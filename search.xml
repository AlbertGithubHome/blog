<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[跟风试试ChatGPT]]></title>
    <url>%2Fblog%2F2023%2F02%2F05%2F%E8%B7%9F%E9%A3%8E%E8%AF%95%E8%AF%95ChatGPT%2F</url>
    <content type="text"><![CDATA[前言其实现在也不算是跟风了，从 ChatGPT 出现至今已经有几个月的时间，这股风似乎已经刮过去了，虽然各种新闻铺天盖地，但因为懒一直没有亲手试一试，这两天尝试了一下，发现要想使用还是有点门槛的，它使用方式很简单，可以把它想象成一个智能机器人，可以回答你的各种提问，当然它比淘宝机器人客服智能多了，它的门槛在于地域限制非常强，中国用户要想使用须得费一番功夫。 什么是ChatGPTChatGPT 以其强大的信息整合和对话能力惊艳了全球，是人工智能研究实验室OpenAI新推出的一种人工智能技术驱动的自然语言处理工具，使用了Transformer神经网络架构，也是GPT-3.5架构，能够在输入序列中捕捉长期依赖性。它还使用了大量的语料库来训练模型，这些语料库包含了真实世界中的对话，以便模型能够更好地理解人类语言。 ChatGPT怎么玩要想使用 ChatGPT 主要有以下几步： 注册 ChatGPT 账号 完成 ChatGPT 手机号验证后登录 输入任意话题，探索 ChatGPT 的强大功能 注册 需要将网络环境切换成国外IP完成整个注册步骤（美国、加拿大、日本等，大陆IP肯定不行，香港澳门 IP 据说也不行） 访问 https://chat.openai.com/auth/login 链接并使用自己的邮箱进行账号注册，我就偷了个懒使用 Microsoft Account 直接登录的 不管是用邮箱注册，还是第三方账号登录都需要打开邮箱查收 OpenAI 账号验证邮件，并完成邮箱验证 验证 登录成功之后需要验证手机号，这一步中国大陆手机号不好使，需要借助短信接码平台 sms-activate.org 具体使用方法比较简单，可以自己试试或者百度一下，其实就是提供一个外国号码给我们验证使用，在网站上可以收验证码，把收到的验证码填到 ChatGPT 平台就可以完成验证了 使用前需要先用邮箱在 sms-activate.org 注册账号后充值，用支付宝可以自动转汇率充值，佣金2.3%，我充值的这段过年时间好像免佣金 一般充值0.2美元就可以，因为免佣金我充了0.5，方便以后再用 选择号码地区时尽量使用印度尼西亚，这个地区验证过了，没有问题 使用直接在最下方的框中输入想问的问题就可以了，下面随便截两个问题，有些问题它还是答不上来的，我曾让它用 Python 画一个猪猪，每次代码生成约10分钟就卡主了，让它画一个兔子结果只画了耳朵。 设计型 开发型 强人所难型 Python调用ChatGPT先去 https://beta.openai.com/account/api-keys 点击 Create new secret key 按钮创建一个API密钥，然后赋值给下面代码中的 openai.api_key 变量直接运行就可以了 1234567891011121314151617import openaidef use_openai(): # Set your API key openai.api_key = "YOUR_API_KEY" # Use the GPT-3 model completion = openai.Completion.create( engine="text-davinci-002", prompt="今天中午吃什么", max_tokens=1024, temperature=0.5 ) # Print the generated text print(completion.choices[0].text)if __name__ == '__main__': use_openai() 总结 ChatGPT 是经过训练的语言模型，以GPT-3.5为基础，其强大的信息整合和对话能力惊艳了全球 ChatGPT 有时很聪明，仿佛是一个拥有了自己的智力，可以应对许多问题，甚至可以直接写代码 ChatGPT 有时却很蠢，特意回避了一些问题，并认为加了一些限制，所以目前来看局限性还很大 ChatGPT 注册登录地址 https://chat.openai.com/auth/login ChatGPT 短信验证可使用的第三方平台 https://sms-activate.org ChatGPT 创建密钥地址 https://beta.openai.com/account/api-keys ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 试图窥探整个世界的运行规律，至今一无所获~ 2023-1-31 00:57:05-->]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>OpenAI</tag>
        <tag>人工智能</tag>
        <tag>智能机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows10彻底关闭自动更新]]></title>
    <url>%2Fblog%2F2023%2F02%2F05%2FWindows10%E5%BD%BB%E5%BA%95%E5%85%B3%E9%97%AD%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[前言在写这篇总结之前，我已经尝试过多次关闭Win10的自动更新了，一般来说更新之后可以使操作系统更加健壮，但总会有一些原因让我们不想更新，比如我最近总是需要远程操作电脑，如果它临时更新会导致我无法继续使用，所以我一般会关掉更新，然后在一个空闲的时间让他尽情更新个够。 网上有很多关于停止自动更新的文章，有些操作起来比较简单，有些则非常繁琐，但大多数教程都不太好使，很多时候就是临时有效，过段时间就又开始自动更新了，这让我想起了很久之前设计的流氓软件。 无赖一个软件会有一个正常的A进程，然后还有一个默默守护着A进程的B进程，当B进程发现一段时间内A进程不见了，就自动启动A进程，如果担心B进程被发现，还可以搞一个默默守护B进程的C进程，子子孙孙无穷尽也。 操作系统的自动更新当然不会这么直白，但是禁用之后还能自动启动，说明一定有其他的东西在默默的守护着他，如果找到这些守护者，把他们干掉，自动更新也就被真正禁止了。 步骤关闭自动更新服务按下快捷键 Win+R 打开运行对话框，输入services.msc，点击『确定』按钮或者直接回车打开服务页面 如图所示，找到 Windows Update 双击打开属性 在 Windows Update 属性页面的『常规』选项卡中，将启动类型设置为选择『禁用』，一次点击『停止』『应用』按钮 我们点击切换到『恢复』选项卡中，将失败后的操作均设置为『无操作』后，点击『应用』按钮 按照相同的方式处理 Update Orchestrator Service 服务，有些电脑中名字是 更新 Orchestrator 服务，也是按照上面的方法设置『常规』和『恢复』两个选项卡就好。 关闭自动更新策略上面的步骤是很多教程中都提到的，一开始耗时，过一段时间发现又开始自动更新了，所以要按照后面的步骤继续操作一下。 按下快捷键 Win+R 打开运行对话框，输入gpedit.msc，点击『确定』按钮或者直接回车打开组策略编辑器 依次打开树形结构中『计算机配置』-『管理模板』-『Windows组件』-『Windows更新』，双击打开右侧『配置自动更新』选项 在弹出的界面中选择『已禁用』，然后点击『应用』『确定』按钮 继续在右侧找到『删除使用所有Windows更新功能的访问权限』选项，双击打开 在弹出的界面中选择『已启用』，然后点击『应用』『确定』按钮 至此就设置完了，以后Windows10就不会偷偷摸摸的自动更新了 效果我司的电脑上安装有安全监测软件，之前我关闭自动更新服务器时该软件只是给我提示个警告，提醒我要保持自动更新的好习惯，但是当我设置完更新组策略之后，安全软件直接提示我已经高危风险了，并且提示将在10分钟后给我自动断网，说明这次的禁用真的生效了，无奈，为了上网我还是还原了更新组策略的设置。 总结 网上的教程虽然多，但要注意甄别，找到好使的方案记下来，方便日后重复操作 关闭Windows自动更新，不仅要关闭自动更新的服务，还要设置更新组策略，防止死灰复燃 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 得不到的永远在躁动，被偏爱的都有恃无恐~]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>update</tag>
        <tag>关闭自动更新，自动更新，service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2022年终总结——人生中最美好的一站]]></title>
    <url>%2Fblog%2F2023%2F02%2F05%2F2022%E5%B9%B4%E7%BB%88%E7%BB%93%E2%80%94%E2%80%94%E4%BA%BA%E7%94%9F%E4%B8%AD%E6%9C%80%E7%BE%8E%E5%A5%BD%E7%9A%84%E4%B8%80%E7%AB%99%2F</url>
    <content type="text"><![CDATA[有一种责任与压力，叫做上有老下有小，但有一种幸福也叫做上有老下有小，当你遭遇挫折与困难时，这些“老小”以及那个同龄的“她”是你坚实的后盾，同时也是你最后的港湾~ 前言2022年已经接近尾声，在经历了多次封控之后终于趁着元旦假期回到老家，和家人聚在了一起，这种平淡的幸福正是我想要的，作为普通平淡的人，我没有拯救世界的梦想，也没达到忧国忧民的高度，我就是芸芸众生中的一员，渴望过着普通而平淡的人生。 我深知这种平凡的生活来之不易，更知许多人为了这份平凡在负重前行，感恩我们这个和平的环境和为了这份和平在努力付出的人，我会好好珍惜。 回顾2022去年依旧设立了不少目标，虽然总体上完成的不太好，但还是要勇敢面对，接下来会逐个回顾一下。 工作上 FLAG 继续踏实做好本职工作，做好工作内容的总结，落实到纸上 学习和了解常见中间件的使用，更多的参与设计的工作 完成度 ： 90% 个人的性格决定了我不能摆烂，所以对待任何事情都比较认真，对待各项工作也是比较谨慎的，一直在踏踏实实的完成，做好日报记录和周报总结，好记性不如烂笔头，有些问题记录下来再回过头查找时会非常方便。 时代在发展，落实到纸上已经不单单指的是“纸”，而是一切可以记录新的媒介，电子版的记录还有一个非常大的好处就是查找信息方便，这可以大大加快我们解决问题的速度，不过我读书时还是喜欢写在纸上，不太喜欢电子版的书籍。 今年工作上有一个比较大的变化就是，从一个被动的执行者转变成了一个初级的管理者，这个身份的转变给我带来了机会，同时也带来了不小的挑战，关于设计工作也一直在进行，因为目前的身份导致我必须做一些决定，所以这部分工作是逃不开的。 通过一些课程学习和与大佬的沟通的交流后发现，管理重点不在“管”而在“理”，管理者并没有身份上的优越感，而在于合理的分配自己拥有的资源，巧妇难为无米之炊，手上什么也没有，最后肯定完不成任务。 在管理者的岗位上，自己能调动的一切皆为“资源”，这些包括时间、金钱、合作、甚至是人员，合理的调配这些才能达到最大的效果，否则就会产生浪费，效果也大大折扣。 这个Flag中的扣分项应该是学习和了解常见中间件的使用，今年确实使用和配置过一些中间件，但是没有好好的系统的学习，这方面后面要加油了。 今年关于工作上的付出基本达到最大化了，最近一段时间为了阶段性版本，大多数都是后半夜才能回家了，很累，但是并不排斥，我喜欢自己做的东西，为了取得好的结果，目前的付出是值得的。 学习上 FLAG 博客总结不能落，继续保持一周一更，完成40篇基础目标 刷题不用太频繁，每周总得有贡献，不可抗拒因素除外 建立自己的技能树，搭配工作总结，统计出自己到底会什么 选取经典开源代码学习，代码量要少一点，毕竟精力有限了 读书、读书、读书，书都买好了 完成度 ： 70% 博客总结flag达标，依旧是40篇踩线通过，今年CSDN改变挺大的，但不影响我在上面记录工作学习内容的初衷，40篇博客内容比较基础，大多数来源于工作之中，也有一部分来源与生活需求，简单记录，记忆珍存，以下是近两年的数据对比。 今年的博客还诞生了第一篇浏览量过10W的文章，《float的精度和取值范围》，目前浏览量到达了117805，这篇总结当初确实花了不少时间，也侧面说明了高质量的文章才能得到粉丝的认可。 今年的CSDN玩出了不少新花样，特别是与博主的互动上，经常会发一些实体的证书，虽然这些东西不值什么钱，但是对博主来说得到认可还是很开心的。 关于刷题这个Flag今年的情况有点拉跨，别说参加比赛了，每周的刷题数也保证不了，时间真的不太够用，虽然这听起来像是在找理由，但确实是客观存在的，毕竟从早上9点工作到凌晨4点，我是真的抽不出时间来刷题了，还是要命的，活着才有未来，噶了就什么都没了。 建立自己的技能树，这方面已经着手开始做了，目前只列举了一个大概的知识框架，今后会随着工作和学习继续补充，主要是总结自己，为今后的职业发展打好基础。 读取经典开源代码这个Flag今年完成的最差了，几乎没有什么进展，阅读代码一直被项目牵着走，代码确实没少读，一直在Code Review，完整的开源项目几乎没有读，不过发现了一个 magic_enum 项目还不错，把他按照项目需求改造了一下，目前正在使用。 读书一直在进行，中间因为身体原因和项目忙的原因暂停过一段时间，不过去年Flag列举的书单都翻了翻，毕竟书都买回来了，但是有些真的不适合精读，比如民法典，有空了翻一翻还是比较有意思的。 股票大作手回忆录（2022-02-12 22:58:39） 顺势而为，不与大盘做对，永远朝着阻力小的方向操作 世界很烦，但你要很可爱（2022-02-28 07:09:45） 不管别人对你如何，请记得一定要对自己好一点儿 洛克菲勒写给儿子的38封信 商人就是靠创造资源、掠夺他人的资源，甚至逼迫他人转让自己的资源来致富的。 两次全球大危机的比较研究 经济发展比较注重效率，收入分配比较关注公平 创华为 —— 任正非传（2022-07-17 11:00:19） 你不要说未来有什么贡献，万一活不到未来，未来的贡献和我们没有关系 孙子兵法与三十六计（没看完） 历史总是在重演，只有胜利的人才能诉说 世界经济简史（看了一点） 原始的经济就不纯粹 民法典（翻阅） 没想到这么薄薄的一本干掉了那么多大部头 活着（2022-12-17 11:22:10） 生活是属于每个人自己的感受，不属于任何别人的看法 《创华为》是一部任正非的传记，这是继《乔布斯传》之后我读的有一本比较厚的传记书籍了，大佬们的经历总是能鼓舞人心，虽然有些描写我觉得不真实，但是所有的经历不太可能都是伪造的，去伪存真，相信那些正能量的内容。 《活着》这本书很出名，但是我读了一遍感觉这本书宣传时给我的印象完全不同，整本书的描写朴实无华，就真的像我等小老百姓在村头树下闲聊，很多情景并没有刻意描写，“她死了”——仅仅三个字就断送了别人姓名，并没有更多的叙述。读过之后我并没有觉得富贵有多惨，这就是那个时代的一个缩影，他不仅不惨，而且也经历过很多人没有经历过的幸福，有一个衣食无忧的童年，有一个不离不弃的妻子，经历战乱却活了下来… 投资上 FLAG 基金和ETF继续定投，适当配置债券固收 股票池里把曾经“瞎选”的股票逐渐出清，依旧拥抱大白马，少折腾 目标收益不太高，8个点，希望不要打脸 完成度：85% 今年的投资严格按照Flag策略执行，基金和ETF持续定投，目前有周定投和月定投两套策略，中概、恒生、红利还设置了智能条件单，不主动干预，实现被动调仓，只是最后的收益结果没有达标(ಥ_ಥ) 曾经瞎选的股票也出清了一部分，有一只最多赔了90%的票拿了5年，今年回本之后也卖掉了，所以只要股票不退市都有回本的机会。 持仓依旧银地保三傻和大白马——万安招富，虽然没挣钱，但今年的回撤控制的还可以，做了一波过山车，4月份感觉股市要崩了，6月份股市要飞了，不仅回本还有了盈利，10月份股市要炸了，12月份又飘起来了，可谓一波三折。 本来在2021年已经预测2022在加息周期中，能保住本金就可以了，没想到又遭遇俄乌战争，疫情反复，股市受到了比较大的冲击，不过结果还是不错的，虽然Flag想达到8%，最终收益达到了6.72%，也还可以接受，不过基金和ETF有点浮亏，吃掉了股票的部分收益，整体上略有盈余。 生活上 FLAG 疫情结束了多回几次老家吧，去看看那些想我又不愿说出口的亲人 注重身体的保养，锻炼提上日程，降低亮红灯的指标 完成度60% 生活上的Flag是完成的最差的，本来想今年疫情结束多回几次老家，结果今天封闭的时间最长，刚刚统计完2022年算上春节假期一共回家14天，达到了历史最低值。 身体的保养和锻炼更是拉跨，年初的时候因为胸腔疼痛，3、4月份吃了两个月的中药来调理，医生告诉我不要熬夜，晚上10点准时睡觉，这几乎是不可能完成的任务，每天有太多的事情需要忙了，不过今年体检各项指标比较稳定，注意血压、视力和肝脏。 有趣的是这个中医岁数不大，但是找他看病的人很多，每次门诊都持续到晚上12点钟，医生门诊持续到12点这还是我第一次遇到，所以我每次挂号都看一下时间，大概每次都赶在周六晚上9点去找他，当有一次他告诉我要10点钟睡觉时，我反问还在忙碌的他为什么不10点睡，他只说了一句：“你比我幸福”。其实就是生活所迫呗，他因为工作无法按时睡觉，我又何尝不是呢？ 今年陪娃的时间不太多，娃娃也顺利的上了幼儿园，认识了很多小伙伴，虽然因为疫情只上了1个多月，但每天还是开开心心的。我因为有两个月封控在家，在一起相处的时间还蛮多的，但是也在忙于工作，居家办公导致生活和工作融在一起，无法分割，个人不太喜欢长时间处在这种环境下，还是喜欢公司办公，回家陪娃的日子。 去年总结中提到的那批多肉，今年一年过得不错，长势喜人，已经成了我的小花园中重要的组成部分，今年12月初的时候又买了一批，还买了几颗文竹，结果花盆到了、营养土到了，这都新年了多肉和文竹还没到，看来得等到春天才能发货了。 展望2023工作 适应自己身份的转变，提升自己的管理技能，做一名合格的管理者 继续做好本职可开发工作，做好工作内容的总结，推动新项目顺利上线 根据自己的技能树框架，查漏补缺，有针对性的学习和探索 学习 继续博客总结，40篇是基础线，2023要超过这个值，尽量一周一篇，放假可休息 新的一年依旧很忙，刷题不强制要求，每周最好有贡献，每月必须有输出 在现有的技能树框架上继续丰富，做到枝繁叶茂，试试找点副业 开源代码还是以项目为驱动，选取经典实现，比如kafka 继续读书，今年书也买好了，数量不多，下半年应该还会买一批 投资 定投和智能条件单保持现状，ETF追加1~2个新的行业赛道 股票池还有几个垃圾票要找机会清理掉，手里的价投股票适当做做趋势 保持对新年经济的乐观，目标收益10个点 生活 疫情放开了，有事没事都趁着假期回家看看 身体很重要，锻炼的很费时，中和一下，可以跳跳绳 周末了陪娃耍一耍，工作尽量不占用周末的时间 总结 旧的一年虽然看起来很拉跨，但我认为是人生中最美好的一站，因为照片人上的人都还在，而我也在成长 新的一年稳中向好，不盲目乐观，也不宜过分悲观，投资环境应该会转好 身体方面自己要多注意，有些情况并不是自己要熬夜，其实多为生活所迫，若衣食无忧，谁甘愿拼搏 珍惜周末时光，珍惜和家人在一起的日子，收起自己的坏脾气 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 往昔总是回忆情，未来光景亦可期。若问此生弥珍贵，还看今朝不分离。]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活，投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[圣诞平安夜了还要继续敲代码吗]]></title>
    <url>%2Fblog%2F2022%2F12%2F25%2F%E5%9C%A3%E8%AF%9E%E5%B9%B3%E5%AE%89%E5%A4%9C%E4%BA%86%E8%BF%98%E8%A6%81%E7%BB%A7%E7%BB%AD%E6%95%B2%E4%BB%A3%E7%A0%81%E5%90%97%2F</url>
    <content type="text"><![CDATA[此时不敲，更待何时~ 前言先不说圣诞节算不算我们的节日，单纯就节日还要不要敲代码这个问题来说，每个程序猿/媛都有自己的答案。如果说你把工作敲代码当做一种负担，那么节假日正好可以作为自己休息的理由，好好休息一下也是不错的选择；如果把敲代码作为一种乐趣，那么节假日难得清净，少了很多杂事的打扰，正好是敲代码的黄金时间段，想一下自己的状态很容易就应该明白了。 正文节日圣诞节很明显不是我们的传统节日，应该被归为“洋节”一类，有些人对这些外来节日特别喜欢，同时也有人对此特别排斥，每个人都有自己的理由，其实不止是对这些节日，人们对于其他任何事物都存在着截然相反的评价，这些评价通常是基于自身利益、过往经验而给出的，它代表了每个人的想法，这些想法不是一成不变的，随着时间的推移，人们对它的评价甚至会走到相反的对立面。 其实我觉得不必过分排斥这些节日，一些人推崇这些节日自然有他们的目的，比如一些商家为了销售业绩往往会推一些节日活动，如果我们也能在这些活动中获得优惠，那么何乐而不为呢？ 无论是中国节还是“洋节”出现的原因基本都是用来纪念一些事件或者寄托某种情感的，但是节日的这些属性在如今这个快节奏的社会环境下渐渐淡化，人们更多的是希望在节日假期中能好好的放松一下。 代码与技术写代码对我来说算是一种爱好，没有达到痴迷的程度，但是有空了还是想随便写点东西，不过今天只写了一点点，毕竟今天周末，多花了点时间陪伴家人，最近一段时间太忙了，心中总是有所亏欠，暂时停下来歇一歇。 每次进入新的领域时总是一头雾水，当渐渐入门以后便会发现一些规律，最近发现linux中也有很多蹭热度的命令，比如 ls 这个命令实在太常用了，可以展示指定目录下的所有文件，lscpu、lsgpu、lsmem、lspci、lsusb 这些命令的出现我觉得就是在蹭 ls 的热度。 除了 ls 还有一个 top 命令用来展示正在运行的进程信息，iftop、iotop、htop、atop 这些命令自然是蹭了它的热度，起一个相似度很高的名字，然后便于自己推广。 上面 ls 和 top 两个被蹭热度的命令，与后续命令还有有相似的功能，比如 ls 展示目录下的所有文件，而 lscpu 是用来展示 cpu 信息的，那么 “ls” 变成了一个基础功能的代表，就是用来展示信息，类似于英语中的词根，以及汉字中的偏旁。 而编程语言的历史上，还有一对完全为了蹭热度没有任何关系的语言，那就是 java 和 javascript 语言，javascript 在当时为了推广自己蹭了流行语言 java 的热度，它俩实际的关系就像是周杰和周杰伦的关系，仅仅是名字长得像罢了。 总结 ls 家族有 lscpu、lsgpu、lsmem、lspci、lsusb 等命令 top 家族有 iftop、iotop、htop、atop 等命令 把写代码当做一种兴趣爱好，其实没有什么节假日，想写就写咯 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当下的今天就是最好的，我不想回退到之前人生中的任何一个时间点。我一直有这种感觉，今天是最好的，所以珍惜现在拥有的一切，我不想让自己产生回退到过去的想法，因为那样肯定是发生了痛苦的事情。恰好今天和她看了《想见你》这部穿越剧，更加深了我的这种想法，今天就是最好的。 2022-12-25 01:37:25]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
        <tag>linux</tag>
        <tag>节日</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx的安装与常用配置]]></title>
    <url>%2Fblog%2F2022%2F12%2F19%2FNginx%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言Nginx 是开源、高性能、高可靠的 Web 和反向代理服务器，而且支持热部署，几乎可以做到 7 * 24 小时不间断运行，出镜率很高，从2004年发布至今，凭借开源的力量，日趋完善。其占用内存少、并发能力强、并且Nginx 是免费的还可以商业化，配置使用也比较简单。 常用功能反向代理Nginx 可以用作正向代理和反向代理，而在做反向代理时，能提供性能稳定，且配置灵活的转发功能 负载均衡Nginx 对于负载均衡策略提供了内置策略和扩展策略。内置策略有轮询，加权轮询，IP哈希。而扩展策略，就可以随心所欲了，只要遵循规则就可以搞出各种花样。 静态资源服务Nginx 可以通过本地文件系统提供服务 web缓存服务器Nginx 可以对不同的文件做缓存处理，配置灵活，支持FastCGI_Cache，可用于对 FastCGI 的动态程序进行缓存。配合着第三方的ngx_cache_purge可以对制定的URL缓存内容进行增删管理。 插播快讯本来今天打算总结一下Nginx的安装和使用的，结果中午的几个未接电话打破了宁静，游戏居然进不去了，看到几个未接电话就觉得不妙了，了解完情况才知道居然是阿里云香港C区的服务器挂了，本来觉得很快就能恢复，但随着时间的推移和官方的不表态，渐渐发现事情没有这么简单，虽然在过去好几个小时以后更新了公告，但这反应速度也太慢了。 现在已经过去了10多个小时（2022-12-18 10:47:22开始），尽管最新的公告说机房已经恢复，产品功能还在陆续恢复，但我们的游戏现在（2022-12-19 02:10:06）依旧处于停服状态，目前的云服务器依旧不可用，我还在等实在是写不下去了，关于Nginx的使用后面再补充吧，现在只希望云服务器能快点恢复。 【已恢复】阿里云香港地域电讯盈科机房制冷设备故障尊敬的客户： 您好！阿里云监控发现香港地域某机房设备异常，影响香港地域可用区C的云服务器ECS、云数据库PolarDB等云产品使用，阿里云工程师已在紧急处理中，非常抱歉给您的使用带来不便，若您有任何问题，请随时联系我们。—进展更新—尊敬的客户： 您好！经排查，阿里云香港地域故障确认系香港PCCW机房制冷设备故障所致，影响香港地域可用区C的云服务器ECS、云数据库、存储产品（对象存储、表格存储等）、云网络产品（全球加速、NAT网关、VPN网关等）等云产品使用。这一故障也影响了香港地域控制台访问和API调用操作，目前阿里云工程师在配合PCCW机房工程师加速处理，部分制冷设备正在恢复中。非常抱歉给您的使用带来不便。若您有任何问题，请随时联系我们。—进展更新—尊敬的客户： 您好！目前阿里云所租用的香港电讯盈科公司机房已修复制冷设备故障，阿里云香港地域所有可用区云产品功能正在陆续恢复正常。对于受本次故障影响的产品，阿里云将根据相关产品的SLA协议进行赔付。非常抱歉给您的使用带来的不便 ，若您有任何问题，请随时联系我们。 总结 Nginx服务器配置方便、高效可靠，可以用作反向代理、负载均衡、静态资源服务、Web缓存服务等 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 生活是属于每个人自己的感受，不属于任何别人的看法~ 阿根廷夺冠了，虽然我从不看球，但这场比赛真的跌宕起伏，不到最后一秒永远不知道会发生什么~ 2022-12-19 02:14:22]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>安装</tag>
        <tag>负载均衡</tag>
        <tag>反向代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动推送消息时附带图片的一种实现方式]]></title>
    <url>%2Fblog%2F2022%2F12%2F03%2F%E8%87%AA%E5%8A%A8%E6%8E%A8%E9%80%81%E6%B6%88%E6%81%AF%E6%97%B6%E9%99%84%E5%B8%A6%E5%9B%BE%E7%89%87%E7%9A%84%E4%B8%80%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言本文是之前总结 pushplus 使用的这篇文章 《借助第三方工具网站完成消息自动推送》 的后续，当时说使用pushplus的微信公众号渠道不允许直接发图片，但是可以将图片链接放到 image 标签中，这就要求我们必须要有一个图床，今天也是从这里开始。 实现途径 注意：请勿使用base64编码的方式把本地图片放到内容中，您可以将本地的图片上传到七牛云等云存储空间上来获取图片的外链地址。 这段内容来自pushplus官网，限制了直接发送图片的方法，并给出了使用图片外链的建议，实现代码非常简单，示例如下： 1234567891011import requestsdef post_wechat_msg(token): url = 'http://www.pushplus.plus/send' new_json = &#123; "token": token, "title": "图片示例", "content": "content":"这是一张图片&lt;br/&gt;&lt;img src='http://www.pushplus.plus/doc/img/push.png' /&gt;", "topic": "9caigroup" &#125; result = requests.post(url, json=new_json) 代码量非常的少，想发别图片就直接修改img标签中的内容就可以了，问题是怎么获取我们图片的外链。 图床与外链如果你没听说过这两个词说明你对建站、发文等了解的比较少，不过也没有关系，这两个词很好理解 图床：就是放置图片的床，可以理解为图片的仓库，里面存储了很多图片外链：就是访问图床里图片的一个链接，可以通过一个网址读取到想要的图片 以上面代码中的例子来看，http://www.pushplus.plus/doc/img/push.png 就是图片的外链，图片push.png被放在 http://www.pushplus.plus/doc/img/ 这个“图床”下面。 如果没有自己创建网站或者跨平台转发博文的经历，可能不太理解图床有啥用，我举个例子试着说明一下，假如你在A平台发了一篇技术文章，在文章中需要一些图片来丰富文章内容，你可以把这些图片在A平台的文章中上传。同时你想在另一B平台上也同步这篇文章，此时把A平台上这篇文章完全复制过去是不行的，因为其中的图片是在A平台上传的，A平台作为这些图片的图床一般会给图片加防盗链，这些图片链接复制到B平台上是显示不出来的，这也是为什么之前的新浪博客图片加了防盗链以后，很多平台上的文章图片都挂了的原因。 现在有一种实现方式就是自己建一个图床，获取得到图片链接就是固定的，也可以限制可以在域名下显示，这样同一篇文章无论是A平台，还是B平台都引用自己图床里的图片，文章同步就方便了很多，但是因为建立自己的图床功能单一，没有cdn加速，维护成本较高，所以一般使用第三方的图床服务，我只用过七牛和Github这两个图床，不过Github有时不太稳定，有需求可以按自己的喜好来选择。 七牛图床说起七牛我用的还比较早，大概在2017年左右就用过了，当时建了一个静态的小破站，需要引用一些图片资源，就了解到了图床与七牛，免费提供10个G的空间，同时可以获取外链，使用比较方便，图片更新有一套方便的API，已经过去5年了，我前两天使用之前的脚本更新时发现还能用，说明还是比较稳定的。 后来出了一个事情导致我放弃了七牛，转身选择了Github图床。这件事就是七牛的规则调整，曾经使用的外链全都过期，要求必须绑定备案过的域名才能使用，当时替换了文章中所有的图片链接，同时兴冲冲的买了一个域名，结果只有域名是不能备案的，必须还要买服务器，各种要求晕晕乎乎的一直没太弄懂，申请了几次的公安备案也因为资料不全未审核未通过，最后无奈放弃了。 1、测试域名有使用限制，在创建空间的时候自动分配，到期30天会自动回收，因此建议您绑定自己的已备案域名2、根据工信部的有关规定，国内接入cdn服务，域名必须要完成备案 这个是七牛官方给出的解释，起初我认为想要达到这个要求完全不用每个使用者绑定自己的域名，其实这些要求无非就是想让cdn使用者对自己上传和分享的资源负责，禁止传播非法内容，这个要求本身是好的，绑定了自己备案的域名如果出现非法内容，可以立马找到使用者，可是不仅仅只有绑定自己域名这一条实现方式。 要求七牛给每个使用者分配一个1级域名不太现实，但是可以做成主域名备案，然后给每个使用者分配二级、甚至三级域名，其实这就是一开始提供的测试域名，使用这些域名分享资源的用户要求必须在平台实名，这样再出现非法内容的时候可以定位到个人，也能达到监管的目的，可是为啥平台放弃了测试域名的长期使用，反而要求每个使用者绑定自己域名呢？ 最近和七牛的工程师反复沟通了解到，其实分配给用户的测试域名是合规的，因为它的主域名肯定完成了备案，但是因为有些用户违规使用，这就对这个主域名造成了影响，一些产品可以和社交软件因为该域名分享的图片资源出现过非法内容，可能对这个域名的资源自动屏蔽，这就对正常用户的使用造成了影响，这也是一个平台发展壮大之后必须面临的问题，林子大了什么鸟都有。 所以七牛把原来提供的域名被定义为测试域名，给30天的使用期限，仅用于调通功能流程，到期自动回收，即使被一些平台限制了也没什么关系，只要确定后续要继续使用，绑定自己的域名就行了，违规使用的用户的不会影响到其他人。 所以我觉的这件事挺无奈的，如果可以一直使用测试域名，对于使用者无疑是很方便的，对于平台方也降低了用户门槛，便于产品的推广，但是这些美好的愿望总因为一些“极个别人”无法实现了，我们只能接受现实，采用目前这种方案。 备案流程因为最近想要在发送通知消息中增加图片，所以不得不再次使用图床了，你问我为啥不用Github的图床，那是因为微信把它给屏蔽用不了了（捂脸），所以我不得不把目光又转回七牛，经过不懈努力，终于把绑定域名的事情搞定了，记录一下，希望能帮助到有同样需求的人。 备案前的准备 购买一个域名 购买一台云服务器 搭建好个人网站 身份证正反照片 手持身份证的照片 开始备案购买域名推荐阿里云和腾讯云，因为后续备案的流程会有通知提醒，可以免去很多查找资料的麻烦，两个平台我都买过域名，但是只在腾讯云上完成过备案，所有后续的流程也是以腾讯云备案为基础的。 购买域名需要先实名，腾讯云上购买之前需要先提交实名模板，审核通过后才能买域名，域名的购买和网上购物没啥区别，挑选自己喜欢的加到购物车，然后付款就可以了，购买成功后会有站内信提醒，通知后续的备案流程，这也是比较方便的地方。 买完域名要买服务器，其实备案不仅仅是对一个域名的备案，而是对这个域名提供的服务内容进行备案，而要提供内容和服务就需要有一台带有外网IP的服务器，然后将域名和服务器进行绑定，统一写到备案内容中，这样你提供的服务和提供服务的这个域名就登记在册了。 腾讯云买服务器同样跟买菜一样，如果只是为了备案，买一台最低配的就可以了，然后再上面搭建一个简易网站，就可以后续的备案流程了，备案的第一步叫管局备案，也叫工信部备案，也是我们常听说的ICP备案。 在提交备案资料后，腾讯云会先进行审核，其中要注意规避游戏内容，提及游戏的网站很容易被退回，不同地区的管局对网站内容要求不一样，比如北京地区现在不允许提交个人博客类网站了，这个再提交前先看一下不同地区的限制，手机号和紧急电话必须时刻保持畅通，提交资料后3-4天会有电话确认，按要求修改好审核通过后就会被提交到管局审核。 提交管局审核之前，要关停域名解析，在审核通过前不允许域名到服务器的解析，提交之后提示20个工作日会收到结果，马上会收到一条短信验证通知，按要求回复后只要等着就可以了，我是在第7天收到的审核通过的通知，速度还是比较快的。 当ICP备案成功后会得到一个 【京/冀/沪 ICP 备 xxxxxxx 号 - 1】的网站身份证，之后腾讯云站内信会通知在30个工作日内完成公安备案，这是不同于ICP备案的另一种备案流程，这一步要求的内容会比较多，不过好在大平台这些基础信息都是提供好的，目前已经按照流程提交了申请，正在等待审核中，希望不要再出什么差错。 图床绑定域名这一步在完成ICP备案通过之后就可以进行了，参考 官方文档 - 如何配置域名的 CNAME 操作就可以了，注意二级域名的命名和使用，步骤并不复杂，我因为拼写错误还耽误了一些时间，绑定成功之后就可以愉快的使用了。 pushplus发图片的实现 申请一个七牛图床账号 将图片上传到七牛平台 复制图片的外链 临时使用可以使用测试域名 长期使用需要绑定备案域名 实名购买一个域名 购买云服务器，搭建一个网站 提交资料，完成ICP备案 在七牛绑定备案好的域名 编写发送图片的代码，在img标签中引用图床中图片的外链 总结 使用 pushplus 发送微信通知消息时可以使用img标签引用图床外链的方式发送包含图片的消息 图床可以选择 pushplus 官方推荐的七牛，可以暂时使用测试域名来提供外链，想长久使用可以绑定自己已经备案的域名 备案域名时需要先买一个域名和云服务器，推荐在阿里云和腾讯云上购买，大平台上购买后的续通知提醒和备案流程会非常顺畅 对于非盈利性的网站备案需要有ICP备案和公安备案，ICP备案购买域名后平台会帮助进行，而公安备案中需要的信息平台方也会提供 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 神州归家，腾飞中华，眼中所见都是大事，逐渐接受自身的平庸，一个普普通通的人，想要过一段平凡的生活，在柴米油盐中慢慢步入黄昏~ 2022-12-4 21:57:22]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pushplus</tag>
        <tag>自动推送</tag>
        <tag>图片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下统计目录下所有文件的行数]]></title>
    <url>%2Fblog%2F2022%2F11%2F30%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%BB%9F%E8%AE%A1%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6%E7%9A%84%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言工作中时常有统计数据行和代码行的需求，虽然能依次打开每个文件数一下，但是这样操作效率太低了，如果是在linux环境中，可以使用一个常用的命令 wc，一起记录一下常见的用法吧。 wc命令wc命令，（全称water closet，洗手间，哦，错了），全称word count？我也没找到，作用就是输出每个文件包含的行数、单词数和字节数，这三项是基础信息也是最常用的信息，此外它还可以输出文件中字符数量、输出文件中最长行的长度。 使用方法非常简单，直接在命令末尾添加文件名就行了 12$ wc README.md 35 60 2460 README.md 上面的信息显示，README.md文件中包含25行内容，存在60个单词，共有2460个字节，可能是因为该文件中包含中文，所以单词数统计不太准确。 常用选项该命令各选项含义如下： - l: 统计行数 - w: 统计字数 - c：统计字节数 - m：统计字符数 - L：统计最长行的长度 其中 - l 使我们统计行数的利器 统计文件行数wc 命令搭配一些其他的命令能更方便的完成统计工作 统计1个文件行数12$ wc -l README.md35 README.md 统计2个文件行数wc 命令后名跟多个文件名时会打印出每个文件的行数，并且在最后一行显示所有文件总行数 1234$ wc -l README.md test.txt 35 README.md 3 test.txt 38 total 统计当前目录下python代码文件数量12$ find ./ -name "*.py" | wc -l317 统计当前目录下python代码文件行数12$ find ./ -name "*.py" | xargs cat | wc -l38538 统计当前目录下python代码文件非空行数12$ find ./ -name "*.py" | xargs cat | grep -v ^$ | wc -l35320 实现的方式就是在输出文件内容的过程中过滤掉空行，但我认为统计代码行数时不应该过滤空行，适当的留白也是优秀代码的一部分 统计当前目录下2种代码文件的总行数一个项目中使用的代码往往不止一种，所以在统计代码行数时要考虑多种代码文件，可以写多个-name选项或者使用正则表达式 123456$ find ./ -regex '.*\.\(py\|h\)' | xargs cat | wc -l39795$ find ./ -regextype posix-extended -regex '.*\.(py|h)' | xargs cat | wc -l39795$ find ./ -name "*.py" -o -name "*.h" | xargs cat | wc -l39795 总结 wc 命令是用来查询文件行数的核心命令，不加选项时默认输出文件行数、单词数、字节数 wc 命令配合 find、xargs、cat 等命令可以统计出目录内指定类型文件的总行数 find 查找多个文件的常用写法 find ./ -regex &#39;.*\.\(py\|h\)&#39; 或 find ./ -name &quot;*.py&quot; -o -name &quot;*.h&quot; 查项目全部代码量终极大招：find ./ -regex &#39;.*\.\(py\|lua\|go\|h\|hpp\|cpp\|c\|cc\)&#39; | xargs cat | wc -l ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 确实挺难的，再坚持一下~ 2022-12-1 17:16:29]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>grep</tag>
        <tag>wc</tag>
        <tag>统计函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[借助第三方工具网站完成消息自动推送]]></title>
    <url>%2Fblog%2F2022%2F11%2F19%2F%E5%80%9F%E5%8A%A9%E7%AC%AC%E4%B8%89%E6%96%B9%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99%E5%AE%8C%E6%88%90%E6%B6%88%E6%81%AF%E8%87%AA%E5%8A%A8%E6%8E%A8%E9%80%81%2F</url>
    <content type="text"><![CDATA[前言寻找消息推送的起因是之前买过一台云服务器，用于开发环境搭建和学习，最近想用它进行一些数据分析，而数据分析的结果如果每次都需要登录服务器来看就有点“太老土”了，所以想找一种使用方便的消息推送模式。提到之前的消息通知最常用的是短信和邮件，但是随着时代的发展，它们的弊端也逐渐暴露出来，短信的内容主要以文本为主，想要发送图片就比较困难，邮件查看起来不是太方便。而微信作为每天都用的社交软件，用它来接收消息就很方便了，所以朝着这个方向寻找，我发现了 pushplus 这个第三方工具。 pushplus是什么pushplus(推送加)是集成了微信、短信、邮件、企业微信、HiFlow连接器、钉钉、飞书等渠道的信息推送平台，只需要调用一个简单的API接口，即可帮助你迅速完成消息推送，使用简单方便。 我主要想用它的微信推送，一开始也想试试短信推送的，但是需要支付每条0.1元的费用所以放弃了（什么年代了，短信收费还这么贵），这里先简单说下它的微信推送的原理，利用的就是微信公众号的消息通道。 什么是消息通道，我的理解就是获取消息的途径，比如短信、邮件、微信、电话等等，这个pushplus本身并没有搭建新的消息通道，而是整合了已有消息通道的使用方式，让消息通知变得简单了，而微信消息推送利用的就是微信公众号这个途径。 我们知道微信公众号可以接收消息，并且微信公众平台也开放了后台接口，允许公众号运营方通过程序接口发送消息给关注者，这就是微信公众号的消息通道。pushplus就是使用了这个通道，他们把微信公众平台的接口进行了封装，开放给所有关注 pushplus 的人，而这些人就可以利用这个通道给自己发送消息，给群组发送消息，甚至给所有的关注者发送消息，当然这些消息都是显示在这个公众号里，要想接收消息必须先关注公众号，或者按照pushplus的规则加入群组。 既然是利用别人家的通道，必然要收到这个通道原有的限制，pushplus也不例外，首先当前微信公众号对模板消息推送有当日100万次的上限，超过这个上限有当日将无法发送模板消息，也就是所有pushplus的用户都将无法使用微信渠道的推送功能，所以pushplus在单日推送次数上做了部分限制。每人每日可用推送条数为200条，当大于200条的时候消息将不在推送。 目前这个每天200条是免费的，但随着用户量的增大，这个限制必然是会更加严格的，按现在计算没人推送200条也只能服务于5000人，不过大部分人是不会发这么多的，短时间应该没问题。另外该网站目前还推出了绑定自己公众号的服务，必须是经过认证的非个人公众号才可以哦，腾讯就是这么要求的，它也没办法，当前还提供了会员制度，可以适当放宽消息推送的次数。 我觉得大家不要反感会员制度，人总是要恰饭的，即使不是为了赚钱，也需要付出成本的，免费提供这项服务是需要服务器资源的，总不能一直永爱发电吧，收个合理的费用方便广大开发者我觉得无可厚非，就像我买的服务器1年一百多，续费成本每年要700多，如果对外提供服务器不收钱总不能靠西北风活着吧。 pushplus测试使用pushplus 的官方网站是 www.pushplus.plus，直接微信扫码就注册成功，同时会关注微信公众号用于接收消息，对于一对一消息和一对多消息都提供了消息发送界面，输入内容直接点击发送按钮就可以了，非常方便。 官网介绍如下： 一对一消息发送 一对一消息发送 微信公众号里看到的消息通知 代码发送一对一发送消息的代码非常简单，也就下面这几行，更复杂的格式还需要研究下： 123456789import requestsdef send_wechat_msg(title, content): token = '2c358fyearacb4581bc92f0c320c728fb' url = 'http://www.pushplus.plus/send?token='+token+'&amp;title='+title+'&amp;content='+content requests.get(url)if __name__ == "__main__": send_wechat_msg('python推送测试消息', '测试消息详细内容') 注意的问题 限制问题。前面已经提到，使用pushplus公众号，所有用户共享100万条消息的限制，随着公司后期的发展这可能不够用 合规问题。因为是自定义推送内容，所以平台要注意合规问题，不仅公众号要进行内容审查，pushplus也需要进行内容审查，否则可能因为一个用户的违规导致所有用户都发不了消息。 消息格式。目前pushplus支持多种消息格式，包括 text、html、json、markdown 等格式，这能满足大部分用户的要求了 发送图片。pushplus不支持直接发送图片，具体方式通过html的 &lt;img&gt; 标签来实现，可以将本地的图片上传到七牛云等云存储空间上来获取图片的外链地址。 总结 消息推送可以选择 pushplus 官网扫码即可使用 pushplus 的微信推送借用了微信公众号的消息通道，会受到的平台原有的100万条消息限制 pushplus 提供给普通用户每天有200条消息的使用权限，仅通过http请求即可方便的使用api 注意后续的平台发展，很可能发展到一定程度后增加更严格的使用限制 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 一道残阳铺水中，半江瑟瑟半江红。可怜九月初三夜，露似真珠月似弓~ 2022-11-20 13:01:16]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>消息推送</tag>
        <tag>微信</tag>
        <tag>短信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下使用rsync命令完成数据同步]]></title>
    <url>%2Fblog%2F2022%2F11%2F13%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8rsync%E5%91%BD%E4%BB%A4%E5%AE%8C%E6%88%90%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[前言linux 环境下说到数据复制第一个映入脑海的命令还是 cp，毕竟它用起来很方便，但是只能完成本地数据的拷贝，如果想实现本地和远程主机之间的数据拷贝可以使用 scp，有些文章会把它翻译成 “ssh copy”，但实际上它的全称是 “secure copy”，不过它确实利用了ssh协议，缺点就是只能全量拷贝，如果想完成增量拷贝可以选择 rsync命令，官网 rsync.samba.org 比较简陋，但很符合GNU的风格。 rsync的特点 可实现增量同步，即只同步发生变化的数据 可保持原文件或目录的权限、时间、软硬连接、属主、组等所有属性均保持不变 支持拷贝特殊文件，如连接文件、设备等 排除指定文件或目录同步的功能，相当于打包命令tar的排除功能 rsync本身不对数据加密，可使用rcp/rsh/ssh等方式来配合传输文件 支持匿名的活认证的进程模式传输，可实现方便安全的数据备份和镜像 rsync 传输模式 本地方式，类似cp，可完成本机数据的拷贝复制 远程方式，类似scp，但可实现增量复制，可以将数据从本地推送至服务端，也可以将数据从服务端拉取到本地 守护进程方式，区分客户端和服务端，可以启动 rsyncd 服务（未使用过需研究） rynsc 命令格式1234567891011Local: rsync [OPTION...] SRC... [DEST]Access via remote shell: Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST:DESTAccess via rsync daemon: Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST] rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST::DEST rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 常用的还是第2种和第3种，第一种是本地复制，一般用cp代替就够了，4~7用于守护进程模式，至今我还没用过 rsync 命令选项12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061-v, --verbose 详细模式输出。-q, --quiet 精简输出模式。-c, --checksum 打开校验开关，强制对文件传输进行校验。-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-r, --recursive 对子目录以递归模式处理。-R, --relative 使用相对路径信息。-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀。-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。-l, --links 保留软链结。-L, --copy-links 想对待常规文件一样处理软链结。--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。--safe-links 忽略指向SRC路径目录树以外的链结。-H, --hard-links 保留硬链结。-p, --perms 保持文件权限。-o, --owner 保持文件属主信息。-g, --group 保持文件属组信息。-D, --devices 保持设备文件信息。-t, --times 保持文件时间信息。-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间。-n, --dry-run现实哪些文件将被传输。-w, --whole-file 拷贝文件，不进行增量检测。-x, --one-file-system 不要跨越文件系统边界。-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节。-e, --rsh=command 指定使用rsh、ssh方式进行数据同步。--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。--delete 删除那些DST中SRC没有的文件。--delete-excluded 同样删除接收端那些被该选项指定排除的文件。--delete-after 传输结束以后再删除。--ignore-errors 及时出现IO错误也进行删除。--max-delete=NUM 最多删除NUM个文件。--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。--force 强制删除目录，即使不为空。--numeric-ids 不将数字的用户和组id匹配为用户名和组名。--timeout=time ip超时时间，单位为秒。-I, --ignore-times 不跳过那些有同样的时间和长度的文件。--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。-T --temp-dir=DIR 在DIR中创建临时文件。--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。-P 等同于 --partial。--progress 显示备份过程。-z, --compress 对备份的文件在传输时进行压缩处理。--exclude=PATTERN 指定排除不需要传输的文件模式。--include=PATTERN 指定不排除而需要传输的文件模式。--exclude-from=FILE 排除FILE中指定模式的文件。--include-from=FILE 不排除FILE指定模式匹配的文件。--version 打印版本信息。--address 绑定到特定的地址。--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。--port=PORT 指定其他的rsync服务端口。--blocking-io 对远程shell使用阻塞IO。-stats 给出某些文件的传输状态。--progress 在传输时显示传输过程。--log-format=formAT 指定日志文件格式。--password-file=FILE 从FILE中得到密码。--bwlimit=KBPS 限制I/O带宽，KBytes per second。-h, --help 显示帮助信息。 常用组合从上面描述的选项可以看到，这个命令的参数特别多，但实际使用时只需要掌握一些常用组合即可： 保留文件原属性拷贝1rsync -avz jerry@82.156.125.169:/data/logs /data/backuplogs 从远程主机将目录 /data/logs 拷贝到本地，对应目录为 /data/backuplogs，保留文件原有属性，包括权限、时间、软硬连接、属主、组等 删除目标目录中S再原目录不存在的文件1rsync -avz --delete --progress /data/logs 82.156.125.169:/data/backuplogs 将本地 /data/logs 目录推送到远程目录 82.156.125.169:/data/backuplogs 下，同时删除在原目录下不存在的文件，这个参数组合特别适合用来完全同步两个目录的内容 限制网速传输1rsync -avz --bwlimit=1024 /data/logs 82.156.125.169:/data/backuplogs/ 同步数据时限制传输速度为1024kBytes/s（就是1MB/s），防止占用过多带宽，可以根据需要调整数值，注意末尾加了一个斜杠 / 表示将原目录同步到目标目录下，而不是与目标目录完全同步，这一点在实践中要注意 断点续传1rsync -avz --partial /data/logs root@82.156.125.169:/data/backuplogs 传输时保留那些因故没有完全传输的文件，来是加快随后的再次传输 总结 本地拷贝数据用 cp，远程拷贝选 scp，如果想实现增量复制拷贝可以使用 rsync rsync 可以将数据从本地推送至服务端，也可以将数据从服务端拉取到本地，也可以使用 daemon 模式 rsync 最常用的命令组合就是 avz，可以保留文件原有属性进行递归拷贝 使用 rsync 命令传输数据时注意目录末尾的斜杠 /，加上它表示目录下的内容，使用时要注意不要覆盖错了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== “人类一思考，上帝就发笑”。虽然我们已经总结了万物，终究还是渺小和无知的代表，但我们可以利用这些渺小的总结，在有限的生命内过的更好，一些看似无法到达和接触的领域，在被划分整理之后显得那么苍白，他想笑就让他去笑吧，毕竟我们只需要为自己的有限生命而负责~ 2022-11-14 00:29:36]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>rsync</tag>
        <tag>cp</tag>
        <tag>数据同步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下查询主板、CPU、内存等硬件信息]]></title>
    <url>%2Fblog%2F2022%2F11%2F06%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9F%A5%E8%AF%A2%E4%B8%BB%E6%9D%BF%E3%80%81CPU%E3%80%81%E5%86%85%E5%AD%98%E7%AD%89%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言如果是在windows系统下，查询电脑硬件会容易的多，可以通过电脑属性、计算机管理等多种图形化界面中查到，如果安装了各种电脑管家，那查询这类信息就更方便了，但如果在linux系统下通常要使用命令来解决，特别是查询服务器配置时，一般不会给服务器安装图形化界面，所以掌握必要的查询命令对于合理使用服务器资源很有必要。 dmidecode这个命令是偶然发现的，之前一般是查询电脑硬件资源的使用情况，比如 top 来看各个进程消耗的CPU和内存，使用 free -h 查询内存总体使用情况，最近电脑内存不太够了，想查询一下内存插槽状况，所以找到了这个命令 dmidecode，英文解释为 “DMI table decoder”，也就是DMI表解码器，可以理解为DMI信息的解释器。 在学习什么是DMI之前，先了解一下 SMBIOS 的概念，BOIS 是英文”Basic Input Output System”的缩略词，它是一组固化到计算机内主板上一个ROM芯片上的程序，它保存着计算机最重要的基本输入输出的程序、开机后自检程序和系统自启动程序，并可以从CMOS中读写系统设置的具体信息，为计算机提供最底层的、最直接的硬件设置和控制。 SMBIOS 全称是”System Management Basic Input/Output System”，表示系统管理基本输入输出系统，是主板或系统制造者以标准格式显示产品管理信息所需遵循的统一规范。 DMI 全称 “Desktop Management Interface”，用于帮助收集电脑系统信息的管理系统，DMI信息的收集必须在严格遵照SMBIOS规范的前提下进行，其设计适用于任何的平台和操作系统，充当了管理工具和系统层之间接口的角色。它建立了标准的可管理系统，更加方便了电脑厂商和用户对系统的了解，同时提供更为友好的用户工作环境。SMBIOS 和 DMI 都是由行业指导机构 “Desktop Management Task Force (DMTF)” 起草的开放性的技术标准。 常用参数 -q：显示会简单点，一些未知的、不活动的和oem指定的条目不显示，元数据和句柄引用被隐藏。 -t: 一个非常重要的参数，用于显示指定类型的条目，可以使用数字，也可以使用一些代表数字组合的单词 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849DMI TYPES The SMBIOS specification defines the following DMI types: Type Information ──────────────────────────────────────────── 0 BIOS 1 System 2 Baseboard 3 Chassis 4 Processor 5 Memory Controller 6 Memory Module 7 Cache 8 Port Connector 9 System Slots 10 On Board Devices 11 OEM Strings 12 System Configuration Options 13 BIOS Language 14 Group Associations 15 System Event Log 16 Physical Memory Array 17 Memory Device 18 32-bit Memory Error 19 Memory Array Mapped Address 20 Memory Device Mapped Address 21 Built-in Pointing Device 22 Portable Battery 23 System Reset 24 Hardware Security 25 System Power Controls 26 Voltage Probe 27 Cooling Device 28 Temperature Probe 29 Electrical Current Probe 30 Out-of-band Remote Access 31 Boot Integrity Services 32 System Boot 33 64-bit Memory Error 34 Management Device 35 Management Device Component 36 Management Device Threshold Data 37 Memory Channel 38 IPMI Device 39 Power Supply 40 Additional Information 41 Onboard Devices Extended Information 42 Management Controller Host Interface 数字组合的单词代表： 123456789101112131415161718Keyword Types──────────────────────────────bios 0, 13system 1, 12, 15, 23, 32baseboard 2, 10, 41chassis 3processor 4memory 5, 6, 16, 17cache 7connector 8slot 9Keywords are matched case-insensitively. The following command lines are equivalent:· dmidecode --type 0 --type 13· dmidecode --type 0,13· dmidecode --type bios· dmidecode --type BIOS -s：可以输出一些指定关键词相关信息，可用关键词如下： 123456789101112131415161718192021222324Valid string keywords are: bios-vendor bios-version bios-release-date system-manufacturer system-product-name system-version system-serial-number system-uuid system-family baseboard-manufacturer baseboard-product-name baseboard-version baseboard-serial-number baseboard-asset-tag chassis-manufacturer chassis-type chassis-version chassis-serial-number chassis-asset-tag processor-family processor-manufacturer processor-version processor-frequency -t参数测试以查看内存信息为例，可以使用 sudo dmidecode -t memory 命令查看内存所有信息，根据man手册知道这个命令包括5、6、16、17四项内容，但实际上我查询到的5、6为空，这个命令在我的电脑上只包含16、17两项 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283$ sudo dmidecode -t memory# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present.Handle 0x0002, DMI type 16, 23 bytesPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 32 GB Error Information Handle: Not Provided Number Of Devices: 2Handle 0x0003, DMI type 17, 84 bytesMemory Device Array Handle: 0x0002 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: ChannelA-DIMM0 Bank Locator: BANK 0 Type: DDR4 Type Detail: Synchronous Speed: 2667 MT/s Manufacturer: Micron Serial Number: 00000000 Asset Tag: None Part Number: 4ATF51264HZ-2G6E1 Rank: 1 Configured Memory Speed: 2667 MT/s Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: Not Specified Module Manufacturer ID: Bank 1, Hex 0x2C Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 4 GB Cache Size: None Logical Size: NoneHandle 0x0004, DMI type 17, 84 bytesMemory Device Array Handle: 0x0002 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: ChannelB-DIMM0 Bank Locator: BANK 2 Type: DDR4 Type Detail: Synchronous Speed: 2667 MT/s Manufacturer: Micron Serial Number: 00000000 Asset Tag: None Part Number: 4ATF51264HZ-2G6E1 Rank: 1 Configured Memory Speed: 2667 MT/s Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: Not Specified Module Manufacturer ID: Bank 1, Hex 0x2C Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 4 GB Cache Size: None Logical Size: None 可以直接指定单独的数字查询，比如查询内存阵列： 12345678910111213$ sudo dmidecode -t 16# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present.Handle 0x0002, DMI type 16, 23 bytesPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 32 GB Error Information Handle: Not Provided Number Of Devices: 2 查询内存设备（内存条）信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374$ sudo dmidecode -t 17# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present.Handle 0x0003, DMI type 17, 84 bytesMemory Device Array Handle: 0x0002 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: ChannelA-DIMM0 Bank Locator: BANK 0 Type: DDR4 Type Detail: Synchronous Speed: 2667 MT/s Manufacturer: Micron Serial Number: 00000000 Asset Tag: None Part Number: 4ATF51264HZ-2G6E1 Rank: 1 Configured Memory Speed: 2667 MT/s Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: Not Specified Module Manufacturer ID: Bank 1, Hex 0x2C Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 4 GB Cache Size: None Logical Size: NoneHandle 0x0004, DMI type 17, 84 bytesMemory Device Array Handle: 0x0002 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: ChannelB-DIMM0 Bank Locator: BANK 2 Type: DDR4 Type Detail: Synchronous Speed: 2667 MT/s Manufacturer: Micron Serial Number: 00000000 Asset Tag: None Part Number: 4ATF51264HZ-2G6E1 Rank: 1 Configured Memory Speed: 2667 MT/s Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: Not Specified Module Manufacturer ID: Bank 1, Hex 0x2C Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 4 GB Cache Size: None Logical Size: None 一共有2个存储插槽，每个插槽上装有一个4G的内存条，最大支持32G内存 -q参数测试测试查询内存信息命令 sudo dmidecode -t 16 时可以看到开头有一些版本信息，查询其他类型时也包含这些信息 123# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present. 如果想屏蔽这些信息就可以使用 -q 参数，还可以屏蔽未知信息，可以对比一下： 1234567$ sudo dmidecode -t 16 -qPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 32 GB Number Of Devices: 2 12345678910111213$ sudo dmidecode -t 16# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present.Handle 0x0002, DMI type 16, 23 bytesPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 32 GB Error Information Handle: Not Provided Number Of Devices: 2 -s参数测试查询BOIS信息: 12345678$ sudo dmidecode -s bios-vendorLENOVO$ sudo dmidecode -s bios-versionN2SET18P (1.12 )$ sudo dmidecode -s bios-release-date04/01/2020 查询系统信息： 12345678$ sudo dmidecode -s system-uuida2d29e4c-2f08-11b2-a85c-8252d66b70f9$ sudo dmidecode -s system-familyThinkPad X390$ sudo dmidecode -s system-manufacturerLENOVO 查询机箱主板信息 1234567891011121314151617$ sudo dmidecode -s chassis-typeNotebook$ sudo dmidecode -s chassis-versionNone$ sudo dmidecode -s chassis-serial-numberPC1JX6X4$ sudo dmidecode -s baseboard-manufacturerLENOVO$ sudo dmidecode -s baseboard-product-name20SDA01ACD$ sudo dmidecode -s baseboard-versionSDK0L77769 WIN 查询处理器信息 12345$ sudo dmidecode -s processor-familyCore i5$ sudo dmidecode -s processor-versionIntel(R) Core(TM) i5-10210U CPU @ 1.60GHz 总结 DMI 在遵守SMBIOS规范的前提下帮助收集电脑系统信息，适用于任何的平台和操作系统，可用 dmidecode 命令查询 dmidecode 最重要的是 -t 参数，可以指定的特定数字类型0-42，可以使用具有组合命令的单词， 这些预定单词不区分大小写： bios、system、baseboard、chassis、processor、memory、cache、connector、slot 还有查询硬件使用情况的 top、htop、free 命令，查询静态信息的 lscpu、lsgpu、lsmem、lspci、lsusb命令等待挖掘 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 管理的本质是管人理事。真诚待人，踏实做事，坚信自己的目标可以实现并全力以赴，用行动表明自己的态度，不要忘了沟通交流，了解每个人的核心诉求，不要大包大揽，学会做应该做的事而不是所有事~ 2022-11-6 23:03:25]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>dmidecode</tag>
        <tag>lscpu</tag>
        <tag>硬件信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk处理/etc/passwd文件]]></title>
    <url>%2Fblog%2F2022%2F10%2F30%2Fawk%E5%A4%84%E7%90%86-etc-passwd%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言awk 是 linux 环境下的一个强大的编程工具，用于对文本和数据进行处理。数据可以来自标准输入、一个或多个文件，或其它命令的输出。同时它支持用户自定义函数和动态正则表达式等先进功能，可以被作为一种编程语言，可以很方便的在命令行中使用，但更多是在脚本来使用，为了熟悉这个命令，我们可以用它来尝试处理 /etc/passwd 文件学习一下常见用法。 awk处理流程awk 这个名字来源于它的三个作者姓氏的第一个字母，分别是Alfred Aho、Brian Kernighan、Peter Weinberger，常用的版本是 gawk， 它是awk的GNU版本，提供了Bell实验室和GNU的一些扩展。 awk 工作流程可分为三个部分： 读输入文件之前执行的代码段（GEGIN块，由BEGIN关键字标识，可选） 主循环执行输入文件的代码段（BODY块，可选） 读输入文件之后的代码段（END块，由END关键字标识，可选） 命令结构: 1awk &apos;BEGIN&#123; commands &#125; pattern&#123; commands &#125; END&#123; commands &#125;&apos; 开始块（BEGIN）1BEGIN &#123;awk-commands&#125; 开始块部分是可选的，可以没有开始块部分，如果存在必须以 BEGIN 开头 开始块就是在程序启动的时候执行的代码部分，并且它在整个过程中只执行一次，我们可以在开始块中初始化一些变量。 主体块（BODY）1pattern &#123;awk-commands&#125; 默认情况下，对于输入的每一行，awk 都会执行命令，我们可以通过 pattern 将其限定在指定的模式中，这部分也是可选的，不存在也是可以的，不过不存在主体的脚本用处不大。 结束块（END）1END &#123;awk-commands&#125; 结束块部分是可选的，可以没有结束块部分，是在程序结束时执行的代码。 如果存在必须以 END 开头 /etc/passwd文件起初 /etc/passwd 文件包含了用户名和密码，但是由于该文件允许所有用户读取，易导致用户密码泄露，因此 Linux 系统将用户的密码信息从 /etc/passwd 文件中分离出来，并单独放到了/etc/shadow 文件中，此文件只有 root 用户拥有读权限，其他用户没有任何权限，这样就一定程度上保证了用户密码的安全性。 /etc/passwd 配置文件的内容用冒号 : 隔开分为7段，分别为： 用户名：账户名字 x：早期这个部分放的是用户登入密码，现在的密码是放入/etc/shadow中的 UID：用户ID，0表示系统管理员，1~999保留给系统使用的ID，1000以上给一般使用者 GID：组ID，0表示系统管理员，1~999保留给系统使用的ID，1000以上给一般使用者 使用者的信息说明 用户家目录：用户登入时所在的目录 默认shell：用户在登入的时候，默认使用的shell类型，如果不能使用shell，则会显示/sbin/nologin /etc/shadow 配置文件的内容用冒号 : 隔开分为9段，分别为： 用户名 经过加密的密码( * 表示用户被锁定，！表示无加密) 最近更改过密码的日期：Linux中的日期是经过1970年1月1号开始累计的日期 密码不能修改的天数，0表示随时可以修改 密码需要重新被修改的天数，通过修改该值可以强制修改密码 密码需要变更的告警天数，7表示系统会向用户发出警告的天数 密码到期后帐号可以使用的时间 帐号失效日期，通过1970年1月1号开始累加的日期，到了时间后无论密码是否过期，该账号就不能再使用了 保留的 如果忘记自己的账户密码，该怎么处理呢？ 对于普通账户的密码遗失，可以通过 root 账户使用 passwd 命令重新设置密码解决，passwd username。 如果 root 账号的密码遗失，则需要重新启动进入单用户模式，系统会提供 root 权限的 bash 接口，此时可以用 passwd 命令修改账户密码。或者通过挂载根目录，修改 /etc/shadow 文件将 root 密码清空，无密码即登陆后再使用 passwd 命令配置 root 密码。 awk 常用处理字段分割及相关变量 $1,$2,$3…$n：awk中用该顺序形式表示files中每行以间隔符号分割的各列的不同字段 $0：表示文本本身 NF：表示当前记录的字段数（列数） $NF：最后一列 $(NF-1)：倒数第二列 FNR/NR：行号 FILENAME：文件名 “\t”：制表符 RS：换行符 “”： 打印字符串 FS：定义间隔符 ~：匹配，与==相比不是精确比较 !~：不匹配，不精确比较 ==：等于，必须全部相等，精确比较 /[0-9][0-9]+/：两个或两个以上数字 -F’[:#]’：定义两个分隔符 常用处理添加表头12345678[root@VM-0-3-centos ~]# awk -F: 'BEGIN&#123;print"用户名\t密码\tUID\tGID\t信息\t家目录\tshell"&#125;&#123;printf $1"\t"$2"\t"$3"\t"$4"\t"$5"\t"$6"\t"$7"\n"&#125;' /etc/passwd用户名 密码 UID GID 信息 家目录 shellroot x 0 0 root /root /bin/bashbin x 1 1 bin /bin /sbin/nologindaemon x 2 2 daemon /sbin /sbin/nologinadm x 3 4 adm /var/adm /sbin/nologinlp x 4 7 lp /var/spool/lpd /sbin/nologinsync x 5 0 sync /sbin /bin/sync 添加行号1234567[root@VM-0-3-centos ~]# awk -F: '&#123;printf NR"\t"$0"\n"&#125;' /etc/passwd1 root:x:0:0:root:/root:/bin/bash2 bin:x:1:1:bin:/bin:/sbin/nologin3 daemon:x:2:2:daemon:/sbin:/sbin/nologin4 adm:x:3:4:adm:/var/adm:/sbin/nologin5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin6 sync:x:5:0:sync:/sbin:/bin/sync 打印第三行第1列和第7列12[root@VM-0-3-centos ~]# awk -F: 'NR==3&#123;print $1"\t"$7&#125;' /etc/passwddaemon /sbin/nologin 打印文件总行数12[root@VM-0-3-centos ~]# awk -F: 'END&#123;print FILENAME"\t"NR&#125;' /etc/passwd/etc/passwd 28 查询不能登录的用户数12[root@VM-0-3-centos ~]# awk -F: 'BEGIN&#123;x=0&#125; &#123;if($7=="/sbin/nologin") x++&#125; END&#123;print FILENAME"\t"x&#125;' /etc/passwd/etc/passwd 21 12[root@VM-0-3-centos ~]# awk '/nologin/' /etc/passwd | wc -l21 查询可以登录的用户123[root@VM-0-3-centos ~]# awk -F: '$NF !~ /nologin$/&#123;print $1&#125;' /etc/passwdrootsync 打印UID小于300的用户1234567[root@VM-0-3-centos ~]# awk -F: '&#123; if($3&lt;300) print $1"\t"$3 &#125;' /etc/passwdroot 0bin 1daemon 2adm 3lp 4sync 5 打印UID在2和6之间的用户1234[root@VM-0-3-centos ~]# awk -F: '($3&gt;2&amp;&amp;$3&lt;6)&#123;print $1"\t"$3 &#125;' /etc/passwdadm 3lp 4sync 5 进行算术运算1234[root@VM-0-3-centos ~]# awk -F: '($3*2 &lt; 6)&#123;print $1"\t"$3 &#125;' /etc/passwdroot 0bin 1daemon 2 利用条件判断统计各类用户数1234[root@VM-0-3-centos ~]# awk -F: '&#123;if($3==0)&#123;i++&#125; else if($3&gt;999)&#123;k++&#125; else&#123;j++&#125;&#125; END&#123;print "管理员个数: "i; print "普通用个数: "k; print "系统用户数: "j&#125;' /etc/passwd管理员个数: 1普通用个数: 1系统用户数: 26 每行数据打印3遍12345678910111213141516[root@VM-0-3-centos ~]# awk -F: '&#123;for(i=1;i&lt;=3;i++) print NR"\t"i"\t"$0&#125;' /etc/passwd1 1 root:x:0:0:root:/root:/bin/bash1 2 root:x:0:0:root:/root:/bin/bash1 3 root:x:0:0:root:/root:/bin/bash2 1 bin:x:1:1:bin:/bin:/sbin/nologin2 2 bin:x:1:1:bin:/bin:/sbin/nologin2 3 bin:x:1:1:bin:/bin:/sbin/nologin3 1 daemon:x:2:2:daemon:/sbin:/sbin/nologin3 2 daemon:x:2:2:daemon:/sbin:/sbin/nologin3 3 daemon:x:2:2:daemon:/sbin:/sbin/nologin4 1 adm:x:3:4:adm:/var/adm:/sbin/nologin4 2 adm:x:3:4:adm:/var/adm:/sbin/nologin4 3 adm:x:3:4:adm:/var/adm:/sbin/nologin5 1 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin5 2 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin5 3 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 统计/etc/passwd中各种类型shell的数量1234567[root@VM-0-3-centos ~]# awk -F: '&#123;shells[$NF]++&#125;END&#123; for(i in shells)&#123;print i,shells[i]&#125;&#125;' /etc/passwd/bin/sync 1/bin/bash 3/sbin/nologin 21/sbin/halt 1/bin/false 1/sbin/shutdown 1 网站访问状态统计1234[root@VM-0-3-centos ~]# netstat -ant | grep :80 | awk '&#123;access_stat[$NF]++&#125;END&#123;for(i in access_stat)&#123;print i, access_stat[i]&#125;&#125;'LISTEN 3CLOSE_WAIT 1ESTABLISHED 1 生成清空arp命令的文本12345678[root@VM-0-3-centos ~]# arp | awk '!/Address/&#123;print "arp -d " $1&#125;'arp -d 169.254.0.2arp -d 169.254.0.3arp -d 169.254.0.79arp -d 172.18.0.3arp -d 169.254.128.8arp -d 169.254.128.9arp -d 10.10.0.17 直接在末尾加管道 bash 就可以执行了，arp | awk &#39;!/Address/{print &quot;arp -d &quot; $1}&#39;| bash 这是一种通用的生成处理命令的方式，先生成命令文本，然后作为bash输入，执行即可。 总结 awk 处理流程分为3个阶段，BEGIN块、BODY块、END块，其中BEGIN块和END块都执行一次，BODY块对每一行输入执行一次 awk 的3个处理块都是可选的，对于待处理的文件，一般至少会包含BODY块 awk 中BODY块的 pattern 用于行过滤，/pattern/ 表示匹配，/!pattern/ 表示不匹配 awk 中BODY块的中也可对指定字段判断是否包含，&#39;$1 ~ /root/&#39; 表示第一列包含root，&#39;$NF !~ /nologin$/&#39; 表示最后一列不包含nologin awk 还可用于待处理命令文本的预处理，先将过滤文本拼装成命令文本，然后利用管道直接通过bash执行 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 最近有些急躁，虽知欲速则不达，但压力之下确实难以平静。之所以这么拼，并不是为了卷谁，也不是为了表现给别人看，只是想有所提升超越昨天的自己，目的也非常俗套，通过提升自身来给自己的小家一个更好的生活。“一室之不治，何以天下家国为？”如果每个人都能把自己的小日子过好，则天下太平! 2022-10-30 21:24:10]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>awk</tag>
        <tag>passwd</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后台运行一个go程序]]></title>
    <url>%2Fblog%2F2022%2F10%2F23%2F%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E4%B8%80%E4%B8%AAgo%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言后台运行在日常开发中比较常用，特别是在部署服务器上，一般都是通过ssh连接到服务器，然后启动后台运行程序，如果程序不支持后台运行，那么当终端断开时程序也就退出了，所以掌握常用的后台运行方式还是比较有用的。 概念提到后台运行，通常会想到 daemon 模式，日常开发时也常常混着说，不过通过查询资料时发现，这两个概念还有些区别： 后台运行：是指进程在操作系统中非显示运行，未关联到任何命令行终端或程序界面，这种方式运行的进程则被称为后台进程。 daemon模式：也叫守护进程，它首先是后台运行，然后它还有守护的职责，若异常退出，可以自动重启服务程序。 所以说 daemon 不仅要时候后台运行，还有守护进程职责，像Windows 和 Linux 中的各种服务，比如MySQL、防火墙、SSH服务等都是后台运行的进程。 常用方式很多产品会部署在linux服务器上，所以相比较而言，后台运行在linux上更常用，而 nohup、&amp;、setsid 等命令就基本上可以达到后台运行的目的，之前写过一篇总结 《linux环境下运行程序常用的nohup和&amp;的区别》，可以简单回忆下： nohup 是no hang up的缩写，就是不挂断的意思，忽略SIGHUP信号，在关闭命令终端后程序依旧运行 &amp; 是只后台运行，即忽略SIGINT信号，也就是按Ctrl+C不会终止程序，但是关闭命令行终端程序终止 而 setsid 是新学到的命令，使用起来也非常的简单，只需要加在待执行命令的前面即可： 1[root@VM-0-3-centos ~]# setsid ping www.baidu.com &gt; out.log 此时关闭当前终端，重新打开另一终端会发现 ping 的进程，同时文件 out.log 文件也一直在更新 12345678[root@VM-0-3-centos ~]# ps -ef | grep pingroot 1692 1 0 23:36 ? 00:00:00 ping www.baidu.comroot 1707 1279 0 23:36 pts/1 00:00:00 grep --color=auto ping[root@VM-0-3-centos ~]# tail -f out.log64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=8 ttl=251 time=9.38 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=9 ttl=251 time=9.36 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=10 ttl=251 time=9.35 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=11 ttl=251 time=9.37 ms 小技巧&amp; 不能让进程永久在后台执行，但是如果在命令前后加上()括起来，一样可以实现nohup ..&amp;的功能，命令就能永久在后台执行了。 12345[root@VM-0-3-centos ~]# (ping www.baidu.com &gt; t.log &amp;)[root@VM-0-3-centos ~]# ps -ef | grep pingroot 3410 1 0 23:44 pts/0 00:00:00 ping www.baidu.comroot 3450 1279 0 23:44 pts/1 00:00:00 grep --color=auto ping 代码级别实现虽然上面的方式很方便，但毕竟只能在 linux 上使用，如果可以通过修改 go 代码在 windows 和 linux 上都实现后台运行那再好不过了，很幸运查到一个go的库 github.com/codyguo/godaemon，使用起来非常方便，只需要在代码中引入这个库，然后启动程序是加入 -d 参数就可以后台运行了 123import ( _ "github.com/codyguo/godaemon") 不过我在使用的过程中发现两个问题，一个是传递的后续参数会莫名消失，另一个是好像关闭终端会导致程序退出，所以我打算看看源码，结果发现源码就只有几行： 1234567891011121314151617181920212223package godaemonimport ( "flag" "fmt" "os" "os/exec")func init() &#123; goDaemon := flag.Bool("d", false, "run app as a daemon with -d=true.") flag.Parse() if *goDaemon &#123; cmd := (os.Args[0], flag.Args()...) if err := cmd.Start(); err != nil &#123; fmt.Printf("start %s failed, error: %v\n", os.Args[0], err) os.Exit(1) &#125; fmt.Printf("%s [PID] %d running...\n", os.Args[0], cmd.Process.Pid) os.Exit(0) &#125;&#125; 这些就是源码的全部了，是不是很吃惊，其实弄懂原理就很好明白了，其中利用了 exec.Command 函数，因为go 中没有 fork 的便利实现，所以可以利用 exec.Command 启动新的进程，这样新启动的进程在当前进程退出后就被系统进程接管了，只不过它处理的参数有点问题，flag.Args()会把所有 - 开头的参数都消耗掉，自己按需实现就可以了 另一个关闭终端会导致程序退出的问题，可以在传入 -d 参数的情况下调用 signal.Ignore(syscall.SIGHUP) 忽略掉 SIGHUP 信号即可 总结 在操作系统中非显示运行，未关联到任何命令行终端或程序界面的进程则被称为后台进程 linux环境下常用来后台运行程序的命令有 hohup、&amp; 和 setsid github.com/codyguo/godaemon 是一个极简的后台运行可用库，仅添加修改一行代码 若想更丰富的功能可以参考 github.com/sevlyar/go-daemon 和 github.com/zh-five/xdaemon 两个库 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生若只如初见，何事秋风悲画扇。等闲变却故人心，却道故人心易变~ 2022-10-24 00:04:29]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nohup</tag>
        <tag>setsid</tag>
        <tag>后台</tag>
        <tag>daemon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由逆序对数引发的解题目录]]></title>
    <url>%2Fblog%2F2022%2F10%2F04%2F%E7%94%B1%E9%80%86%E5%BA%8F%E5%AF%B9%E6%95%B0%E5%BC%95%E5%8F%91%E7%9A%84%E8%A7%A3%E9%A2%98%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言如果你恰巧看到了我的上一篇总结《排序算法系列之（七）——分分合合的归并排序》会发现我把搁置了3年半的排序系列又续更了，起因是最近刷题时遇到了逆序对数求解，而解这类问题常用的方法之一就是归并排序，究竟是怎样的一道题呢？我们可以先试着解决一下。 逆序对数题目非常简短，描述内容如下： 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。 示例 1: 12输入: [7,5,6,4]输出: 5 限制： 10 &lt;= 数组长度 &lt;= 50000 作者：LeetCode-Solution链接：https://leetcode.cn/problems/shu-zu-zhong-de-ni-xu-dui-lcof/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 求解过程题目要求不难理解，即使你不知道什么是逆序对也能很容易的根据描述写出下面暴力的解法： 123456789101112class Solution &#123; int ans = 0;public: int reversePairs(vector&lt;int&gt;&amp; nums) &#123; for (int i = 0; i &lt; nums.size(); i++) &#123; for (int j = i + 1; j &lt; nums.size(); j++) &#123; if (nums[i] &gt; nums[j]) ++ans; &#125; &#125; return ans; &#125;&#125;; 不出意外的获得了 TLE，看看限制范围也能猜到这题暴力肯定不让过，如果数组长度小于 100 还可以考虑搏一搏，而这道题必须用更巧妙的方法才行，而归并排序就是解法之一： 12345678910111213141516171819202122232425262728293031class Solution &#123; int ans = 0;public: void mergeCnt(vector&lt;int&gt;&amp; v, int left, int right, vector&lt;int&gt;&amp; t) &#123; if (left &gt;= right) return; int i = left, mid = (left+right) / 2, j = mid + 1, k = 0; mergeCnt(v, left, mid, t); mergeCnt(v, mid+1, right, t); while(i &lt;= mid &amp;&amp; j &lt;= right) &#123; if (v[i] &lt;= v[j]) t[k++] = v[i++]; else &#123; t[k++] = v[j++]; ans += mid - i + 1; // 相比归并排序只多了这一行 &#125; &#125; while (i &lt;= mid) t[k++] = v[i++]; while (j &lt;= right) t[k++] = v[j++]; copy(t.begin(), t.begin() + right - left + 1, v.begin() + left); &#125; int reversePairs(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;int&gt; t(n); mergeCnt(nums, 0, n-1, t); return ans; &#125;&#125;; 相比原始的归并排序只增加了一行代码，就得到了一个优于暴力解法O(N^2)的O(NlogN)的解法，具体解释可以去看相关题解，本文的总结目的不是解一道题，而是想给自己保留一个有趣的解题目录。 解题目录刷题大计最近两年断断续续的在进行着，题解也有写过一些，尝试过多种方式，内容比较零散，复习时比较头疼，没有一个完整的大纲和复习线路，所以打算单开一篇总结，持续收集一些有意思的题目，方便后续的复习和拿来即用。 题目的标签主要有两个方面，一是本身的题目知识点类型，另一种是解题用到的解法类型，比如上面提到的这道题，从题目看归为逆序对数，从解法看可以归为排序解法和离散化树状数组解法，所以后面可以会看到一个题目出现在多个目录中的情况，只是分类依据不同而已。 题目分类逆序对数 51. 数组中的逆序对 解法分类归并排序 51. 数组中的逆序对 离散化树状数组 51. 数组中的逆序对 差分数组 2406. 将区间分为最少组数 滑动窗口 2379. 得到 K 个黑块的最少涂色次数 Trie树 2416. 字符串的前缀分数和 1233. 删除子文件夹 动态规划 19. 秋叶收藏集 ……持续补充 总结 有时看似无关的两件事居然关系紧密，比如归并排序加一行代码就可以求解逆序对数 分类、总结、重新分类，在不断分类中重新认识这个世界 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 不积跬步，无以至千里；不积小流，无以成江海。放眼于未来，着眼于脚下，一味计划而不行动，最终醒来只会发现是梦一场~ 2022-10-5 00:49:14]]></content>
      <categories>
        <category>OJ</category>
      </categories>
      <tags>
        <tag>题解</tag>
        <tag>OJ</tag>
        <tag>解题目录</tag>
        <tag>逆序对数</tag>
        <tag>归并排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法系列之（七）——分分合合的归并排序]]></title>
    <url>%2Fblog%2F2022%2F10%2F03%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B9%8B%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E5%88%86%E5%88%86%E5%90%88%E5%90%88%E7%9A%84%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言再一次总结基础的排序算法，印象里距离上一次总结排序也没过多久，查询后才发现上一篇总结《排序算法系列之（六）——逐步砍掉树杈的堆排序》到现在已经过去了3年多的时间，真是时光荏苒，岁月如梭啊，这次想起总结排序起因并不是排序，而是查找逆序数对，而解决逆序数对通常使用的两种方式是归并排序和离散化树状数组，所以我又把归并排序捡起来了 归并排序温故而知新，知识有时候就是这么神奇，多年以后再次看到归并排序，才发现以前掌握的归并排序并不全面，之前理解的归并排序主要放在“并”这个操作，但是其实归并排序中还有很重要的一环，那就是“分”，先分后合才是完整的归并排序。 归并排序是建立在归并操作上的一种有效，稳定的排序算法，该算法是采用分治法的一个非常典型的应用，首先将整个序列一分为二得到两个序列，然后将分出的两个序列排好序后再合并，得到完全有序的序列。对于子序列如果元素大于1则继续进行一分为二，分别排序再合并的操作，直至有序，算法本身是通过一个递归的概念定义。当然反过来通过递推也可以实现，相邻区间两两合并，最终至全部有序也是可以的。 提到一分为二很容易联想到快速排序，在最优的情况下待排序的数组每次被一分为二，将小于中间值的元素全部移到数组前半段，将大于中间值的元素全部移到数组后半段，完成一趟快排，然后对每段分别采用相同的策略从而达到整理有序，这是快排的思想，听起来有些部分确实和归并排序很像，但是区别也是很大的，比如快排不需要合并的操作，另外快排是一种不稳定的排序。 为了简单一点，我们还是采用递归的版本来描述一下归并排序，先画一个图，直观的看下归并排序中二分天下是怎么操作的，先分裂： 12345678910111213141516171819graph TB A[2,3,9,4,7,12,6,1,11,5]--&gt;B[2,3,9,4,7]; A[2,3,9,4,7,12,6,1,11,5]--&gt;C[12,6,1,11,5]; B[2,3,9,4,7]--&gt;D[2,3,9]; B[2,3,9,4,7]--&gt;E[4,7]; C[12,6,1,11,5]--&gt;F[12,6,1]; C[12,6,1,11,5]--&gt;G[11,5]; D[2,3,9]--&gt;H[2,3]; D[2,3,9]--&gt;I[9]; E[4,7]--&gt;J[4]; E[4,7]--&gt;K[7]; F[12,6,1]--&gt;L[12,6]; F[12,6,1]--&gt;M[1]; G[11,5]--&gt;N[11]; G[11,5]--&gt;O[5]; H[2,3]--&gt;P[2]; H[2,3]--&gt;Q[3]; L[12,6]--&gt;R[12]; L[12,6]--&gt;S[6]; idx_0 idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 2 3 9 4 7 12 6 1 11 5 排序后再合并： 12345678910111213141516171819graph BT A[1,2,3,4,5,6,7,9,11,12]--&gt;B[2,3,4,7,9]; A[1,2,3,4,5,6,7,9,11,12]--&gt;C[1,5,6,11,12]; B[2,3,4,7,9]--&gt;D[2,3,9]; B[2,3,4,7,9]--&gt;E[4,7]; C[1,5,6,11,12]--&gt;F[1,6,12]; C[1,5,6,11,12]--&gt;G[5,11]; D[2,3,9]--&gt;H[2,3]; D[2,3,9]--&gt;I[9]; E[4,7]--&gt;J[4]; E[4,7]--&gt;K[7]; F[1,6,12]--&gt;L[6,12]; F[1,6,12]--&gt;M[1]; G[11,5]--&gt;N[11]; G[11,5]--&gt;O[5]; H[2,3]--&gt;P[2]; H[2,3]--&gt;Q[3]; L[6,12]--&gt;R[12]; L[6,12]--&gt;S[6]; 排序过程上面的图可以很清楚的看出怎样划分以及合并的结果，但是对于描述怎样每个子数组怎样从无序变成有序不太明显，同时也忽略了合并的算法，所以下面用语言简单描述下，其实每个子数组怎样从无序变成有序这个并不需要关心，因为数组是一个元素是必定有序，当数组包含两个元素时，说明它是由两个单元素数组“归并”而成，所以我们需要掌握的是归并的算法。同理，4个元素的数组是由两个双元素有序数组归并而成，同样说明需要掌握的只有归并的逻辑。 下面我们具体操作一下，以 [2,3,4,7,9] 和 [1,5,6,11,12] 两个子数组归并成最终有序数组为例，模拟一次从小到大排序。 首先我们假设有两个指针L 和 R 分别指向两个数组的首个元素，另有一个数组 M 为结果数组，初始为空 M = []，下面开始操作，初始数据如下： 123[2(L),3,4,7,9][1(R),5,6,11,12][] 比较两个指针元素，找出两个指针所指的更小的元素，放入结果数组后指针加一，R 指针元素更小，元素1放入结果数组，指针向后加一 123[2(L),3,4,7,9][1,5(R),6,11,12][1] 继续步骤1，这次 L 指针元素更小，元素2放入结果数组，指针向后加一 123[2,3(L),4,7,9][1,5(R),6,11,12][1,2] 继续比较，还是 L 指针元素更小，元素3放入结果数组，指针向后移动 123[2,3,4(L),7,9][1,5(R),6,11,12][1,2,3] 继续比较，仍然是 L 指针元素更小，元素4放入结果数组，指针向后移动 123[2,3,4,7(L),9][1,5(R),6,11,12][1,2,3,4] 继续比较，这次变成 R 指针元素更小，元素5放入结果数组，指针向后移动 123[2,3,4,7(L),9][1,5,6(R),11,12][1,2,3,4,5] 继续比较，仍然是 R 指针元素更小，元素6放入结果数组，指针向后移动 123[2,3,4,7(L),9][1,5,6,11(R),12][1,2,3,4,5,6] 继续比较，现在是 L 指针元素更小，元素7放入结果数组，指针向后移动 123[2,3,4,7,9(L)][1,5,6,11(R),12][1,2,3,4,5,6,7] 继续比较，仍是 L 指针元素更小，元素9放入结果数组，指针向后移动 123[2,3,4,7,9](L)[1,5,6,11(R),12][1,2,3,4,5,6,7,9] 现在 L 指针已经走到了数组的最后，而 R 所在的数组还有元素，说明 R 中剩余的元素肯定都比 L 最后一个元素都大，所以直接把 R 中剩余元素11和12放到结果数组就完成了排序 123[2,3,4,7,9](L)[1,5,6,11,12](R)[1,2,3,4,5,6,7,9,11,12] 从排序过程也可以看出，使用归并排序需要一个额外的结果数组来完成合并操作，下面我们用代码来实现一下这个算法。 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/*功能： 交换两个变量参数： element1--被交换的第一个元素的地址 element2--被交换的第二个元素的地址返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void swap_data(int* element1, int* element2)&#123; int middle_value = *element1; *element1 = *element2; *element2 = middle_value;&#125;/*功能： 将数组的两个有序段[left,mid]和[mid+1,right]合并成[left,right]区间完整有序参数： array--表示待排序的数组，此处会退化成指针 left--数组第一段开始的索引 mid--数组第一段结束的索引 right--数组第二段结束的索引返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void merge(int array[], int left, int mid, int right)&#123; int* temp = (int*)malloc((right-left+1)*4); int i = 0, l = left, r = mid + 1; while (l &lt;= mid &amp;&amp; r &lt;= right) &#123; if (array[l] &lt;= array[r]) temp[i++] = array[l++]; else temp[i++] = array[r++]; &#125; //第一段仍有元素没加到结果 while (l &lt;= mid) temp[i++] = array[l++]; //第二段仍有元素没加到结果 while (r &lt;= right) temp[i++] = array[r++]; //结果赋值回原数组 for (int j = 0; j &lt;= right - left; j++) array[left+j] = temp[j]; free(temp);&#125;/*功能： 归并排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 left--数组第一个待排序元素索引 right--数组最后一个待排序元素索引返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void merge_sort(int array[], int left, int right)&#123; if (right &gt; left) &#123; int mid = (right + left) / 2; merge_sort(array, left, mid); merge_sort(array, mid+1, right); merge(array, left, mid, right); &#125;&#125; 代码分析归并排序的核心的是分割和归并，分割时采取递归一分为二就可以，然后归并才是体现算法精髓的地方，合并时通常使用两个指针来分别指向数组的两段，通过不断比较元素大小将两段有序数组合并成一段，在排序过程中使用了额外的空间，这也是归并排序的劣势，例子中为了方便每次合并时都申请了新数组，其实可以优化一下，在排序开始申请一个临时数组就可以，中间合并时使用同一个临时数组就可以。 运行测试在线编辑器是一个很方便的测试代码的环境，如果想本地调试一下，也可以直接下载归并排序–源码，在本地编译后进行调试，其实边看代码边调试是理解算法思路很有效的方式。 总结 归并排序的核心思想是不断二分后合并两段有序子数组达到最终有序 归并排序因为是优先比较相邻的元素，所以是稳定的排序算法 归并排序使用了临时空间，最大与原数组等长，优化时可以在排序前申请一个就可以服务于整个排序过程 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 生活的点点滴滴随时间流淌，它们不是逝去，恰恰是它们组成了我们的生活，我们可以选择躺平，可以选择奋斗，可以选择全力以赴，当我们还可以选择的时候，请感恩你所拥有的一切吧，特别是对已经攥在手里的东西，深深地表达一下感激，勿等失去徒伤悲。 2022-10-4 02:20:05]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
        <tag>归并</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本来打算完整安装一次redmine最终还是放弃了]]></title>
    <url>%2Fblog%2F2022%2F10%2F01%2F%E6%9C%AC%E6%9D%A5%E6%89%93%E7%AE%97%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85%E4%B8%80%E6%AC%A1redmine%E6%9C%80%E7%BB%88%E8%BF%98%E6%98%AF%E6%94%BE%E5%BC%83%E4%BA%86%2F</url>
    <content type="text"><![CDATA[前言redmine 是最近工作中经常使用的项目管理软件，因为平时主要使用跟踪一些开发进度和bug修复情况，平时使用并不算重度，体验还不错，最近因为需要停电重启，所以想借着这个机会学习一下怎么安装，看了不少教程，各种安装方法，因为依赖项确实有点多，至今也没按照官网wiki安装成功（战术捂脸），不过从中发现了很多新的知识点，还是记录一下。 安装参考其实官方的wiki写的就不错，但是有关依赖项的部分提及的很少，还有就是没有说怎么安装 ruby，先把写的比较好的文档列举在此，有时间再试试，其实也快安装成功了，就是云主机内存太低，有一步安装需要编译，总是内存不足… redmine/wiki/redmineinstall CentOS 7 安装 Redmine 4.1 Linux下redmine安装使用 centos安装ruby及更换gem的源 bitnami redmine一键式安装 centos 安装 redmine mysql ruby on rails How to setup Redmine 4 on Ubuntu 18.04 Linux 下安装 Redmine linux搭建redmine：bitnami-redmine-4.1.1-4-linux-x64-installer.run Redmine安装神器：Bitnami redmine 的安装指导（Linux的安装方法） Redmine packaged by Bitnami 以上列举的教程前几个还不错，虽然没安装完整，但是接触了ruby、nginx、mysql这些依赖项，也了解了不少新东西，大概知道运行 redmine 需要哪些东西了，完成安装只是时间和硬件问题，咳咳~ 其中有几篇文章建议利用 bitnami 来简化 redmine 的安装，看步骤确实方便了很多，但是不幸的是 Bitnami 在2021年6月30日之前停止对大多数Linux本地安装程序的支持，我已经找不到安装程序 bitnami-redmine-4.0.5-0-linux-x64-installer.run了 Bitnami plans on discontinuing the support for the majority of Native Installers for Linux by June 30th 2021. Please read this blog post for more information. Ruby已经多次接触到 Ruby 了，上次是升级gitlab服务器的时候，新版本的gitlab需要安装ruby依赖项，这次安装redmine同样需要安装Ruby，对于Ruby的安装省事的方法可以通过Linux对应包管理工具直接安装，但是版本通常达不到要求，所以我们总是需要采用复杂的方式才可以安装Ruby，先了解下Ruby是什么。 Ruby 是一种跨平台、面向对象的动态类型编程语言。Ruby 体现了表达的一致性和简单性，它不仅是一门编程语言，更是表达想法的一种简练方式。 Ruby 的作者于 1993 年 2 月 24 日开始编辑 Ruby，直至 1995 年 12 月才正式公开发布。之所以称为 Ruby，是因为 Perl 的发音与 6 月的诞生石 pearl（珍珠）相同，因此 Ruby 以 7 月的诞生石 ruby（红宝石）命名。 Ruby 的特性与 Smalltalk、Perl 和 Python 类似。Perl、Python 和 Smalltalk 是脚本语言。Smalltalk 是一个真正的面向对象语言。Ruby，与 Smalltalk 一样，是一个完美的面向对象语言。使用 Ruby 的语法比使用 Smalltalk 的语法要容易得多。 RailsRuby on Rails（官方简称为 Rails，亦被简称为 RoR），是一个使用 Ruby 语言写的开源 Web 应用框架，它是严格按照 MVC 结构开发的。它努力使自身保持简单，来使实际的应用开发时的代码更少，使用最少的配置。 Rails 的设计原则包括“不做重复的事”和“惯例优于设置” 。 Ruby on Rails 是一种结合 Ruby 语言与 Rails 平台的一种网页程序框架，Ruby 语言以自然、简洁、快速著称，全面支持面向对象程序设计，而 Rails 则是 Ruby 广泛应用方式之一，在 Rails 平台上设计出一套独特的 MVC 开发架构，采取模型（Model）、视图（View）、控制器（Controller）分离的开发方式，不但减少了开发中的问题，更简化了许多繁复的动作。 RubyGemsRubyGems 是 Ruby 的一个包管理器，它提供一个分发 Ruby 程序和库的标准格式，还提供了一个管理程序包安装的工具，它将一个 Ruby 应用程序打包到一个 gem 里，作为一个安装单元，旨在方便地管理 gem 安装的工具，以及用于分发 gem 的服务器，这类似于 Ubuntu 下的apt-get，Centos 的 yum，Python 的 pip，大约创建于2003年11月，从Ruby 1.9版起成为Ruby标准库的一部分。 RVM在linux上安装Ruby在之前提到了，可以通过对应系统的包管理软件来直接安装，比如在CentOS上可以直接运行 sudo yum install ruby 命令来安装，但是这种方式通常安装的版本比较低，无法满足需要，所以你也可以采用源码编译的方式安装。 如果从源码级别安装已经超出了你的能力范围还可以使用 RVM 来安装，它是 Ruby 的版本管理工具，类似 nodejs的 nvm 工具，可以方便的安装和配置当前系统使用的Ruby版本，安装RVM的命令如下： 12$ gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB$ curl -sSL https://get.rvm.io | bash -s stable 若安装报错先输入以下命令再继续： 12$ command curl -sSL https://rvm.io/mpapis.asc | gpg2 --import -$ command curl -sSL https://rvm.io/pkuczynski.asc | gpg2 --import - 之后就可以利用 rvm 命令来安装 Ruby 了 12$ rvm install 2.6$ ruby -v 另辟蹊径安装 redmine 很困难，难道我们就没办法了吗？不！我们还有 docker，一条命令 docker run -d --name some-redmine -p 8080:3000 redmine 搞定，首先会下载所需要的镜像，之后直接启动，通过 http://IP:8080/ 直接就可以访问了，真香~ 总结 redmine 是一个项目管理web软件，依赖Rails、MySQL、Nginx等组件或服务 Ruby on Rails 是一个使用 Ruby 语言写的开源 Web 应用框架，严格按照 MVC 结构开发 可以通过RVM安装Ruby， RVM 是 Ruby 的版本管理工具，可以方便的安装和配置当前系统使用的Ruby版本 如果参照官方的wiki实在安装不上redmine，可以通过 docker 来安装，虽然看起来像个玩具，但是真的能用啊 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 黄河远上白云间，一片孤城万仞山。羌笛何须怨杨柳，春风不度玉门关。 2022-10-3 02:33:26]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
        <tag>redmine</tag>
        <tag>ruby</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如期而至的SVN服务器迁移引来一个大瓜XAMPP]]></title>
    <url>%2Fblog%2F2022%2F09%2F13%2F%E5%A6%82%E6%9C%9F%E8%80%8C%E8%87%B3%E7%9A%84SVN%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%81%E7%A7%BB%E5%BC%95%E6%9D%A5%E4%B8%80%E4%B8%AA%E5%A4%A7%E7%93%9CXAMPP%2F</url>
    <content type="text"><![CDATA[前言之前对于版本控制工具 svn 和 git 更多的是操作命令的使用，而最近逐步开始接触使用环境的搭建了，对于一些私有项目还是需要在内部服务器上搭建一个 svn 或者 git 服务器的，前段时间因为 SVN 服务器硬盘空间告急，所以 SVN 数据迁移被提上了日程，经过几个月的准备，终于要动手了，期间也讨论了几个方案，结果最终迁移完发现，和我想的完全不一样啊，看来还是我太年轻了，世界真大~ 方案评估进行重大行动前总要有个可行性分析，虽然这不算特别重大，但终归会影响一个项目组的工作，所以要尽可能考虑周全，避免造成数据损失，影响开发进度，所以一开始基于目前数据仓库的现状提出了几种迁移的方案： 原机器增加硬盘：经运维同学确认机器太老，不支持扩展了，此路不通 采用 svnadmin dump 这种官方推荐的备份方式：这种方式采用 svnadmin dump、svndumpfilter、svnadmin load 等命令可以实现仓库备份、还原、甚至定向过滤等目的，但这种方式适合数据少短历史的仓库，如果版本比较大，如版本数增长数十万，那么dump的过程将非常慢，恢复过程更耗时，另外还有一个难点，这种方式要求磁盘空间足够，但是目前仓库占用90%，剩余的10%用来备份绝对不够的，所以此种方法也基本行不通 丢弃提交历史，本地客户端更新最新数据，然后提交到新配的服务器上，做0版提交：此种方法可以绕过磁盘不足的问题，但这种方式丢掉了历史记录，并且改变了仓库的uuid，测试时发现无论是 switch 还是 relocate 命令都无法直接切换，基本上要重新下载一份数据了。 把整个系统连同数据直接镜像到新配置的服务器上：这种方式可以保留完整的提交记录，同时不需要老硬盘提供太大的空间，但操作难度上还需运维同学评估。 其他：使用 svnadmin hotcopy 进行全量拷贝，备份过程较快，灾难恢复也很快；使用 svnsync 制作2个镜像库，须在 svn1.4 版本以上使用。这些基本都是备份的方式，是保证数据安全应该采取的策略，可以作为参考。 所以综上优先选择全盘镜像，其次选择用最新的数据做0版的方式，除此之外如果用 svnadmin 因硬盘受限就只能划分成一个个子目录来迁移了。 前奏之前我接触到的svn服务器基本就是安装svnserve就好，修改用户的访问权限可以直接在服务器上编辑access配置文件，后来了解到了 svnmanager 才知道原来有个可以修改这些配置文件的网页啊，用起来方便了许多，这次迁移之前我还是停留在这个层面上，结果运维同学搭建好环境后，我用 ps 命令找了半天也没找到 svnserve 进程，然后在历史记录里发现了 xampp 这货 接着便查找了很多关于 xampp 的知识，才发现之前还是弱了，这些基本都没了解过，经过这一次迁移，接触到了几十个命令、相关工具和配置文件，特此记录一下整个过程，方便今后复习和拿来就用。 最终迁移的方案选择的是数据整体拷贝，通过网络从旧机器发送到新机器上，然后在新机器上搭建svn服务器环境，这种方式好处很多：不依赖原机器剩余的硬盘空间、传输方便、可以完整保留历史记录、不用重新配置权限文件、修改域名绑定后对所有使用者几乎无感，即使需要重新认证，输入自己的svn用户名密码就搞定了，非常方便。 svn客户端访问服务端时可以通过svnserve、svnserver+ssh、Apache等多种方式访问。svnserve是一个小巧、轻便的服务器程序，设置简单，可以使用 subversion 专有的协议进行访问；但因为本身不提供加密通讯的功能，安全性低，可以通过ssh建立在安全隧道后调用svnserve程序；当然也可以通过http访问，利用Apache通过mod_dav_svn访问版本库，进而进行svn的操作。 我平时用的较多的也是通过 http 协议来访问和更新 svn 仓库的，可以直接搭建 Apache 来实现，也可以使用很方便的 xammp 来实现，本质上访问svn数据和使用svnmanager管理用户和权限都是网页服务，所以这两项都可以利用xammp，那么接下来简单了解下什么是 xammp XAMMPXAMPP（Apache+MariaDB+PHP+PERL）是一个功能强大的建站集成软件包。这个软件包原来的名字是 LAMPP（Linux+Apache+MySQL+PHP+PERL），但是为了避免误解，最新的几个版本就改名为 XAMPP 了。 XAMMP 的出现源于人们对安装网页服务时的挫败感，许多人通过他们自己的经验认识到安装 Apache 服务器是件不容易的事儿，如果您想添加 MySQL、PHP 和 Perl等环境那就更困难了。XAMPP 是一个易于安装且包含 MySQL、PHP 和 Perl 的 Apache 发行版，非常容易安装和使用：只需下载，解压缩，启动即可，最近的版本把解压缩这一步替换成了运行脚本安装，其实和解压缩一样。 因为软件包非常全，一下就解决了访问、数据存储、数据管理、插件安装管理等多个方面，我查看了其中一些软件的配置文件，比如MySQL，无论是配置路径还是数据存储路径都是在 XAMPP 安装路径下，如果想在新的机器上部署一套完全一样的服务，只需要拷贝整个安装目录到新机器上就行了（前提是一些安装时创建的新用户要有），保险的做法是在新机器上安装一次，再用旧机器数据覆盖上去就行了。 前面夸了 XAMPP 这么久，那么它有没有缺点呢？当然有！缺点就是不安全，XAMPP仅用于开发目的，它具有某些配置设置（比如MySQL没有密码），使本地开发变得容易，如果你想让你搭建的XAMPP可以从互联网访问，这样并不是一个好的做法，可以使用WAMP, MAMP或LAMP等替换方案，这些是类似的软件包，更适合生产环境。 不知道是不是因为仅用于开发环境，网络上很难找到在Linux 环境下安装 XAMPP + subversion + svnmanager 的完整教程，在 windows 下安装倒是有几篇，所以我追随运维同学使用历史命令列表，尝试完整搭建一次 svn 服务，这可是从几百个命令列表中不断尝试出来的。 搭建svn服务系统环境如下： [root@VM-0-3-centos /]# hostnamectl Static hostname: VM-0-3-centos Icon name: computer-vm Chassis: vm Machine ID: 5467bde017714ffcad6d449b4a1fbbbc Boot ID: 8aa2f3bd14104190bc11e39bf2831052 Virtualization: kvm Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-1127.19.1.el7.x86_64 Architecture: x86-64 准备软件包可以自行去官网下载： xampp-linux-x64-5.6.40-1-installer.run subversion-1.10.8-bin.tar.gz svnmanager-1.10.tar.gz 安装必要环境和工具123yum install -y epel-releaseyum install vim net-tools telnet wget bind-utils ipmitool ntp rsyncyum install libserf 安装xampp给脚本添加可执行命令后，直接运行即可： 12chmod +x xampp-linux-x64-5.6.40-1-installer.run./xampp-linux-x64-5.6.40-1-installer.run 安装过程还真是挺容易的，一直输入 Y 就可以了 12345678910111213141516171819202122232425262728293031323334[root@VM-0-3-centos software]# ./xampp-linux-x64-5.6.40-1-installer.run----------------------------------------------------------------------------Welcome to the XAMPP Setup Wizard.----------------------------------------------------------------------------Select the components you want to install; clear the components you do not wantto install. Click Next when you are ready to continue.XAMPP Core Files : Y (Cannot be edited)XAMPP Developer Files [Y/n] :YIs the selection above correct? [Y/n]: Y----------------------------------------------------------------------------Installation DirectoryXAMPP will be installed to /opt/lamppPress [Enter] to continue:----------------------------------------------------------------------------Setup is now ready to begin installing XAMPP on your computer.Do you want to continue? [Y/n]: Y----------------------------------------------------------------------------Please wait while Setup installs XAMPP on your computer. Installing 0% ______________ 50% ______________ 100% #########################################----------------------------------------------------------------------------Setup has finished installing XAMPP on your computer. 运行xampp进入到目录下启动服务 12cd /opt/lampp./xampp start 报下面的错误： 1234567891011121314151617181920212223242526272829# ./xampp start/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/bash: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directoryid: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/share/xampp/xampplib: line 11: test: -ne: unary operator expected/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directoryXAMPP: netstat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directorynetstat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/bin/httpd: error while loading shared libraries: librt.so.1: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directoryXAMPP: hostname: error while loading shared libraries: libnsl.so.1: cannot open shared object file: No such file or directorynetstat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directoryXAMPP: netstat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directorycat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory 编辑xampp1vim xammp 将文件中 export LD_ASSUME_KERNEL=2.2.5 内容改成 export LD_ASSUME_KERNEL=2.8.0，再次启动时正常 12345[root@VM-0-3-centos lampp]# ./xampp startStarting XAMPP for Linux 5.6.40-1...XAMPP: Starting Apache...ok.XAMPP: Starting MySQL...ok.XAMPP: Starting ProFTPD...ok. 访问xampp浏览器输入IP后回车，显示页面如下，XAMPP 就安装好了，是不是很方便 安装subversion直接解压，移动到指定的位置即可 123tar zxvf subversion-1.10.8-bin.tar.gzmv subversion /usr/local/cp /usr/local/subversion/*.so /opt/lampp/modules/ 安装svnmanagersvnmanager 依赖 VersionControl_SVN-0.5.1，所以需要先安装这个组件： 12cd /opt/lampp/bin./pear install VersionControl_SVN-0.5.1 输出信息如下： 123456[root@VM-0-3-centos bin]# ./pear install VersionControl_SVN-0.5.1WARNING: channel "pear.php.net" has updated its protocols, use "pear channel-update pear.php.net" to updatedownloading VersionControl_SVN-0.5.1.tgz ...Starting to download VersionControl_SVN-0.5.1.tgz (31,559 bytes).........done: 31,559 bytesinstall ok: channel://pear.php.net/VersionControl_SVN-0.5.1 解压 svnmanager-1.10.tar.gz 移动到指定位置： 12tar zxvf svnmanager-1.10.tar.gzmv svnmanager-1.10 /opt/lampp/htdocs/svnmanager 创建svn仓库目录新建svn必要的目录，并调整目录权限 1234mkdir -p /export/svnrepos/configmkdir -p /export/svnrepos/reposmkdir -p /export/svnrepos/trashchown -R daemon:daemon /export/svnrepos/ 修改配置文件首先修改Apache的主配置文件： 12cp /opt/lampp/etc/httpd.conf /opt/lampp/etc/httpd.conf.bakvim /opt/lampp/etc/httpd.conf 搜索 LoadModule 在后面添加如下内容 12345678910111213141516171819# add for svnLoadModule dav_svn_module modules/mod_dav_svn.soLoadModule authz_svn_module modules/mod_authz_svn.so&lt;Location /svn&gt; DAV svn SVNParentPath /export/svnrepos/repos # Allow the index page to list all the repositories it contains SVNListParentPath On # Require SSL connection for password protection. # SSLRequireSSL AuthType Basic # Message to give to the committer AuthName "AW SVN" # File listing users with write (commit) access AuthzSVNAccessFile /export/svnrepos/config/svn_access_file AuthUserFile /export/svnrepos/config/svn_passwd_file Require valid-user&lt;/Location&gt; 修改svnmanger配置文件 12cp /opt/lampp/htdocs/svnmanager/config.php.linux /opt/lampp/htdocs/svnmanager/config.phpvim /opt/lampp/htdocs/svnmanager/config.php 按照自己的本地目录修改成以下配置： 12345678910111213141516171819//Shell command's$htpassword_cmd = "/opt/lampp/bin/htpasswd";$svn_cmd = "/usr/local/subversion/bin/svn";$svnadmin_cmd = "/usr/local/subversion/bin/svnadmin";//Subversion locations$svn_config_dir = "/export/svnrepos/config/";$svn_repos_loc = "/export/svnrepos/repos/";$svn_passwd_file = "/export/svnrepos/config/svn_passwd_file";$svn_access_file = "/export/svnrepos/config/svn_access_file";//If the following is set, removing a repository will cause it to be//moved to this location rather than being deleted.$svn_trash_loc = "/export/svnrepos/trash/";//Data Source Name (only tested with mysql and sqlite!!)//$dsn = "mysqli://svnmanageruser:xxxpass@localhost/svnmanager"; 基本上看注释和变量名都能明白各个配置的作用，简单解释几个： $htpassword_cmd：用于加密svn用户密码的，svn支持明文密码，使用加密后密码更安全一些 $svn_passwd_file：保存SVN仓库所有用户的密码文件，svnmanager增删改用户时不仅修改自己的数据库，也会同步修改此文件 $svn_access_file：保存SVN仓库所有权限访问规则，svnmanager增删改用户和组的权限时不仅修改自己的数据库，也会同步修改此文件 $dsn：svnmanager自己访问MySQL的用户名、密码、及数据库名字 为svnmanager创建MySQL用户直接在命令行用mysql命令就可以登录MySQL，无需密码，这也是前面提到的XAMPP不安全的地方，好在设置的是只允许本地访问 12cd /opt/lammp/bin./mysql 展示如下： 12345678910[root@VM-0-3-centos bin]# ./mysqlWelcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 13Server version: 10.1.38-MariaDB Source distributionCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; 为svnmanager创建MySQL用户，并授予权限： 1234CREATE DATABASE svnmanager;CREATE USER 'svnmanageruser'@'localhost' IDENTIFIED BY 'xxxpass';GRANT ALL PRIVILEGES ON db . * TO 'svnmanageruser'@'localhost';FLUSH PRIVILEGES; 执行结果如下： 12345678910111213141516171819202122232425262728293031MariaDB [(none)]&gt; CREATE DATABASE svnmanager;Query OK, 1 row affected (0.00 sec)MariaDB [(none)]&gt; CREATE USER 'svnmanageruser'@'localhost' IDENTIFIED BY 'xxxpass';Query OK, 0 rows affected (0.04 sec)MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON svnmanager . * TO 'svnmanageruser'@'localhost';Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || phpmyadmin || svnmanager || test |+--------------------+6 rows in set (0.01 sec)MariaDB [(none)]&gt; use svnmanagerDatabase changedMariaDB [svnmanager]&gt; show tables;Empty set (0.00 sec)MariaDB [svnmanager]&gt; 重启xammp服务1/opt/lampp/xammp restart 展示如下： 12345678[root@VM-0-3-centos ~]# /opt/lampp/xampp restartRestarting XAMPP for Linux 5.6.40-1...XAMPP: Stopping Apache...ok.XAMPP: Stopping MySQL...ok.XAMPP: Stopping ProFTPD...ok.XAMPP: Starting Apache...ok.XAMPP: Starting MySQL...ok.XAMPP: Starting ProFTPD...ok. 访问svnmanager在浏览器输入IP地址/svnmanager/ 访问如下： 登录svnmanagersvnmanager安装后默认使用admin用户登录，密码也是admin，这个可以在 /opt/lampp/htdocs/svnmanager/config.php 文件中配置，当我们添加新用户后，admin这个自动废弃。 添加新用户的步骤：『admin登录』-&gt; 『User Admin』 -&gt; 『Add a new user to the repository system』 -&gt; 『输入新用户信息和自己的密码』 -&gt; 『Confirm』 添加完新用户就可以使用新用户登录了，还可以创建新的仓库并给用户授予权限 创建新仓库的步骤：『Repository Admin』 -&gt; 『Create a new Repository』 -&gt; 『输入新库名和描述』 -&gt; 『Confirm』 给用户授予访问仓库的读写权限步骤：『Repository Admin』 -&gt; 『Change User Privileges of a Repository』 -&gt; 『Repository Name select』 -&gt; 『选择用户、路径，勾选读写权限』 -&gt; 『Confirm』 此外还有一些svnmanager操作可以查询官方文档，一般会把所有用户分成组管理，并把各个库的权限授予不同的组，这个实践一下就清楚了。 可能遇到的错误如果你遇到了以下错误，一定是配置访问MySQL服务的用户或权限不匹配了，检查纠正过来就好 1234567891011121314151617Fatal Error[2] mysqli_real_connect(): (HY000/1044): Access denied for user &apos;svnmanageruser&apos;@&apos;localhost&apos; to database &apos;svnmanager&apos; (@line 86 in file /opt/lampp/htdocs/svnmanager/prado-2.0.3/framework/Data/adodb/drivers/adodb-mysqli.inc.php).Debug Backtrace#1 -- pradoErrorHandler(...)#2 adodb-mysqli.inc.php:86 -- mysqli_real_connect(...)#3 adodb.inc.php:416 -- ADODB_mysqli-&gt;_connect(...)#4 adodb.inc.php:3713 -- ADOConnection-&gt;Connect(...)#5 TAdodb.php:318 -- ADONewConnection(...)#6 TAdodb.php:113 -- TAdodb-&gt;open()#7 DataModule.php:31 -- TAdodb-&gt;__call(...)#8 DataModule.php:31 -- TAdodb-&gt;MetaTables()#9 TApplication.php:584 -- DataModule-&gt;onLoad(...)#10 TApplication.php:629 -- TApplication-&gt;loadModule(...)#11 TApplication.php:482 -- TApplication-&gt;loadPage(...)#12 index.php:5 -- TApplication-&gt;run() 查看服务器目录信息查看一下服务器目录下信息，之前我们建立了三个空文件夹 config、repos、trash，现在已经有很多数据了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@VM-0-3-centos export]# tree svnrepossvnrepos|-- config| |-- auth| | |-- svn.simple| | |-- svn.ssl.client-passphrase| | |-- svn.ssl.server| | `-- svn.username| |-- config| |-- README.txt| |-- servers| |-- svn_access_file| `-- svn_passwd_file|-- repos| `-- firstrepos| |-- conf| | |-- authz| | |-- hooks-env.tmpl| | |-- passwd| | `-- svnserve.conf| |-- db| | |-- current| | |-- format| | |-- fsfs.conf| | |-- fs-type| | |-- min-unpacked-rev| | |-- revprops| | |-- revs| | |-- transactions| | |-- txn-current| | |-- txn-current-lock| | |-- txn-protorevs| | |-- uuid| | `-- write-lock| |-- format| |-- hooks| | |-- post-commit.tmpl| | |-- post-lock.tmpl| | |-- post-revprop-change.tmpl| | |-- post-unlock.tmpl| | |-- pre-commit.tmpl| | |-- pre-lock.tmpl| | |-- pre-revprop-change.tmpl| | |-- pre-unlock.tmpl| | `-- start-commit.tmpl| |-- locks| | |-- db.lock| | `-- db-logs.lock| `-- README.txt`-- trash |-- testrepos1-2022-09-17T10:51:04+02:00 | |-- conf | | |-- authz | | |-- hooks-env.tmpl | | |-- passwd | | `-- svnserve.conf | |-- db | | |-- current | | |-- format | | |-- fsfs.conf | | |-- fs-type ... ... config 目录下增加了管理权限的文件 svn_access_file 和用户密码的文件 svn_passwd_file，repos目录下新增了刚刚新增加的库 firstrepos，trash 目录下是我刚刚删除的无用的仓库。 检出新增加的svn库使用subversion客户端填写 https://IP/svn/firstrepos 地址可以下载我们最新的库 firstrepos，需要输入刚刚授予了权限的用户和密码。 注意事项各个组件的配置文件 The main XAMPP configuration files are located as follows:Apache configuration file: \xampp\apache\conf\httpd.conf, \xampp\apache\conf\extra\httpd-xampp.confPHP configuration file: \xampp\php\php.iniMySQL configuration file: \xampp\mysql\bin\my.iniFileZilla Server configuration file: \xampp\FileZillaFTP\FileZilla Server.xmlApache Tomcat configuration file: \xampp\tomcat\conf\server.xmlApache Tomcat configuration file: \xampp\sendmail\sendmail.iniMercury Mail configuration file: \xampp\MercuryMail\MERCURY.INI 配置svn服务过程可能修改的文件123/opt/lampp/etc/httpd.conf/opt/lampp/htdocs/svnmanager/config.php/opt/lampp/etc/extra/httpd-ssl.conf 这配置文件 /opt/lampp/etc/httpd.conf 中还可以通过 LimitRequestBody、LimitXMLRequestBody 等参数限制访问和提交的数据量 配置服务器时间如果发现svn提交记录时间不匹配，一般是因为svn服务器时间错误导致的，可以使用 date 命令修改，也可以配置一个 ntp 服务器来解决 ntp update -s xxx.xxx.xxx.xx 指定ntp服务的IP或域名，默认使用UDP 123端口ntpq -p 或 ntpstat 可以查看ntp同步状态 备份svn库在迁移之前我们讨论了很多种方案，其中也包括一些备份方式，其实最简单的备份就是拷贝，所以我们可以使用 rsync 命令通过网络备份，把 svn 库的主目录 svnrepos 和 /opt/lampp 同步到备份机器，如果svn服务器出现了问题，可以立即用备份机提供服务。 主动生成密码文件/opt/lampp/bin/htpasswd -c /export/svnrepos/config/svn_passwd_file test 命令可以创建svn_passwd_file文件，并添加第一个用户test，执行命令后会提示输入两次新密码，切记不要在已经存在用户的svn仓库中使用，否则会被覆盖 其他问题如果搭建好的svnmanager可以创建用户和仓库，但是无法授予权限，网页错误中包含 libserf，可以通过 yum install libserf 命令解决。 如果添加好用户授予权限后，无法通过svn客户端更新，可以用检查 /export/svnrepos/config/svn_passwd_file 和 /export/svnrepos/config/svn_access_file 文件归属是否正确为 daemon，如果为文件归属 root，svnmanager 无法修改文件内容，也就会导致用户权限分配不正确 如果想限制访问svn的IP，可以启用 firewalld 服务或者安装使用 iptables 服务 总结 XMAPP 可以方便的提供WEB服务，可以把网站放到 /opt/lampp/htdocs 目录下 可以利用 XAMPP + subversion + svnmanager 的组合提供SVN服务，方便的创建仓库和管理各目录权限 CentOS 安装具体的环境前，可以安装一些通用的软件，yum install -y epel-release， EPEL全称Extra Packages for Enterprise Linux，由 Fedora 社区打造，为 RHEL 及衍生发行版 CentOS 等提供高质量软件包的项目 systemctl 是一个常用的服务命令，比如关闭 firewalld 服务 systemctl stop firewalld，查询状态 systemctl status iptables ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 岁月没有在某些人的脸上留下痕迹，但却对我特别照顾，好像在我这里过的特别匆忙，认真想想自己确实对保留青春阳光付出甚少，甚至连洗脸的时间都一再压缩，也难怪它还给我这些沧桑，今后多花点时间注意一些吧，你认真的注视着它，它便不会轻易溜走~ 2022-9-17 20:47:42]]></content>
      <categories>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>SVN</tag>
        <tag>XAMPP</tag>
        <tag>svnmanager</tag>
        <tag>ntp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于竞赛，CSDN还有很长的路要走]]></title>
    <url>%2Fblog%2F2022%2F09%2F09%2F%E5%85%B3%E4%BA%8E%E7%AB%9E%E8%B5%9B%EF%BC%8CCSDN%E8%BF%98%E6%9C%89%E5%BE%88%E9%95%BF%E7%9A%84%E8%B7%AF%E8%A6%81%E8%B5%B0%2F</url>
    <content type="text"><![CDATA[前言 CSDN 编程竞赛·第五期总结AlbertS 41 52.5 52m36s 竞赛勋章 虽说这篇总结写在第五期之后，但是我却是从第三期就开始参加了的，前两期没有注意到就错过了，好在最近这几期比赛成绩很喜人，第三期6分，第四期67.5，第五期52.5，我承认我有点水，可是我也是能在力扣竞赛里常年水两题的选手啊，这个6分（满分100）真的惊到我了，从最近几期的参赛体验来看，CSDN在竞赛这个方向上还有很长的路要走。 第三期其实我也不算竞赛小白啦，毕竟之前在力扣参加过很多次了，因为平时总在CSDN上写总结，所以看到这里也有竞赛就想来试试，结果第一次有点手足无措，开头居然是4道Java和前端的选择题，我可是C++后端搬运工，嗯，我承认确实不会，根据经验选一选总能对一道题吧，既然选择题是特定语言的，那我直接写大题吧，一段操作猛如虎，一看分数2.5，当然分数不是立马出来的，我感觉答得还可以，结果过了几天出成绩是6分~ 吐槽开始： 要求关闭聊天软件，应该是防止作弊，关就关吧，这倒也无所谓 不能切屏，必须一直保持在答题状态，这有点严格了吧，万一这会我有事要处理呢，其实通常就是处理一些聊天信息 不能复制粘贴，你说不能从其他地方复制答案我可以理解，但是复制自己的代码也不行是要闹哪样 我知道为啥不能粘贴自己刚写的代码了，因为判断不出来是从哪里复制的，可是已经限制切屏了呀 测试用例简陋，有时看不太懂题全靠用例，好嘛，这里看不懂题就算了，用例可有可无 验证流程麻烦，好几个关闭按钮切来切去的 主函数不用给我提供，我又不能改，写在那有啥用 头文件要自己加，这是考算法来还是考背诵呢？（我确实被惯得不想写包含头文件了） 第四期这期已经取消了选择题，直接是4道编程题，还是这样答起来顺利一点，各种语言都行，哪个顺手就用哪个了 吐槽开始： 因为取消了选择题，一时间竟找不到槽点了(#^.^#)，必须编一条 测试用例依旧简单的可怜 第五期我知道为啥第四期表现好了，因为在憋大招对付第五期，因为早饭吃的迟了，晚来了半小时参赛，结果发现进不去，正想报个bug发现帖子里都说进不去，这可有意思了，总共就一两千人参加，应该不是服务器性能不行吧，肯定是页面bug了 虽然大部分人都进不去，但还是有个别人进去比赛了的，已经出了成绩，20分暂居第一名，如果能保持到比赛结束就开心了，官方一直在回帖说解决问题，反正到比赛结束我也没进去。 一直到下午的1点之后，距离开始已经过去了5个小时，我终于进去了，告诉我比赛结束，分数是0，居然不能答题了，赶紧发帖找客服处理，好在工作人员及时赶到，帮我重置了一下，可以正常进入了。 吐槽开始： 千人左右的比赛服务器就炸了 出现问题后修复过程缓慢 发帖提示我违规，但是不说哪里有问题，我也不知道哪句话说错了 通过BUG交流，沟通效率低下 优势与不足前面吐槽了那么多，接下来说说优点吧： 答题参与时间相对自由，仅限制了答题时长2小时候，不严格限制开始时间 分数按照通过的用例比例来给，没有采用非1即0的赋分方式，照顾了很多缺乏竞赛经验的选手，但是这样做未必是正确的 参加基本上都有奖品可以拿，调动了选手们的积极性，礼不在重，有就行 出现问题后尽力解决，虽然解决的慢了点，但是让人看到了负责人的一面 缺点前面已经说了那么多，就不再赘述了，重点说一下答题环境需要改进，给出一些建议吧，其实编程竞赛已经不是新鲜事物了，有那么多好的平台可以学，比如力扣，每次的比赛都很多人参加的，把好的地方学过来就行： 删除无关代码，只保留必要的输入参数就行了，把main函数给我也没用，又不允许本地调试 限制切屏这条尽力优化，限制复制自己代码这条必须去掉，我调整一下逻辑还要重新敲一遍体验未免太差了 比赛之后可以查看别人提交的代码，用于学习提升 赛后有针对这些题的讨论渠道，可以了解到自己有哪些不足，知道什么样的用例通不过 写包含头文件的这件事不太重要，重点放在逻辑实现上，现在谁还不是面向搜索引擎编程了，只要找到方法，头文件算什么难事 总结 继续参加这个比赛，督促自己进步，同时也促进平台进步 如果自己不知道怎么做，就照着表现好的学，前面有那么多优秀的产品可以借鉴 产品上线还是要充分测试，像服务器炸了这种事最好不要出现了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 『雄关漫道真如铁 而今迈步从头越』豪气如虹，真的很有才~ 2022-9-10 11:50:37]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>竞赛</tag>
        <tag>CNDN</tag>
        <tag>活动</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单总结下近期遇到的网络概念gcp、anycast IP、vlan]]></title>
    <url>%2Fblog%2F2022%2F09%2F04%2F%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%E4%B8%8B%E8%BF%91%E6%9C%9F%E9%81%87%E5%88%B0%E7%9A%84%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5gcp%E3%80%81anycast-IP%E3%80%81vlan%2F</url>
    <content type="text"><![CDATA[前言 你知道得越多，就越会意识到自己知道得多么少 知识这个东西总是越学越越多，特别是你去接触一个新领域的时候，会忽然发现之前的自己是多么的狂妄，多么的无知，最近有一些网络相关的需求，这沟通过程中接触到了一些新的概念，比如gcp、anycast IP、vlan等等，初听这些词确实不太理解，所以查了一些资料，对了解到的内容进行一个简单的总结。 gcpGCP 全称 Google Cloud Platform，即谷歌云平台，再这样一个万物皆可云的发展趋势下，云服务发展突飞猛进，作为海外乃至全球的科技巨头，谷歌在云服务领域有着极强的优势，特别是在海外环境下被广泛使用，为计算、存储、网络、大数据、机器学习和物联网（IoT）以及云管理、安全和开发人员工具提供服务，云计算产品包括： Google Compute Engine，这是一种基础架构即服务 (IaaS) 产品，可为用户提供用于工作负载托管的虚拟机实例。 Google App Engine，这是一种平台即服务 (PaaS) 产品，可让软件开发人员访问 Google 的可扩展托管。开发人员还可以使用软件开发工具包 (SDK) 来开发在 App Engine 上运行的软件产品 Google Cloud Storage，这是一个云存储平台，旨在存储大型非结构化数据集。Google 还提供数据库存储选项，包括用于NoSQL 非关系存储的Cloud Datastore 、用于MySQL 完全关系存储的Cloud SQL 和 Google 的原生 Cloud Bigtable 数据库。 Google Container Engine，它是运行在 Google 公共云中的Docker容器的管理和编排系统，是基于 Google Kubernetes 容器编排引擎。 Google Cloud Pub/Sub 是一种托管的实时消息传递服务，允许在应用程序之间交换消息，是一种应用程序开发和集成服务。 Google Cloud Endpoints 允许开发人员创建基于RESTful API 的服务，然后让 Apple iOS、Android 和 JavaScript 客户端可以访问这些服务。 其他产品包括任播 DNS 服务器、直接网络互连、 负载平衡、监控和日志服务。 Anycast任播(Anycast)，又称为选播、泛播或任意播，是IPv6中定义的一种新型通信服务，是IPv6中三大通信方式之一，该定义最早由C Partridge在RFC 1546中首次提出，但是RFC1546对任播的描述仅仅是一个实验性的服务。 Anycast指IPV6协议中一个发送方同最近的一组接收方之间的通信，当一个单播地址被分配到多于一个的接口上时，发到该接口的报文被网络路由到由路由协议度量的“最近”的目标接口上。 与Unicast和Multicast类似，Anycast也是IP网络的一种通信模式。 Multicast指多播，它是指网络中一个节点发出的信息被多个节点收到。与此相对的有单播Unicast和广播Broadcast，前者是指一个节点发出的信息只被一个节点收到，后者是指一个节点发出的信息被子网内所有节点收到 Anycast 允许源结点向一组目标结点中的一个结点发送数据报，而这个结点由路由系统选择，对源结点透明；同时，路由系统就近选择结点为源结点提供服务，从而在一定程度上为源结点提供了更好的服务也减轻了网络负载。 实现上分布的服务共享相同的IP地址，同时在IP层进行透明的服务定位，这使得各种网络服务特别是应用层服务具有更强的透明性，而路由系统选择了“最近”的服务，缩短了服务响应的时间，同时减轻了网络负载，同时在网络上冗余分布相同的服务，路由系统可以提供机制选择负载相对轻的带宽相对高的路径来转发报文，这样就给用户带来了两个方面的好处： 减弱了分布式拒绝服务攻击（DDOS） 减弱了网络拥塞给用户带来的影响 Anycast IPAnycast IP 是集Multicast（多播IP）和Unicast（单播IP）特性于一身的特殊IP地址类型，在Anycast 这种通信模式下，同时存在多个有效的数据包接收端，但是就某一个特定IP数据包而言，仅有一个接收端主机收到了此数据包。 客户分散在多地又需要就近接入的服务，需要IP不同的多个地域部署机器、配置 DNS 实现负载均衡，十分繁琐。使用 Anycast 的 IP 后，无需每个地域都配置 IP，后端维护一套逻辑即可，各地域请求直接用专线加速到后端机器。 Anycast 的 IP 能起到游戏加速器的作用，游戏请求就近进入云服务，走云服务的内网专线到达游戏服务器，极大缩短经过的公网路径，减少了延时、抖动、丢包。此外跟传统加速比，IP 入口无需额外部署流量接收设备，且IP无需区分地域，简化了 DNS 部署。 vlanVLAN 全称 Virtual Local Area Network，即虚拟局域网。 VALN 是一组逻辑上的设备和用户，这些设备和用户并不受物理位置的限制，可以根据功能、部门及应用等因素将它们组织起来，相互之间的通信就好像它们在同一个网段中一样，由此得名虚拟局域网。 VLAN可以为信息业务和子业务、以及信息业务间提供一个相符合业务结构的虚拟网络拓扑架构并实现访问控制功能。与传统的局域网技术相比较，VLAN技术更加灵活，它具有以下优点： 网络设备的移动、添加和修改的管理开销减少；可以控制广播活动；可提高网络的安全性。 当一个交换机上的所有端口中有至少一个端口属于不同网段的时候，当路由器的一个物理端口要连接2个或者以上的网段的时候，就是VLAN发挥作用的时候，这就是VLAN的目的。 VLAN限制网络上的广播，将网络划分为多个VLAN可减少参与广播风暴的设备数量。VLAN分段可以防止广播风暴波及整个网络。VLAN可以提供建立防火墙的机制，防止交换网络的过量广播。 WAN：Wide Area Network，广域网 LAN：Local Area Network，局域网 WLAN：Wireless Local Area Network，无线局域网 总结 GCP 全称 Google Cloud Platform，指的是谷歌云平台 Anycast IP 可以实现多地用户就近接入服务，提升用户体验 VLAN 指虚拟局域网，可以更合理划分网络资源，控制广播活动，提高安全性 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 飞鸟尽，良弓藏；狡兔死，走狗烹。缘何众人都执着登顶？恐被藏与烹而已~ 2022-9-4 01:49:39]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>gcp</tag>
        <tag>anycast</tag>
        <tag>vlan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rm -rf 真是删库跑路的一把好手]]></title>
    <url>%2Fblog%2F2022%2F08%2F26%2Frm-rf-%E7%9C%9F%E6%98%AF%E5%88%A0%E5%BA%93%E8%B7%91%E8%B7%AF%E7%9A%84%E4%B8%80%E6%8A%8A%E5%A5%BD%E6%89%8B%2F</url>
    <content type="text"><![CDATA[前言上回书说到《一个月黑风高的夜晚紧急完成gitlab服务器数据迁移》，因为数据迁移后原数据还是存在的，该分区硬盘快满了，进而影响了原目录下的日志存储，既然数据已经迁移到新的路径了，那原来的库直接删掉就好了，往往就是这么不经意间做了一个令人十分后怕的决定。 删库说干就干，连上服务器就开始操作了，为了避免搞错了，我还打开了另一个ssh窗口，对照着正在使用的git库，来一步步查找原来路径下已经废弃的仓库，嗯，终于找到了，对比各种信息没啥问题，两个窗口相互对照，十分“保险”。 rm -rf xxx 走你，一切都安静了，好了退出当前路径检查一下空间大小，咦？路径怎么不对，好像删的是正在使用的那个库哎！服了，还真是受到了惊吓啊！背后发凉啊！gitlab网站访问一下，嗯，果然找不到了，拜拜！ 跑路既然库都删完了，要不跑路吧？ 算了，能跑到哪呢？先回去看看能不能找回来吧~ 恢复rm -rf 恢复硬盘数据是别想了，一般会让你卸载硬盘，断网，防止擦除，用第三方工具等，这我之前都演练过，几乎没什么用，这个时候需要冷静，先理智的分析一下： 既然是git库，我本地也是有的，要不我把我的库推上去试试？虽然没有那么新，但也差不了几个提交了，不过远程库都被我删了，我如果推上去一个新库，别人是不是直接访问不了，或者引发冲突呢？ 想起之前迁移的时候我还备份了数据目录呢，那这样，先把备份的数据恢复到误删除的目录下，然后我再找一个本地的拉取到了最新状态git库推上去，既然想清楚了，那就动手吧。 通知相关人员先不要拉取和推送数据 把一月前备份的git-data目录中对应数据通过 rsync 命令拷贝到误删除目录，这时通过gitlab网站已经能看到数据了，只是数据是一个月前的 跳到版本发布机，上面的Git库数据是最新的，按照分支把版本发布机上的git数据逐个推送到gitlab服务器 再次打开gitlab网站发现一切恢复如初，真是…… 感想rm -rf 命令真是删库跑路的一把好手，一点也不拖泥带水，更无回收站这个后悔药可以吃，所以在服务器上对文件使用了这个命令，基本上等于判了死刑，但是git库真是一个好东西，分布式的存储可以保证每个人那都有完整的仓库，只要能找到一个最新的就行。 为了保证我能有最新的库可以用，我赶紧在 jenkins 上新建了两个定时任务，每天定时把仓库拉取到最新，防止类似意外的发生。 后续其实这个后续和删库这件事没有任何关系，如果非得说有什么关系，就是它们都属于“灾难”，删库刚刚处理完，紧接着游戏玩家出现登录不上的问题，一开始以为是网络波动，因为我登录过程也不太顺畅，直到玩家发来了录屏，我才发现这个问题又有的查了。 玩家所说的无法登录并不是真的登不进去，而是登录之后加载完读条刚要进场景，直接退到登录界面，查询网络消息发现每次登录后几秒钟，网络连接自动断开，但是断开前的通讯流程日志显示的延迟信息，又说明网络状况良好，一头雾水。 最后耗时两天，在收集了各种线索以后，发现是升级Unity版本后，在法语、俄语、乌克兰语作为系统语言时，对c#的字符串处理逻辑要求更加严格，如果不做处理沿用之前的写法，很容易出现崩溃错误，因为有try-catch处理，表现出来就是直接断网会登录界面，统一设置语言处理函数时修复了此问题。 身心俱疲~ 总结 使用 rm -rf 命令还是要谨慎，谨慎，再谨慎 如果真的删库了，也不一定非得跑路，先冷静想想有没有补救的措施 语言、字符集、编码真的是相互纠结，至此我的bug库里又收录了系统语言运行时，神奇 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 北风卷地白草折，胡天八月即飞雪~ 2022-8-26 23:41:18]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>Git</tag>
        <tag>rm</tag>
        <tag>删除</tag>
        <tag>恢复</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git查看历史记录及修改内容]]></title>
    <url>%2Fblog%2F2022%2F08%2F21%2Fgit%E6%9F%A5%E7%9C%8B%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95%E5%8F%8A%E4%BF%AE%E6%94%B9%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[前言在 Git 中没有真正的方法来做任何事情，这就是它的妙处！比如查看修改内容这件事，有些人会想到 git log，有些人会想到 git show，最近我又学到一个 git whatchanged，实现目的方法多种多样，各种途径任君挑选。 刚开始步入软件开发行业时喜欢捣鼓各种软件，进行各种个性化配置，任意修改快捷键，这样在开发过程中确实会舒服一些，但是换了一个环境时（电脑重装或在别人的机器），就好像一个什么也不会的傻子一样，所以慢慢的我开始强迫自己熟悉软件自己的快捷键和各种命令，这样在重装系统时会免去很多麻烦，并且因为一些命令用习惯了，在编写部署脚本时也不会总是写出不能识别的简写命令了。 修改文件为了测试各种的查找修改记录的命令，我先进行一次包含增加、修改、删除的提交，然后对比来看各个命令的作用，实际修改如下： 在文件address.txt中增加两行数据 清空文件age.txt中4行数据 修改文件name.txt中一行数据 增加带有3行数据的phone.txt文件 删除带有2行数据的story.txt文件 在执行了 git add . 命令后，可以用 git diff --staged 查看即将提交的文件修改，展示如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849$ git diff --stageddiff --git a/address.txt b/address.txtindex e69de29..8f9d6e6 100644--- a/address.txt+++ b/address.txt@@ -0,0 +1,2 @@+beijing+shanghai\ No newline at end of filediff --git a/age.txt b/age.txtindex 58f78c9..e69de29 100644--- a/age.txt+++ b/age.txt@@ -1,4 +0,0 @@-12-16-17-15\ No newline at end of filediff --git a/name.txt b/name.txtindex ac37a53..0ed306a 100644--- a/name.txt+++ b/name.txt@@ -1,4 +1,4 @@ tom-alice+jerry bily andy\ No newline at end of filediff --git a/phone.txt b/phone.txtnew file mode 100644index 0000000..241bcdb--- /dev/null+++ b/phone.txt@@ -0,0 +1,3 @@+110+120+119\ No newline at end of filediff --git a/story.txt b/story.txtdeleted file mode 100644index 0d89902..0000000--- a/story.txt+++ /dev/null@@ -1,2 +0,0 @@-King-Wolf\ No newline at end of file 关于 git diff 对于很多使用 git 管理代码的小伙伴再熟悉不过了，但是其中有些细节还是需要学习的： diff --git a/name.txt b/name.txt 这一行是说以下展示 name.txt 文件修改前后的信息，a/name.txt 和 b/name.txt 分别表示修改前后的文件名 index ac37a53..0ed306a 100644 这一行表示文件修改前后的 object，100644 表示这是一个常规文件，文件权限为 644，使用 git cat-file -p可以查看文件内容： 1234567891011$ git cat-file -p ac37a53tomalicebilyandy$ git cat-file -p 0ed306atomjerrybilyandy a/name.txt``` 表示修改前的文件，```+++ b/name.txt``` 表示修改后的文件12345678910114. `@@ -1,4 +1,4 @@` 这一句应该是最有意思的，也是不用一看懂的一行，其中开头和结尾的 `@@` 为固定格式，`-1,4` 表示修改前的 `1~4`行，`+1~4`表示修改后的`1~4`行，这句话的意思就是，以下内容展示了修改前 `1~4`行到修改后 `1~4`行的文件变化，因为我们只修改了一行，所以修改前后行数不变，如果新增行数和删除行数不同，那么这个位置展示的行数也是不同的，例如 `phone.txt` 文件的变化 `@@ -0,0 +1,3 @@`5. 最后就是文件具体的变化了，新增内容前面是加号 `+`，删除内容前面是减号 `-`： ```bash tom -alice +jerry bily andy 查询文件修改当我们把修改的内容提交以后，这条修改属于存入了仓库的历史之中，git diff 就无法再查看文件的变化了，而需要使用 git log 或 git show 来查看文件修改的内容，接下来我们来看看这些命令都能用来干嘛。 git showgit show 默认展示最近一次提交的修改，与执行 git commit 命令之前的 git diff --staged 查看得到的绝大部分内容相同，只是在开头位置包含最新提交的信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455$ git showcommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example datadiff --git a/address.txt b/address.txtindex e69de29..8f9d6e6 100644--- a/address.txt+++ b/address.txt@@ -0,0 +1,2 @@+beijing+shanghai\ No newline at end of filediff --git a/age.txt b/age.txtindex 58f78c9..e69de29 100644--- a/age.txt+++ b/age.txt@@ -1,4 +0,0 @@-12-16-17-15\ No newline at end of filediff --git a/name.txt b/name.txtindex ac37a53..0ed306a 100644--- a/name.txt+++ b/name.txt@@ -1,4 +1,4 @@ tom-alice+jerry bily andy\ No newline at end of filediff --git a/phone.txt b/phone.txtnew file mode 100644index 0000000..241bcdb--- /dev/null+++ b/phone.txt@@ -0,0 +1,3 @@+110+120+119\ No newline at end of filediff --git a/story.txt b/story.txtdeleted file mode 100644index 0d89902..0000000--- a/story.txt+++ /dev/null@@ -1,2 +0,0 @@-King-Wolf\ No newline at end of file git show 还可以加数字，比如 git show -3 就是展示最近3次提交修改信息。 git show --stat 可以查看最新提交的修改文件，如果想查看指定提交的修改文件信息，可以在后面跟上commit-id，例如 git show --stat 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee 12345678910111213$ git show --statcommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data address.txt | 2 ++ age.txt | 4 ---- name.txt | 2 +- phone.txt | 3 +++ story.txt | 2 -- 5 files changed, 6 insertions(+), 7 deletions(-) 这个展示信息可以清楚的看出哪些文件新增了内容，哪些文件删除了内容，并且展示了文件增伤行数的比例，但是有一点它不够清晰，那就是无法看出哪些是新增的文件，哪些是删除的文件，比如 age.txt 和 story.txt 都显示删除了数据，但实际上 story.txt 整个文件都从仓库中删除了，要想看出文件增删状态可以使用接下来展示的这个命令 git whatchanged git whatchangedgit whatchanged 可以展示出文件的增删状态和权限修改，默认分页展示所有提交记录，可以后面加数字来展示最近几次的文件增删状态： 123456789101112$ git whatchanged -1commit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data:100644 100644 e69de29 8f9d6e6 M address.txt:100644 100644 58f78c9 e69de29 M age.txt:100644 100644 ac37a53 0ed306a M name.txt:000000 100644 0000000 241bcdb A phone.txt:100644 000000 0d89902 0000000 D story.txt 从这个文件中就可以看出 address.txt、age.txt、name.txt 三个文件被修改了，phone.txt 是新增加的，story.txt 文件被删除了，如果在命令后面加上选项 --stat 作用就和 git show 一样了，兜兜转转回到原点~ 12345678910111213$ git whatchanged -1 --statcommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data address.txt | 2 ++ age.txt | 4 ---- name.txt | 2 +- phone.txt | 3 +++ story.txt | 2 -- 5 files changed, 6 insertions(+), 7 deletions(-) git log根据我个人的理解，git show 注重查看一次提交中修改的内容，而 git log 主要用于查找历史提交的脉络，但这不是绝对的，因为git做一件事，没有绝对的一种方式，你也可以用 git log 来实现 git show。 展示最近一次提交 123456$ git log -1commit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data 展示其他分支最近一次提交 123456$ git log -1 devcommit 62cc52cbc7f9581fa825b443aba3481083459656 (dev)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 12:02:11 2022 +0800 init git repository 展示修改的文件列表及文件修改的统计 12345678910111213$ git log -1 --statcommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data address.txt | 2 ++ age.txt | 4 ---- name.txt | 2 +- phone.txt | 3 +++ story.txt | 2 -- 5 files changed, 6 insertions(+), 7 deletions(-) 展示每次修改的文件列表 123456789101112$ git log -1 --name-onlycommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example dataaddress.txtage.txtname.txtphone.txtstory.txt 展示修改的文件列表和显示状态 123456789101112$ git log -1 --name-statuscommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example dataM address.txtM age.txtM name.txtA phone.txtD story.txt 展示指定作者提交的记录 123456$ git log -1 --author="albert"commit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data 单行展示提交的记录 123$ git log --oneline2447e2b (HEAD -&gt; master) update example data62cc52c (dev) init git repository 展示指定日期之前的提交记录 123456789101112$ git log --before='2022-08-22'commit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example datacommit 62cc52cbc7f9581fa825b443aba3481083459656 (dev)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 12:02:11 2022 +0800 init git repository 展示一天之内的提交记录 123456789101112$ git log --since=1.day.agocommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example datacommit 62cc52cbc7f9581fa825b443aba3481083459656 (dev)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 12:02:11 2022 +0800 init git repository 展示指定包含指定内容的提交记录 123456$ git log --grep=updatecommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data 展示指定不包含指定内容的提交记录 123456$ git log --grep=update --invert-grepcommit 62cc52cbc7f9581fa825b443aba3481083459656 (dev)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 12:02:11 2022 +0800 init git repository 终极大招，图形化展示，其实用的并不多 12345678$ git log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --date=relative --all* 2447e2b - (HEAD -&gt; master) update example data (77 minutes ago) &lt;albert&gt;* 62cc52c - (dev) init git repository (2 hours ago) &lt;albert&gt;* 719ec7a - (refs/stash) WIP on master: 83f00c5 init git repository (2 hours ago) &lt;albert&gt;|\| * 9c87e06 - index on master: 83f00c5 init git repository (2 hours ago) &lt;albert&gt;|/* 83f00c5 - init git repository (5 hours ago) &lt;albert&gt; 总结 查看最近一次修改的内容使用 git show 查看最近一次修改的文件使用 git show --stat 或 git log -1 --name-only 查看最近一次修改的文件状态使用 git whatchanged -1 或 git log -1 --name-status 图形化显示git提交记录使用 git log --graph ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 轻生的人到底是勇敢还是懦弱呢？虽说我未经历过他的人生不该轻易评判，但就我主观来思考这类事，大抵是因为对生活失去了信心。人固有一死，或重于泰山，或轻于鸿毛。而遇到困难就选择轻生的人是自私且不负责任的，若你无牵无挂走了也就走了，倘若还有家人岂不是还要拿出额外一笔钱给你办个葬礼？连死都不怕了，还有什么是过不去的呢？ 2022-8-21 21:19:13 &lt;!–我曾走过山，走过水，其实只是借助它们走过我的生命；我看着天，看着地，其实只是借助它们确定我的位置；我爱着他，爱着你，其实只是借助别人实现了我的爱欲。 ——史铁生《务虚笔记》– &gt;]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>log</tag>
        <tag>版本控制</tag>
        <tag>Git</tag>
        <tag>历史记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参考开源项目实现一个简易的C++枚举转字符串的函数]]></title>
    <url>%2Fblog%2F2022%2F08%2F07%2F%E5%8F%82%E8%80%83%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%9A%84C-%E6%9E%9A%E4%B8%BE%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言前段时间接触了 magic_enum 这个开源库，代码量不算太多，是一个但头文件的枚举操作库，关于如何使用还写了一篇总结 《推荐一个C++枚举转字符串的开源项目magic_enum》，当时觉得这个库很棒，但是对于我当前枚举转化字符串的需求还说还是太臃肿了，所以决定改造一下，这不今天过来填坑了。 改造一开始还没太理解开源库的原理，认为原来的实现限制太大，为了实现后面字符串转枚举，获取所有枚举名等需求，不得不限定一个枚举的范围，这个范围在 magic_enum 这个开源库中是 [-128, 128]，所以当我开始改造时打算把这个范围去掉，但是当我真正弄懂它的原理后，才发现这个范围是必须指定的，不然无法在编译期预处理，从而达到枚举值转换成字符串的目的。 认识到这一点以后，我也不再纠结范围的限制，设定了一个 [0, 31] 的常用枚举范围，相比于原来 [-128, 128] 的范围缩小了不少，这样能加快编译的速度，参考这个开源库和一些网络上关于这个库的讲解，我也实现了一个功能单一的简洁的枚举转字符串的函数 Enum2String，大约70行代码，使用起来还是比较方便的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;array&gt;#include &lt;string&gt;#include &lt;utility&gt;#include &lt;string_view&gt;template &lt;typename E, E V&gt;constexpr auto PrettyName()&#123; std::string_view name&#123;__PRETTY_FUNCTION__, sizeof(__PRETTY_FUNCTION__) - 2&#125;; name.remove_prefix(name.find_last_of(" ") + 1); if (name.front() == '(') name.remove_prefix(name.size()); return name;&#125;template &lt;typename E, E V&gt;constexpr bool IsValidEnum()&#123; return !PrettyName&lt;E, V&gt;().empty();&#125;template &lt;int... Seq&gt;constexpr auto MakeIntegerSequence(std::integer_sequence&lt;int, Seq...&gt;)&#123; return std::integer_sequence&lt;int, (Seq)...&gt;();&#125;constexpr auto NormalIntegerSequence = MakeIntegerSequence(std::make_integer_sequence&lt;int, 32&gt;());template &lt;typename E, int... Seq&gt;constexpr size_t GetEnumSize(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::array&lt;bool, sizeof...(Seq)&gt; valid&#123;IsValidEnum&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; constexpr std::size_t count = [](decltype((valid)) v) constexpr noexcept-&gt;std::size_t &#123; auto cnt = std::size_t&#123;0&#125;; for (auto b : v) if (b) ++cnt; return cnt; &#125;(valid); return count;&#125;template &lt;typename E, int... Seq&gt;constexpr auto GetAllValidValues(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::size_t count = sizeof...(Seq); constexpr std::array&lt;bool, count&gt; valid&#123;IsValidEnum&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; constexpr std::array&lt;int, count&gt; seq&#123;Seq...&#125;; std::array&lt;int, GetEnumSize&lt;E&gt;(NormalIntegerSequence)&gt; values&#123;&#125;; for (std::size_t i = 0, v = 0; i &lt; count; ++i) if (valid[i]) values[v++] = seq[i]; return values;&#125;template &lt;typename E, int... Seq&gt;constexpr auto GetAllValidNames(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::array&lt;std::string_view, sizeof...(Seq)&gt; names&#123;PrettyName&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; std::array&lt;std::string_view, GetEnumSize&lt;E&gt;(NormalIntegerSequence)&gt; validNames&#123;&#125;; for (std::size_t i = 0, v = 0; i &lt; names.size(); ++i) if (!names[i].empty()) validNames[v++] = names[i]; return validNames;&#125;template &lt;typename E&gt;constexpr std::string_view Enum2String(E V)&#123; constexpr auto names = GetAllValidNames&lt;E&gt;(NormalIntegerSequence); constexpr auto values = GetAllValidValues&lt;E&gt;(NormalIntegerSequence); constexpr auto size = GetEnumSize&lt;E&gt;(NormalIntegerSequence); for (size_t i = 0; i &lt; size; ++i) if (static_cast&lt;int&gt;(V) == values[i]) return names[i]; return std::to_string(static_cast&lt;int&gt;(V));&#125; 函数使用这个Enum2String函数使用也非常方便，直接把枚举变量作为参数传进去就可以了: 1234567891011121314151617#include "myenum.h"#include &lt;iostream&gt;enum class Color&#123; RED, GREEN, BLUE,&#125;;int main()&#123; Color c = Color::BLUE; std::cout &lt;&lt; Enum2String(c) &lt;&lt; std::endl; return 0;&#125; 编译运行后的结果为： 12$ g++ enumtest.cpp -std=c++17 &amp;&amp; ./a.outColor::BLUE 各函数的作用前面提到过，我这个库还是参考 magic_enum 这个开源库的源码及网上对它的讲解来实现的，只不过精简了大部分我用不到的内容，仅实现了我想要的枚举转字符串的功能，并且大部分都在编译器求值，仅 Enum2String 函数中遍历的部分只能在运行时才能计算求得，所以效率还算不错，各个模板函数作用明确，下面简单描述下： 12345678template &lt;typename E, E V&gt;constexpr auto PrettyName()&#123; std::string_view name&#123;__PRETTY_FUNCTION__, sizeof(__PRETTY_FUNCTION__) - 2&#125;; name.remove_prefix(name.find_last_of(" ") + 1); if (name.front() == '(') name.remove_prefix(name.size()); return name;&#125; PrettyName() 函数是利用 __PRETTY_FUNCTION__ 这个宏来截取最终我们想要的字符串，如果不做处理，__PRETTY_FUNCTION__ 的值会是这样： constexpr auto PrettyName() [with E = Color; E V = Color::BLUE] 靠近结尾的 Color::BLUE 正是我们想要得到的字符串，所以我们可以按照自己的需要把它截取出来。 12345template &lt;typename E, E V&gt;constexpr bool IsValidEnum()&#123; return !PrettyName&lt;E, V&gt;().empty();&#125; IsValidEnum() 函数是用于判断一个枚举名字是否有效，如果截取的最终名字为空，则认为此枚举无效。 1234567template &lt;int... Seq&gt;constexpr auto MakeIntegerSequence(std::integer_sequence&lt;int, Seq...&gt;)&#123; return std::integer_sequence&lt;int, (Seq)...&gt;();&#125;constexpr auto NormalIntegerSequence = MakeIntegerSequence(std::make_integer_sequence&lt;int, 32&gt;()); MakeIntegerSequence() 用于生成一个范围是 [0, 32) 的整数数列。 123456789101112template &lt;typename E, int... Seq&gt;constexpr size_t GetEnumSize(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::array&lt;bool, sizeof...(Seq)&gt; valid&#123;IsValidEnum&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; constexpr std::size_t count = [](decltype((valid)) v) constexpr noexcept-&gt;std::size_t &#123; auto cnt = std::size_t&#123;0&#125;; for (auto b : v) if (b) ++cnt; return cnt; &#125;(valid); return count;&#125; GetEnumSize() 用于遍历数列范围内的各个整数，找出有效的枚举有多少个。 1234567891011template &lt;typename E, int... Seq&gt;constexpr auto GetAllValidValues(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::size_t count = sizeof...(Seq); constexpr std::array&lt;bool, count&gt; valid&#123;IsValidEnum&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; constexpr std::array&lt;int, count&gt; seq&#123;Seq...&#125;; std::array&lt;int, GetEnumSize&lt;E&gt;(NormalIntegerSequence)&gt; values&#123;&#125;; for (std::size_t i = 0, v = 0; i &lt; count; ++i) if (valid[i]) values[v++] = seq[i]; return values;&#125; GetAllValidValues() 用于遍历数列范围内各个整数，找出全部有效枚举值，返回包含有效值的数组。 123456789template &lt;typename E, int... Seq&gt;constexpr auto GetAllValidNames(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::array&lt;std::string_view, sizeof...(Seq)&gt; names&#123;PrettyName&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; std::array&lt;std::string_view, GetEnumSize&lt;E&gt;(NormalIntegerSequence)&gt; validNames&#123;&#125;; for (std::size_t i = 0, v = 0; i &lt; names.size(); ++i) if (!names[i].empty()) validNames[v++] = names[i]; return validNames;&#125; GetAllValidNames() 用于遍历数列范围内各个整数，找出全部有效枚举值的名字，返回包含这些名字的数组。 12345678910template &lt;typename E&gt;constexpr std::string_view Enum2String(E V)&#123; constexpr auto names = GetAllValidNames&lt;E&gt;(NormalIntegerSequence); constexpr auto values = GetAllValidValues&lt;E&gt;(NormalIntegerSequence); constexpr auto size = GetEnumSize&lt;E&gt;(NormalIntegerSequence); for (size_t i = 0; i &lt; size; ++i) if (static_cast&lt;int&gt;(V) == values[i]) return names[i]; return std::to_string(static_cast&lt;int&gt;(V));&#125; Enum2String() 用于从编译期生成的数组中遍历寻找枚举值等于参数的枚举值名字，如果枚举值无效或者超出范围就范围对应的整数字符串。 总结 magic_enum 是个很不错的库，但他相对于我的需求来说显得太大了 根据自己的需求改造开源库，一方面巩固了知识，另一方面也更适合自己的要求 constexpr 这个东东可以在编译期求值，后面可以多花点时间研究一下 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 原始财富的积累的真的是太难了，那些趁着各种东风各种红利的人们是幸运的，运气也是人生的一部分，而大部分没有这运气的人们想要积累财富，必然要付出十倍甚至上百倍的努力，这些不可选择也无需抱怨，只要踏踏实实往前走就好了~ 2022-8-7 16:19:20]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>字符串</tag>
        <tag>enum</tag>
        <tag>开源项目</tag>
        <tag>枚举</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[std::uniform_real_distribution的一个bug引发的服务器崩溃]]></title>
    <url>%2Fblog%2F2022%2F08%2F06%2Fstd-uniform-real-distribution%E7%9A%84%E4%B8%80%E4%B8%AAbug%E5%BC%95%E5%8F%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B4%A9%E6%BA%83%2F</url>
    <content type="text"><![CDATA[前言近日发生一次线上游戏服务器宕机问题，通过日志和core文件信息定位到崩溃的函数，但是崩溃的位置却是一段很长时间都没有改动过的代码，起初怀疑是配置数据的问题，但仔细查看之后均正常，然后又怀疑是玩家旧数据异常导致，但是分析代码逻辑后也没发现问题，最后是一个同事发现生成随机数的代码有bug，导致数组越界了，还真是个意想不到的原因。 崩溃问题崩溃出现在从数组中随机一个数的逻辑中，其中用到了 std::uniform_real_distribution 这个模板类，示例代码如下： 123456789vector&lt;int&gt; v&#123;1, 3, 5, 6&#125;;std::random_device rd;std::mt19937 gen(rd());std::uniform_real_distribution&lt;float&gt; dis(0, 1.0f);int n = static_cast&lt;int&gt;(dis(gen) * v.size());return v[n]; 之前也了解过 std::uniform_real_distribution&lt;float&gt; dis(0, 1.0f); 这个用法，他应该返回的范围是 [0, 1.0) 内左闭右开的浮点数，所以最终计算出的 n 的值应该为 [0, n-1] 范围内的整数，所以这段代码不应该有问题，但是问题却恰恰出现在 std::uniform_real_distribution 的身上。 std::uniform_real_distribution&lt;&gt; 的bugstd::uniform_real_distribution 这个模板类定义在头文件 &lt;random&gt; 中，是C++11新加的内容，定义如下： 12template&lt; class RealType = double &gt;class uniform_real_distribution; 可产生均匀分布在区间 [a, b) 上的随机浮点值 x。 但是这个函数有个bug，它有时候会返回边界值b，也就是说实际范围变成了 [a, b]。 可以通过 cppreference.com - uniform_real_distribution查到，具体描如下： It is difficult to create a distribution over the closed interval [a,b] from this distribution. Using std::nextafter(b, std::numeric_limits::max()) as the second parameter does not always work due to rounding error. Most existing implementations have a bug where they may occasionally return b (GCC #63176 LLVM #18767 MSVC STL #1074). This was originally only thought to happen when RealType is float and when LWG issue 2524 is present, but it has since been shown that neither is required to trigger the bug. 关于这个bug还可以看一下这个帖子的讨论： Is 1.0 a valid output from std::generate_canonical? 看得时候注意一下这段描述 This problem can also occur with std::uniform_real_distribution; the solution is the same, to specialize the distribution on double and round the result towards negative infinity in float. bug 重现方法这个bug有多种变体，其中一个就是说它和 generate_canonical 产生随机数有关 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;limits&gt;#include &lt;random&gt;int main()&#123; std::mt19937 rng; std::seed_seq sequence&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; rng.seed(sequence); rng.discard(12 * 629143 + 6); float random = std::generate_canonical&lt;float, std::numeric_limits&lt;float&gt;::digits&gt;(rng); if (random == 1.0f) &#123; std::cout &lt;&lt; "Bug!\n"; &#125; return 0;&#125; 此段代码在编译器 g++ 5.4.0 上编译执行时能重现，但是在 g++ 10.0.3 上已经被修复无法重现了，再看下面这段代码： 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;random&gt;int main()&#123; std::random_device rd; std::mt19937 gen(rd()); std::uniform_real_distribution&lt;&gt; dis(0, 1.0f); while (true) &#123; float f = dis(gen); if (f &gt;= 1.0) &#123; std::cout &lt;&lt; "BUG\n"; break; &#125; &#125; return 0;&#125; 这段代码无论是 g++ 5.4.0 版本还是 g++ 10.0.3 都能重现打印出 BUG，这个问题在于模板默认是 double 类型，最后转化成 float 来使用，我按照建议之前帖子中的建议，都改成 double 来使用，之后一直运行了10来天再没出现过随机到边界值的问题。 总结 标准库中的内容很权威，但是不保证一定是正确的，可以持有怀疑态度 std::uniform_real_distribution的历史版本是有bug，几乎各个编译器都出现过随机到边界值的情况 这个bug其实在文档中已经指出了，所以大家看文档时还是要仔细一点，往往使用不规范也容易造成bug ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 适当的放松是生活的调味剂，有时候真的需要肆意挥霍一下，一张一弛，生活之道~ 2022-8-7 01:30:30]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>随机数</tag>
        <tag>uniform_real_distribution</tag>
        <tag>bug</tag>
        <tag>random</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN如何删除文件名包含空格的文件]]></title>
    <url>%2Fblog%2F2022%2F07%2F31%2FSVN%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E5%90%8D%E5%8C%85%E5%90%AB%E7%A9%BA%E6%A0%BC%E7%9A%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言SVN 和 Git 两个常用的版本管理工具都有 add 和 commit 命令，但是这两个命令的含义是不同的，git add 命令添加的是变化的文件，不管是增加的文件、删除的文件还是更新的文件，使用 git add 命令之后都可以放到暂存区，而 svn add 命令只能影响新增和变化的文件，对原在库中现已删除的文件没有影响，想要真正删除文件需要在执行 svn commit 命令之前先执行 svn delete 命令。 SVN删除文件SVN 删除文件需要指定文件名字，而使用在一些自动化脚本中时，被删除的文件都是其他脚本自动删除的，要想获得这些文件的名字，可以利用 svn st 命令来查询，然后搭配管道再使用 svn del 命令来删除这些文件。 12345678$ svn st! test/A.txt! test/B.txt! test/C Blank.txt! test/DD.txtM test/E.txt? test/F.txtA test/G.txt svn st 命令的结果对每个变化文件显示一行，第一列是文件状态，第二列及后面的部分是文件名 !：表示已经删除的文件 M：表示文件内容发生变化的文件 ?：表示一个新文件，不在原版本库中 A：表示本次新增的文件 要想删除文件只需要把 ! 开头的文件删除就可以了，常规命令如下： 1svn st | grep ^! | awk '&#123;print " --force "$2&#125;' | xargs -r svn del 这个命令就是要生成 svn del --force test/A.txt 命令然后执行，这样就能达到删除文件的目的了 基于这个目的，一般自动化部署的机器上的svn提交命令如下： 123svn st | grep ^! | awk '&#123;print " --force "$2&#125;' | xargs -r svn delsvn add . --no-ignore --forcesvn commit -m "build message" 被删除的文件名带空格上述命令能处理绝大部分的情况，但是如果被删除的文件中包含空格，那么执行命令时就会报错: 12$ svn st | grep ^! | awk '&#123;print " --force "$2&#125;' | xargs -r svn delsvn: E125001: 'test/C' does not exist 为什么会报错的呢？就是说因为文件 test/C Blank.txt 的名字中包含了空格，使用 awk &#39;{print &quot; --force &quot;$2}&#39; 把空格后半部分的文件名丢掉了，所以只要处理一下这种情况就可以了，处理命令如下： 1svn st | grep ^! | awk '&#123;$1="";print $0 &#125;' | awk '$1=$1' | awk '&#123;print " --force ""\""$0"\""&#125;' | xargs -r svn del svn st 显示个文件状态 grep ^! 过滤出原在版本库中但现已删除的文件 awk &#39;{$1=&quot;&quot;;print $0 }&#39; 去掉每行信息中第一列的状态值 awk &#39;$1=$1&#39; 去掉数据开头和结尾的空格 awk &#39;{print &quot; --force &quot;&quot;\&quot;&quot;$0&quot;\&quot;&quot;}&#39; 拼接文件名，并在前后添加引号，--force &quot;test/C Blank.txt&quot; xargs -r svn del 利用管道传递文件名将文件删除 总结 svn st 命令可以查询 SVN 当前目录下各个文件的状态 svn del 命令用于删除原版本库中的文件 如果待删除的文件名包含空格可使用 svn st | grep ^! | awk &#39;{$1=&quot;&quot;;print $0}&#39; | awk &#39;$1=$1&#39; | awk &#39;{print &quot; --force &quot;&quot;\&quot;&quot;$0&quot;\&quot;&quot;}&#39; | xargs -r svn del ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 互发声明并非只是打打嘴架，应该是在相互试探着对方的底线，各自盘算着多方的利益，双方都是极其聪明的人，背后也都有强大的团队提供支持，博弈啊博弈，每一方都会选择对自己最有利的行动~ 2022-7-31 20:13:21]]></content>
      <categories>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>del</tag>
        <tag>SVN</tag>
        <tag>add</tag>
        <tag>commit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个月黑风高的夜晚紧急完成gitlab服务器数据迁移]]></title>
    <url>%2Fblog%2F2022%2F07%2F30%2F%E4%B8%80%E4%B8%AA%E6%9C%88%E9%BB%91%E9%A3%8E%E9%AB%98%E7%9A%84%E5%A4%9C%E6%99%9A%E7%B4%A7%E6%80%A5%E5%AE%8C%E6%88%90gitlab%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[前言真是无巧不成书，白天刚刚讨论了一天SVN服务器迁移事情，晚上突然被告知 gitlab 服务无法访问了，赶紧连接到服务器上查看，好家伙，磁盘还剩下可怜的 98K，SVN 不管了，先把 gitlab 搞定吧，毕竟第二天的游戏更新包都打不出来了，确认是磁盘不足问题后已经是9点半了，月亮都升起老高了，赶紧开始修复吧。 修复过程关于修复过程，网上有很多文章写了具体的步骤，都非常的相似，基本来自于官方文档的翻译，不过抄来抄去的内容产生了一些错误，甚至是在关键的步骤上出错，所以在看这些文章是要特别注意，如果你的英文还不错的话，建议看一下官方文档 为了记录过程我也不免俗套的把这些步骤写一写，方便日后拿来即用，关于每步的作用我会进行简述，并且提一下注意事项。 总体步骤：在配置文件中把存放数据的路径改一个新目录，把原数据移动到新的目录，重启gitlab服务即可 备份数据虽然只是做数据迁移，不涉及切换物理机，也不会更换服务版本，但是为了保险起见，还是先把数据备份一份，现在遇到的问题是 gitlab 是默认安装的，数据存放在 /var/opt/gitlab/git-data 目录下，仓库存放在子目录 repositories 里面，这个目录一般空间都不大，很容易出现磁盘满的情况，目录结构如下： 123456789101112131415git-data└── repositories ├── +gitaly │ ├── cache │ ├── state │ └── tmp └── @hashed ├── 19 ├── 35 ├── 3f ├── 45 ├── 4a ├── 4b ├── ... └── f5 真正的仓库数据就在 @hashed 目录下众多子目录下，再往下走就是一个个 .git目录了，结构与我们本地的 .git 目录一致。 备份的命令需要记录一下: 1gitlab-rake gitlab:backup:create 备份的路径可以在配置文件 /etc/gitlab/gitlab.rb 中修改： 12345[root@code-server ~]# vim /etc/gitlab/gitlab.rbgitlab_rails['manage_backup_path'] = truegitlab_rails['backup_path'] = "/data/gitlab/backups" // gitlab备份目录gitlab_rails['backup_archive_permissions'] = 0644 // 生成的备份文件权限gitlab_rails['backup_keep_time'] = 7776000 // 备份保留天数为90天 备份从21:30开始一直到凌晨1点左右结束，200G 数据用时3个半小时，真是太慢了，大部分的时间都卡在备份了 Dumping lfs objects 了 1234567891011121314151617181920212223242526272829303132333435363738394041[root@gitlab-server ~]# gitlab-rake gitlab:backup:create2022-07-25 21:27:34 +0800 -- Dumping database ...Dumping PostgreSQL database gitlabhq_production ... [DONE]2022-07-25 21:27:42 +0800 -- done2022-07-25 21:27:42 +0800 -- Dumping repositories ... * xxxx/project (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x) ... * xxxx/project (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x) ... [DONE] * xxxx/project.wiki (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x.wiki) ... * xxxx/project.wiki (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x.wiki) ... [EMPTY] [SKIPPED] * xxxx/project.design (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x.design) ... * xxxx/project.design (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x.design) ... [EMPTY] [SKIPPED] ... ...2022-07-25 22:16:14 +0800 -- done2022-07-25 22:16:14 +0800 -- Dumping uploads ...2022-07-25 22:16:15 +0800 -- done2022-07-25 22:16:15 +0800 -- Dumping builds ...2022-07-25 22:16:15 +0800 -- done2022-07-25 22:16:15 +0800 -- Dumping artifacts ...2022-07-25 22:16:15 +0800 -- done2022-07-25 22:16:15 +0800 -- Dumping pages ...2022-07-25 22:16:15 +0800 -- done2022-07-25 22:16:15 +0800 -- Dumping lfs objects ...2022-07-26 00:41:39 +0800 -- done2022-07-26 00:41:39 +0800 -- Dumping container registry images ...2022-07-26 00:41:39 +0800 -- [DISABLED]Creating backup archive: 1658767299_2022_07_26_xx.xx.xx-ee_gitlab_backup.tar ... doneUploading backup archive to remote storage ... skippedDeleting tmp directories ... donedonedonedonedonedonedonedoneDeleting old backups ... skippingWarning: Your gitlab.rb and gitlab-secrets.json files contain sensitive dataand are not included in this backup. You will need these files to restore a backup.Please back them up manually.Backup task is done. 修改存放目录这一步注意创建新目录后修改权限，并修改配置文件 12345[root@gitlab ~]# mkdir -p /data/gitlab-data[root@gitlab ~]# chown -R git:root /data/gitlab-data[root@gitlab ~]# vim /etc/gitlab/gitlab.rb# 把注释取消然后指定新的仓库存储位置，原配置是# git_data_dirs(&#123; "default" =&gt; &#123; "path" =&gt; "/mnt/nas/git-data" &#125; &#125;)git_data_dirs(&#123; "default" =&gt; &#123; "path" =&gt; "/data/gitlab-data" &#125; &#125;) /data/gitlab-data 是新挂载的目录，目标路径和其子路径必须不能为软链接。 关闭服务迁移数据前关闭服务主要是怕别人再使用，其实已经凌晨应该没有人用了，为了保险起见还是先关了吧 1gitlab-ctl stop 关闭时会自动关闭多个服务项 12345678910111213141516[root@gitlab-server gitlab]# gitlab-ctl stopok: down: alertmanager: 0s, normally upok: down: gitaly: 0s, normally upok: down: gitlab-exporter: 0s, normally upok: down: gitlab-workhorse: 0s, normally upok: down: grafana: 0s, normally upok: down: logrotate: 0s, normally upok: down: nginx: 1s, normally upok: down: node-exporter: 0s, normally upok: down: postgres-exporter: 1s, normally upok: down: postgresql: 0s, normally upok: down: prometheus: 0s, normally upok: down: puma: 0s, normally upok: down: redis: 0s, normally upok: down: redis-exporter: 1s, normally upok: down: sidekiq: 0s, normally up 迁移数据1rsync -av /var/opt/gitlab/git-data/repositories /data/gitlab-data/ 末尾的/要有，这一点很多文章都提到了，但是它们给的例子中有没有，很容易造成迷惑，这一步其实就是把 repositories 完整的拷贝到 /data/gitlab-data/ 目录下，并保留原文件和目录的属性，不加 / 就少了一级目录，不过也有文章让使用 cp 或者 mv 命令迁移数据，虽然也能达到目的，但是不如 rsync 合适，他们无法保留原属性。 200G 数据拷贝了半小时，比备份操作快多了 刷新配置1gitlab-ctl reconfigure 这一步我在执行前以为瞬间就能完成，运行后才发现这个命令检查了很多数据，在控制台输出了一大堆内容，可以注意一下有没有报错信息。 启动服务1gitlab-ctl start 启动之后需要等大概一分钟才能访问，否则会报服务器内部错误，应该是有些服务还没完全开始工作吧 12345678910111213141516[root@gitlab-server gitlab]# gitlab-ctl startok: run: alertmanager: (pid 17961) 1sok: run: gitaly: (pid 17984) 0sok: run: gitlab-exporter: (pid 18023) 0sok: run: gitlab-workhorse: (pid 18025) 1sok: run: grafana: (pid 18054) 0sok: run: logrotate: (pid 18063) 1sok: run: nginx: (pid 18082) 0sok: run: node-exporter: (pid 18111) 1sok: run: postgres-exporter: (pid 18120) 0sok: run: postgresql: (pid 18129) 0sok: run: prometheus: (pid 18136) 1sok: run: puma: (pid 18151) 0sok: run: redis: (pid 18157) 1sok: run: redis-exporter: (pid 18173) 0sok: run: sidekiq: (pid 18183) 1s 总结 gitlab 服务器配置文件是 /etc/gitlab/gitlab.rb gitlab 配置文中设置备份目录 gitlab_rails[&#39;manage_backup_path&#39;]，存储目录 git_data_dirs gitlab-rake gitlab:backup:create 用于备份 gitlab 关闭组件 gitlab-ctl stop，开启组件 gitlab-ctl start，重启组件 gitlab-ctl restart gitlab-ctl reconfigure 用于刷新配置 gitlab 迁移数据命令 rsync -av /var/opt/gitlab/git-data/repositories /data/gitlab-data/ ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 曾经幅员辽阔，东征西讨的元朝占据了欧亚大陆，但持续时间不过百年，在历史长河中只是星星点点，若要在这长河中留下痕迹，必然要做出影响全人类的重大成就，而你我绝大多数都是普通人，承认自己是普通人的那一刻你已经成长了~ 2022-7-30 16:59:18]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>gitlab</tag>
        <tag>数据迁移</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐一个C++枚举转字符串的开源项目magic_enum]]></title>
    <url>%2Fblog%2F2022%2F07%2F23%2F%E6%8E%A8%E8%8D%90%E4%B8%80%E4%B8%AAC-%E6%9E%9A%E4%B8%BE%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AEmagic-enum%2F</url>
    <content type="text"><![CDATA[前言C++11引入了强类型的枚举类 enum class 用来代替旧风格枚举enum，新引入的 enum class 具有诸多优点：防止命名空间污染，不能隐式的转换为整型，防止不同类型的枚举相互赋值，支持前置声明。当然它也不是只有优点，因为类型不能隐式转换成int，所以在使用或者输出时需要使用 static_cast 进行转换，不过即便使用 static_cast 可以转换后输出，也不便于我们辨识枚举的值，如果想输出枚举定义时的名字就需要使用一些魔法了。 magic_enum因为C++本身不支持反射，或者说反射能力极弱，所以想反射我们必须自己实现一些东西，比如 UE 引擎就为C++写了一套自己的反射标签，而我们想获得枚举定义时的名字就需要自己记录了，因为编译后的枚举一般都转化成了整数，一个简单粗暴的想法是在定义时为每个枚举值同时指定一个同名字符串，构成map存储下来，不过我们不想每次都自己来做这件事，要是有人能帮忙就好了，这不它来了， magic_enum 就可以帮你实现这个愿望。 简单介绍magic_enum 是一个单头文件的开源库，使用方便，可以轻松帮你实现打印枚举值定义时名字的需求，另外除了可以实现这个功能，还可以根据字符串生成枚举值，根据整数生成枚举值，获取枚举值数组，获取枚举值和名字对应的数字组等等，简直是一个封装了枚举操作的宝库。 具体使用直接引用头文件 magic_enum.hpp，然后调用函数 magic_enum::enum_name(enum_xxx) 即可： 1234567891011121314151617181920212223#include &lt;iostream&gt;#include "magic_enum.hpp"enum class WeekDay&#123; WD_SUNDAY = 0, WD_MONDAY, WD_TUESDAY, WD_WEDNESDAY, WD_THURSDAY, WD_FRIDAY, WD_SATURDAY,&#125;;int main()&#123; WeekDay day = WeekDay::WD_MONDAY; std::cout &lt;&lt; "enum value: " &lt;&lt; static_cast&lt;std::underlying_type&lt;WeekDay&gt;::type&gt;(day) &lt;&lt; "\n"; std::cout &lt;&lt; "enum name: " &lt;&lt; magic_enum::enum_name(day) &lt;&lt; "\n"; return 0;&#125; 编译运行如下： 123$ g++ testenum.cpp -std=c++17 &amp;&amp; ./a.outenum value: 1enum name: WD_MONDAY 原理简述很神奇对不对，其实枚举值转换成字符串这一步，是是利用了函数模板和 __PRETTY_FUNCTION__ 组合使用获得到的，也就是对 __PRETTY_FUNCTION__ 进行截取得到的字符串。 __PRETTY_FUNCTION__ 在预编译阶段会替换成带有参数的函数名，比如 constexpr auto magic_enum::detail::n() [with E = WeekDay; E e = WD_MONDAY] 从中截取出 WD_MONDAY 就可以了。 局限性为了实现从字符串到枚举值的转换，这个库的内部定义了一个整数范围，默认从-128到128，用于遍历查找字符串对应的枚举值是多少，并且在代码中加了 static_assert 来判断范围，如果超过了这个范围就会报编译错误，这个范围可以通过修改源码中的 MAGIC_ENUM_RANGE_MIN 和 MAGIC_ENUM_RANGE_MAX 重新编译来修改，在这个范围之外还有个最大值 std::numeric_limits&lt;std::uint16_t&gt;::max 的限制。 这个最大限制也是可以改的，不过我尝试把 MAGIC_ENUM_RANGE_MAX 改到上限值 32767 之后编译时间明显变成，编译过程变得异常的慢，单个文件编译30秒，所以不建议把这个值调太大。 对比一下不使用 magic_enum 和使用它之后生成的汇编代码，从100多行扩充到1000多行，利用 type_traits 生成了大量的函数： 1234567891011121314151617181920... .type _ZN10magic_enum6detail7names_vI7WeekDayEE, @gnu_unique_object .size _ZN10magic_enum6detail7names_vI7WeekDayEE, 112_ZN10magic_enum6detail7names_vI7WeekDayEE: .quad 9 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_0EEE .quad 9 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_1EEE .quad 10 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_2EEE .quad 12 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_3EEE .quad 11 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_4EEE .quad 9 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_5EEE .quad 11 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_6EEE .weak _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_0EEE... 简化与改进其实我最想要的还是通过枚举值转化转化成字符串名称的功能，可以将这个开源库简化一下，仅保留这个功能，这样也不会有范围限制了，感觉这个库为了实现从字符串到枚举值转换背负了太多，去掉它会很清爽。 总结 magic_enum 是一个开源的、单头文件的、枚举操作工具箱 magic_enum 可以实现枚举值到字符串、字符串到枚举值、获取所有枚举名等多种操作 magic_enum 本身对枚举值有范围限制，默认是 [-128, 128], 可通过 MAGIC_ENUM_RANGE_MIN 和 MAGIC_ENUM_RANGE_MAX 修改 不建议将 magic_enum 默认枚举范围改的太大，这会明显拖慢编译时间 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 世界上有那么多美好，不要跟自己过不去，总是揪着那些角落里的肮脏不放。我们无法选择抓到什么牌，但可以决定怎么把已经抓到手的牌打出去，摆烂是一天，奋斗也是一天，究竟要怎么做，取决于你自己~ 2022-7-24 02:30:58]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>enum</tag>
        <tag>枚举类</tag>
        <tag>magic_enum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++17使用std::optional表示一个可能存在的值]]></title>
    <url>%2Fblog%2F2022%2F07%2F17%2FC-17%E4%BD%BF%E7%94%A8std-optional%E8%A1%A8%E7%A4%BA%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%83%BD%E5%AD%98%E5%9C%A8%E7%9A%84%E5%80%BC%2F</url>
    <content type="text"><![CDATA[前言平时写代码会遇到一种传递参数特殊值标记特殊流程，或者函数返回值存在魔法数的情况，很需要一种标记参数或返回值状态的结构，那么在 C++17 标准下提供了 std::optional 这个模板类，可以表示一个值不存在的状态，一起来看看用法吧。 返回一个bool值以下例子纯属虚构，只为说明问题，无实际意义 1234567891011121314151617181920212223bool getBoolVal(int a, int b)&#123; int* n = new int; if (!n) return false; *n = 1; if (a + *n &gt; b) return true; else return false;&#125;int main()&#123; if (getBoolVal(10, 9)) std::cout &lt;&lt; 1 &lt;&lt; std::endl; else std::cout &lt;&lt; 1 &lt;&lt; std::endl; return 0;&#125; 这个例子中的函数 getBoolVal 本意是想返回一个 bool 类型的判断结果，但是函数中有一些异常情况时，比如申请内存异常时，也会返回一个bool值，这是与原判断结果语义不同的，所以需要单独返回这种情况，如果也放到同一个返回值中会导致含义模糊，这时可以考虑使用引用变量参数来返回实际比较结果。 12345678910111213141516171819202122232425262728293031bool getBoolVal(int a, int b, bool&amp; ret)&#123; int* n = new int; if (!n) return false; *n = 1; if (a + *n &gt; b) ret = true; else ret = false; return true;&#125;int main()&#123; bool ret = false; if (getBoolVal(10, 9, ret)) std::cout &lt;&lt; "error" &lt;&lt; std::endl; else &#123; if (ret) std::cout &lt;&lt; 1 &lt;&lt; std::endl; else std::cout &lt;&lt; 0 &lt;&lt; std::endl; &#125; return 0;&#125; 这个引用参数 ret 使用起来有点不方便，那把两个值都返回怎么样，虽然C++不允许有多个返回值，但可以把它们包装成 std::pair 或者 std::tuple 来返回，再来改写一下： 1234567891011121314151617181920212223242526272829std::pair&lt;bool, bool&gt; getBoolVal3(int a, int b)&#123; int* n = new int; if (!n) return &#123;false, false&#125;; *n = 1; if (a + *n &gt; b) return &#123;true, true&#125;; else return &#123;true, false&#125;;&#125;int main()&#123; auto [err, ret] = getBoolVal(10, 9); if (err) std::cout &lt;&lt; "error" &lt;&lt; std::endl; else &#123; if (ret) std::cout &lt;&lt; 1 &lt;&lt; std::endl; else std::cout &lt;&lt; 0 &lt;&lt; std::endl; &#125; return 0;&#125; 这种方法把实际的返回值，搭配一个表示状态的 bool 变量，组成 std::pair 进行返回，基本上得到而来语义明确的目的，但是看起来还是不太优雅，而 std::optional 可以帮助我们实现类似的需求，并且代码看起来能更简洁一点。 使用 std::optional 改写std::optional 本身是一个模板类：会有一个 std::nullopt 12template &lt;class T&gt;class optional; 它内部有两种状态，要么有一个T类型的值，要么用 std::nullopt 表示没有值，查看一个 std::optional 对象是否有值，可以用 has_value() 进行判断，当一个 std::optional 有值时，可以通过用指针的方式(*号和-&gt;号)来使用它，或者用 value()函数取它的值，下面我们用它来改写一下之前的实现： 123456789101112131415161718192021222324252627282930std::optional&lt;bool&gt; getBoolVal4(int a, int b)&#123; int* n = new int; if (!n) return std::nullopt; *n = 1; if (a + *n &gt; b) return true; else return false;&#125;int main()&#123; std::optional&lt;bool&gt; ret = getBoolVal(10, 9); if (ret.has_value()) std::cout &lt;&lt; "error" &lt;&lt; std::endl; else &#123; if (ret.value()) std::cout &lt;&lt; 1 &lt;&lt; std::endl; else std::cout &lt;&lt; 0 &lt;&lt; std::endl; &#125; return 0;&#125; 使用了 std::optional 之后就把 bool 类型之前的两态变成了三态，很多类似的逻辑也被封装成了函数，使用它之后代码更清晰了，从此可以告别一些烦人的魔法数了，一些函数参数也可以使用 std::optional 来包装，用法类似，在此就不展开说了。 总结 std::optional 是一个模板类，可以表示一个可能存在的值 std::optional 的内部有两种状态，要么表示一个T类型的值，要么用 std::nullopt 表示没有值 可以用 has_value() 判断一个 std::optional 是否有值，然后用 value() 函数取它表示的值 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 子未经历过，安知此文是鸡汤，子非我，安知我不知此文是鸡汤。意见向左的人往往在内心互道傻X，而现实生活中哪有什么绝对的对错，只是出发点和眼界不同罢了，即使是真理也有适用的环境，“两点之间线段最短”，这一定是对的吗？ 2022-7-17 23:26:03]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>optional</tag>
        <tag>可选值</tag>
        <tag>魔法数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偶然在github开源项目中发现了.travis.yml这货]]></title>
    <url>%2Fblog%2F2022%2F07%2F02%2F%E5%81%B6%E7%84%B6%E5%9C%A8github%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%8F%91%E7%8E%B0%E4%BA%86-travis-yml%E8%BF%99%E8%B4%A7%2F</url>
    <content type="text"><![CDATA[前言偶然在一个github开源项目中发现了.travis.yml这货，然后一发不可收拾，翻了翻之前看的几个开源库都有这个文件，并且最近经常看到它，这被称为“巴德尔-迈因霍夫现象”，是一种认知偏见，即在第一次注意到某一事物后，有一种更频繁地注意到它的倾向，导致某人相信它有很高的频率，既然这样索性就深入研究了一下这个文件，发现它原来是用于持续集成的。 持续集成持续集成是一种 DevOps（Development和Operations的组合词）软件开发实践。采用持续集成时，开发人员会定期将代码变更合并到一个中央存储库中，之后系统会自动运行构建和测试操作。每次集成都通过自动化的构建（包括编译，发布，自动化测试）来进行验证，从而尽早地发现集成错误，提高软件质量，并减少验证和发布新软件更新所需的时间。 持续集成是将构建并测试的过程自动化，在你提交代码时，持续集成服务能够自动触发构建与测试过程，并反馈结果，加快开发周期，同时减少脏代码的引入，而持续集成工具有很多，常见的包括 Jenkins、Gitlab-CI、Travis CI 和 AppVeyor，github上项目的持续集成可以选择使用 Travis CI，也有项目使用 AppVeyor，它们都是开源持续集成云服务。 YAML.travis.yml 是 github 用于说明持续集成步骤配置文件，使用的语言是 YAML。它是一种可读性非常高，与程序语言数据结构非常接近，同时具备丰富的表达能力和可扩展性，并且易于使用的数据标记语言。经常会拿它和 XML 和 JSON 进行对比，YAML 比 XML 语法简洁得多，但是没有 XML 的标签概念，而 JSON 语法是 YAML 1.2 的子集，非常接近 YAML1.0 与 YAML1.1 的子集。 YAML 可以简单表达清单、散列表，标量等数据结构。它使用空白符号缩进，适合用来表达或编辑数据结构、各种配置文件、倾印调试内容、文件大纲等，配置文件后缀为通常为 .yml，比如：.travis.yml。 关于具体的语法本文就不展开说了，网上自行搜索一下，不同类型的项目的配置通常有自己的规范，可以参照travis官方配置说明，下面展示一个 .travis.yml 文件 12345678910111213141516171819language: cppsudo: falseos: - linux - osxcompiler: - gcc - clangscript: - $CC ccronexpr.c ccronexpr_test.c -I. -Wall -Wextra -std=c89 -DCRON_TEST_MALLOC -o a.out &amp;&amp; ./a.out - $CXX ccronexpr.c ccronexpr_test.c -I. -Wall -Wextra -std=c++11 -DCRON_TEST_MALLOC -o a.out &amp;&amp; ./a.outnotifications: email: on_success: always 第一次使用 .travis.yml要想学会一件事必须反复强化记忆，所以我决定自己写个.travis.yml来使用一次，刚开始语法还不太熟悉，所以我打算在一些开源项目的文件基础上来修改，需求也比较简单，只要能实现我上传到github的代码能自动编译就可以了。 注册登录travis登陆 travis 官网，直接用github账号登陆即可，这样 travis 可以直接关联登录的github账号，自动获取你的仓库信息。 登陆之后，点击settings，然后激活 Travis CI 勾选需要持续集成的仓库。 编写代码为了方便测试，我们只编写一个简单的 HelloWolrd.cpp 测试文件好了 1234567#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; "Hello World!" &lt;&lt; std::endl; return 0;&#125; 编写.travis.yml我只写了一个文件，要求只要编译 gcc 通过就行了，暂时也不需要邮件通知 123456789101112language: cppsudo: falseos: - linuxcompiler: - gccscript: - $CXX wolrd/code/c++/HelloWorld.cpp -o a.out &amp;&amp; ./a.out 推送代码启动Travis CI123456789101112131415161718Albert@home-pc MINGW64 /d/data/maingit/HelloWorld (master)$ git add .Albert@home-pc MINGW64 /d/data/maingit/HelloWorld (master)$ git commit -m&quot;test travis ci&quot;[master 8e92384] test travis ci 2 files changed, 2 insertions(+), 2 deletions(-)Albert@home-pc MINGW64 /d/data/maingit/HelloWorld (master)$ git pushEnumerating objects: 13, done.Counting objects: 100% (13/13), done.Delta compression using up to 4 threadsCompressing objects: 100% (6/6), done.Writing objects: 100% (7/7), 604 bytes | 302.00 KiB/s, done.Total 7 (delta 3), reused 0 (delta 0)remote: Resolving deltas: 100% (3/3), completed with 3 local objects.To github.com:AlbertGithubHome/HelloWorld.git af45c67..8e92384 master -&gt; master 推送之后travis-ci网站会自动启动，构建过程和结果如下： 第一次尝试失败，检查发现编译文件的路径写错了，修改后再次推送，成功构建的界面如下： 然后就可以在编译状态按钮后面领取这样一个标签，它可以根据项目构建状态实时变化，快把它加到项目的README文件里吧。 [![Build Status](https://app.travis-ci.com/AlbertGithubHome/HelloWorld.svg?branch=master)](https://app.travis-ci.com/AlbertGithubHome/HelloWorld) 总结 .travis.yml 是使用 Travis CI 持续集成服务的配置文件，使用 YAML 语言编写 YAML 比 XML 语法简洁得多，但是没有 XML 的标签概念，而 JSON 语法是 YAML 1.2 的子集 GitHub 和 Travis CI 是一对好基友，几乎不用额外的配置，只要按照官方语法写好 .travis.yml 文件即可 可以把Travis CI 看成一个机器人，每当我们 push 代码时，这个机器人会按照既定流程帮我们自动构建和检测 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 卅是一个阶段，更是一个开始~ 2022-7-3 01:08:33]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>CI</tag>
        <tag>CD</tag>
        <tag>YAML</tag>
        <tag>travis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11中的noexcept说明符和操作符]]></title>
    <url>%2Fblog%2F2022%2F06%2F26%2FC-11%E4%B8%AD%E7%9A%84noexceptno%E7%9A%84%E8%AF%B4%E6%98%8E%E7%AC%A6%E5%92%8C%E6%93%8D%E4%BD%9C%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[前言noexcept 这个说明符看起来很厉害的样子，给人一种函数加上它就可以不产生异常的感觉，但事实真的如此吗？它真的能消除一切异常吗？答案是不能！它只是函数的一种说明，作为开发者我们需要自己保证标记了 noexcept 的函数不产生异常。啥？这不是没事找事吗？ 为什么要用 noexcept既然给函数添加了 noexcept 说明符并不能杜绝异常的产生，那我们为什么还要用它呢？我个人的理解是为了优化，因为异常存在的意义就是用来干脏活累活的，为了实现异常的处理，我们编写逻辑的每一次函数调用，编译器都会生成一堆外围的处理代码，而当我们把一个函数标记为 noexcept 时，表示开发者向运行时保证调用这个函数不会抛出异常，这意味着这些脏活累活都不用干了，如果声明了 noexcept 的函数还试图抛出异常，C++会调用std::terminate函数终止程序运行。 添加 noexcept 说明符后编译器是否会进行优化，我们可以通过生成的汇编代码来看一下，编译器是 gcc 12.1，先看下面这段代码： 123456789101112struct Obj&#123; ~Obj();&#125;void inner();void entrance()&#123; Obj instance; inner();&#125; 函数 entrance() 先定义一个Obj类型的对象，然后调用一个 inner() 函数，假如 inner() 在执行过程中抛出了异常，我们需要在保证对象实例 instance 被正确析构后才能退出 entrance() 函数，为了实现了这个保证，编译器真的是操碎了心，可以看一下生成汇编代码： 123456789101112131415161718192021entrance(): push rbp mov rbp, rsp push rbx sub rsp, 24 call inner() lea rax, [rbp-17] mov rdi, rax call Obj::~Obj() [complete object destructor] jmp .L4 mov rbx, rax lea rax, [rbp-17] mov rdi, rax call Obj::~Obj() [complete object destructor] mov rax, rbx mov rdi, rax call _Unwind_Resume.L4: mov rbx, QWORD PTR [rbp-8] leave ret 从这段汇编代码中可以发现，有两句是调用了析构函数 Obj::~Obj()，其中第二次就是为了异常发生时准备的，接下来我们加上 noexcept 后代码变成了下面这样： 123456789101112struct Obj&#123; ~Obj();&#125;;void inner() noexcept;void entrance()&#123; Obj o; inner();&#125; 源代码几乎没有变化，而生成的汇编代码清爽了太多： 12345678910entrance(): push rbp mov rbp, rsp sub rsp, 16 call inner() lea rax, [rbp-1] mov rdi, rax call Obj::~Obj() [complete object destructor] leave ret 对比来看，以下这部分代码一直都是编译器在默默付出，现在你可以通过 noexcept 帮它减负了 12345678910 jmp .L4 mov rbx, rax lea rax, [rbp-17] mov rdi, rax call Obj::~Obj() [complete object destructor] mov rax, rbx mov rdi, rax call _Unwind_Resume.L4: mov rbx, QWORD PTR [rbp-8] 实际测试 noexcept不处理异常首先实验一下不处理异常时的情景： 12345678910111213141516171819202122#include &lt;iostream&gt;struct Obj&#123; ~Obj() &#123; std::cout &lt;&lt; "~Obj" &lt;&lt; std::endl; &#125;;&#125;;void inner()&#123; throw 1;&#125;int main()&#123; Obj o; inner(); return 0;&#125; 编译运行结果如下： 1234albert@home-pc:/mnt/d/data/cpp/testExcept$ g++ testexcept.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testExcept$ ./a.outterminate called after throwing an instance of 'int'Aborted (core dumped) 可以看到程序直接崩溃了，在抛出一个异常之后被终止了。 正常捕获异常再实验一次不加 noexcept 时常规处理异常的方式，代码如下： 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;struct Obj&#123; ~Obj() &#123; std::cout &lt;&lt; "~Obj" &lt;&lt; std::endl; &#125;;&#125;;void inner()&#123; throw 1;&#125;int main()&#123; Obj o; try &#123; inner(); &#125; catch(int) &#123; std::cout &lt;&lt; "catch exception." &lt;&lt; std::endl; &#125; return 0;&#125; 编译运行结果如下： 1234albert@home-pc:/mnt/d/data/cpp/testExcept$ g++ testexcept.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testExcept$ ./a.outcatch exception.~Obj 异常被捕获，Obj对象被正常析构，程序正常退出了 添加noexcept标记给函数加上noexcept标记，再测试一次： 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;struct Obj&#123; ~Obj() &#123; std::cout &lt;&lt; "~Obj" &lt;&lt; std::endl; &#125;;&#125;;void inner() noexcept&#123; throw 1;&#125;int main()&#123; Obj o; try &#123; inner(); &#125; catch(int) &#123; std::cout &lt;&lt; "catch exception." &lt;&lt; std::endl; &#125; return 0;&#125; 编译运行结果如下： 1234albert@home-pc:/mnt/d/data/cpp/testExcept$ g++ testexcept.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testExcept$ ./a.outterminate called after throwing an instance of 'int'Aborted (core dumped) 我们看到加了noexcept如果抛出异常，程序会被直接终止，try...catch 语句就不起作用了，所以 noexcept 不能随便加，一定得保证确实没有异常才可以应用此优化。 noexcept操作符noexcept本身后面可以加一个表达式，返回一个bool值，用来判定一个函数是否会抛出异常，可以看一下这个例子： 123456789101112131415161718#include &lt;iostream&gt;void inner() noexcept&#123; throw 1;&#125;void test();void test2() noexcept;int main()&#123; std::cout &lt;&lt; std::boolalpha &lt;&lt; "Is inner() noexcept? " &lt;&lt; noexcept(inner()) &lt;&lt; std::endl &lt;&lt; "Is test() noexcept? " &lt;&lt; noexcept(test()) &lt;&lt; std::endl &lt;&lt; "Is tes2() noexcept? " &lt;&lt; noexcept(test2()) &lt;&lt; std::endl;&#125; 编译运行后的结果如下： 12345albert@home-pc:/mnt/d/data/cpp/testExcept$ g++ testexcept.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testExcept$ ./a.outIs inner() noexcept? trueIs test() noexcept? falseIs tes2() noexcept? true test() 和 test2() 函数的判断应该没有什么疑问，而 inner()明明会抛出异常，而 noexcept操作符却返回了false，这是因为我们在函数后面加上了 noexcept 说明符，这时就需要我们自己保证函数不会抛出异常了，如果你硬要抛出异常那编译器也没有办法了。 noexcept的优缺点优点调用标记为 noexcept 的函数时不需要额外记录exception handler，所以编译器拥有更高的自由度，便于生成更加高效的执行代码。 缺点noexcept关键字啊会影响接口的灵活性，如果基类某个虚函数设置为noexcept，派生类重写虚函数时也必须遵守，派生类中只要有一个函数遗漏了noexcept约定，就可能会导致整个程序在发生异常时被终止。 如果标记为noexcept的函数调用了第三方库的函数，就需要对这些第三方库都做一层封装，保证处理了第三方库所有可能抛出的异常，增大了工作量。 总结 noexcept 是一个说明符同时也是一个操作符 noexcept 作为说明符放在函数名后面，表明次函数不会抛出异常，等同于 noexcept(true) noexcept 作为操作符时，可以用来判断一个函数是否会抛出异常，用法为 noexcept(funcName) 当使用 noexcept 标记函数时，我们需要自己保证函数不会抛出异常，这样可以生成更高效的代码 如果标记了 noexcept函数还是抛出了异常，那么程序会直接调用 std::abort() 终止程序，try...catch都没用 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 曾经接受教育努力学习是打破阶级固化的长矛，如今教育更多表现为阻碍阶级跨越的护盾，不能否认好好学习可以让我们的生活变得更好，但寒门再难出贵子确实是摆在我们面前的现实，马太效应，强者愈强、弱者愈弱。条条大路通罗马，而有些人就生在罗马，我们只能靠自己微弱的锋利，去尝试突破那一层层禁锢。 2022-6-26 17:23:42]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>noexcept</tag>
        <tag>terminate</tag>
        <tag>try-catch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++17新语法之if和switch语句中可以初始化变量啦]]></title>
    <url>%2Fblog%2F2022%2F06%2F18%2FC-17%E6%96%B0%E8%AF%AD%E6%B3%95%E4%B9%8Bif%E5%92%8Cswitch%E8%AF%AD%E5%8F%A5%E4%B8%AD%E5%8F%AF%E4%BB%A5%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%98%E9%87%8F%E5%95%A6%2F</url>
    <content type="text"><![CDATA[前言今天在看一些c++代码时偶然间发现一些示例，在if语句的小括号内居然出现了分号，难道这一段是伪代码吗？之前从来没见过这种写法，然后写了一个例子，用最新的编译器编译后发现真的是正常的代码，并且可以成功运行，然后脑袋有个声音一闪而过，是不是一直就支持这种写法，只是我不知道而已，后来几经周折，终于发现真相，原来这其实是C++17新的语法。 if 语句本来的样子C++17之前 if 语句的小括号内只能放判断逻辑，if (condition) { /* ... */ } 也就是这样，所以如果想使用一个变量进行判断，那么这个变量必须在if语句之前进行定义，比如我们常常使用的查找map元素的逻辑： 1234567&#123; auto it = mp.find("key"); if (it != mp.end()) return it-&gt;second; else return defaultValue;&#125; if 语句新能力从C++17开始，if 语句的小括号之中也可以初始化变量了，语法为 if (initializer; condition) { /* ... */ }，这样做的好处就是可以更加精细的控制变量的作用域，使代码看起来更加紧凑，比如上面的例子可以改写为： 123456&#123; if (auto it = mp.find("key"); it != mp.end()) return it-&gt;second; else return defaultValue;&#125; 最直观的感受就是少了一行代码，但是更重要的特点还是他把变量 it 的作用域限制在了 if 语句之内。 另外就是能一定程度上解决代码缩进层数太多的问题，这也是实际开发过程中遇到的问题，比如下面这段代码： 12345678910111213&#123; auto it = mp.find("key"); if (it != mp.end()) return it-&gt;second; else &#123; int cfg = GetConfig(); if (cfg) return defaultValue1; else return defaultValue2; &#125;&#125; 在C++17之前的代码中，因为 if 语句中不能包含初始化变量的逻辑，所以会造成 if 嵌套层层递进，当这种条件太多时，缩进层数太多导致代码可读性变差，使用C++17的语法再改进一下就变成了下面这样，逻辑更加清晰了： 12345678&#123; if (auto it = mp.find("key"); it != mp.end()) return it-&gt;second; else if (int cfg = GetConfig(); cfg) return defaultValue1; else return defaultValue2;&#125; switch 语句也是相同的能力扩展，就不再展开说了，这里贴一个 cppreference - switch 上的例子： 12345678910111213141516171819202122232425&#123; struct Device &#123; enum State &#123; SLEEP, READY, BAD &#125;; auto state() const &#123; return m_state; &#125; /*...*/ private: State m_state&#123;&#125;; &#125;; switch (auto dev = Device&#123;&#125;; dev.state()) &#123; case Device::SLEEP: /*...*/ break; case Device::READY: /*...*/ break; case Device::BAD: /*...*/ break; &#125;&#125; 似曾相识在go中这个在if中既初始化又进行判断的语法，之前在go中也用过，类似的需求在go中的写法是： 1234567func xxx() &#123; if value := mp["key"]; value != nil &#123; return value &#125; else &#123; return defaultValue &#125;&#125; 是不是很像？其实在golang中更规范的用法是使用map查找的第二个返回值来判断元素是否存在，就像下面这样： 1234567func xxx() &#123; if value, ok := mp["key"]; ok &#123; return value &#125; else &#123; return defaultValue &#125;&#125; 关于语言的思考看到上面这种golang和c++17的对比，会发现语言之间都在相互促进，这些所谓的高级语言，总能在其中一种语言中发现另一种语言的影子，也就是说在某些方面上他们是“趋同”的，可能在不久的将来会出现一种“超高级语言”，类似于C++模板机制，C++、Golang、Python 这些语言作为一种类型，传递到超高级语言之中，编写或者描述完功能逻辑后，自动生成对应语言的代码： 12345SuperLanguageTemplate&lt;C++/Golang/Python&gt;[AutoGenerateByFollowingDescription]&#123; / * ... * /&#125; 总结 if 和 switch 语句都可以在逻辑判断前初始化变量，变量的定义域可以更精细的控制 编程语言之间相互“借鉴”，部分语法长得越来越像，最后不知道会不会归于统一 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生并不是不断失去，反过来看，其实人生应该是不断拥有，1秒，2秒，3秒，这些本就不一定属于你的时间，作为馈赠出现在了你的生命里~ 2022-6-19 01:44:55]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>if</tag>
        <tag>switch</tag>
        <tag>initializer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解git裸仓库并利用post-receive自动化部署]]></title>
    <url>%2Fblog%2F2022%2F06%2F12%2F%E4%BA%86%E8%A7%A3git%E8%A3%B8%E4%BB%93%E5%BA%93%E5%B9%B6%E5%88%A9%E7%94%A8post-receive%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前言【裸仓库】指的是使用 git init --bare 命令得到的仓库，是对这种操作结果的一种直译，这个词对于刚接触 git 软件的小伙伴来说可能是第一次听说，而我也是最近实际操作了几次才渐渐理解，下面解释一下什么是裸仓库，以及为什么要使用它，有理解不对的地方还请大家指正。 普通库和裸仓库普通库在解释裸仓库之前，还是先来看看 git init命令创建一个普通仓库的目录结构: 123456789101112131415161718192021222324252627282930313233[root@VM-0-3-centos data]# git init simpleInitialized empty Git repository in /data/simple/.git/[root@VM-0-3-centos data]# cd simple/[root@VM-0-3-centos simple]# touch README.md[root@VM-0-3-centos simple]# cd ..[root@VM-0-3-centos data]# tree -a simple/simple/|-- .git| |-- branches| |-- config| |-- description| |-- HEAD| |-- hooks| | |-- applypatch-msg.sample| | |-- commit-msg.sample| | |-- post-update.sample| | |-- pre-applypatch.sample| | |-- pre-commit.sample| | |-- prepare-commit-msg.sample| | |-- pre-push.sample| | |-- pre-rebase.sample| | `-- update.sample| |-- info| | `-- exclude| |-- objects| | |-- info| | `-- pack| `-- refs| |-- heads| `-- tags`-- README.md10 directories, 14 files 通过上述命令操作后可以看到，git init simple 操作之后，创建了一个名为 simple 的库，simple 目录下还有一个 .git 子目录，其中包含了git系统常用的文件，在 .git 目录外是我们的工作区，可以存放我们库中待更新的文件，修改之后可以通过 git add，git commit 等命令更新 .git 中的内容，简单来说普通库就是在工作目录 simple 中还包括一个 .git 目录，下面添加一个文件试试。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@VM-0-3-centos simple]# git add README.md[root@VM-0-3-centos simple]# git commit -m"add readme file"[master (root-commit) 9a9b255] add readme file 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README.md[root@VM-0-3-centos simple]# tree . -a.|-- .git| |-- branches| |-- COMMIT_EDITMSG| |-- config| |-- description| |-- HEAD| |-- hooks| | |-- applypatch-msg.sample| | |-- commit-msg.sample| | |-- post-update.sample| | |-- pre-applypatch.sample| | |-- pre-commit.sample| | |-- prepare-commit-msg.sample| | |-- pre-push.sample| | |-- pre-rebase.sample| | `-- update.sample| |-- index| |-- info| | `-- exclude| |-- logs| | |-- HEAD| | `-- refs| | `-- heads| | `-- master| |-- objects| | |-- 9a| | | `-- 9b255b81e994fa9af2b9c7ecbd852eb716ad6c| | |-- e6| | | `-- 9de29bb2d1d6434b8b29ae775ad8c2e48c5391| | |-- f9| | | `-- 3e3a1a1525fb5b91020da86e44810c87a2d7bc| | |-- info| | `-- pack| `-- refs| |-- heads| | `-- master| `-- tags`-- README.md16 directories, 22 files[root@VM-0-3-centos simple]# 添加文件之后，.git 目录中的内容发生了变化，多了3个新的object。 裸仓库还是先从目录结构入手，我们使用 git init --bare 命令创建一个裸仓库，目录结构如下： 1234567891011121314151617181920212223242526272829[root@VM-0-3-centos data]# git init --bare bare.gitInitialized empty Git repository in /data/bare.git/[root@VM-0-3-centos data]# tree bare.git/ -abare.git/|-- branches|-- config|-- description|-- HEAD|-- hooks| |-- applypatch-msg.sample| |-- commit-msg.sample| |-- post-update.sample| |-- pre-applypatch.sample| |-- pre-commit.sample| |-- prepare-commit-msg.sample| |-- pre-push.sample| |-- pre-rebase.sample| `-- update.sample|-- info| `-- exclude|-- objects| |-- info| `-- pack`-- refs |-- heads `-- tags9 directories, 13 files[root@VM-0-3-centos simple]# 从目录结构来看裸仓库和普通库很像，但是仔细对比你会发现，这个裸仓库相比普通库少了一层目录，库目录 bare.git 内直接就是之前普通库 .git 目录下的内容，也就是说在 git 目录外层没有了工作目录来进行文件的增删改操作，那么我们仿照普通库操作在这个目录下提交一个文件会怎样呢？ 1234567[root@VM-0-3-centos data]# cd bare.git/[root@VM-0-3-centos bare.git]# touch README.md[root@VM-0-3-centos bare.git]# git add README.mdfatal: This operation must be run in a work tree[root@VM-0-3-centos bare.git]# git statusfatal: This operation must be run in a work tree[root@VM-0-3-centos bare.git]# 通过操作发现这个裸仓库不允许增删改库内的文件，甚至连 git status 这种命令都无法使用，统一提示了 fatal: This operation must be run in a work tree 这句话，告诉用户这些命令都必须在工作区内操作，既然不能修改，那么这个裸仓库就是“只读”的，那么它还有什么用呢？ 虽然裸仓库不允许直接修改，但是可以作为服务端远程仓库，在本地克隆这个远程仓库之后再进行修改，这也是最常见的应用方式，总结来说，普通库和裸仓库的区别就是：普通库拥有工作目录，并且工作目录中可以存放正常编辑和提交的文件，而裸库只存放这些文件的commit记录，不允许用户直接在上面进行各种git操作。 使用裸仓库前面提到裸仓库不能直接修改，但是我们可以采取修改克隆后库文件的方式达到更新的目的，下面列举两种常见的方式： 使用 git remote add 方式关联这种方式需要我们先在本地初始化一个普通库，再使用 git remote add 命令建立关联（PowerShell命令行操作，git命令是相同的）： 12345678910111213141516171819202122232425262728293031323334PS-Win D:\data\maingit\test&gt; git init barebyremoteInitialized empty Git repository in D:/data/maingit/test/barebyremote/.git/PS-Win D:\data\maingit\test&gt; cd .\barebyremote\PS-Win D:\data\maingit\test\barebyremote&gt; git remote add origin root@82.156.125.196:/data/bare.gitPS-Win D:\data\maingit\test\barebyremote&gt; new-item README.md 目录: D:\data\maingit\test\barebyremoteMode LastWriteTime Length Name---- ------------- ------ -----a---- 2022/6/12 16:51 0 README.mdPS-Win D:\data\maingit\test\barebyremote&gt; git add .\README.mdPS-Win D:\data\maingit\test\barebyremote&gt; git commit -m"add readme file"[master (root-commit) f1c41db] add readme file 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README.mdPS-Win D:\data\maingit\test\barebyremote&gt; git push -u origin masterEnumerating objects: 3, done.Counting objects: 100% (3/3), done.Writing objects: 100% (3/3), 223 bytes | 223.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)To 82.156.125.196:/data/bare.git * [new branch] master -&gt; masterBranch 'master' set up to track remote branch 'master' from 'origin'.PS-Win D:\data\maingit\test\barebyremote&gt; git log -3commit f1c41db4699f71e9750d8d6aa2c01875ac6d4a14 (HEAD -&gt; master, origin/master)Author: albert &lt;albert@163.com&gt;Date: Sun Jun 12 16:51:34 2022 +0800 add readme filePS-Win D:\data\maingit\test\barebyremote&gt; 使用 git clone 直接克隆使用克隆方式时，按照普通库来操作就可以（PowerShell命令行操作，git命令是相同的）： 123456789101112131415161718192021PS-Win D:\data\maingit\test&gt; git clone root@82.156.125.196:/data/bare.git barebycloneCloning into 'barebyclone'...remote: Counting objects: 3, done.remote: Total 3 (delta 0), reused 0 (delta 0)Receiving objects: 100% (3/3), done.PS-Win D:\data\maingit\test&gt; cd .\barebyclone\PS-Win D:\data\maingit\test\barebyclone&gt; git log -3commit f1c41db4699f71e9750d8d6aa2c01875ac6d4a14 (HEAD -&gt; master, origin/master, origin/HEAD)Author: albert &lt;albert@163.com&gt;Date: Sun Jun 12 16:51:34 2022 +0800 add readme filePS-Win D:\data\maingit\test\barebyclone&gt; ls 目录: D:\data\maingit\test\barebycloneMode LastWriteTime Length Name---- ------------- ------ -----a---- 2022/6/12 16:57 0 README.md 为什么要使用裸仓库既然裸仓库相比于普通库只是少了工作目录，那么我们直接用普通库作为远程仓库可不可以呢？结论是可以，但是不建议，我们来实际操作一下，利用刚刚的建立的 simple 作为远端库，我们在本地clone后修改，再上传看看会遇到什么问题。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152PS-Win D:\data\maingit\test&gt; git clone root@82.156.125.196:/data/simple simpleremote: Counting objects: 3, done.remote: Total 3 (delta 0), reused 0 (delta 0)Receiving objects: 100% (3/3), done.PS-Win D:\data\maingit\test&gt; cd .\simple\PS-Win D:\data\maingit\test\simple&gt; git log -3commit 9a9b255b81e994fa9af2b9c7ecbd852eb716ad6c (HEAD -&gt; master, origin/master, origin/HEAD)Author: albert &lt;albert@example.com&gt;Date: Sun Jun 12 15:53:30 2022 +0800 add readme filePS-Win D:\data\maingit\test\simple&gt; new-item .gitignore 目录: D:\data\maingit\test\simpleMode LastWriteTime Length Name---- ------------- ------ -----a---- 2022/6/12 17:20 0 .gitignorePS-Win D:\data\maingit\test\simple&gt; git add .\.gitignorePS-Win D:\data\maingit\test\simple&gt; git commit -m"add gitignore file"[master b5a679f] add gitignore file 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 .gitignorePS-Win D:\data\maingit\test\simple&gt; git pushEnumerating objects: 3, done.Counting objects: 100% (3/3), done.Delta compression using up to 4 threadsCompressing objects: 100% (2/2), done.Writing objects: 100% (2/2), 263 bytes | 263.00 KiB/s, done.Total 2 (delta 0), reused 0 (delta 0)remote: error: refusing to update checked out branch: refs/heads/masterremote: error: By default, updating the current branch in a non-bare repositoryremote: error: is denied, because it will make the index and work tree inconsistentremote: error: with what you pushed, and will require 'git reset --hard' to matchremote: error: the work tree to HEAD.remote: error:remote: error: You can set 'receive.denyCurrentBranch' configuration variable toremote: error: 'ignore' or 'warn' in the remote repository to allow pushing intoremote: error: its current branch; however, this is not recommended unless youremote: error: arranged to update its work tree to match what you pushed in someremote: error: other way.remote: error:remote: error: To squelch this message and still keep the default behaviour, setremote: error: 'receive.denyCurrentBranch' configuration variable to 'refuse'.To 82.156.125.196:/data/simple ! [remote rejected] master -&gt; master (branch is currently checked out)error: failed to push some refs to 'root@82.156.125.196:/data/simple'PS-Win D:\data\maingit\test\simple&gt; 克隆之后正常的修改和提交都没有问题，但是 git push的时候报错，原因提示 ! [remote rejected] master -&gt; master (branch is currently checked out)，提示当前的 master 分支是检出状态，不允许直接推送。 仔细想想就会有些思路，普通库实际上包含两份数据的，一份在 .git 目录中以object形式存在，一份在工作目录中以源文件形式存在，我们每次使用 git 命令，可以保证工作目录内文件和 .git 目录数据是一致的，但是如果将普通库作为远端时，在下游提交数据时，远端库中的 .git 目录会直接更新，但是工作区却不知道此时谁在用，不能直接更新覆盖，这就造成了数据不一致的情况。 如果非得使用普通库作为服务端仓库，那么可以参照上面报错的建议，在采用额外方式保证一致性的同时，修改服务端库的 receive.denyCurrentBranch 这个git配置项，或者将服务端分支切换到一个无人使用的分支上，这样下游端就可以直接推送了。 1234567[root@VM-0-3-centos data]# cd simple/[root@VM-0-3-centos simple]# git checkout -b unlessSwitched to a new branch 'unless'[root@VM-0-3-centos simple]# git branch -a master* unless[root@VM-0-3-centos simple]# 1234567891011121314151617PS-Win D:\data\maingit\test\simple&gt; pwdPath----D:\data\maingit\test\simplePS-Win D:\data\maingit\test\simple&gt; git pushEnumerating objects: 3, done.Counting objects: 100% (3/3), done.Delta compression using up to 4 threadsCompressing objects: 100% (2/2), done.Writing objects: 100% (2/2), 263 bytes | 263.00 KiB/s, done.Total 2 (delta 0), reused 0 (delta 0)To 82.156.125.196:/data/simple 9a9b255..b5a679f master -&gt; masterPS-Win D:\data\maingit\test\simple&gt; 自动化部署利用 post-receive 进行自动化部署的原理就是，git 本身提供了一些脚本接口，在某些 git 操作发生时，会调用预定脚本执行命令，相当于给 git 用户开放了接口，我们可以修改 post-receive 脚本，在修改提交后自动部署最新内容，进一步实现自动化集成。 因为前面已经介绍了很多有关裸仓库的知识，接下来我只叙述操作步骤，看了之前的介绍，这部分内容应该没什么难度了。 需求服务端建立裸仓库，在接收到新的提交时，自动将项目部署到/data/publish/game 目录下 操作步骤服务端远端操作建立裸仓库 /data/repo/game.git，对应部署目录是 /data/publish/game 123456[root@VM-0-3-centos data]# mkdir -p /data/repo[root@VM-0-3-centos data]# mkdir -p /data/publish/game[root@VM-0-3-centos data]# cd repo/[root@VM-0-3-centos repo]# git init --bare game.gitInitialized empty Git repository in /data/repo/game.git/[root@VM-0-3-centos repo]# 新建 /data/repo/game.git/hooks/post-receive 脚本，可以拷贝 post-receive.sample 进行修改，脚本内编写内容如下： 123456789# 指定部署目录DIR=/data/publish/gamegit --work-tree=$&#123;DIR&#125; clean -fd# 强制检出git --work-tree=$&#123;DIR&#125; checkout --force# 运行启动脚本cd $&#123;DIR&#125;chmod 755 start.sh./start.sh 客户端本地操作本地项目普通库目录结构如下，启动脚本为 start.sh： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051albert@home-pc MINGW64 /d/data/maingit/test/game (master)$ tree game/ -agame/├── .git│ ├── COMMIT_EDITMSG│ ├── config│ ├── description│ ├── HEAD│ ├── hooks│ │ ├── applypatch-msg.sample│ │ ├── commit-msg.sample│ │ ├── fsmonitor-watchman.sample│ │ ├── post-update.sample│ │ ├── pre-applypatch.sample│ │ ├── pre-commit.sample│ │ ├── pre-merge-commit.sample│ │ ├── prepare-commit-msg.sample│ │ ├── pre-push.sample│ │ ├── pre-rebase.sample│ │ ├── pre-receive.sample│ │ └── update.sample│ ├── index│ ├── info│ │ └── exclude│ ├── logs│ │ ├── HEAD│ │ └── refs│ │ └── heads│ │ └── master│ ├── objects│ │ ├── 53│ │ │ └── dd8b65afe02329eb73cbe142b9359ffd2c4c70│ │ ├── 68│ │ │ └── 31f81503989c192a10b47ecf48bc6bfe7c2cf4│ │ ├── 81│ │ │ └── aaa9093e1d32996c53766fa5f943e3ea6c79b0│ │ ├── e6│ │ │ └── 9de29bb2d1d6434b8b29ae775ad8c2e48c5391│ │ ├── info│ │ └── pack│ └── refs│ ├── heads│ │ └── master│ └── tags├── README.md└── start.sh16 directories, 27 filesalbert@home-pc MINGW64 /d/data/maingit/test/game (master)$ cat start.shcp README.md test.txt 与远端裸仓库建立关联1234567891011121314albert@home-pc MINGW64 /d/data/maingit/test/game (master)$ git remote add origin root@82.156.125.196:/data/repo/game.gitalbert@home-pc MINGW64 /d/data/maingit/test/game (master)$ git push -u origin masterEnumerating objects: 4, done.Counting objects: 100% (4/4), done.Delta compression using up to 4 threadsCompressing objects: 100% (2/2), done.Writing objects: 100% (4/4), 286 bytes | 286.00 KiB/s, done.Total 4 (delta 0), reused 0 (delta 0)To 82.156.125.196:/data/repo/game.git * [new branch] master -&gt; masterBranch 'master' set up to track remote branch 'master' from 'origin'. 至此自动化部署环境已建立，当本地 game 仓库推送更新时，远端服务器会自动更新部署 快速回顾文中主要命令收于此节，方便自己后期快速查找操作 服务端远程新建裸仓库 12cd /data/repogit init --bare game.git 本地库与远端库建立关联 1git remote add origin root@82.156.125.196:/data/repo/game.git 新建或修改 hooks 目录下 post-receive 脚本 123456DIR=/data/publish/gamegit --work-tree=$&#123;DIR&#125; clean -fdgit --work-tree=$&#123;DIR&#125; checkout --forcecd $&#123;DIR&#125;chmod 755 start.sh./start.sh 总结 裸仓库是一个只包含提交记录，没有工作目录的仓库，适合用来做服务端远程仓库 裸仓库不能直接在仓库中执行修改文件的git命令，可以在客户端克隆之后修改之后再进行提交 自动化部署利用了git服务器提供的脚本接口，当新的推送达到时会调用 post-receive 脚本 配置自动化部署环境时需要注意，如果没有配置ssh免密码登陆，需要在push代码的时候输入密码 另外自动化部署时要注意各个文件及目录的权限，因为要运行脚本，要保证推送用户有足够的运行权限 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 每个人都有自己的选择，很多看似突如其来的决定，往往都是深思熟虑后的结果，每个人在自己的旅途中不断的分类、选择、分类、选择，无法逃离的坚持到最后一刻~ 2022-6-12 20:19:30]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>bare</tag>
        <tag>clone</tag>
        <tag>post_receive</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下常用的查找命令find、which、grep]]></title>
    <url>%2Fblog%2F2022%2F06%2F05%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%B8%B8%E7%94%A8%E7%9A%84%E6%9F%A5%E6%89%BE%E5%91%BD%E4%BB%A4find%E3%80%81which%E3%80%81grep%2F</url>
    <content type="text"><![CDATA[前言查找是运维工作的很重要的一部分，不管是文件查找，还是内容查找，在日常开发维护过程中都常常用到，本文把一些日常用到的查找命令总结到一起，通过对比来学习异同点，进而达到 增强记忆的目的。 本文只是想对常用命令进行一个罗列，并不会对每个命令进行详细的解释，如果想看更详细的用法，直接查询 man 手册是一个不错的选择，我们接下来会说到通用文件查找的 find 命令，快速定位文件的 locate 命令，仅用于搜索程序和文档的 whereis 命令，用于查找系统命令的 which 命令，最后是用于文件内容查找的 grep 命令。 find命令格式1find [指定目录] 搜索条件 [指定动作] 具体示例 全局查找tendis文件所在目录 12[root@VM-0-3-centos ~]# find / -name tendis/root/tendis 当前目录按指定名找到tendis并打印文件信息 12[root@VM-0-3-centos ~]# find . -name tendis -ls918146 4 drwxr-xr-x 4 root root 4096 May 1 2021 ./tendis 全局查找test开头的文件 12345678910[root@VM-0-3-centos ~]# find / -name 'test*'/boot/grub2/i386-pc/testspeed.mod/boot/grub2/i386-pc/test.mod/boot/grub2/i386-pc/test_blockarg.mod/boot/grub2/i386-pc/testload.mod/usr/lib/modules/3.10.0-1127.19.1.el7.x86_64/kernel/drivers/ntb/test/usr/lib/python2.7/site-packages/jinja2/tests.pyc/usr/lib/python2.7/site-packages/jinja2/tests.py/usr/lib/python2.7/site-packages/jinja2/testsuite... 当前目录下查找所有的目录 12345678[root@VM-0-3-centos ~]# find . -type d../tendis./tendis/scripts./tendis/bin./tendis/bin/deps./extundelete-0.2.4... 查找大于1M的文件 123456789[root@VM-0-3-centos ~]# find . -size +1M -ls918152 164712 -rwxr-xr-x 1 root root 168663910 Dec 17 2020 ./tendis/bin/tendisplus_static918151 18036 -rwxr-xr-x 1 root root 18464898 Dec 17 2020 ./tendis/bin/binlog_tool918148 2576 -rwxr-xr-x 1 root root 2635759 Dec 17 2020 ./tendis/bin/redis-cli918150 10896 -rwxr-xr-x 1 root root 11154937 Dec 17 2020 ./tendis/bin/deps/libstdc++.so.6918145 165076 -rwxr-xr-x 1 root root 169036319 Dec 17 2020 ./tendis/bin/tendisplus1311915 1860 -rw-r--r-- 1 root root 1904320 Nov 28 2021 ./extundelete-0.2.4/src/extundelete-extundelete.o1311926 1296 -rwxr-xr-x 1 root root 1323360 Nov 28 2021 ./extundelete-0.2.4/src/extundelete... 查找10分钟内修改的普通文件 123[root@VM-0-3-centos ~]# find . -type f -mmin -10./b.txt./.bash_history locatelocate 也是用来查找文件的，只不过它不是通过文件系统来找，而是通过自己的数据库来找，默认在 /var/lib/mlocate/mlocate.db，每天自动更新一次，所以查不到最新变动的文件，可以手动通过 updatedb 来更新数据库（我查了一下才2M很小的）。 命令格式1locate [选项] [匹配串] 具体示例 查找家目录下包含te的文件 123456789101112131415[root@VM-0-3-centos ~]# locate ~/te/root/tendis/root/test.iso/root/tendis/bin/root/tendis/file.xml/root/tendis/scripts/root/tendis/bin/binlog_tool/root/tendis/bin/deps/root/tendis/bin/redis-cli/root/tendis/bin/tendisplus/root/tendis/bin/tendisplus_static/root/tendis/bin/deps/libstdc++.so.6/root/tendis/scripts/start.sh/root/tendis/scripts/stop.sh/root/tendis/scripts/tendisplus.conf 不区分大小写查找 12345678910111213141516[root@VM-0-3-centos ~]# locate -i ~/tE/root/TE.txt/root/tendis/root/test.iso/root/tendis/bin/root/tendis/file.xml/root/tendis/scripts/root/tendis/bin/binlog_tool/root/tendis/bin/deps/root/tendis/bin/redis-cli/root/tendis/bin/tendisplus/root/tendis/bin/tendisplus_static/root/tendis/bin/deps/libstdc++.so.6/root/tendis/scripts/start.sh/root/tendis/scripts/stop.sh/root/tendis/scripts/tendisplus.conf whereiswhereis 只能用于二进制文件、man手册和源代码文件的搜索，默认返回所有信息。 命令格式1whereis [-bmsBMS] 匹配串 具体示例 查找二进制程序 ls 12[root@VM-0-3-centos ~]# whereis -b lsls: /usr/bin/ls 查找 grep 所有信息 12[root@VM-0-3-centos ~]# whereis grepgrep: /usr/bin/grep /usr/share/man/man1/grep.1.gz whichwhich 是在 PATH 变量中找到第一个匹配的命令并返回，这能帮助我们确认多个相同命令时用的是哪一个。 命令格式1which [选项] 匹配串 具体示例 打印当前使用的gcc程序，打印所有可加 -a 参数 1234[root@VM-0-3-centos ~]# which gcc/usr/bin/gcc[root@VM-0-3-centos ~]# which gcc -a/usr/bin/gcc grepgrep 不算是单纯查找文件的命令，更多的是用于从文件中过滤指定内容。 命令格式1grep [选项] 匹配串 [指定文件] 具体示例 过滤包含指定字符串的行 1234[root@VM-0-3-centos ~]# grep "which" w.txt which - shows the full path of (shell) commands. which [options] [--] programname [...] This man page is generated from the file which.texinfo. 显示匹配行之后的2行 123456789101112[root@VM-0-3-centos ~]# grep "which" w.txt -A 2 which - shows the full path of (shell) commands.SYNOPSIS which [options] [--] programname [...]DESCRIPTION-- This man page is generated from the file which.texinfo.OPTIONS-- 当前目录下查找包含 wonderful 的文件 123[root@VM-0-3-centos ~]# grep -r "wonderful" ../.rediscli_history:hset life family wonderful./.bash_history:grep -r "wonderful" . | head 总结 find命令查找文件最全面 find . -name tendis -ls locate 命令查找最快，locate -i /etc/redis，可用 updatedb 命令更新数据库 whereis 命令可以查找二进制、man手册、源码，whereis -b grep which 可以从PATH路径下找到第一个匹配的二进制程序 grep 一个强大的过滤命令，也可用于找文件 grep -r &quot;wonderful&quot; . ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 幸福感从比较中诞生，亦从比较中消亡，并且与比较双方的关系紧密程度高度相关。我有一块糖，而你没有，我就很幸福，转身发现他有10块糖，然后嘴里的糖瞬间就不甜了~ 2022-6-5 23:21:58]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>find</tag>
        <tag>查找</tag>
        <tag>grep</tag>
        <tag>locate</tag>
        <tag>which</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（十一）：替换字符串中包含百分号%的子串]]></title>
    <url>%2Fblog%2F2022%2F05%2F15%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E6%9B%BF%E6%8D%A2%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E5%8C%85%E5%90%AB%E7%99%BE%E5%88%86%E5%8F%B7-%E7%9A%84%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[前言今天这篇总结是之前批处理替换字符串的延伸问题，同样来源于读者的提问，要处理的问题是被替换的子串中如果有百分号 % 要怎样替换，因为 % 在批处理脚本中也比较特殊，如果要想表示一个 % 字符，那么在给变量赋值时需要写成 %% 的样子，用两个表示一个，类似于进行转义，因为在批处理中， %开头的内容通常表示一个变量。 之前也处理过一些替换问题，列举如下，不过今天的问题需要新的解法。 《.bat批处理（六）：替换字符串中匹配的子串》 《.bat批处理（九）：替换带有等号=的字符串的子串》 问题示例 将字符串 https://blog.csdn.net/alb%3crtsh/articl%3c/d%3ctails/124760925 中的 %3c 替换成字母 e 问题比较明确，就是因为被替换的子串中包含了 % 导致常规的替换写法 %a:b=c% 的写法失效了。 解决方法既然子串中包含 % 会影响变量替换字符串的写法，我们就可以考虑换一种变量写法，用 ! 代替 %，看到这很多人应该反应过来了，那就是启用延迟变量扩展，这个我就不展开说了，之前总结过，可以看一下这篇文章《.bat批处理（八）：各种形式的变量%0、%i、%%i、var、%var%、!var!的含义和区别》。 示例代码1234567891011@echo offrem 将输入字符串中的%3c替换成字母eSET INPUT_PARAM=%1setlocal EnableDelayedExpansionecho -echo replace result is !INPUT_PARAM:%%3c=e!echo -pause 运行结果12345D:\data\bat&gt;replace%.bat https://blog.csdn.net/alb%3crtsh/articl%3c/d%3ctails/124760925-replace result is https://blog.csdn.net/albertsh/article/details/124760925-请按任意键继续. . . 总结 批处理脚本中的替换语法不仅可以写成 %a:b=c%，还可以写成 !a:b=c!的形式 批处理脚本执行机制是会按行执行，在执行之前会先预处理 开启延迟环境变量扩展setlocal enabledelayedexpansion，变量会在用到时再估值，不会预处理了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 公平不一定平等，平等也不代表公平。究竟什么是秩序，往往强者指定规则，弱者小心遵守，达到一个稳态，那就是秩序~ 2022-5-15 23:02:49]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下常用的网络命令ping、telnet、traceroute、tcpdump]]></title>
    <url>%2Fblog%2F2022%2F05%2F10%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%B8%B8%E7%94%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E5%91%BD%E4%BB%A4ping%E3%80%81telnet%E3%80%81traceroute%E3%80%81tcpdump%2F</url>
    <content type="text"><![CDATA[前言因特网(Internet)的前身是美国国防部高级研究计划局(ARPA)用于军事目的的通信网络，真的就是一个内部的工具出圈成了改变世界的事物。网这个词用的很形象也很贴切，如今的世界任何事情都离不开这个大网了，特别是之前炒的非常火爆的万物互联概念，更是把所有事物都挂在了“网”上，虽然这张大网某些节点偶尔会不太通畅，但这并不能阻断消息的往来。之前都在讲全球化，谁能想到短短几年“逆全球化”居然越来越热。 1983年1月1日被认为是互联网的官方生日，在此之前，各种计算机网络没有相互通信的标准方式。但从这一天开始，用于军事目的的阿帕网和国防数据网络正式定义为TCP/IP标准，建立了一种统一的通信协议，它允许不同网络上不同类型的计算机相互”交谈”，互联网就此诞生了。 网络如今人们已经离不开网络了，不管是每天工作，还是日常娱乐，网络无处不在，连两岁的宝宝看到动画片转圈圈都知道是网卡了，而进行应用开发和游戏开发的搬砖小哥儿们更是无人能逃脱网络问题，遇到网络问题不可怕，重启电脑、重启路由器，重启光猫，问题很可能就解决了。 但，总有那么几个问题是重启解决不了的，所以我们还是得掌握一些正常的工具来排查问题，那么接下来就列举几个常用的命令，真的很常见，大神请绕路~ 网络命令ping这应该是用的最多的一个网络命令了吧，“ping一下通不通”，这句话经常在日常开发调试中被提及，ping 命令常常用来测试，指定的两台机器之间的网络是否可以连通，命令格式如下： 1ping [OPTIONS] 域名或IP地址 一些常用的选项： -4：只使用 IPv4 -6：只使用 IPv6 -c count：发送多少个测试包之后停止，linux环境下如果不加这个参数会一直发包 -i interval：指定发包的的间隔时间 用常用网站测试一下： 1234567891011[root@VM-0-3-centos ~]# ping -c 5 -i 2 www.baidu.comPING www.a.shifen.com (110.242.68.3) 56(84) bytes of data.64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=1 ttl=251 time=10.3 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=2 ttl=251 time=10.2 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=3 ttl=251 time=10.2 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=4 ttl=251 time=10.2 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=5 ttl=251 time=10.2 ms--- www.a.shifen.com ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 8007msrtt min/avg/max/mdev = 10.217/10.243/10.311/0.132 ms 测试信息的含义： 数据 含义 -c 5 发送5个测试包 -i 2 每个测试包发送间隔2s www.baidu.com 指定的目标地址 www.a.shifen.com (110.242.68.3) 实际的目标主机的主机名和IP地址 56(84) bytes ICMP数据部分的大小56字节，加上8字节的ICMP头，则ICMP包大小为64字节，再加上20字节的IP头，IP包大小为84字节 64 bytes ICMP数据包大小 icmp_seq=1 ICMP包序号 ttl=251 剩余生存时间，生存时间是指数据包被路由器丢弃之前允许通过的网段数量，由发送主机设置的，以防止数据包在互联网络上永不终止的循环，每经过一个路由器至少将TTL减 1 time=10.3 ms 响应时间，这个时间越小，连接速度越快 — www.a.shifen.com ping statistics — 统计信息分割线，以下为统计信息 5 packets transmitted 发送数据包的数量 5 received 接收到的数据包的数量 0% packet loss 数据包的丢失率 time 8007ms 整个过程消耗的总时间 rtt min/avg/max/mdev = 10.217/10.243/10.311/0.132 ms 最小响应时间/平均响应时间/最大响应时间/响应时间的平均差 ping 命令全称 Packe InterNet Groper， 翻译为因特网包探索器，是一个用于测试网络连接状况的程序。该令会使用ICMP（Internet Control Message Protocol）传输协议，向特定的目标主机发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 ICMP协议是IP层的附属协议，是介于IP层和TCP层之间的协议，一般认为属于IP层协议，也就是网络层协议。ICMP用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。 注意：在Windows命令行中执行ping命令，默认只发送4个数据包就停止，这个行为和Linux环境下是不同的 telnettelnet是电信(telecommunications)和网络(networks)的联合缩写，是Internet远程登陆服务的标准协议，为用户提供了在本地计算机上完成远程主机工作的能力。在终端使用者的电脑上使用telnet程序，可以连接到服务器，直接在telnet程序中输入命令，这些命令会在服务器上运行，就像直接在服务器的控制台上输入一样，进而可以在本地就能控制远端的服务器。 telnet位于OSI模型的第7层，属于应用层上的一种协议，使用端口23，底层基于TCP协议。传输的数据和口令是明文形式，相对来说不安全，存在很大的安全隐患，不再用于通过公共网络访问网络设备和服务器。 现在使用更加安全的ssh代替telnet进行远程管理终端，ssh传输方式是以加密形式传输，并且功能比telnet更齐全，而telnet主要作用变成了查看某个端口是否可访问，常用命令格式如下： 1telnet IP或域名 端口 测试一下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@VM-0-3-centos ~]# telnet www.baidu.com 80Trying 110.242.68.4...Connected to www.baidu.com.Escape character is '^]'.^]telnet&gt; send ?ao Send Telnet Abort outputayt Send Telnet 'Are You There'brk Send Telnet Breakec Send Telnet Erase Characterel Send Telnet Erase Lineescape Send current escape characterga Send Telnet 'Go Ahead' sequenceip Send Telnet Interrupt Processnop Send Telnet 'No operation'eor Send Telnet 'End of Record'abort Send Telnet 'Abort Process'susp Send Telnet 'Suspend Process'eof Send Telnet End of File Charactersynch Perform Telnet 'Synch operation'getstatus Send request for STATUS? Display send optionstelnet&gt; ?Commands may be abbreviated. Commands are:close close current connectionlogout forcibly logout remote user and close the connectiondisplay display operating parametersmode try to enter line or character mode ('mode ?' for more)open connect to a sitequit exit telnetsend transmit special characters ('send ?' for more)set set operating parameters ('set ?' for more)unset unset operating parameters ('unset ?' for more)status print status informationtoggle toggle operating parameters ('toggle ?' for more)slc change state of special charaters ('slc ?' for more)z suspend telnet! invoke a subshellenviron change environment variables ('environ ?' for more)? print help informationtelnet&gt; qConnection closed.[root@VM-0-3-centos ~]# 一般情况下测试80端口是否可用，看到第3行的 Connected to www.baidu.com. 就够了，如果想使用telnet工具发送数据，可以按组合键 Ctrl+]进入输入命令的模式，按字母 q 可以退出。 注意：在Windows环境下如果端口可以连通会显示一个无任何信息的黑框，按组合键 Ctrl+]才会有反应，如果端口不通则会提示超时 nc上一小节说过，telnet是基于TCP的应用层协议，所以只能检测TCP端口是否正常，如果想检测一个UDP端口是否可用，使用telnet命令是办不到的，这时候就可以使用 nc 命令来实现，详细的用法可以参考之前的文章《网络工具nc的常见功能和用法》，下面只列举一下检查UDP端口的命令参数： 12albert@home-pc:~$ nc -nvuz 82.156.125.169 666Connection to 82.156.125.169 666 port [udp/*] succeeded! traceroutetraceroute是一个路由跟踪命令，用于追踪数据包在网络上的传输时的全部路径，IPv4是发送的探测包大小是60字节，而IPv6默认是80字节，这个可以通过查询 man 手册来查询。 通过traceroute我们可以探测出数据从当前计算机到另一台主机是走了什么样网络路径，不过相同的数据包每次由从相同的出发点到相同目的地走的路径可能并相同，但大部分时候所走的路由是一样的，所以可以帮助我们了解网络状况。 traceroute的原理是利用逐步设置 ttl 参数进行参数，这个参数全称time-to-live, 指当前数据包在网络中存在的是时间，而时间不是我们平常所说的时间，指的是数据包在网络环境中最多可以被中转的次数，每经过一个路由设备就要减1，减到0则说明数据包超时，要给原地址一个包含自身信息的回应。 既然这样就可以利用 ttl 的特点来探测出路径，先将第一个数据包的 ttl 设置为 1，到达第一个路由设备后 ttl 减为0，返回给发送数据的起始设备，这样我们就找到了网络路径中的第一个点。然后将下一个数据包的 ttl 设置为2，就可以探测出路径上的第二个路由设备，直到找到目的地址，整个探测过程结束。 1traceroute [OPTIONS] IP或域名 一些常用的选项： -4：强制使用 IPv4 追踪，默认自动选择 -6：强制使用 IPv6 追踪，默认自动选择 -I：使用 ICMP 回应进行探测 -T：使用 TCP 同步进行探测 -m max_ttl：指定 time-to-live 的最大值，默认是30 测试如下： 12345678910111213141516[root@VM-0-3-centos ~]# traceroute www.baidu.com -Ttraceroute to www.baidu.com (110.242.68.4), 30 hops max, 60 byte packets 1 9.61.26.129 (9.61.26.129) 0.464 ms 0.626 ms 0.788 ms 2 9.61.119.152 (9.61.119.152) 0.843 ms 1.085 ms 1.282 ms 3 * * * 4 10.200.44.221 (10.200.44.221) 1.130 ms 10.162.5.252 (10.162.5.252) 1.089 ms 10.200.44.205 (10.200.44.205) 1.385 ms 5 61.49.142.157 (61.49.142.157) 2.842 ms 61.49.142.153 (61.49.142.153) 0.923 ms 61.49.142.149 (61.49.142.149) 1.467 ms 6 123.126.0.217 (123.126.0.217) 2.031 ms 61.148.7.157 (61.148.7.157) 2.057 ms 202.96.13.5 (202.96.13.5) 5.015 ms 7 125.33.186.17 (125.33.186.17) 2.499 ms 124.65.194.161 (124.65.194.161) 6.422 ms 124.65.194.157 (124.65.194.157) 3.125 ms 8 * * * 9 110.242.66.190 (110.242.66.190) 10.030 ms 110.242.66.166 (110.242.66.166) 10.261 ms 110.242.66.162 (110.242.66.162) 10.751 ms10 * * *11 * * *12 * * *13 * * *14 * * 110.242.68.4 (110.242.68.4) 9.245 ms 从上面的测试结果来看，每一行都是一个记录，每个纪录表示一跳，从我的机器到 www.baidu.com 一共经过了14个设备才到达，不过并不是每次探测的结果都一样，这个是会变化的，可以看到每行有3个以ms为单位时间，是因为该命令每次默认发送3个探测数据包，这3个时间就是网关响应后返回的时间。 另外在输出信息中部分节点显示 * * *，关于这种现象，我找到几种说法，遇到了需要针对具体情况进行分析： 设备防火墙封掉了ICMP的返回信息，我们得不到什么相关的数据包返回数据 由于回送TTL超时信息的时候，CPU生成这个返回包必须被打断，为保证其它工作的正常进行，每隔一秒才会处理traceroute，所以可能会看到中间一路 * * *，但却看得到最后的destination. 这时往往是路由设备CPU太忙或者中间路由器不回送TTL超时包的原因（感觉不太靠谱）。 虚拟机nat路由器，默认丢弃port&gt;32767的包，导致看不到中间路由的信息 注意：在Windows环境下，相同功能的命令为Tracert，而不是traceroute tcpdumptcpdump命令是一个网络嗅探，它可以打印所有经过该设备网卡的数据包的信息，也可以使用-w选项将数据包保存到文件中，方便以后分析，功能与Windows平台上的Wiresh相同，生成的数据文件也可以传送到Windows平台，使用 Wireshark 软件进行分析。 命令格式非常简单，必要时加一些参数就可以： 1tcpdump [OPTIONS] 一些常用的选项： -c count：接受count个数据包之后退出 -i：指定网卡 -v：打印较详细的信息用于分析 -w：结果写到文件之中 tcp：过滤出tcp数据 port xxxx：仅打印指定端口的数据 测试一下： 1234567891011121314151617[root@VM-0-3-centos ~]# tcpdump -c 6 -vtcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes10:30:45.344349 IP (tos 0x10, ttl 64, id 47149, offset 0, flags [DF], proto TCP (6), length 188) VM-0-3-centos.ssh &gt; 68.128.126.124.broad.bjtelecom.net.6183: Flags [P.], cksum 0x9127 (correct), seq 14744777:14744925, ack 1170924064, win 340, length 14810:30:45.344782 IP (tos 0x0, ttl 64, id 39263, offset 0, flags [DF], proto UDP (17), length 73) VM-0-3-centos.35281 &gt; 183.60.83.19.domain: 48669+ PTR? 68.128.126.124.in-addr.arpa. (45)10:30:45.344826 IP (tos 0x10, ttl 64, id 47150, offset 0, flags [DF], proto TCP (6), length 188) VM-0-3-centos.ssh &gt; 68.128.126.124.broad.bjtelecom.net.6183: Flags [P.], cksum 0x5e8b (correct), seq 148:296, ack 1, win 340, length 14810:30:45.353844 IP (tos 0xa0, ttl 251, id 62658, offset 0, flags [DF], proto TCP (6), length 40) 68.128.126.124.broad.bjtelecom.net.6183 &gt; VM-0-3-centos.ssh: Flags [.], cksum 0x6909 (correct), ack 296, win 258, length 010:30:45.424218 IP (tos 0x0, ttl 56, id 9062, offset 0, flags [DF], proto UDP (17), length 121) 183.60.83.19.domain &gt; VM-0-3-centos.35281: 48669 1/0/0 68.128.126.124.in-addr.arpa. PTR 68.128.126.124.broad.bjtelecom.net. (93)10:30:45.424327 IP (tos 0x0, ttl 64, id 17697, offset 0, flags [DF], proto UDP (17), length 68) VM-0-3-centos.40741 &gt; 183.60.82.98.domain: 15078+ PTR? 3.0.10.10.in-addr.arpa. (40)6 packets captured13 packets received by filter0 packets dropped by kernel 因为不能一直盯着某个设备的网络情况，所以会将数据打印到文件中便于日后分析，常常将tcpdump命令后台运行，比如可以写成下面这样： 1nohup tcpdump -i eth0 tcp port 8080 -v -w dm8080.cap &amp; 在Windows平台一般就直接使用Wiresh软件啦，非常方便 总结 设备一旦接入网络很难“独善其身” ping 命令可以测试目的IP是否可达 ping www.baidu.com telnet 命令可以测试指定的tcp端口是否可达 telnet 110.242.68.4 80 nc 命令可以测试指定udp端口是否可达 nc -nvuz 110.242.68.4 666 tracerout 命令可以探测网络路径 traceroute www.baidu.com tcpdump 命令可以收集所有经过网卡的数据包 tcpdump -i eth0 tcp port 8080 -v ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== “王侯将相，宁有种乎？”讲的是权利；“天下兴亡，匹夫有责！”说的是义务；权力和义务的辩证关系体现于此，二者统一，不可分离~ 2022-5-13 20:59:38]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>ping</tag>
        <tag>telnet</tag>
        <tag>traceroute</tag>
        <tag>tcpdump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11新式洗牌std::shuffle与老式洗牌函数std::random_shuffle的区别]]></title>
    <url>%2Fblog%2F2022%2F05%2F03%2FC-11%E6%96%B0%E5%BC%8F%E6%B4%97%E7%89%8Cstd-shuffle%E4%B8%8E%E8%80%81%E5%BC%8F%E6%B4%97%E7%89%8C%E5%87%BD%E6%95%B0std-random-shuffle%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言洗牌算法是项目开发中常用的一种算法，它和随机数有着密不可分的关系，比如我们从报名参与活动的前10个用户中选取一个人发放幸运奖，这时可以从[1, 10] 范围内随机一个数来确定幸运儿；如果是抽取两个人呢？那就随机两次！是的，确实可以这样做，但是随着随机次数的增多，后面随机的数字很可能和前面一样，这时就要重复随机才能解决。 想想现实生活中我们可以怎么做，取红桃A-红桃10一共10张扑克牌，然后把它们随机洗一洗牌，如果需要取3个幸运儿，那么只需取扑克牌的前三张就可以了，这样很容易就取到了不重复的3个数字，当然你从中间随机抽取也是可以的，这样操作在逻辑实现上要复杂一点点。 洗牌算法其实洗牌利用的是一个排列的概念，学过排列组合的知识以后我们很清楚，n个元素排列的种类数一共是: $$A^n_n$$ 也就是n!，这是个恐怖的数字，比如打印N个数字的全排列，它的是时间复杂度就是O(N!)，这个谁也没办法优化，因为打印这些排列情况就需要O(N!)这么多的时间，而洗牌就是保证从这些元素组成的全排列中等概率的选取一种排列。 把所有的排列情况列举出来，然后从中选择一个所需时间是O(N!)，这显然是不现实的，所以很多大神们进行了优化，出现了多种洗牌算法，下面我只列举一种比较好理解的 Knuth-Durstenfeld Shuffle 算法。 改洗牌算法可简单表述为：一个拥有n个元素的初始序列，将最后一个数和该序列的前 n 个数中的随机一个数进行交换（如果随机结果是和第n个数交换，相当于没换），然后倒数第二个数和该序列的前 n - 1 个数中的随机一个数进行交换，以此类推，直到将该序列第一个数操作完，就完成了洗牌，该算法保证了每个元素在每个位置的概率都是相等的，时间复杂度为O(N)。 举个例子就像下面这样： 初始序列是 A、B、C、D、E、F，为了便于和刚才的算法思路对应描述，索引从1开始 第一轮从1-6个位置中随机一个和最后的 F 交换，假如随机到位置3，也就是和 C 交换，结果为： A B F D E C 概率是P=1/6，也就是随机一个数字的概率 第二轮从1-5个位置中随机一个和倒数第二个元素 E 交换，假如随机到的是位置2，也就是和 B 交换，结果为： A E F D B C 概率是P=(5/6)*(1/5)=1/6，为什么这么算呢？要想和 B 交换必须第一轮随机不到B才可以，所以要在前面乘以 5/6 第三轮从1-2个位置中随机一个和倒数第二个元素 D 交换，假如随机到的还是位置2，也就是和 E 交换，结果为 A D F E B C 概率是P=(5/6)*(4/5)*(1/4)=1/6，有了第二轮这个就应该明白了吧 依次类推，直到操作完第五次随机交换，整个洗牌算法也就完成了，伪代码也就几行 1234for (int i = vec.size() - 1; i &gt; 0; i--)&#123; std::swap(vec[i], vec[std::rand() % (i + 1)]);&#125; std::random_shuffle使用这个函数需要引用头文件 &lt;algorithm&gt;，共有以下几个重载函数： 12345678910template&lt; class RandomIt &gt;void random_shuffle( RandomIt first, RandomIt last ); //(deprecated in C++14)(removed in C++17)template&lt; class RandomIt, class RandomFunc &gt;void random_shuffle( RandomIt first, RandomIt last, RandomFunc&amp; r ); //(until C++11) //(deprecated in C++14)(removed in C++17)template&lt; class RandomIt, class RandomFunc &gt;void random_shuffle( RandomIt first, RandomIt last, RandomFunc&amp;&amp; r ); //(since C++11) //(deprecated in C++14)(removed in C++17) 从文档来看 std::random_shuffle 这个函数的实现在C++14标准中已经不推荐使用，在C++17中已经被移除了，函数定义如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * @brief Randomly shuffle the elements of a sequence. * @ingroup mutating_algorithms * @param __first A forward iterator. * @param __last A forward iterator. * @return Nothing. * * Reorder the elements in the range @p [__first,__last) using a random * distribution, so that every possible ordering of the sequence is * equally likely. */ template&lt;typename _RandomAccessIterator&gt; inline void random_shuffle(_RandomAccessIterator __first, _RandomAccessIterator __last) &#123; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt; _RandomAccessIterator&gt;) __glibcxx_requires_valid_range(__first, __last); if (__first != __last) for (_RandomAccessIterator __i = __first + 1; __i != __last; ++__i) &#123; // XXX rand() % N is not uniformly distributed _RandomAccessIterator __j = __first + std::rand() % ((__i - __first) + 1); if (__i != __j) std::iter_swap(__i, __j); &#125; &#125; /** * @brief Shuffle the elements of a sequence using a random number * generator. * @ingroup mutating_algorithms * @param __first A forward iterator. * @param __last A forward iterator. * @param __rand The RNG functor or function. * @return Nothing. * * Reorders the elements in the range @p [__first,__last) using @p __rand to * provide a random distribution. Calling @p __rand(N) for a positive * integer @p N should return a randomly chosen integer from the * range [0,N). */ template&lt;typename _RandomAccessIterator, typename _RandomNumberGenerator&gt; void random_shuffle(_RandomAccessIterator __first, _RandomAccessIterator __last,#if __cplusplus &gt;= 201103L _RandomNumberGenerator&amp;&amp; __rand)#else _RandomNumberGenerator&amp; __rand)#endif &#123; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt; _RandomAccessIterator&gt;) __glibcxx_requires_valid_range(__first, __last); if (__first == __last) return; for (_RandomAccessIterator __i = __first + 1; __i != __last; ++__i) &#123; _RandomAccessIterator __j = __first + __rand((__i - __first) + 1); if (__i != __j) std::iter_swap(__i, __j); &#125; &#125; 以上函数实现来源于文件 /usr/include/c++/5/bits/stl_algo.h，看源码时发现一个问题，原来标准库中的代码也是空格和Tab混用，复制过来的时候我还专门整理了一下。 第一个仅两个参数的函数中首先验证了迭代器的类型和范围的有效性，同时使用了 std::rand() 函数来随机选择了一个需要交换的元素，而拥有三个参数的函数逻辑几乎一样，只是使用了自定义传入的随机函数来选择需要交换的元素，所以洗牌算法的核心逻辑就是这个随机函数。 rand 和 srand这两个是C标准函数，在C++中被放在头文件 &lt;cstdlib&gt; 之中，搜索到的函数声明如下： 123456__BEGIN_NAMESPACE_STD/* Return a random integer between 0 and RAND_MAX inclusive. */extern int rand (void) __THROW;/* Seed the random number generator with the given number. */extern void srand (unsigned int __seed) __THROW;__END_NAMESPACE_STD 其中 std::rand() 是用于返回一个介于[0, RAND_MAX] 范围的伪随机整型值，RAND_MAX 的值最小为 32767，也就是有符号short的最大值，我查到的版本库中的值是2147483647，即有符号int的最大值。 std::srand() 的作用是为 st::rand() 这个伪随机数生成器设置种子，如果在调用 std::srand() 之前使用了 std::rand()，种子默认为1，相当于调用了 std::srand(1)，rand通常不是线程安全的函数，依赖于具体的实现。 另外你可能还见过 random 和 srandom 等函数，他们通常是另一个标准（BSD）的随机函数，比如下面这段描述： 123456789/* These are the functions that actually do things. The `random', `srandom', `initstate' and `setstate' functions are those from BSD Unices. The `rand' and `srand' functions are required by the ANSI standard. We provide both interfaces to the same random number generator. *//* Return a random long integer between 0 and RAND_MAX inclusive. */extern long int random (void) __THROW;/* Seed the random number generator with the given number. */extern void srandom (unsigned int __seed) __THROW; 如果是在 POSIX 平台你可能还会遇到 rand_r(int *seed) 函数。 需要注意的是， std::rand() 生成的是一个伪随机序列，如果随机种子相同，则得到的序列也是相同的，这也是 std::rand 不建议使用的原因，建议是使用C++11随机数生成工具来替换它。 伪随机序列也并不是“一无是处”，两个进程可以通过设置相同的随机数种子来产生相同的序列，比如可以用于服务器和客户端做帧同步时产生随机数，这样的随机数产生是同步可控的。 下面举个 std::rand() 使用的例子 12345678910111213141516#include &lt;iostream&gt;#include &lt;cstdlib&gt;int main()&#123; std::srand(1); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; std::srand(1); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; std::srand(1); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 12345albert@home-pc:/mnt/d/data/cpp/testrandom$ g++ testrandom.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testrandom$ ./a.out180428938318042893831804289383 我们可以看到因为随机种子相同，生成的随机数都是同一个，为了使的生成的序列更随机，通常使用当前时间戳 std::time(nullptr) 作为随机种子，然后再生成随机序列： 1234567891011121314151617#include &lt;iostream&gt;#include &lt;cstdlib&gt;#include &lt;ctime&gt;int main()&#123; std::srand(std::time(nullptr)); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; std::srand(std::time(nullptr)); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; std::srand(std::time(nullptr)); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 12345albert@home-pc:/mnt/d/data/cpp/testrandom$ g++ testrandom.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testrandom$ ./a.out177757254117775725411777572541 怎么还是相同的呢？那是因为 std::time(nullptr) 函数返回的时间戳单位是秒，在一秒中内的时间种子是相同的，所以返回的序列也是相同的，通常的使用方法是在程序启动时设置一次时间种子就可以了，并不需要每次都进行设置，而 random_shuffle 中使用了 std::rand() 函数，如果不手动设置时间种子，每次同一时间洗同一副牌，得到的结果也是相同的，所以这也是random_shuffle被后续版本移除的一个原因。 随机数生成器和分布器random是C++11提供的一个头文件，其中包含多个随机数生成工具，可以使用生成器和分布器的组合产生随机数，其中包含随机数生成器和分布器的多个类实现，分为以下两种： Uniform random bit generators (URBGs)：均匀随机位生成器，也就是生成均匀分布随机数的对象，可以生成伪随机序列，也可生成真正的随机数序列Random number distributions：随机数分布器，用于将URBGs产生的随机数转换为某种特定数学概率分布的序列，如均匀分布、正态分布、泊松分布等 常见的生成器： linear_congruential_engine: 线性同余生成算法，是最常用也是速度最快的，随机效果一般 mersenne_twister_engine: 梅森旋转算法，随机效果最好 subtract_with_carry_engine: 滞后Fibonacci算法 常见的适配器，我理解的它的作用是生成器的二次加工厂，对生成器结果进行特定操作 discard_block_engine: 丢弃一些数 independent_bits_engine: 将序列打包成指定位数的块 shuffle_order_engine: 调整序列顺序 预定义的随机数生成器，利用通用生成器和适配器组合出的流行特定生成器： minstd_rand minstd_rand0 mt19937: mt是因为这个伪随机数产生器基于Mersenne Twister算法，19937来源于产生随的机数的周期长可达到2^19937-1。 mt19937_64 ranlux24_base ranlux48_base ranlux24 ranlux48 knuth_b default_random_engine: 编译器可以自行实现 以上随机数引擎需要一个整型参数作为种子，对于给定的随机数种子，伪随机数生成器总会生成相同的序列，这在测试的时候是相当有用的。而在实际使用时，需要设置随机树作为种子来产出不同的随机数，推荐使用 std::random_device 的值作为随机数种子。 std::random_device 是一个使用硬件熵源的非确定性随机数发生器，不可预测。 常见的分布器： uniform_int_distribution: 均匀离散分布 uniform_real_distribution: 均匀实数分布 bernoulli_distribution: 伯努利分布 binomial_distribution: 二项式分布 geometric_distribution: 几何分布 negative_binomial_distribution: 负二项式分布 poisson_distribution: 泊松分布 exponential_distribution: 指数分布 gamma_distribution: 伽玛分布 weibull_distribution: 威布尔分布 extreme_value_distribution: 极值分配 normal_distribution: 正态分布 lognormal_distribution: 对数正态分布 chi_squared_distribution: 卡方分布 cauchy_distribution: 柯西分布 fisher_f_distribution: Fisher F分布 student_t_distribution: 学生T分布 discrete_distribution: 离散分布 piecewise_constant_distribution: 分段常数分布 piecewise_linear_distribution: 分段线性分布 下面举个生成器和分布器组合生成随机常用例子，以下为模拟掷骰子生成点数的实现： 12345678910111213#include &lt;iostream&gt;#include &lt;random&gt;int main()&#123; std::mt19937 gen(std::random_device&#123;&#125;()); std::uniform_int_distribution&lt;&gt; dist(1, 6); for (int i = 0; i &lt; 10; ++i) std::cout &lt;&lt; dist(gen) &lt;&lt; std::endl; return 0;&#125; 编译运行结果如下： 123456789101112albert@home-pc:/mnt/d/data/cpp/testrandom$ g++ testrandom.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testrandom$ ./a.out3241541134 std::shuffle终于又转回来了，去随机数那一块儿溜了半天，终于回到了洗牌函数，这个函数是C++11版本才加入的，函数定义如下： 12345678910111213141516171819202122232425262728293031323334353637/** * @brief Shuffle the elements of a sequence using a uniform random * number generator. * @ingroup mutating_algorithms * @param __first A forward iterator. * @param __last A forward iterator. * @param __g A UniformRandomNumberGenerator (26.5.1.3). * @return Nothing. * * Reorders the elements in the range @p [__first,__last) using @p __g to * provide random numbers.*/template&lt;typename _RandomAccessIterator, typename _UniformRandomNumberGenerator&gt; void shuffle(_RandomAccessIterator __first, _RandomAccessIterator __last, _UniformRandomNumberGenerator&amp;&amp; __g) &#123; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt; _RandomAccessIterator&gt;) __glibcxx_requires_valid_range(__first, __last); if (__first == __last) return; typedef typename iterator_traits&lt;_RandomAccessIterator&gt;::difference_type _DistanceType; typedef typename std::make_unsigned&lt;_DistanceType&gt;::type __ud_type; typedef typename std::uniform_int_distribution&lt;__ud_type&gt; __distr_type; typedef typename __distr_type::param_type __p_type; __distr_type __d; for (_RandomAccessIterator __i = __first + 1; __i != __last; ++__i) std::iter_swap(__i, __first + __d(__g, __p_type(0, __i - __first))); &#125; 这种实现和之前 std::random_shuffle 函数实现很类似，只是随机数部分有些不同，它的第3个参数需要的是一个均匀随机数生成器URBGs，一个常见的使用方法如下： 1234567891011121314151617#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;iterator&gt;#include &lt;random&gt;#include &lt;vector&gt;int main()&#123; std::vector&lt;int&gt; vec&#123;1, 2, 3, 4, 5, 6&#125;; std::mt19937 gen(std::random_device&#123;&#125;()); std::shuffle(vec.begin(), vec.end(), gen); std::copy(vec.begin(), vec.end(), std::ostream_iterator&lt;int&gt;(std::cout, " ")); return 0;&#125; 编译后运行结果如下： 123albert@home-pc:/mnt/d/data/cpp/testrandom$ g++ testrandom.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testrandom$ ./a.out3 5 1 2 6 4 randint结尾了顺便说一下偶然看到的一个实验性函数 std::experimental::randint，用于生成指定范围内的一个随机数，目前还没有进入标准，不过看起来使用很方便了，后续有可能被纳入标准吧，贴一下 cppreference 上的例子 std::experimental::randint 如下： 12345678#include &lt;iostream&gt;#include &lt;experimental/random&gt;int main()&#123; int random_number = std::experimental::randint(100, 999); std::cout &lt;&lt; "random 3-digit number: " &lt;&lt; random_number &lt;&lt; '\n';&#125; 总结 std::random_shuffle 可以只传递一个待洗牌的区间，函数内会使用默认的 std::rand 函数来完成随机元素的选择，依赖全局状态 std::random_shuffle 也可以传入自定义的随机函数，不过这个函数在C++14表中已经不建议时使用了，在C++17标准中已经被移除 std::shuffle 是C++11标准添加的，也是推荐使用的洗牌函数，它的第三个参数需要传递一个均匀随机数生成器对象 C++11中的&lt;random&gt;头文件中提供了很多生成随机数的工具，需要搭配生成器和分布器来使用 mt19937 名字看起来有点怪，但它是常用的生成器，mt表示它基于Mersenne Twister算法，19937源于产生随的机数的周期长可达到2^19937-1 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当被误解时，解释或者争论都是没有用的，有些事情就解释不清楚，或者根本无法解释，甚至没有人会听你解释，想一想，真的什么也做不了，就像一句名言说的，你永远叫不醒一个装睡的人，那个故意误解你的人又怎会听你解释~ 2022-5-3 21:02:56]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>shuffle</tag>
        <tag>洗牌</tag>
        <tag>随机数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[恶搞一下std::forward函数]]></title>
    <url>%2Fblog%2F2022%2F04%2F20%2F%E6%81%B6%E6%90%9E%E4%B8%80%E4%B8%8Bstd-forward%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言关于 std::forward 的用法在之前的文章 《C++11中std::move和std::forward到底干了啥》已经总结过了，它被称为完美转发函数，用于函数模板中完成参数转发任务，当形参为左值引用时把它转发成左值，而在形参成为右值引用时把它转发成右值，依靠了引用折叠规则和 std::remove_reference 模板。 前段时间看到std::forward的源代码时突然有发现有些疑问，后来弄明白了决定换个花样试一试，不过在“恶搞”这个函数之前，先来看一看使用模板的规则，我们以模板函数为例，看看模板是怎么用的。 函数模板12345template&lt;class T&gt;T Add(T a, T b)&#123; return a + b;&#125; 这是一个非常简单的模板函数，直接传入参数就可以调用这个函数做加法运算，就像下面这样： 12345678910111213141516#include &lt;iostream&gt;template&lt;class T&gt;T Add(T a, T b)&#123; return a + b;&#125;int main(void)&#123; std::cout &lt;&lt; Add(2020, 2) &lt;&lt; std::endl; std::cout &lt;&lt; Add(3.0, 2.1) &lt;&lt; std::endl; std::cout &lt;&lt; Add(std::string("happy"), std::string(" holiday")) &lt;&lt; std::endl; return 0;&#125; 函数的运行结果如下： 20225.1happyholiday 我们在调用模板函数时虽然没有指定模板 T 的类型，但是编译器会自动推导，分别生成以下三个函数： 1234567891011121314int Add(int a, int b)&#123; return a + b;&#125;double Add(double a, double b)&#123; return a + b;&#125;std::string Add(std::string a, std::string b)&#123; return a + b;&#125; 但是当我们采用以下的方式调用函数的时候就会出现编译错误 1std::cout &lt;&lt; Add(3, 2.1) &lt;&lt; std::endl; 123456789101112albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testt.cpp --std=c++11testt.cpp: In function ‘int main()’:testt.cpp:13:28: error: no matching function for call to ‘Add(int, double)’ std::cout &lt;&lt; Add(3, 2.1) &lt;&lt; std::endl; ^testt.cpp:5:3: note: candidate: template&lt;class T&gt; T Add(T, T) T Add(T a, T b) ^testt.cpp:5:3: note: template argument deduction/substitution failed:testt.cpp:13:28: note: deduced conflicting types for parameter ‘T’ (‘int’ and ‘double’) std::cout &lt;&lt; Add(3, 2.1) &lt;&lt; std::endl; ^ 编译器给出的错误很明显，那就是没有匹配 Add(int, double) 的函数生成，这个模板只提供了一个类型参数，遇到这种情况应该怎么办呢？我们知道 int 可以隐式转换成 double 类型，那就让它默认生成一个类型为 double 的模板函数 Add(int, double) 就可以了，所以这种情况下把调用函数写成下面这样就可以成功编译了： 1std::cout &lt;&lt; Add&lt;double&gt;(3, 2.1) &lt;&lt; std::endl; 通过这个例子我们发现有些情况下，这个模板函数的参数类型必须显式传递，接下来我们再来熟悉一下 std::forward 函数。 forwawrd 函数定义先来复习一下函数的定义： 12345678910111213141516171819202122232425/** * @brief Forward an lvalue. * @return The parameter cast to the specified type. * * This function is used to implement "perfect forwarding". */template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp; __t) noexcept &#123; return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125;/** * @brief Forward an rvalue. * @return The parameter cast to the specified type. * * This function is used to implement "perfect forwarding". */template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept &#123; static_assert(!std::is_lvalue_reference&lt;_Tp&gt;::value, "template argument" " substituting _Tp is an lvalue reference type"); return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125; 以上两个模板函数就是用来实现完美转发左值引用和右值引用的，那么你可以试试，当调用第一个函数的时候能不能推导出 _Tp 是什么类型，我之前的疑惑也在这里，这个函数的参数 typename std::remove_reference&lt;_Tp&gt;::type&amp; __t 和模板参数类型 _Tp 看起来关系很密切，但好像又没关系，因为虽然知道参数类型 typename std::remove_reference&lt;_Tp&gt;::type&amp; __t 是个左值引用，但是你并不知道 _Tp 是什么类型，它还是需要显式来指定的，我们接下来试一试和我们想的一不一样。 forwawrd 完美转发直接拿一个之前写过的完美转发例子吧，代码如下： 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;utility&gt;void Print(int&amp; val)&#123; std::cout &lt;&lt; "lvalue refrence: val=" &lt;&lt; val &lt;&lt; std::endl;&#125;void Print(int&amp;&amp; val)&#123; std::cout &lt;&lt; "rvalue refrence: val=" &lt;&lt; val &lt;&lt; std::endl;&#125;template&lt;typename T&gt;void TPrint(T&amp;&amp; t)&#123; return Print(std::forward&lt;T&gt;(t));&#125;int main()&#123; int date = 2022; TPrint(date); TPrint(501); return 0;&#125; 编译运行之后的结果如下： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testf.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.outlvalue refrence: val=2022rvalue refrence: val=501 通过结果我们发现 std::forward 函数在拥有万能引用参数的模板函数中实现了完美转发，左值转发后调用了参数为左值引用的函数，右值转发后调用了参数为右值引用的函数，这时如果我们把调用 std::forward 的地方改一下，去掉指定的参数类型 T，写成 return Print(std::forward(t));，然后编译看看会发生什么 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testf.cpp --std=c++11testf.cpp: In instantiation of ‘void TPrint(T&amp;&amp;) [with T = int&amp;]’:testf.cpp:23:16: required from heretestf.cpp:17:30: error: no matching function for call to ‘forward(int&amp;)’ return Print(std::forward(t)); ^In file included from /usr/include/c++/5/bits/stl_pair.h:59:0, from /usr/include/c++/5/bits/stl_algobase.h:64, from /usr/include/c++/5/bits/char_traits.h:39, from /usr/include/c++/5/ios:40, from /usr/include/c++/5/ostream:38, from /usr/include/c++/5/iostream:39, from testf.cpp:1:/usr/include/c++/5/bits/move.h:76:5: note: candidate: template&lt;class _Tp&gt; constexpr _Tp&amp;&amp; std::forward(typename std::remove_reference&lt;_From&gt;::type&amp;) forward(typename std::remove_reference&lt;_Tp&gt;::type&amp; __t) noexcept ^/usr/include/c++/5/bits/move.h:76:5: note: template argument deduction/substitution failed:testf.cpp:17:30: note: couldn't deduce template parameter ‘_Tp’ return Print(std::forward(t)); ^In file included from /usr/include/c++/5/bits/stl_pair.h:59:0, from /usr/include/c++/5/bits/stl_algobase.h:64, from /usr/include/c++/5/bits/char_traits.h:39, from /usr/include/c++/5/ios:40, from /usr/include/c++/5/ostream:38, from /usr/include/c++/5/iostream:39, from testf.cpp:1:/usr/include/c++/5/bits/move.h:87:5: note: candidate: template&lt;class _Tp&gt; constexpr _Tp&amp;&amp; std::forward(typename std::remove_reference&lt;_From&gt;::type&amp;&amp;) forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept ^/usr/include/c++/5/bits/move.h:87:5: note: template argument deduction/substitution failed:testf.cpp:17:30: note: couldn't deduce template parameter ‘_Tp’ return Print(std::forward(t)); ^testf.cpp:17:33: error: return-statement with a value, in function returning 'void' [-fpermissive] return Print(std::forward(t)); ^testf.cpp: In instantiation of ‘void TPrint(T&amp;&amp;) [with T = int]’:testf.cpp:24:15: required from heretestf.cpp:17:30: error: no matching function for call to ‘forward(int&amp;)’ return Print(std::forward(t)); ^In file included from /usr/include/c++/5/bits/stl_pair.h:59:0, from /usr/include/c++/5/bits/stl_algobase.h:64, from /usr/include/c++/5/bits/char_traits.h:39, from /usr/include/c++/5/ios:40, from /usr/include/c++/5/ostream:38, from /usr/include/c++/5/iostream:39, from testf.cpp:1:/usr/include/c++/5/bits/move.h:76:5: note: candidate: template&lt;class _Tp&gt; constexpr _Tp&amp;&amp; std::forward(typename std::remove_reference&lt;_From&gt;::type&amp;) forward(typename std::remove_reference&lt;_Tp&gt;::type&amp; __t) noexcept ^/usr/include/c++/5/bits/move.h:76:5: note: template argument deduction/substitution failed:testf.cpp:17:30: note: couldn't deduce template parameter ‘_Tp’ return Print(std::forward(t)); ^In file included from /usr/include/c++/5/bits/stl_pair.h:59:0, from /usr/include/c++/5/bits/stl_algobase.h:64, from /usr/include/c++/5/bits/char_traits.h:39, from /usr/include/c++/5/ios:40, from /usr/include/c++/5/ostream:38, from /usr/include/c++/5/iostream:39, from testf.cpp:1:/usr/include/c++/5/bits/move.h:87:5: note: candidate: template&lt;class _Tp&gt; constexpr _Tp&amp;&amp; std::forward(typename std::remove_reference&lt;_From&gt;::type&amp;&amp;) forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept ^/usr/include/c++/5/bits/move.h:87:5: note: template argument deduction/substitution failed:testf.cpp:17:30: note: couldn't deduce template parameter ‘_Tp’ return Print(std::forward(t)); ^testf.cpp:17:33: error: return-statement with a value, in function returning 'void' [-fpermissive] return Print(std::forward(t)); 这次出了一个很长的编译错误，看来还是需要指定类型的，既然是需要指定的，那我们指定成其他的有没有问题呢？比如写成下面这样： 1return Print(std::forward&lt;float&gt;(t)); 编译运行结果如下，都变成了右值引用： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testf.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.outrvalue refrence: val=2022rvalue refrence: val=501 再改成下面这样： 1return Print(std::forward&lt;T&amp;&gt;(t)); 编译运行结果如下，都变成了左值引用： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testf.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.outlvalue refrence: val=2022lvalue refrence: val=501 完美转发失效上面的这两个例子能说明完美转发失效了吗？这倒也不能说明，第一个例子全都转发成了右值引用，第二个例子全部转发成了左值引用，和我们指定的类型是一致的，也算实现了完美转发，只不过通过这些例子更加深入的理解了完美转发的含义，就是能保证转化成指定的类型，如果指定的类型是个万能引用，就会根据原始类型来完成转发，本次探索之旅到此也就结束了，解答疑惑是个有趣的事情。 总结 std::forward 的本质还是进行强制类型转换，它会把传入的参数转发成指定的类型 完美转发其实是可以脱离左值右值概念的，这也是对完美转发更加深入的理解吧 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 每一点付出终有回报，每一滴汗水从不会白流，可能你看不见也摸不着，但其实它已经悄然声息的改变了你，改变了你周围的点点滴滴~ 2022-5-1 22:58:21]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>forward</tag>
        <tag>完美转发</tag>
        <tag>funny</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++可变参数模板的展开方式]]></title>
    <url>%2Fblog%2F2022%2F04%2F04%2FC-%E5%8F%AF%E5%8F%98%E5%8F%82%E6%95%B0%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%B1%95%E5%BC%80%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言可变参数模板（variadic templates）是C++11新增的强大的特性之一，它对模板参数进行了高度泛化，能表示0到任意个数、任意类型的参数。相比C++98/03这些类模版和函数模版中只能含固定数量模版参数的“老古董”，可变模版参数无疑是一个巨大的进步。 如果是刚接触可变参数模板可能会觉得比较抽象，使用起来会不太顺手，使用可变参数模板时通常离不开模板参数的展开，所以本文来列举一些常用的模板展开方式，帮助我们来对可变参数模板有一个初步的了解。 可变参数模板的定义可变参数模板和普通模板的定义类似，在写法上需要在 typename 或 class 后面带上省略号...，以下为一个常见的可变参数函数模板： 12345template &lt;class... T&gt;void func(T... args)&#123; //...&#125; 上面这个函数模板的参数 args 前面有省略号，所以它就是一个被称为模板参数包（template parameter pack）的可变模版参数，它里面包含了0到N个模版参数，而我们是无法直接获取 args 中的每个参数的，只能通过展开参数包的方式来获取参数包中的每个参数，这也是本文要重点总结的内容。 参数包的展开参数包展开的方式随着c++语言的发展也在与时俱进，我们以实现一个可变参格式化打印函数为例，列举一些常用的方式： 递归函数方式展开1234567891011121314151617181920#include &lt;iostream&gt;void FormatPrint()&#123; std::cout &lt;&lt; std::endl;&#125;template &lt;class T, class ...Args&gt;void FormatPrint(T first, Args... args)&#123; std::cout &lt;&lt; "[" &lt;&lt; first &lt;&lt; "]"; FormatPrint(args...);&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 这种递归展开的方式与递归函数的定义是一样的，需要递归出口和不断调用自身，仔细看看这个函数模板是不是都满足啦？递归出口就是这个无模板参数的 FormatPrint，并且在有参模板中一直在调用自身，递归调用的过程时这样的 FormatPrint(4,3,2,1) -&gt; FormatPrint(3,2,1) -&gt; FormatPrint(2,1) -&gt; FormatPrint(1) -&gt; FormatPrint()，输出内容如下： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out[1][2][3][4][good][2][hello][4][110] 逗号表达式展开123456789101112131415#include &lt;iostream&gt;template &lt;class ...Args&gt;void FormatPrint(Args... args)&#123; (void)std::initializer_list&lt;int&gt;&#123; (std::cout &lt;&lt; "[" &lt;&lt; args &lt;&lt; "]", 0)... &#125;; std::cout &lt;&lt; std::endl;&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 这种方式用到了C++11的新特性初始化列表（Initializer lists）以及很传统的逗号表达式，我们知道逗号表达式的优先级最低，(a, b) 这个表达式的值就是 b，那么上述代码中(std::cout &lt;&lt; &quot;[&quot; &lt;&lt; args &lt;&lt; &quot;]&quot;, 0)这个表达式的值就是0，初始化列表保证其中的内容从左往右执行，args参数包会被逐步展开，表达式前的(void)是为了防止变量未使用的警告，运行过后我们就得到了一个N个元素为0的初始化列表，内容也被格式化输出了： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out[1][2][3][4][good][2][hello][4][110] 说到这顺便提一下，可以使用sizeof...(args)得到参数包中参数个数。 enable_if方式展开12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;tuple&gt;#include &lt;type_traits&gt;template&lt;std::size_t k = 0, typename tup&gt;typename std::enable_if&lt;k == std::tuple_size&lt;tup&gt;::value&gt;::type FormatTuple(const tup&amp; t)&#123; std::cout &lt;&lt; std::endl;&#125;template&lt;std::size_t k = 0, typename tup&gt;typename std::enable_if&lt;k &lt; std::tuple_size&lt;tup&gt;::value&gt;::type FormatTuple(const tup&amp; t)&#123; std::cout &lt;&lt; "[" &lt;&lt; std::get&lt;k&gt;(t) &lt;&lt; "]"; FormatTuple&lt;k + 1&gt;(t);&#125;template&lt;typename... Args&gt;void FormatPrint(Args... args)&#123; FormatTuple(std::make_tuple(args...));&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; C++11的enable_if常用于构建需要根据不同的类型的条件实例化不同模板的时候。顾名思义，当满足条件时类型有效。可作为选择类型的小工具，其广泛的应用在 C++ 的模板元编程（meta programming）之中，利用的就是SFINAE原则，英文全称为Substitution failure is not an error，意思就是匹配失败不是错误，假如有一个特化会导致编译时错误，只要还有别的选择，那么就无视这个特化错误而去选择另外的实现，这里的特化概念不再展开，感兴趣可以自行了解，后续可以单独总结一下。 在上面的代码实现中，基本思路是先将可变模版参数转换为std::tuple，然后通过递增参数的索引来选择恰当的FormatTuple函数，当参数的索引小于tuple元素个数时，会不断取出当前索引位置的参数并输出，当参数索引等于总的参数个数时调用另一个模板重载函数终止递归，编译运行输入以下内容： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out[1][2][3][4][good][2][hello][4][110] 折叠表达式展开（c++17）1234567891011121314#include &lt;iostream&gt;template&lt;typename... Args&gt;void FormatPrint(Args... args)&#123; (std::cout &lt;&lt; ... &lt;&lt; args) &lt;&lt; std::endl;&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 折叠表达式（Fold Expressions）是C++17新引进的语法特性，使用折叠表达式可以简化对C++11中引入的参数包的处理，可以在某些情况下避免使用递归，更加方便的展开参数，如上述代码中展示的这样可以方便的展开参数包，不过输出的内容和之前的有些不一样： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++17albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out1234good2hello4110 对比结果发现缺少了格式化的信息，需要以辅助函数的方式来格式化： 123456789101112131415161718192021#include &lt;iostream&gt;template&lt;typename T&gt;string format(T t) &#123; std::stringstream ss; ss &lt;&lt; "[" &lt;&lt; t &lt;&lt; "]"; return ss.str();&#125;template&lt;typename... Args&gt;void FormatPrint(Args... args)&#123; (std::cout &lt;&lt; ... &lt;&lt; format(args)) &lt;&lt; std::endl;&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 这次格式化内容就被加进来了： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++17albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out[1][2][3][4][good][2][hello][4][110] 这样好像还是有点麻烦，我们可以把折叠表达式和逗号表达式组合使用，这样得到的代码就简单多啦，也能完成格式化输出的任务： 1234567891011121314#include &lt;iostream&gt;template&lt;typename... Args&gt;void FormatPrint(Args... args)&#123; (std::cout &lt;&lt; ... &lt;&lt; (std::cout &lt;&lt; "[" &lt;&lt; args, "]")) &lt;&lt; std::endl;&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 总结 Variadic templates 是C++11新增的强大的特性之一，它对模板参数进行了高度泛化 Initializer lists 是C++11新加的特性，可以作为函数参数和返回值，长度不受限制比较方便 Fold Expressions 是C++17新引进的语法特性，可以方便的展开可变参数模板的参数包 可变参数模板的参数包在C++11的环境下，可以利用递归、逗号表达式、enable_if等方式进行展开 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有些人苦中作乐，而有些人却是身在福中不知福。人性本贪婪，只是度不同。我虽知福，奈何要想一家安稳还差的太多~ 2022-4-5 22:02:27]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>可变参数</tag>
        <tag>template</tag>
        <tag>模板</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epoll的LT模式（水平触发）和ET模式（边沿触发）]]></title>
    <url>%2Fblog%2F2022%2F03%2F27%2Fepoll%E7%9A%84LT%E6%A8%A1%E5%BC%8F%EF%BC%88%E6%B0%B4%E5%B9%B3%E8%A7%A6%E5%8F%91%EF%BC%89%E5%92%8CET%E6%A8%A1%E5%BC%8F%EF%BC%88%E8%BE%B9%E6%B2%BF%E8%A7%A6%E5%8F%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言epoll的触发模式是个引发讨论非常多的话题，网络上这方面总结的文章也很多，首先从名字上就不是很统一，LT模式常被称为水平触发、电平触发、条件触发，而ET模式常被称为边缘触发、边沿触发等，这些都是从英文翻译过来的，只不过翻译的时候有些差异，LT全称 level-triggered，ET全称 edge-triggered。 虽然这个知识点热度很高，但很多人对于它的理解总是差那么一点，特别是在面试的时候，很多面试者总是处于一种回忆和背诵的状态，其实这两种模式真的不需要去死记硬背，下面说说我个人对这两种模式的理解和记忆方法。 名称的记忆每次提到ET（边沿触发）首先映入我脑海的是大学里《数字逻辑电路》这门课程，里面会提到低电平、高电平，当电平从低到高时会有一个上升沿，而电平从高到低时会有一个下降沿，这个“沿”就是边沿触发时提到的“边沿”，跟马路边的马路牙子是同一种概念，也就是指状态变化的时候。提起上升沿和下降沿我还是印象很深的，当时我可是占用了好几节课的时间用Verilog语言写了一个显示“HELLO WORLD”的仿真波形，依靠的就是电平变化中的“沿”。 状态变化LT模式和ET模式可以类比电平变化来学习，但是在实际应用中概念却不是完全一样的，在epoll的应用中涉及到关于IO的读写，而读写的状态变化有哪些呢？可读、不可读、可写、不可写，其实就是这四种状态而已，以socket为例。 可读：socket上有数据不可读：socket上没有数据了可写：socket上有空间可写不可写：socket上无空间可写 对于水平触发模式，一个事件只要有，就会一直触发。对于边缘触发模式，只有一个事件从无到有才会触发。 LT模式对于读事件 EPOLLIN，只要socket上有未读完的数据，EPOLLIN 就会一直触发；对于写事件 EPOLLOUT，只要socket可写（一说指的是 TCP 窗口一直不饱和，我觉得是TCP缓冲区未满时，这一点还需验证），EPOLLOUT 就会一直触发。 在这种模式下，大家会认为读数据会简单一些，因为即使数据没有读完，那么下次调用epoll_wait()时，它还会通知你在上没读完的文件描述符上继续读，也就是人们常说的这种模式不用担心会丢失数据。 而写数据时，因为使用 LT 模式会一直触发 EPOLLOUT 事件，那么如果代码实现依赖于可写事件触发去发送数据，一定要在数据发送完之后移除检测可写事件，避免没有数据发送时无意义的触发。 ET模式对于读事件 EPOLLIN，只有socket上的数据从无到有，EPOLLIN 才会触发；对于写事件 EPOLLOUT，只有在socket写缓冲区从不可写变为可写，EPOLLOUT 才会触发（刚刚添加事件完成调用epoll_wait时或者缓冲区从满到不满） 这种模式听起来清爽了很多，只有状态变化时才会通知，通知的次数少了自然也会引发一些问题，比如触发读事件后必须把数据收取干净，因为你不一定有下一次机会再收取数据了，即使不采用一次读取干净的方式，也要把这个激活状态记下来，后续接着处理，否则如果数据残留到下一次消息来到时就会造成延迟现象。 这种模式下写事件触发后，后续就不会再触发了，如果还需要下一次的写事件触发来驱动发送数据，就需要再次注册一次检测可写事件。 数据的读取和发送关于数据的读比较好理解，无论是LT模式还是ET模式，监听到读事件从socket开始读数据就好了，只不过读的逻辑有些差异，LT模式下，读事件触发后，可以按需收取想要的字节数，不用把本次接收到的数据收取干净，ET模式下，读事件触发后通常需要数据一次性收取干净。 而数据的写不太容易理解，因为数据的读是对端发来数据导致的，而数据的写其实是自己的逻辑层触发的，所以在通过网络发数据时通常都不会去注册监可写事件，一般都是调用 send 或者 write 函数直接发送，如果发送过程中， 函数返回 -1，并且错误码是 EWOULDBLOCK 表明发送失败，此时才会注册监听可写事件，并将剩余的服务存入自定义的发送缓冲区中，等可写事件触发后再接着将发送缓冲区中剩余的数据发送出去。 代码实践基础代码以下为一个epoll触发模式测试的基础代码，也不算太长，直接拿来就可以测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158#include &lt;sys/socket.h&gt; //for socket#include &lt;arpa/inet.h&gt; //for htonl htons#include &lt;sys/epoll.h&gt; //for epoll_ctl#include &lt;unistd.h&gt; //for close#include &lt;fcntl.h&gt; //for fcntl#include &lt;errno.h&gt; //for errno#include &lt;iostream&gt; //for coutclass fd_object&#123;public: fd_object(int fd) &#123; listen_fd = fd; &#125; ~fd_object() &#123; close(listen_fd); &#125;private: int listen_fd;&#125;;/*./epoll for lt modeand./epoll 1 for et mode*/int main(int argc, char* argv[])&#123; //create a socket fd int listen_fd = socket(AF_INET, SOCK_STREAM, 0); if (listen_fd == -1) &#123; std::cout &lt;&lt; "create listen socket fd error." &lt;&lt; std::endl; return -1; &#125; fd_object obj(listen_fd); //set socket to non-block int socket_flag = fcntl(listen_fd, F_GETFL, 0); socket_flag |= O_NONBLOCK; if (fcntl(listen_fd, F_SETFL, socket_flag) == -1) &#123; std::cout &lt;&lt; "set listen fd to nonblock error." &lt;&lt; std::endl; return -1; &#125; //init server bind info int port = 51741; struct sockaddr_in bind_addr; bind_addr.sin_family = AF_INET; bind_addr.sin_addr.s_addr = htonl(INADDR_ANY); bind_addr.sin_port = htons(port); if (bind(listen_fd, (struct sockaddr *)&amp;bind_addr, sizeof(bind_addr)) == -1) &#123; std::cout &lt;&lt; "bind listen socket fd error." &lt;&lt; std::endl; return -1; &#125; //start listen if (listen(listen_fd, SOMAXCONN) == -1) &#123; std::cout &lt;&lt; "listen error." &lt;&lt; std::endl; return -1; &#125; else std::cout &lt;&lt; "start server at port [" &lt;&lt; port &lt;&lt; "] with [" &lt;&lt; (argc &lt;= 1 ? "LT" : "ET") &lt;&lt; "] mode." &lt;&lt; std::endl; //create a epoll fd int epoll_fd = epoll_create(88); if (epoll_fd == -1) &#123; std::cout &lt;&lt; "create a epoll fd error." &lt;&lt; std::endl; return -1; &#125; epoll_event listen_fd_event; listen_fd_event.data.fd = listen_fd; listen_fd_event.events = EPOLLIN; if (argc &gt; 1) listen_fd_event.events |= EPOLLET; //add epoll event for listen fd if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_fd, &amp;listen_fd_event) == -1) &#123; std::cout &lt;&lt; "epoll ctl error." &lt;&lt; std::endl; return -1; &#125; while (true) &#123; epoll_event epoll_events[1024]; int n = epoll_wait(epoll_fd, epoll_events, 1024, 1000); if (n &lt; 0) break; else if (n == 0) //timeout continue; for (int i = 0; i &lt; n; ++i) &#123; if (epoll_events[i].events &amp; EPOLLIN)//trigger read event &#123; if (epoll_events[i].data.fd == listen_fd) &#123; //accept a new connection struct sockaddr_in client_addr; socklen_t client_addr_len = sizeof(client_addr); int client_fd = accept(listen_fd, (struct sockaddr*)&amp;client_addr, &amp;client_addr_len); if (client_fd == -1) continue; socket_flag = fcntl(client_fd, F_GETFL, 0); socket_flag |= O_NONBLOCK; if (fcntl(client_fd, F_SETFL, socket_flag) == -1) &#123; close(client_fd); std::cout &lt;&lt; "set client fd to non-block error." &lt;&lt; std::endl; continue; &#125; epoll_event client_fd_event; client_fd_event.data.fd = client_fd; client_fd_event.events = EPOLLIN | EPOLLOUT; if (argc &gt; 1) client_fd_event.events |= EPOLLET; if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &amp;client_fd_event) == -1) &#123; std::cout &lt;&lt; "add client fd to epoll fd error." &lt;&lt; std::endl; close(client_fd); continue; &#125; std::cout &lt;&lt; "accept a new client fd [" &lt;&lt; client_fd &lt;&lt; "]." &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; "EPOLLIN event triggered for client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "]." &lt;&lt; std::endl; char recvbuf[1024] = &#123; 0 &#125;; int m = recv(epoll_events[i].data.fd, recvbuf, 1, 0); // only read 1 bytes when read event triggered if (m == 0 || (m &lt; 0 &amp;&amp; errno != EWOULDBLOCK &amp;&amp; errno != EINTR)) &#123; if (epoll_ctl(epoll_fd, EPOLL_CTL_DEL, epoll_events[i].data.fd, NULL) != -1) std::cout &lt;&lt; "the client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] disconnected." &lt;&lt; std::endl; close(epoll_events[i].data.fd); &#125; std::cout &lt;&lt; "recv data from client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] and data is [" &lt;&lt; recvbuf &lt;&lt; "]." &lt;&lt; std::endl; &#125; &#125; else if (epoll_events[i].events &amp; EPOLLOUT) &#123; if (epoll_events[i].data.fd == listen_fd) //trigger write event continue; std::cout &lt;&lt; "EPOLLOUT event triggered for client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "]." &lt;&lt; std::endl; &#125; &#125; &#125; return 0;&#125; 简单说下这段代码的测试方法，可以使用 g++ testepoll.cpp -o epoll 进行编译，编译后通过 ./epoll 运行为LT模式，通过 ./epoll et模式运行为ET模式，我们用编译好的epoll程序作为服务器，使用nc命令来模拟一个客户端。 测试分类 编译后直接./epoll，然后在另一个命令行窗口用 nc -v 127.0.0.1 51741 命令模拟一次连接，此时 ./epoll 会产生大量的 EPOLLOUT event triggered for client fd ...，那是因为在LT模式下，EPOLLOUT会被一直触发。 123456789101112albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epollstart server at port [51741] with [LT] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].... 注释包含 EPOLLOUT event triggered for client fd 输出内容的第152行代码，编译后 ./epoll运行，然后在另一个命令行窗口用 nc -v 127.0.0.1 51741 模拟一次连接后，输入abcd回车，可以看到服务器./epoll输出内容，EPOLLIN被触发多次，每次读取一个字节。 1234567891011121314albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epollstart server at port [51741] with [LT] mode.accept a new client fd [5].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [a].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [b].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [c].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [d].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is []. 还原刚才注释的那行代码，编译后执行 ./epoll et 启动服务器，然后在另一个命令行窗口用 nc -v 127.0.0.1 51741 模拟一次连接后，然后在另一个命令行窗口用 nc -v 127.0.0.1 51741 模拟一次连接，服务器窗口显示触发了EPOLLOUT事件 1234albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epoll etstart server at port [51741] with [ET] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5]. 在此基础上，从刚刚运行 nc命令的窗口中输入回车、输入回车、输出回车，那么epoll服务器窗口看到的是触发了三次EPOLLIN事件，每次收到一个回车: 12345678910111213albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epoll etstart server at port [51741] with [ET] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is []. 但是如果在nc模拟的客户端里输出abcd回车，那么在epoll服务器窗口触发一次EPOLLIN事件接收到一个a之后便再也不会触发EPOLLIN了，即使你在nc客户端在此输入也没有用，那是因为在接受的缓冲区中一直还有数据，新数据来时没有出现缓冲区从空到有数据的情况，所以在ET模式下也注意这种情况。 123456789101112131415albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epoll etstart server at port [51741] with [ET] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [a]. 怎么解决ET触发了一次就不再触发了改代码呗，ET模式在连接后触发一次EPOLLOUT，接收到数据时触发一次EPOLLIN，如果数据没收完，以后这两个事件就再也不会被触发了，要想改变这种情况可以再次注册一下这两个事件，时机可以选择接收到数据的时候，所以可以修改这部分代码： 12345678910111213141516else&#123; std::cout &lt;&lt; "EPOLLIN event triggered for client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "]." &lt;&lt; std::endl; char recvbuf[1024] = &#123; 0 &#125;; int m = recv(epoll_events[i].data.fd, recvbuf, 1, 0); // only read 1 bytes when read event triggered if (m == 0 || (m &lt; 0 &amp;&amp; errno != EWOULDBLOCK &amp;&amp; errno != EINTR)) &#123; if (epoll_ctl(epoll_fd, EPOLL_CTL_DEL, epoll_events[i].data.fd, NULL) != -1) std::cout &lt;&lt; "the client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] disconnected." &lt;&lt; std::endl; close(epoll_events[i].data.fd); &#125; std::cout &lt;&lt; "recv data from client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] and data is [" &lt;&lt; recvbuf &lt;&lt; "]." &lt;&lt; std::endl;&#125; 添加再次注册的逻辑： 1234567891011121314151617181920212223else&#123; std::cout &lt;&lt; "EPOLLIN event triggered for client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "]." &lt;&lt; std::endl; char recvbuf[1024] = &#123; 0 &#125;; int m = recv(epoll_events[i].data.fd, recvbuf, 1, 0); // only read 1 bytes when read event triggered if (m == 0 || (m &lt; 0 &amp;&amp; errno != EWOULDBLOCK &amp;&amp; errno != EINTR)) &#123; if (epoll_ctl(epoll_fd, EPOLL_CTL_DEL, epoll_events[i].data.fd, NULL) != -1) std::cout &lt;&lt; "the client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] disconnected." &lt;&lt; std::endl; close(epoll_events[i].data.fd); &#125; epoll_event client_fd_event; client_fd_event.data.fd = epoll_events[i].data.fd; client_fd_event.events = EPOLLIN | EPOLLOUT; if (argc &gt; 1) client_fd_event.events |= EPOLLET; epoll_ctl(epoll_fd, EPOLL_CTL_MOD, epoll_events[i].data.fd, &amp;client_fd_event); std::cout &lt;&lt; "recv data from client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] and data is [" &lt;&lt; recvbuf &lt;&lt; "]." &lt;&lt; std::endl;&#125; 这次以 ./epoll et 方式启动服务器，使用 nc -v 127.0.0.1 51741 模拟客户端，输入abc回车发现，epoll服务器输出显示触发的事件变了： 1234567891011121314albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epoll etstart server at port [51741] with [ET] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [a].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [b].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [c].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLOUT event triggered for client fd [5]. 总结 LT模式会一直触发EPOLLOUT，当缓冲区有数据时会一直触发EPOLLIN ET模式会在连接建立后触发一次EPOLLOUT，当收到数据时会触发一次EPOLLIN LT模式触发EPOLLIN时可以按需读取数据，残留了数据还会再次通知读取 ET模式触发EPOLLIN时必须把数据读取完，否则即使来了新的数据也不会再次通知了 LT模式的EPOLLOUT会一直触发，所以发送完数据记得删除，否则会产生大量不必要的通知 ET模式的EPOLLOUT事件若数据未发送完需再次注册，否则不会再有发送的机会 通常发送网络数据时不会依赖EPOLLOUT事件，只有在缓冲区满发送失败时会注册这个事件，期待被通知后再次发送 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 即使是在灿烂的阳光下也会有黑暗的角落，不能因为角落的阴暗就忽略阳光下的美好，我们要做的不是把黑暗面放大，而是要做阳光的传递者，哪怕是一面面镜子，通过反射来照亮那星星点点的黑暗，认清自己，不与黑暗为伍，那绝不是你自甘堕落的借口。 两千光束已然出发~ 2022-4-4 18:25:50]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>epoll</tag>
        <tag>LT</tag>
        <tag>ET</tag>
        <tag>水平触发</tag>
        <tag>边沿触发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结下各种常见树形结构的定义及特点（二叉树、AVL树、红黑树、Trie树、B树、B+树）]]></title>
    <url>%2Fblog%2F2022%2F03%2F13%2F%E6%80%BB%E7%BB%93%E4%B8%8B%E5%90%84%E7%A7%8D%E5%B8%B8%E8%A7%81%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AE%9A%E4%B9%89%E5%8F%8A%E7%89%B9%E7%82%B9%2F</url>
    <content type="text"><![CDATA[前言树形结构相比于数组、链表、队列和栈等线性结构要复杂的多，因为树本身的概念就比较多，通过设定一些条件和限制就可以定义出一种新类型的树，结果造成了树的“变化多端”，所以要学习一种树要从树的定义入手，然后根据定义和特点来熟悉各种树适合的场景，这样就可以做到“树尽其用”目的了。 一棵普通的树树形结构和现实中的树很像，只不过现实中的树根长在地上，而树形结构再展示的时候一般把树根画在“天上”，树形结构中数据元素之间存在着“一对多”的关系，具有以下特点： 没有父节点的节点称为根节点 除空树外每棵树只有一个根节点 每个节点都只有有限个子节点或无子节点 每个非根节点有且只有一个父节点 树里面没有环路，如果从一个节点出发，除非往返，否则无法回到起点 相关术语 根节点：最顶层的节点就是根结点，它是整棵树的源头 叶子节点：在树下端的节点，就是其子节点个数为0的节点 节点的度：指定节点有几个分叉就说这个节点的度是几 树的度：只看根结点，树的度等价于根节点的度 节点高度：指从这个节点到叶子节点的距离（一共经历了几个节点） 节点深度：指从这个节点到根节点的距离（一共经历了几个节点） 树的高度：指所有节点高度的最大值 树的深度：指所有节点深度的最大值 节点的层：从根节点开始，假设根节点为第1层，根节点的子节点为第2层，依此类推 二叉树二叉树是对普通树形结构进行限定得到的一种特殊的树，规定树中节点的度不大于2，当节点有两个子节点，也就是有两颗子树时，它们有左右之分，分别被称为左子树和右子树，左子树和右子树又同样都是二叉树。 二叉树性质 二叉树的第i层上至多有2^(i-1)（i≥1）个节点 深度为h的二叉树中至多含有2^h-1个节点 若在任意一棵二叉树中，有n个叶子节点，有m个度为2的节点，则必有n=m+1 具有n个节点的满二叉树深为log(2n+1) 若对一棵有n个节点的完全二叉树进行顺序编号（1≤i≤n），那么，对于编号为i（i≥1）的节点 当i=1时，该节点为根，它无双亲节点 当i&gt;1时，该节点的双亲节点的编号为i/2 若2i≤n，则有编号为2i的左节点，否则没有左节点 若2i+1≤n，则有编号为2i+1的右节点，否则没有右节点 二叉树特例完美二叉树（Perfect Binary Tree）：除了叶子结点之外的每一个结点都有两个孩子，每一层都被完全填充 完全二叉树（Complete Binary Tree）：除了最后一层之外的其他每一层都被完全填充，并且所有结点都保持向左对齐 完满二叉树（Full Binary Tree）： 除了叶子结点之外的每一个结点都有两个孩子结点 二叉查找树二叉查找树是一种特殊的二叉树，又称为排序二叉树、二叉搜索树、二叉排序树等等，它实际上是数据域有序的二叉树，即对树上的每个结点，都满足其左子树上所有结点的数据域均小于或等于根结点的数据域，右子树上所有结点的数据域均大于根结点的数据域。 AVL树平衡二叉树是由前苏联的两位数学家G.M.Adelse-Velskil和E.M.Landis联合提出，因此一般也称作AVL树，AVL树本质还是一棵二叉查找树，只是在其基础上增加了“平衡”的要求，需保证其左子树与右子树的高度之差的绝对值不超过1，其中左子树与右子树的高度因子之差称为平衡因子。 对于AVL树，不管我们是执行插入还是删除操作，只要不满足上面的条件，就要通过旋转来保持平衡。由于旋转比较耗时，所以AVL树适合用于插入与删除次数比较少，但查找多的情况。 特点及应用所有节点的左右子树高度差不超过1，广泛用于Windows NT内核中 红黑树红黑树也是一颗二叉查找树，需要为每个节点存储节点的颜色，可以是红或黑。通过对任何一条从根到叶子的路径上各个节点着色的方式的限制，来确保没有一条路径会比其它路径长出两倍，因此，红黑树是一种弱平衡二叉树。 由于是弱平衡二叉树，那么在相同的节点情况下，AVL树的高度小于等于红黑树的高度，相对于要求严格的AVL树来说，它的旋转次数少，所以对于插入，删除操作较多的情况下，用红黑树的查找效率会更高一些。 特点 每个节点非红即黑 根节点是黑的 每个叶子节点（叶子节点即树尾端NULL节点）都是黑的 每条路径都包含相同的黑节点 如果一个节点是红的，那么它的两儿子都是黑的 对于任意节点而言，其到叶子点的每条路径都包含相同数目的黑节点 应用 广泛用于C++的STL中，如 map 和 set 是用红黑树实现的 Linux的进程调度用红黑树管理进程控制块，进程的虚拟内存空间都存储在一颗红黑树上，每个虚拟内存空间都对应红黑树的一个节点 IO多路复用的 epoll 采用红黑树组织管理socket fd，以支持快速的增删改查 Nginx中用红黑树管理定时器，可以快速得到距离当前最小的定时器 Java的TreeMap的用红黑树实现 Trie树Trie树又被称为前缀树、字典树是一种用于快速检索的多叉树结构。字典树把字符串看成字符序列，根据字符串中字符序列的先后顺序构造从上到下的树结构，树结构中的每一条边都对应着一个字符。字典树上存储的字符串被视为从根节点到某个节点之间的一条路径，并在终点节点上做个标记”该节点对应词语的结尾”，正因为有终点节点的存在，字典树不仅可以实现简单的存储字符串，还可以实现字符串的映射，只需要将相对应的值悬挂在终点节点上即可。 特点及应用Trie的核心思想是空间换时间，有如下基本性质： 根节点不包含字符，除根节点外每一个节点都只包含一个字符 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串 每个节点的所有子节点包含的字符都不相同 字典树能够利用字符串中的公共前缀，这样可能会节省内存，利用字符串的公共前缀可以减少查询字符串的时间，能够最大限度的减少无谓的字符串比较，同时在查询的过程中不需要预知待查询字符串的长度，沿着字典树的边进行匹配，查询效率比较高，但是如果系统中存在大量字符串并且这些字符串基本没有前缀，相应的字典树内存消耗也会很大。正是由于字典树的这些特点，字典树被用于统计、排序和保存大量的字符串（不仅限于字符串），还可用于搜索引擎的关键词提示功能。 B树B树是一个多路平衡查找树，B树的出现是为了弥合不同的存储级别之间的访问速度上的巨大差异，实现高效的I/O。平衡二叉树的查找效率是非常高的，并可以通过降低树的深度来提高查找的效率。但是当数据量非常大，树的存储的元素数量是有限的，这样会导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下，同时数据量过大会导致内存空间不够容纳平衡二叉树所有结点的情况，而B树是解决这个问题的很好的结构。 要想了解B树需要了解一个很重要的概念，B树中所有节点的度的最大值称为B树的阶，记为m，这是一个跟重要值，也就是说m阶B树指的是节点度最大为m的B树。 定义及特点 每个节点最多只有m个子节点 根结点的儿子数为[2, m] 除根结点以外的非叶子结点的儿子数为[m/2, m]，向上取整 非叶子结点的关键字个数=子节点数-1 所有叶子都出现在同一层 k个关键字把节点拆成k+1段，分别指向k+1个儿子，同时满足查找树的大小关系 非叶子节点中不仅包含索引，也会包含数据 应用B树是一种平衡的多路查找树，主要用作文件的索引。其优势是当你要查找的值恰好处在一个非叶子节点时，查找到该节点就会成功并结束查询，有很多基于频率的搜索是选用B树，越频繁查询的结点越往根上走，前提是需要对查询做统计，而且要对key做一些变化。 B+树B+树是b树的一种变体，查询性能更好，m阶的b+树具有以下特征： 有n棵子树的非叶子结点中含有n个关键字（b树是n-1个），这些关键字不保存数据，只用来索引，所有数据都保存在叶子节点（b树是每个关键字都保存数据） 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接 所有的非叶子结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字 通常在b+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点 同一个数字会在不同节点中重复出现，根节点的最大元素就是b+树的最大元素 B+树的优势及应用 B+tree的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多，相对来说IO读写次数也就降低了。 由于非叶子结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 B+树支持范围遍历，只要遍历叶子节点就可以实现整棵树的遍历，而在数据库中基于范围的查询是非常频繁的，这一点要明显由于B树。 由于拥有以上特点，B+广泛应用于文件存储系统以及数据库系统中。 总结 树是一种常见的非线性结构，拥有众多变种 二叉树是树形结构的一大类，每个节点最多拥有两个子节点树，左右子树顺序固定 AVL树是平衡二叉树，任意节点的左右子树高度差最大为1 红黑树是弱平衡二叉树，每个节点记录的自己的颜色，用来控制左右子树高度不大于2倍 Trie树又叫字典树，是一种用于快速检索的多叉树结构 B树是一种多路平衡树，用于提高了磁盘IO性能，多用于文件系统的索引 B+树是对B树的改进，仅在叶子节点存储数据，相比于B树更加矮胖，支持范围遍历 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 世间从来没有什么『感同身受』，每个人面对相同的事件和意外都会因为家庭背景、个人经历的差异而有不同的反应，更不要说那些没经历过的人，即使你曾经真的经历过类似的事情，那么在被漫长的时间洗礼之后，一切都会淡化许多，所以“未经他人苦，莫劝他人善。你若经我苦，未必有我善”~ 2022-3-13 22:47:31]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>树的特点</tag>
        <tag>红黑树</tag>
        <tag>字典树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单梳理下git的使用感受，思考git中最重要的是什么]]></title>
    <url>%2Fblog%2F2022%2F02%2F13%2F%E7%AE%80%E5%8D%95%E6%A2%B3%E7%90%86%E4%B8%8Bgit%E7%9A%84%E4%BD%BF%E7%94%A8%E6%84%9F%E5%8F%97%EF%BC%8C%E6%80%9D%E8%80%83git%E4%B8%AD%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[前言工作中使用git是从两年前开始的，之前一直add -&gt; commit -&gt;push常规操作，真正在工作中使用之后才渐渐理解了git的强大，这种理解是建立在不断解决问题的基础上的，不断的处理遇到的问题，就像升级打怪一样，对git的理解也越来越全面。因为在使用git之前一直用svn作为版本控制工具，所以对git和svn的区别也有了自己的认识，关于两者的区别网上的文章一搜一大把，我就不重复了，我仅仅从自己的理解来描述下两者的不同。 git和svn关于git和svn的区别，网上的文章确实很多，大多数会提到分布式、存储方式、版本号、完整性等方面，而我今天要写的区别是两者提交记录的结构。 既然作为版本控制工具，那么每次历史提交都必须可以追溯和回退，在svn中提交记录时线性的，以时间轴为参考基准，所有提交按照时间先后排列，因为svn记录必须提交到服务器才能生效，所有服务器相当于各个svn客户端的总控，各个svn提交到服务器时线性排列，且必须将本地文件状态更新成和服务器相同时才能修改提交。 正因为在svn中有服务器负责总控操作，所以能保证时间最新的提交记录就是整个svn最新的状态，提交记录不依赖客户端时间，完全由服务器时间进行排序。 在git中没有这样的总控服务器，虽然一般情况下每个代码库都会有统一的托管服务器，但是它的作用任何一个git客户端都能代替，因为git是可以离线提交的，托管服务器只是我们用来存储代码的地方，与svn服务器按时间排序的做法大不相同。 git的提交记录通常是一个树形结构，个别时候会变成有“起点”和“终点”的网状结构，在git中时间只具备参考意义，并不能决定提交记录的先后，如果你对这一点还心存怀疑，可能你是个svn的重度用户，一时还没理解git操作原理。 对于这个问题可以举个例子，操作同一个文件，在svn中2月13日修改一次，2月14日修改一次，那么2月15日看这个文件一定是2月14日修改后的状态；而在git中，同样是那个文件分别在2月13日和2月14日修改一次，2月15日文件的状态取决了两次修改是否在同一分支，以及合并时是怎样处理的，这种错位随着时间的延长和多分支的合并，往往对时间的依赖“微乎其微”，此时再也不能用时间来衡量提交的先后了。 如果一开始就是git，上面提到的这个问题还不太明显，但是用惯了svn再使用git，处理历史回溯问题时往往容易找错方向，经常通过时间过滤出来的内容并不是自己想要的，这一点在实际操作中需要注意。 git最重要的是什么相信这个问题每个人都有自己的答案，有人认为是分布式，有人认为是切换分支很方便，而我的答案是 commit 的设计哲学，我觉得这是git中的精髓，git中的commit就像一个链表中的元素，用来将自身和其他的commit串联到一起，形成branch、tag、HEAD 等等。 我们可以通过 git log 命令来看一条 commit： 123456$ git log -1commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6. 这条commit id 为 7bf665f125a4771db095c83a7ad6ed46692cd314，这在整个库是惟一的，通过 git log 可以看到这次提交的时间、作者、简要说明等信息，那么这次提交和库是什么关系呢？ 通过括号中的内容可以知道当前提交是这个库的6.0分支，同时为标签6.0.6，也与远端的6.0分支同步。 使用 git cat-file 命令可以进一步查询这个commit的组织形式： 1234567$ git cat-file -p HEADtree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dauthor Oran Agra &lt;oran@redislabs.com&gt; 1595156420 +0300committer Oran Agra &lt;oran@redislabs.com&gt; 1595268506 +0300Redis 6.0.6. 可以发现这次提交包含了 tree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3，同时它的父提交就是 parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957d，有了这两个id就可以递推出当前版本内容和这个历史记录。 通过 tree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3 可以递归找出当前版本中的所有文件： 12345678910111213141516171819202122$ git cat-file -p c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3040000 tree 6608d88fe6a7a25b137b869040103ab261310da4 .github100644 blob e445fd2017bb0c13af2f40cd7f24afefdb603ade .gitignore100644 blob 484aeb62186033d32e9a4bdf12434cb6b8c56fb5 00-RELEASENOTES100644 blob 7af2593407805c308cc25739ac9c6520031de60f BUGS100644 blob 000edbeaf0270bf3b9e457274ab092b02b176b84 CONTRIBUTING100644 blob a381681a1c2524ed586c6a87dfeb9ccdf1e86ded COPYING100644 blob 3083f1afd50c34e1139ab1577510a17e968b0ed4 INSTALL100644 blob 3727894624fdabf72995e6f94998a2cad359f760 MANIFESTO100644 blob e614ede891f2dd183a3ae41ea1ac3b63fe2e7634 Makefile100644 blob 55537e01fe862dd200ebe1078033122facfc854e README.md100644 blob 2d020d0ceb0ddc7fd0bb2a6185e57a9afd5aef79 TLS.md040000 tree 43ccdd93a80b35e03160d9db34f1e844a62a74b4 deps100644 blob 8c53f015a20934bdb41c77152fd32a557d719fae redis.conf100755 blob ade1bd09a539ecd8dcdd09e59a658539dab9bce6 runtest100755 blob 27829a5fe8afacf893fe9bafc4245971ce375d6c runtest-cluster100755 blob f6cc0a2589dea0f95b77b226e54200a29b8237ae runtest-moduleapi100755 blob 3fb1ef61561289b2bf8622e49645f66dab83eeea runtest-sentinel100644 blob 4ca5e5f8fc5abe2938c66a6851bba0c90058620f sentinel.conf040000 tree e3b3338a7c60eafb3d9c19d3784e2482beea1d4b src040000 tree af5de133fa0a0da30fe487be40783ef9644fba6d tests040000 tree 5a82556097d23f0c16a8e5432d464f2ab434fd2a utils 通过 parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957d 可以找出上一次提交，进而递归找出所有的提交，要注意有些commit的parent不止一个： 1234567$ git cat-file -p a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dtree 1adcf548620c6134f7d5fd072c05b981d0f36118parent e15528bf1da1f1232fd08801ad382c915be94662author Oran Agra &lt;oran@redislabs.com&gt; 1595162001 +0300committer Oran Agra &lt;oran@redislabs.com&gt; 1595268506 +0300Run daily CI on PRs to release a branch 这个commit的设计真的很神奇，一个个commit串起来就是一个branch，本质来讲branch只是commit的一个别名，包括HEAD也是，而 tag 也是对commit的一个描述，在不加描述信息时和commit也是一样的。 123456789101112131415$ git cat-file -p 6.0.6tree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dauthor Oran Agra &lt;oran@redislabs.com&gt; 1595156420 +0300committer Oran Agra &lt;oran@redislabs.com&gt; 1595268506 +0300Redis 6.0.6.$ git cat-file -p HEADtree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dauthor Oran Agra &lt;oran@redislabs.com&gt; 1595156420 +0300committer Oran Agra &lt;oran@redislabs.com&gt; 1595268506 +0300Redis 6.0.6. 所以理解了commit的定位以后，所有切换分支、切换tag、操作HEAD，本质上都是在对commit进行操作，这些操作的参数完全可以用commit id来替换HEAD、branch name、tag name等等。 总结 svn的提交记录是一个按时间排序的线性结构，git的提交记录是一个参考时间的树状结构 git记录中时间先后不能代表commit修改的先后，回溯查找时要注意这一点才能解释很多疑惑 git中的commit我认为是它的精髓，通过commit的串联和别名，形成分支、标签、HEAD等多种元素，隐藏了细节，方便了操作 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 什么才是精彩的人生？扬在脸上的自信、长在心底的善良、融进血里的骨气、刻进生命里的坚强~ 2022-2-13 23:19:05]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>commit</tag>
        <tag>git</tag>
        <tag>branch</tag>
        <tag>stash</tag>
        <tag>tag</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下随时照看服务器进程的ps和top命令]]></title>
    <url>%2Fblog%2F2022%2F01%2F23%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%9A%8F%E6%97%B6%E7%85%A7%E7%9C%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E7%A8%8B%E7%9A%84ps%E5%92%8Ctop%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言在linux环境服务器下通常是不会安装图形界面的，这时如果想看程序是否在正常运行着，就不能像在windows上一样去查看任务管理器，而是需要通过命令来查询程序的进程运行情况，ps 就是一个查询linux环境下进程运行状态的命令，请注意这个可不是我们P图用的那个工具哦。 ps命令ps 命令在man手册中描述是 “report a snapshot of the current processes”，其实它的作用就是上报程序运行时的一个快照。 ps displays information about a selection of the active processes. If you want a repetitive update of the selection and the displayed information, use top(1) instead. man手册中的这句话也很好理解，ps 命令执行一次就会显示一次所选进程的信息，若想重复刷新显示所选进程的信息，可以使用 top 命令，从作用上来看，可以简单理解为 top 命令是一个反复执行的 ps 命令。 语法格式1ps [options] ps 命令的格式很简单，但是为什么感觉同一个查询进程的需求，每个人写出来的命令都不太一样呢？其实这是因为有好几种语法规范，无论怎么写都可以，甚至交叉混合的写命令都是能识别的，比如最常见的查询所有进程有些人会写成 ps -ef，而有的人会写成 ps aux，这两种都是可以的。 语法规范ps -ef 是使用了UNIX选项，是一种标准语法，而 ps aux使用了BSD选项，是一种BSD语法，另外还有一种加两个短横线的写法，例如 ps --pid 3678，这是一种GNU长选项。 常用的ps选项 显示所有的进程，一般搭配grep使用 1ps aux | grep processname 显示指定的列，可以使用 -eo 来指列名，我最常用的 cmd 是启动时的命令，lstart 是启动时的时间 12[root@VM-0-3-centos ~]# ps -eo cmd,lstart | grep sshdusr/sbin/sshd -D Tue Feb 9 11:13:55 2021 以树形结构显示进程 1ps -axjf 查询指定用户的进程 1ps -fu root 按cpu使用率降序排序 1ps -aux --sort -pcpu 按内存使用率降序排序 1ps -aux --sort +pmem ps命令的表头1234567891011[root@VM-0-3-centos ~]# ps -aux --sort -pcpuUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 17444 0.4 3.7 994796 71276 ? Sl 2021 181:54 /usr/local/qcloud/YunJing/YDEyes/YDServiceroot 8572 0.3 0.7 614824 14416 ? Sl Jan20 21:35 barad_agentroot 1 0.0 0.1 191084 3336 ? Ss 2021 76:42 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root 2 0.0 0.0 0 0 ? S 2021 0:09 [kthreadd]root 4 0.0 0.0 0 0 ? S&lt; 2021 0:00 [kworker/0:0H]root 1063 0.0 0.0 110208 792 tty1 Ss+ 2021 0:00 /sbin/agetty --noclear tty1 linuxroot 1064 0.0 0.0 110208 772 ttyS0 Ss+ 2021 0:00 /sbin/agetty --keep-baud 115200,38400,9600 ttyS0 vt220root 2388 0.0 0.1 157696 1952 pts/0 R+ 00:27 0:00 ps -aux --sort -pcpuroot 6 0.0 0.0 0 0 ? S 2021 13:09 [ksoftirqd/0] 执行 ps 命令通常有以上几列：USER、PID、%CPU、%MEM、VSZ、RSS、TTY、STAT、START、TIME、COMMAND USER：进程所属用户名PID：进程ID%CPU：进程CPU使用率，注意所有CPU使用率加起来可以超过100%，如果有4个CPU，在不考虑多核的情况下，最大应该为400%%MEM：进程内存使用率，可以配合 free -h 命令使用VSZ：进程使用的虚拟内存量RSS：进程占用的固定内存量TTY：进程在哪个终端上运行，若与终端无关，则显示 ?STAT：进程启动的时间START：进程状态，是一个组合值，每个符号有单独含义，见下文TIME：进程使用CPU运行的时间COMMAND：启动进程时执行的命令的名称和参数 进程状态这一部分是要是对man手册的翻译内容 1234567891011121314151617181920Here are the different values that the s, stat and state output specifiers (header "STAT" or "S") will display to describe the state of a process: D uninterruptible sleep (usually IO) R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) T stopped by job control signal t stopped by debugger during the tracing W paging (not valid since the 2.6.xx kernel) X dead (should never be seen) Z defunct ("zombie") process, terminated but not reaped by its parent For BSD formats and when the stat keyword is used, additional characters may be displayed: &lt; high-priority (not nice to other users) N low-priority (nice to other users) L has pages locked into memory (for real-time and custom IO) s is a session leader l is multi-threaded (using CLONE_THREAD, like NPTL pthreads do) + is in the foreground process group 常规状态码D: 无法中断的休眠状态（通常 IO 的进程）R: 正在运行可中在队列中可过行的；S: 处于休眠状态T: 停止或被追踪W: 进入内存交换（从内核2.6开始无效）X: 死掉的进程（基本很少见）Z: 僵尸进程，想杀死通常需要重启系统 BSD格式状态码&lt;: 优先级高的进程N: 优先级较低的进程L: 有些页被锁进内存s: 一个会话进程的领导者（在它之下有子进程）l: 多进程的（使用 CLONE_THREAD, 类似 NPTL pthreads）+: 位于后台的进程组 进程相关的后续命令 查询进程的可执行文件所在位置 1cat /proc/进程id 按进程名查询进程id 1pidof 进程名 top命令语法格式1top [options] top 命令的man手册巨长，它的作用是动态显示进程状态，实际上就是一份份静态数据间隔显示而已，虽然他也有很多参数，但是通常仅使用一个 top 命令即可。 常用的top选项和快捷键 显示top帮助 1top -hV 按照CPU使用率排序 运行top命令后，输入大写字母P，也就是按快捷键 ctrl+p 按照内存使用率排序 运行top命令后，输入大写字母M，也就是按快捷键 ctrl+m 展开多核cpu显示 运行top命令后，输入数字1 切换显示命令名称和完整命令行 运行top命令后，输入小写字母c 切换显示平均负载和启动时间信息 运行top命令后，输入小写字母l top命令的表头12345678910111213[root@VM-0-3-centos ~]# toptop - 00:07:35 up 0 min, 0 users, load average: 0.52, 0.58, 0.59Tasks: 4 total, 1 running, 3 sleeping, 0 stopped, 0 zombie%Cpu(s): 3.4 us, 8.0 sy, 0.0 ni, 88.4 id, 0.0 wa, 0.2 hi, 0.0 si, 0.0 stKiB Mem : 8248540 total, 3284196 free, 4727868 used, 236476 buff/cacheKiB Swap: 13107196 total, 13080444 free, 26752 used. 3379816 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 8940 316 272 S 0.0 0.0 0:00.15 init 10 root 20 0 8940 224 176 S 0.0 0.0 0:00.00 init 11 albert 20 0 15024 3568 3468 S 0.0 0.0 0:00.24 bash 38 albert 20 0 15900 1960 1420 R 0.0 0.0 0:00.04 top 执行 top 命令后默认有这些列 PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND，它们的含义分表是： PID：进程idUSER：进程所有者用户名PR：优先级NI：NICE值.负值表示高优先级，正值表示低优先级VIRT：进程使用的虚拟内存总量，单位KB，VIRT=SWAP+RESRES：进程使用的未被换出的物理内存大小，单位KB，RES=CODE+DATASHR：共享内存大小，单位KB，S：进程状态，D=不可中断的睡眠状态，R=运行，S=睡眠，T=跟踪/停止，Z=僵尸进程%CPU：CPU使用百分比%MEM、内存使用百分比TIME+：进程使用的CPU时间总计,单位1/100秒COMMAND：启动命令 总结 显示所有进程可以使用 ps -ef 或者 ps aux，他们通常是后续过滤的第一步 按cpu使用率降序排序使用 ps aux --sort -pcpu，按内存使用率降序排序使用 ps aux --sort +pmem top 命令结果的第一行是整体信息，包括了当前时间，系统已运行时间，登录用户数，系统负载等信息 top 命令中按大写字母 P 会按照CPU排序，按大写字母 M 会按照内存排序 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 小孩子才有对错，长大了都是利益。做选择的本质是分类，权衡利弊的过程是分类，趋利避害的行事生活更是分类，所以遇到选择我们才要考虑方方面面，因为这样我们才有足够的理由来把它归为有利的一类，还是有害的一类~ 2022-1-28 00:41:05]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ps</tag>
        <tag>命令</tag>
        <tag>top</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过WindowsStore安装QuickLook小工具方便文件预览]]></title>
    <url>%2Fblog%2F2022%2F01%2F09%2F%E9%80%9A%E8%BF%87WindowsStore%E5%AE%89%E8%A3%85QuickLook%E5%B0%8F%E5%B7%A5%E5%85%B7%E6%96%B9%E4%BE%BF%E6%96%87%E4%BB%B6%E9%A2%84%E8%A7%88%2F</url>
    <content type="text"><![CDATA[前言话说Windows商店一直没有什么存在感，普通大众从互联网野蛮生长开始就一直被各大流氓软件控制着连接互联网的门户，下载软件去官网只是少数人拿来伸张正义的说辞，有些官网的访问速度确实…不过随着手机的普及，应用商店的下载安装方式逐渐被人们所接受，而Windows商店也渐渐进入了人们的视野，毕竟又这么个官方的东西，总比满大街搜索捆绑的软件要方便的多。 今天主要是想记录一下QuickLook这个软件，毕竟小而美的工具可以极大的提高生产力，或者是摸鱼的能力，这个软件可以在你想要预览某个文档或者图片时，轻轻敲一个空格就搞定，比如想看一个压缩包的内容，只需敲个空格就可以。 Windows商店这个小工具在windows商店里就有，搜索安装比较方便，但是我发现我电脑里的WindowsStore不见了，还真是神奇，前一段时间还用过的，不知不觉就消失了，所以先安装Windows商店吧。 首先按组合键Win+X，在弹出菜单中左键单击 Windows PowerShell(管理员)选项，输入命令 Get-AppxPackage -allusers | Select Name, PackageFullName，之后会在命令窗口中显示可以安装的软件包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202# Albert at HOME-PC in C:\Windows\system32 [19:55:50]$ Get-AppxPackage -allusers | Select Name, PackageFullNameName PackageFullName---- ---------------1527c705-839a-4832-9118-54d4Bd6a0c89 1527c705-839a-4832-9118-54d4Bd6a0c89_10.0.18362.387_neutral_neutral_cw5...c5e2524a-ea46-4f67-841f-6a9465d9d515 c5e2524a-ea46-4f67-841f-6a9465d9d515_10.0.18362.387_neutral_neutral_cw5...E2A4F912-2574-4A75-9BB0-0D023378592B E2A4F912-2574-4A75-9BB0-0D023378592B_10.0.18362.387_neutral_neutral_cw5...F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE_10.0.18362.387_neutral_neutral_cw5...InputApp InputApp_1000.18362.387.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AAD.BrokerPlugin Microsoft.AAD.BrokerPlugin_1000.18362.387.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AccountsControl Microsoft.AccountsControl_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.AsyncTextService Microsoft.AsyncTextService_10.0.18362.387_neutral__8wekyb3d8bbweMicrosoft.BioEnrollment Microsoft.BioEnrollment_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.CredDialogHost Microsoft.CredDialogHost_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.ECApp Microsoft.ECApp_10.0.18362.387_neutral__8wekyb3d8bbweMicrosoft.LockApp Microsoft.LockApp_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.MicrosoftEdgeDevToolsClient Microsoft.MicrosoftEdgeDevToolsClient_1000.18362.387.0_neutral_neutral_...Microsoft.MicrosoftEdge Microsoft.MicrosoftEdge_44.18362.387.0_neutral__8wekyb3d8bbweMicrosoft.PPIProjection Microsoft.PPIProjection_10.0.18362.387_neutral_neutral_cw5n1h2txyewyMicrosoft.Win32WebViewHost Microsoft.Win32WebViewHost_10.0.18362.387_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.Apprep.ChxApp Microsoft.Windows.Apprep.ChxApp_1000.18362.387.0_neutral_neutral_cw5n1h...Microsoft.Windows.AssignedAccessLockApp Microsoft.Windows.AssignedAccessLockApp_1000.18362.387.0_neutral_neutra...Microsoft.Windows.CallingShellApp Microsoft.Windows.CallingShellApp_1000.18362.387.0_neutral_neutral_cw5n...Microsoft.Windows.CapturePicker Microsoft.Windows.CapturePicker_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.Windows.CloudExperienceHost Microsoft.Windows.CloudExperienceHost_10.0.18362.387_neutral_neutral_cw...Microsoft.Windows.ContentDeliveryManager Microsoft.Windows.ContentDeliveryManager_10.0.18362.387_neutral_neutral...Microsoft.Windows.Cortana Microsoft.Windows.Cortana_1.13.0.18362_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.NarratorQuickStart Microsoft.Windows.NarratorQuickStart_10.0.18362.387_neutral_neutral_8we...Microsoft.Windows.OOBENetworkCaptivePortal Microsoft.Windows.OOBENetworkCaptivePortal_10.0.18362.387_neutral__cw5n...Microsoft.Windows.OOBENetworkConnectionFlow Microsoft.Windows.OOBENetworkConnectionFlow_10.0.18362.387_neutral__cw5...Microsoft.Windows.ParentalControls Microsoft.Windows.ParentalControls_1000.18362.387.0_neutral_neutral_cw5...Microsoft.Windows.PeopleExperienceHost Microsoft.Windows.PeopleExperienceHost_10.0.18362.387_neutral_neutral_c...Microsoft.Windows.PinningConfirmationDialog Microsoft.Windows.PinningConfirmationDialog_1000.18362.387.0_neutral__c...Microsoft.Windows.SecHealthUI Microsoft.Windows.SecHealthUI_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.Windows.SecureAssessmentBrowser Microsoft.Windows.SecureAssessmentBrowser_10.0.18362.387_neutral_neutra...Microsoft.Windows.ShellExperienceHost Microsoft.Windows.ShellExperienceHost_10.0.18362.387_neutral_neutral_cw...Microsoft.Windows.StartMenuExperienceHost Microsoft.Windows.StartMenuExperienceHost_10.0.18362.387_neutral_neutra...Microsoft.Windows.XGpuEjectDialog Microsoft.Windows.XGpuEjectDialog_10.0.18362.387_neutral_neutral_cw5n1h...Microsoft.XboxGameCallableUI Microsoft.XboxGameCallableUI_1000.18362.387.0_neutral_neutral_cw5n1h2tx...Windows.CBSPreview Windows.CBSPreview_10.0.18362.387_neutral_neutral_cw5n1h2txyewywindows.immersivecontrolpanel windows.immersivecontrolpanel_10.0.2.1000_neutral_neutral_cw5n1h2txyewyWindows.PrintDialog Windows.PrintDialog_6.2.1.0_neutral_neutral_cw5n1h2txyewyMicrosoft.Advertising.Xaml Microsoft.Advertising.Xaml_10.1808.3.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.1.7 Microsoft.NET.Native.Framework.1.7_1.7.25531.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.1.7 Microsoft.NET.Native.Framework.1.7_1.7.25531.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.27011.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.1.7 Microsoft.NET.Native.Runtime.1.7_1.7.25531.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.1.7 Microsoft.NET.Native.Runtime.1.7_1.7.25531.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.2.2 Microsoft.NET.Native.Runtime.2.2_2.2.27011.0_x64__8wekyb3d8bbweMicrosoft.Services.Store.Engagement Microsoft.Services.Store.Engagement_10.0.18101.0_x64__8wekyb3d8bbweMicrosoft.Services.Store.Engagement Microsoft.Services.Store.Engagement_10.0.18101.0_x86__8wekyb3d8bbweMicrosoft.UI.Xaml.2.0 Microsoft.UI.Xaml.2.0_2.1810.18004.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.26905.0_x64__8wekyb3d8bbweMicrosoft.Wallet Microsoft.Wallet_2.4.18324.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.27323.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.27323.0_x86__8wekyb3d8bbweRealtekSemiconductorCorp.RealtekAudioControl RealtekSemiconductorCorp.RealtekAudioControl_1.3.179.0_x64__dt26b99r8h8gjMicrosoft.Print3D Microsoft.Print3D_3.3.791.0_x64__8wekyb3d8bbweMicrosoft.XboxSpeechToTextOverlay Microsoft.XboxSpeechToTextOverlay_1.21.13002.0_x64__8wekyb3d8bbweMicrosoft.Xbox.TCUI Microsoft.Xbox.TCUI_1.24.10001.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.27810.0_x86__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.27810.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.0 Microsoft.UI.Xaml.2.0_2.1810.18004.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.27912.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.1.7 Microsoft.NET.Native.Framework.1.7_1.7.27413.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.27912.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Framework.1.7 Microsoft.NET.Native.Framework.1.7_1.7.27413.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.2.1 Microsoft.NET.Native.Runtime.2.1_2.1.26424.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.2.1 Microsoft.NET.Native.Runtime.2.1_2.1.26424.0_x64__8wekyb3d8bbweMicrosoft.Messaging Microsoft.Messaging_4.1901.10241.1000_x64__8wekyb3d8bbweMicrosoft.Advertising.Xaml Microsoft.Advertising.Xaml_10.1811.1.0_x86__8wekyb3d8bbweMicrosoft.Advertising.Xaml Microsoft.Advertising.Xaml_10.1811.1.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.1 Microsoft.NET.Native.Framework.2.1_2.1.27427.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.1 Microsoft.NET.Native.Framework.2.1_2.1.27427.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.2 Microsoft.UI.Xaml.2.2_2.21909.17002.0_x86__8wekyb3d8bbweMicrosoft.UI.Xaml.2.2 Microsoft.UI.Xaml.2.2_2.21909.17002.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.27810.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.27810.0_x86__8wekyb3d8bbweMicrosoft.UI.Xaml.2.1 Microsoft.UI.Xaml.2.1_2.11906.6001.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.1 Microsoft.UI.Xaml.2.1_2.11906.6001.0_x86__8wekyb3d8bbweMicrosoft.Services.Store.Engagement Microsoft.Services.Store.Engagement_10.0.19011.0_x86__8wekyb3d8bbweMicrosoft.Services.Store.Engagement Microsoft.Services.Store.Engagement_10.0.19011.0_x64__8wekyb3d8bbweInputApp InputApp_1000.18362.449.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AAD.BrokerPlugin Microsoft.AAD.BrokerPlugin_1000.18362.449.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AccountsControl Microsoft.AccountsControl_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.AsyncTextService Microsoft.AsyncTextService_10.0.18362.449_neutral__8wekyb3d8bbweMicrosoft.BioEnrollment Microsoft.BioEnrollment_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.CredDialogHost Microsoft.CredDialogHost_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.ECApp Microsoft.ECApp_10.0.18362.449_neutral__8wekyb3d8bbweMicrosoft.LockApp Microsoft.LockApp_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.MicrosoftEdgeDevToolsClient Microsoft.MicrosoftEdgeDevToolsClient_1000.18362.449.0_neutral_neutral_...Microsoft.Windows.Apprep.ChxApp Microsoft.Windows.Apprep.ChxApp_1000.18362.449.0_neutral_neutral_cw5n1h...Microsoft.Windows.AssignedAccessLockApp Microsoft.Windows.AssignedAccessLockApp_1000.18362.449.0_neutral_neutra...Microsoft.Windows.CallingShellApp Microsoft.Windows.CallingShellApp_1000.18362.449.0_neutral_neutral_cw5n...Microsoft.Windows.CapturePicker Microsoft.Windows.CapturePicker_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.Windows.OOBENetworkCaptivePortal Microsoft.Windows.OOBENetworkCaptivePortal_10.0.18362.449_neutral__cw5n...Microsoft.Windows.OOBENetworkConnectionFlow Microsoft.Windows.OOBENetworkConnectionFlow_10.0.18362.449_neutral__cw5...Microsoft.Windows.ParentalControls Microsoft.Windows.ParentalControls_1000.18362.449.0_neutral_neutral_cw5...Microsoft.Windows.PinningConfirmationDialog Microsoft.Windows.PinningConfirmationDialog_1000.18362.449.0_neutral__c...Microsoft.XboxGameCallableUI Microsoft.XboxGameCallableUI_1000.18362.449.0_neutral_neutral_cw5n1h2tx...1527c705-839a-4832-9118-54d4Bd6a0c89 1527c705-839a-4832-9118-54d4Bd6a0c89_10.0.18362.449_neutral_neutral_cw5...c5e2524a-ea46-4f67-841f-6a9465d9d515 c5e2524a-ea46-4f67-841f-6a9465d9d515_10.0.18362.449_neutral_neutral_cw5...E2A4F912-2574-4A75-9BB0-0D023378592B E2A4F912-2574-4A75-9BB0-0D023378592B_10.0.18362.449_neutral_neutral_cw5...F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE_10.0.18362.449_neutral_neutral_cw5...Microsoft.MicrosoftEdge Microsoft.MicrosoftEdge_44.18362.449.0_neutral__8wekyb3d8bbweMicrosoft.PPIProjection Microsoft.PPIProjection_10.0.18362.449_neutral_neutral_cw5n1h2txyewyMicrosoft.Win32WebViewHost Microsoft.Win32WebViewHost_10.0.18362.449_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.CloudExperienceHost Microsoft.Windows.CloudExperienceHost_10.0.18362.449_neutral_neutral_cw...Microsoft.Windows.ContentDeliveryManager Microsoft.Windows.ContentDeliveryManager_10.0.18362.449_neutral_neutral...Microsoft.Windows.NarratorQuickStart Microsoft.Windows.NarratorQuickStart_10.0.18362.449_neutral_neutral_8we...Microsoft.Windows.PeopleExperienceHost Microsoft.Windows.PeopleExperienceHost_10.0.18362.449_neutral_neutral_c...Microsoft.Windows.SecHealthUI Microsoft.Windows.SecHealthUI_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.Windows.SecureAssessmentBrowser Microsoft.Windows.SecureAssessmentBrowser_10.0.18362.449_neutral_neutra...Microsoft.Windows.ShellExperienceHost Microsoft.Windows.ShellExperienceHost_10.0.18362.449_neutral_neutral_cw...Microsoft.Windows.StartMenuExperienceHost Microsoft.Windows.StartMenuExperienceHost_10.0.18362.449_neutral_neutra...Microsoft.Windows.XGpuEjectDialog Microsoft.Windows.XGpuEjectDialog_10.0.18362.449_neutral_neutral_cw5n1h...Windows.CBSPreview Windows.CBSPreview_10.0.18362.449_neutral_neutral_cw5n1h2txyewyMicrosoft.UI.Xaml.2.3 Microsoft.UI.Xaml.2.3_2.32002.13001.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.3 Microsoft.UI.Xaml.2.3_2.32002.13001.0_x86__8wekyb3d8bbweAppUp.IntelGraphicsControlPanel AppUp.IntelGraphicsControlPanel_3.3.0.0_x64__8j3eq9eme6cttMicrosoft.NET.Native.Runtime.2.2 Microsoft.NET.Native.Runtime.2.2_2.2.28604.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.2.2 Microsoft.NET.Native.Runtime.2.2_2.2.28604.0_x64__8wekyb3d8bbweMicrosoft.EdgeDevtoolsPlugin Microsoft.EdgeDevtoolsPlugin_10.0.18362.449_neutral_neutral_cw5n1h2txyewyCanonicalGroupLimited.Ubuntu16.04onWindows CanonicalGroupLimited.Ubuntu16.04onWindows_2020.1604.14.0_x64__79rhkp1f...Microsoft.NET.Native.Runtime.1.7 Microsoft.NET.Native.Runtime.1.7_1.7.27422.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.1.7 Microsoft.NET.Native.Runtime.1.7_1.7.27422.0_x64__8wekyb3d8bbweMicrosoft.XboxGameOverlay Microsoft.XboxGameOverlay_1.54.4001.0_x64__8wekyb3d8bbweMicrosoft.XboxIdentityProvider Microsoft.XboxIdentityProvider_12.67.21001.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.4 Microsoft.UI.Xaml.2.4_2.42007.9001.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.4 Microsoft.UI.Xaml.2.4_2.42007.9001.0_x86__8wekyb3d8bbweMicrosoft.ScreenSketch Microsoft.ScreenSketch_10.2008.2277.0_x64__8wekyb3d8bbweMicrosoft.OneConnect Microsoft.OneConnect_5.2011.3081.0_x64__8wekyb3d8bbweMicrosoft.DesktopAppInstaller Microsoft.DesktopAppInstaller_1.4.3161.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.29512.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.29512.0_x86__8wekyb3d8bbweMicrosoft.Office.OneNote Microsoft.Office.OneNote_16001.13328.20478.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.5 Microsoft.UI.Xaml.2.5_2.52012.2002.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.5 Microsoft.UI.Xaml.2.5_2.52012.2002.0_x86__8wekyb3d8bbweMicrosoft.WindowsAlarms Microsoft.WindowsAlarms_10.2101.28.0_x64__8wekyb3d8bbweMicrosoft.WindowsCamera Microsoft.WindowsCamera_2021.105.10.0_x64__8wekyb3d8bbweMicrosoft.WebMediaExtensions Microsoft.WebMediaExtensions_1.0.40831.0_x64__8wekyb3d8bbweMicrosoft.WebpImageExtension Microsoft.WebpImageExtension_1.0.41203.0_x64__8wekyb3d8bbweMicrosoft.WindowsStore Microsoft.WindowsStore_12104.1001.1.0_x64__8wekyb3d8bbweMicrosoft.BingWeather Microsoft.BingWeather_4.46.31121.0_x64__8wekyb3d8bbweMicrosoft.WindowsCalculator Microsoft.WindowsCalculator_10.2103.8.0_x64__8wekyb3d8bbweMicrosoft.WindowsSoundRecorder Microsoft.WindowsSoundRecorder_10.2103.28.0_x64__8wekyb3d8bbweMicrosoft.MSPaint Microsoft.MSPaint_6.2105.4017.0_x64__8wekyb3d8bbweMicrosoft.Microsoft3DViewer Microsoft.Microsoft3DViewer_7.2105.4012.0_x64__8wekyb3d8bbweMicrosoft.HEIFImageExtension Microsoft.HEIFImageExtension_1.0.40978.0_x64__8wekyb3d8bbwe1527c705-839a-4832-9118-54d4Bd6a0c89 1527c705-839a-4832-9118-54d4Bd6a0c89_10.0.18362.1533_neutral_neutral_cw...c5e2524a-ea46-4f67-841f-6a9465d9d515 c5e2524a-ea46-4f67-841f-6a9465d9d515_10.0.18362.1533_neutral_neutral_cw...E2A4F912-2574-4A75-9BB0-0D023378592B E2A4F912-2574-4A75-9BB0-0D023378592B_10.0.18362.1533_neutral_neutral_cw...F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE_10.0.18362.1533_neutral_neutral_cw...InputApp InputApp_1000.18362.1533.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AAD.BrokerPlugin Microsoft.AAD.BrokerPlugin_1000.18362.1533.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AccountsControl Microsoft.AccountsControl_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.AsyncTextService Microsoft.AsyncTextService_10.0.18362.1533_neutral__8wekyb3d8bbweMicrosoft.BioEnrollment Microsoft.BioEnrollment_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.CredDialogHost Microsoft.CredDialogHost_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.ECApp Microsoft.ECApp_10.0.18362.1533_neutral__8wekyb3d8bbweMicrosoft.EdgeDevtoolsPlugin Microsoft.EdgeDevtoolsPlugin_10.0.18362.1533_neutral_neutral_cw5n1h2txyewyMicrosoft.LockApp Microsoft.LockApp_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.MicrosoftEdgeDevToolsClient Microsoft.MicrosoftEdgeDevToolsClient_1000.18362.1533.0_neutral_neutral...Microsoft.MicrosoftEdge Microsoft.MicrosoftEdge_44.18362.1533.0_neutral__8wekyb3d8bbweMicrosoft.PPIProjection Microsoft.PPIProjection_10.0.18362.1533_neutral_neutral_cw5n1h2txyewyMicrosoft.Win32WebViewHost Microsoft.Win32WebViewHost_10.0.18362.1533_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.Apprep.ChxApp Microsoft.Windows.Apprep.ChxApp_1000.18362.1533.0_neutral_neutral_cw5n1...Microsoft.Windows.AssignedAccessLockApp Microsoft.Windows.AssignedAccessLockApp_1000.18362.1533.0_neutral_neutr...Microsoft.Windows.CallingShellApp Microsoft.Windows.CallingShellApp_1000.18362.1533.0_neutral_neutral_cw5...Microsoft.Windows.CapturePicker Microsoft.Windows.CapturePicker_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.Windows.CloudExperienceHost Microsoft.Windows.CloudExperienceHost_10.0.18362.1533_neutral_neutral_c...Microsoft.Windows.ContentDeliveryManager Microsoft.Windows.ContentDeliveryManager_10.0.18362.1533_neutral_neutra...Microsoft.Windows.Cortana Microsoft.Windows.Cortana_1.13.1.18362_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.NarratorQuickStart Microsoft.Windows.NarratorQuickStart_10.0.18362.1533_neutral_neutral_8w...Microsoft.Windows.OOBENetworkCaptivePortal Microsoft.Windows.OOBENetworkCaptivePortal_10.0.18362.1533_neutral__cw5...Microsoft.Windows.OOBENetworkConnectionFlow Microsoft.Windows.OOBENetworkConnectionFlow_10.0.18362.1533_neutral__cw...Microsoft.Windows.ParentalControls Microsoft.Windows.ParentalControls_1000.18362.1533.0_neutral_neutral_cw...Microsoft.Windows.PeopleExperienceHost Microsoft.Windows.PeopleExperienceHost_10.0.18362.1533_neutral_neutral_...Microsoft.Windows.PinningConfirmationDialog Microsoft.Windows.PinningConfirmationDialog_1000.18362.1533.0_neutral__...Microsoft.Windows.SecHealthUI Microsoft.Windows.SecHealthUI_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.Windows.SecureAssessmentBrowser Microsoft.Windows.SecureAssessmentBrowser_10.0.18362.1533_neutral_neutr...Microsoft.Windows.ShellExperienceHost Microsoft.Windows.ShellExperienceHost_10.0.18362.1533_neutral_neutral_c...Microsoft.Windows.StartMenuExperienceHost Microsoft.Windows.StartMenuExperienceHost_10.0.18362.1533_neutral_neutr...Microsoft.Windows.XGpuEjectDialog Microsoft.Windows.XGpuEjectDialog_10.0.18362.1533_neutral_neutral_cw5n1...Microsoft.XboxGameCallableUI Microsoft.XboxGameCallableUI_1000.18362.1533.0_neutral_neutral_cw5n1h2t...Windows.CBSPreview Windows.CBSPreview_10.0.18362.1533_neutral_neutral_cw5n1h2txyewyMicrosoft.VP9VideoExtensions Microsoft.VP9VideoExtensions_1.0.41182.0_x64__8wekyb3d8bbweMicrosoft.WindowsMaps Microsoft.WindowsMaps_10.2104.2.0_x64__8wekyb3d8bbweMicrosoft.SkypeApp Microsoft.SkypeApp_15.72.94.0_x86__kzf8qxf38zg5cMicrosoft.StorePurchaseApp Microsoft.StorePurchaseApp_12105.5555.19.0_x64__8wekyb3d8bbweMicrosoft.GetHelp Microsoft.GetHelp_10.2105.41472.0_x64__8wekyb3d8bbweMicrosoft.XboxGamingOverlay Microsoft.XboxGamingOverlay_5.721.5282.0_x64__8wekyb3d8bbweMicrosoft.YourPhone Microsoft.YourPhone_1.21042.143.0_x64__8wekyb3d8bbweMicrosoft.ZuneVideo Microsoft.ZuneVideo_10.21061.10121.0_x64__8wekyb3d8bbweMicrosoft.ZuneMusic Microsoft.ZuneMusic_10.21061.10121.0_x64__8wekyb3d8bbweMicrosoft.MicrosoftStickyNotes Microsoft.MicrosoftStickyNotes_4.1.6.0_x64__8wekyb3d8bbwe Microsoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.30035.0_x64__8wekyb3d8bbwe Microsoft.XboxApp Microsoft.XboxApp_48.78.15001.0_x64__8wekyb3d8bbwe Microsoft.MixedReality.Portal Microsoft.MixedReality.Portal_2000.21051.1282.0_x64__8wekyb3d8bbwe Microsoft.People Microsoft.People_10.2105.4.0_x64__8wekyb3d8bbwe Microsoft.MicrosoftOfficeHub Microsoft.MicrosoftOfficeHub_18.2106.12410.0_x64__8wekyb3d8bbwe Microsoft.WindowsTerminal Microsoft.WindowsTerminal_1.9.1942.0_x64__8wekyb3d8bbwe AppUp.IntelGraphicsExperience AppUp.IntelGraphicsExperience_1.100.3370.0_x64__8j3eq9eme6ctt Microsoft.Getstarted Microsoft.Getstarted_10.4.41811.0_x64__8wekyb3d8bbwe Microsoft.Windows.Photos Microsoft.Windows.Photos_2021.21070.22007.0_x64__8wekyb3d8bbwe Microsoft.MicrosoftSolitaireCollection Microsoft.MicrosoftSolitaireCollection_4.10.7290.0_x64__8wekyb3d8bbwe Microsoft.LanguageExperiencePackzh-CN Microsoft.LanguageExperiencePackzh-CN_18362.41.126.0_neutral__8wekyb3d8... microsoft.windowscommunicationsapps microsoft.windowscommunicationsapps_16005.14326.20090.0_x64__8wekyb3d8bbwe Microsoft.WindowsFeedbackHub Microsoft.WindowsFeedbackHub_1.2106.1801.0_x64__8wekyb3d8bbwe Microsoft.UI.Xaml.2.7 Microsoft.UI.Xaml.2.7_7.2109.13004.0_x86__8wekyb3d8bbwe Microsoft.UI.Xaml.2.7 Microsoft.UI.Xaml.2.7_7.2109.13004.0_x64__8wekyb3d8bbwe Microsoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.30704.0_x86__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.30704.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.30704.0_x86__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.30704.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.6 Microsoft.UI.Xaml.2.6_2.62112.3002.0_x86__8wekyb3d8bbweMicrosoft.UI.Xaml.2.6 Microsoft.UI.Xaml.2.6_2.62112.3002.0_x64__8wekyb3d8bbwe 搜索 Store 可以找到 Microsoft.WindowsStore Microsoft.WindowsStore_12104.1001.1.0_x64__8wekyb3d8bbwe 这一项，具体包名根据不同电脑搜索结果而定。 然后运行安装命令就可以了，注意把其中的 Microsoft.WindowsStore_12104.1001.1.0_x64__8wekyb3d8bbwe内容替换成自己搜索出的结果，Add-appxpackage... 命令运行完之后Windows商店就出现了。 12? # Albert at HOME-PC in C:\Windows\system32 [20:00:00]$ Add-appxpackage -register "C:\Program Files\WindowsApps\Microsoft.WindowsStore_12104.1001.1.0_x64__8wekyb3d8bbwe\appxmanifest.xml" -disabledevelopmentmode QuickLook双击打开Windows商店，搜索QuickLook直接安装就可以，一个很不错的小工具，这是来自商店的介绍 官网传送门 总结 Win+x 快速打开管理菜单，Win+i快速打开设置界面 PowerShell中运行 Get-AppxPackage -allusers | Select Name, PackageFullName 列举可安装的软件包 Add-appxpackage -register &quot;C:\Program Files\WindowsApps\****\appxmanifest.xml&quot; -disabledevelopmentmode 安装指定软件** QuickLook快速预览小程序，值得一试 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 元宇宙，一个人人都想分一杯羹的概念，理想很美好，但是现实往往很残酷，当一个人很上头的时候，任何语言都会显得苍白无力，不要只是头脑发热的一直去想，看看别人都是怎么做的，有多少人已经退了出来？人类往往很诚实，但是这不表现在言语上~ 2022-1-9 23:24:47]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>windows</tag>
        <tag>商店</tag>
        <tag>软件</tag>
        <tag>QuickLook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2021年终总结——脚踏实地，为下一次腾飞积蓄力量]]></title>
    <url>%2Fblog%2F2022%2F01%2F09%2F2021%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E8%84%9A%E8%B8%8F%E5%AE%9E%E5%9C%B0%EF%BC%8C%E4%B8%BA%E4%B8%8B%E4%B8%80%E6%AC%A1%E8%85%BE%E9%A3%9E%E7%A7%AF%E8%93%84%E5%8A%9B%E9%87%8F%2F</url>
    <content type="text"><![CDATA[有一种幸福叫照片上的人都还在~ 前言又到了一年一度的交卷时刻，去年的flag可以改个年份继续提上日程了。2021年，平平淡淡的一年，平淡到蜗居在这个寸土寸金的帝都，都没回过几次家。不过平淡并非不好，这份平淡恰恰是一种幸福，感恩我们所在的这个和平的国度，感恩所有为了这份和平付出努力的人们。 人是会慢慢成长的，很多曾经不懂的事情一瞬间全都明白了，有些事情已经晚了，可是还有很多事情可以补救，人生就是不断拿一些东西去换另一些东西的旅程，这种交换无时无刻不在进行着，随着时间这个魔法师渐渐推移，我们突然想换回来了，才发现做了半生的无用功。 太年轻的人 他总是不满足固执地不愿停下 远行的脚步望着高高的天走了长长的路忘了回头看 她有没有哭 生活还要继续，把握当下才能赢得未来，踏踏实实迈出每一步，不需要鲜花和掌声，其实踩出的每一个脚印都是实实在在的生活。 回顾2021既然是年终总结，总要回忆一下去年过的怎么样，最简单的还是来看看去年flag的大脸盛况，拿出我的八股文框架，开整~ 工作上 FLAG 脚踏实地做好本职工作 额外挤出时间去尝试技术提升（优化、解决痛点） 在熟悉业务的同时更多参与设计的工作，拓宽自己的认知范围 完成度：85% 一直觉得“靠谱”这个词对程序员来说是一个很高的评价，而我也在向这个方向默默努力着，事事有反馈是一个人的态度，同时也是一种良好的品质，在过去的一年中没有出大问题，也没有出彩，总体上算是中规中矩，也实现了自己“脚踏实地做好本职工作”的目标。 在技术提升方面，也在进一步努力，优化和解决痛点的工作一直在做，但相比较来说有些欠缺，技术提升和补充这方面主要体现在两部分，一部分来源于日常工作，需求推动进步，为了实现需求而提升自己，虽然进步不大，但贵在学以致用。 另一部分来源于面试，今年发生了角色的转换，我从一个求职者变成了面试官，这是一个可以锻炼自己的宝贵的机会，阅人然后识己，通过准备面试，翻看笔试题，同时观察候选人的表现，来发掘出自己需要提升的部分，循序渐进，总结提升，比如《树的带权路径长度和哈夫曼树》、《完全图与强连通图的那些坑》 都是在面试之后总结得来的。 至于更多的参与设计工作，今年也做了不少努力，得益于项目组的调整以及和老大的热心支持，今年抽出了一些时间来做偏向底层的设计工作，将自己的想法融入项目之中，静等花开。 学习上 FLAG 博客总结继续，基本保持在1周一篇，可以适当偷懒，一年懒10次可以产出40篇 开源代码还是要继续学习，libevent需要详细看一下，今年的出镜率太高 读2本技术类书籍，可以是开阔眼界的，也可以是现有技能提升的 读2本经济学、金融理财相关的书籍 完成度：70% 关于博客总结这个flag真的是把偷懒发挥到了极致，到年底正好产出40篇总结，内容比较分散，基本上都是日常工作学习中遇到的问题，弄明白之后总结到一起，方便后续翻看查找。 开源代码看的相对较少，这部分还需继续努力，libevent因为懒没有看进去，只在年初看了几个数据结构，不靠问题驱动着看代码还是挺难的，特别是这种代码量比较多的库，所以后面转变思路，从一些代码量少的小工具入手，比如这个神奇的grep命令 读书方面，并没有完整看完两本技术书籍，但是其他类型的书籍确实看了不少，如果加上各种故事书都堆成小山了： 小狗钱钱 富爸爸穷爸爸 半小时漫画中国史 半小时漫画中国史2 半小时漫画中国史3 半小时漫画中国史4 半小时漫画中国史5 半小时漫画世界史 硬笔行书字帖 ——《樂墨書院》 不吼不叫培养好孩子 领导力的本质 ——向松下幸之助和稻盛和夫学习 素直之心，以人为本，换位思考，尊重人性，经营哲学 深度思考——不断逼近问题的本质 一本书读懂财报 每一个企业都有它所处行业的烙印 闭环思维——让靠谱成为习惯 我会为你的过程鼓掌，但我只会给你的结果买单 底层逻辑——半秒钟看透问题本质 一切以结果为导向，没有功劳，苦劳也是无用的 刷题方面，今天有点虎头蛇尾了，前几个月还在努力紧跟大佬的脚步，后几个月因为工作、生活中的各种事情吧，刷题速度直线下降，人的精力是有限的，作为一个已经步入社会的人，像学生一样每天拿出大把时间来高强度刷题是不现实的，只能自己来平衡时间了，一段时间刷几道，避免太手生就好了。 生活上 FLAG 陪娃娃，陪家人，工作内容适应后可以多拿出一点时间和家人在一起（需要比2020多一些） 投资理财还是要多花一点时间研究下，目标7%（靠工资是不可能财富自由的，必须开源才行） 注重身体的保养，身体是本钱，可不能把身体搞垮了，愿丢掉体检时的小毛病~ 完成度：65% 陪伴是最长情的告白，今年调整了工作学习方式，尽可能把工作学习时间放到工作日，然后周末节假日就是一心一意陪家人。工作日疯狂输出，周末节假日休养生息，这种方式也不错。 利用放假时间和小娃娃一起玩耍，比如买几盆多肉，一起体会栽种花草的乐趣，一起做丢手绢、木头人的游戏，宝宝欢喜的不得了，只要有人陪着疯，什么电视、手机都抛到脑后，所以当自家娃娃沉迷于某些东西不能自拔时，需要反思一下自己有没有认真的陪过她。 我抱起砖头就没法抱你，放下砖头就没法养你，在这样高速发展的社会，社会分工相当明确，再想守着自己家的一亩三分地来自给自足是不可能了，我们都想争做人上人，但是吃的苦中苦只有自己清楚。 谈到投资理财，我人没了，之前作为小韭菜在股市摸爬滚打、追涨杀跌赚了一些银子，今年转变思路，拥抱大白马，等待低估值修复，结果成了一个被人割的老韭菜，不过暂时不打算调仓，重仓银地保、中丐互怜，投资是反人性的，只要握紧头寸不撒手，你就别想割到我。 人的情绪是波动的，想想一个没有任何消息的公司股价为什么会起起伏伏，是因为每个人对后市的股价走向做了预测，然后根据预测进行买入卖出操作，这种短期预测毫无意义，但是价格在价值周围波动这是市场规律，顺大势，勿逆行。 今年的股票成绩略微惨淡，其实和去年也差不多，坐了几波小板车，加上ETF和基金，浮亏一点点，在今天价值投资被埋在谷底的年头，也还可以吧，继续出清垃圾股。 展望2022工作 继续踏实做好本职工作，做好工作内容的总结，落实到纸上 学习和了解常见中间件的使用，更多的参与设计的工作 学习 博客总结不能落，继续保持一周一更，完成40篇基础目标 刷题不用太频繁，每周总得有贡献，不可抗拒因素除外 建立自己的技能树，搭配工作总结，统计出自己到底会什么 选取经典开源代码学习，代码量要少一点，毕竟精力有限了 读书、读书、读书，书都买好了 投资 基金和ETF继续定投，适当配置债券固收 股票池里把曾经“瞎选”的股票逐渐出清，依旧拥抱大白马，少折腾 目标收益不太高，8个点，希望不要打脸 生活 疫情结束了多回几次老家吧，去看看那些想我又不愿说出口的亲人 注重身体的保养，锻炼提上日程，降低亮红灯的指标 总结 身体有些吃不消了，适当缩减睁着眼睛的时间吧 工作学习分不开，所以两者会共同产出一颗技能树，为后续选择提供腾飞的力量 我觉得是时候把投资这项单独拿出来了，靠搬砖是不能实现自由梦的，需要学会借力搬砖 噢，忘了一点，去年出现在年终总结中的那颗茉莉花挂了，我补两张今年养的多肉吧，希望他能抗住今年的flag~ 梦想 每个人都有自己梦想，而我的梦很简单，那里有一个大大的书房，中间摆一个简单的书桌，一把带轮子的小椅，后面是占满整面墙的书柜，书柜上摆放着我喜欢的技术、财经、历史方面的书籍。书桌上斜放着两台电脑，看书累了就写写自己喜欢的代码，我并不喜欢庞大的程序，更热衷于一些实用的小工具，比如像grep这种锋利的小匕首。偶尔小宝宝会跑进来要我陪她玩，我会给它讲讲正在写哪些有趣的东西，玩一玩自己开发的游戏，是的，她很喜欢，一个简单的小球都玩得不亦乐乎~ 当然了大大的玻璃窗前必须摆满喜欢的花草，他们尽情的吮吸窗外的阳光，同时为书屋提供了生命的颜色，花架旁边是喷壶和铲子，虽然看起来不像是书房里的东西，但是我想把它们摆在那里，这里仿佛是另一番天地。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 空有悲天悯人之心，却无普度众生之力，不愿最后伸出那无助的手，要在可以选择时努力做一个施粥人。 2021-12-31 23:43:27]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows设置防火墙允许指定应用正常使用网络]]></title>
    <url>%2Fblog%2F2021%2F12%2F26%2FWindows%E8%AE%BE%E7%BD%AE%E9%98%B2%E7%81%AB%E5%A2%99%E5%85%81%E8%AE%B8%E6%8C%87%E5%AE%9A%E5%BA%94%E7%94%A8%E6%AD%A3%E5%B8%B8%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[前言很长一段时间以来，防火墙这个神奇的“玩意”在我的电脑上基本属于名存死亡，因为做网络软件开发的缘故，经常需要别人连接我的电脑来进行测试，打开Windows的防火墙常常出现网络无法联通的情况，所以在我的电脑上防火墙基本处于关闭状态，就像下面这样： 但这确实不是一个好的习惯，最近学习了一些网络安全相关的课程，发现打开防火墙还是很有必要的，它可以避免我们遭受一些攻击，最起码可以降低受到攻击的概率，所以我还是决定打开防火墙，同时设置防火墙对自己的开发的应用放行，也就是把应用放到防火墙白名单里，这样就既安全又方便了。 什么是Windows防火墙 防火墙是一项协助确保信息安全的设备，会依照特定的规则，允许或限制传输的数据通过。防火墙可以是一台专属的硬件也可以是架设在一般硬件上的一套软件。总而言之，防火墙就是帮助计算机网络于其内、外网之间构建一道相对隔绝的保护屏障。 这段文字来自于百科，翻译的直白一点就是说：你的电脑就相当于一个城池，然后网络上的病毒、木马等恶意程序就相当于敌对势力派来的间谍，想混进城去，而防火墙就相当于城门口设置的关卡哨兵用来盘查筛选，放合适的人进城，同时放合法的人出去，既要满足城内人与外界的交流，又要防止恶意的坏人蒙混过关。 设置允许通过防火墙的应用要想设置指定应用允许通过防火墙，需要先打开防火墙功能再设置才会生效，下面以Windows10为例操作一次。 打开控制面板 按键盘上的 Win 键，然后输入控制面板，在筛选结果中点击“控制面板”选项： 点击系统和安全选项 点击Windows Defender防火墙选项 其实前面这三步可以简化为，按Win+R快捷键，输入firewall.cpl回车即可 点击启用或关闭Windows Defender防火墙选项 勾选启用 Windows Defender 防火墙 选项，点击确定按钮 点击允许应用或功能通过 Windows Defender 防火墙选项 找到目标应用，把后面两个选项勾选上，然后点击确定即可 这里以 WeChat 为例，在专用 和 公用两个复选框上打钩，然后单击下面的确定按钮就可以了，如果列表中没有想要添加白名单的应用，还可以点击确定按钮上面的 允许其他应用(R)按钮来自己添加。 防火墙中的专用和公用防火墙中的专用 和 公用是针对网络而言的，Windows中给出的说明：专用网络，例如家庭或工作网络；公用网络，比如机场和咖啡店中的网络。那么是不是当我在家里连网时就是专用网络，而在咖啡厅连接网络时就自动识别为公用网络呢？ Windows当然没有这么智能，连接新的网络时属于什么类型的网络需要你自己选择，也就是说你自己要做出判断，如果是自己家的网络，基本是上独享的安全的，那就可以设置为专用网络，如果是连接的公共Wifi那必须要选择公用网络。 而防火墙中对专用网络和公用网络有不同的设置，这个比较好理解，还是以前面城池和哨兵的比喻为例，防火墙作为哨兵在不同的环境下有不同的标准，比如在和平时期，对非常可疑的人才禁止入城，而在战时，除了必要的粮草供应，其他的普通商人可能都不允许入内了。 所以当一个应用无法联网时，需要检查一下应用设置白名单中是在哪个网络环境下，如果是可以信赖的应用，通常在专用网络和公用网络都会设置允许通过防火墙。 总结 快捷键 Win+R 输入 firewall.cpl 后回车，可以快速打开防火墙设置 Windows中的专用网络和公用网络需要自己在连接网络时设置，系统本身没有准确分辨的能力 防火墙中对专用网络和公用网络有不同的设置，相当于在不同的网络环境下设置了不同的安全级别 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 默默的付出是否值得？一查问题就消耗几个小时，修复个BUG前前后后花了几天，有时会假设站在高处，望着此时的自己，这样做是否值得？成年人的世界往往只看性价比，如果说花了几天时间却一无所获，那之前的付出确实不值得，也就是做了一件性价比很低的事情。但是这个结果是做完才知道的，在做之前并不能准确判断是否值得，无法进行“挑选”，也不应该抱着我不做总有人会做的态度，既然做就全力以赴，若无果也仅仅是我们“赌”输了而已。但要记住，现实的世界只记得功劳，没有结论、没有反馈、毫无借鉴意义的苦劳一文不值~ 2021-12-26 23:54:19]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>windows</tag>
        <tag>防火墙</tag>
        <tag>firewall</tag>
        <tag>白名单</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git常用配置——git show/diff tab 显示宽度]]></title>
    <url>%2Fblog%2F2021%2F12%2F19%2Fgit%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94git-show-diff-tab-%E6%98%BE%E7%A4%BA%E5%AE%BD%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[前言平时在代码开发过程中使用编辑器时一般会使用空格来代替Tab符，而Tab的宽度通常设置为4个空格的宽度。在使用git diff 命令时发现有些老代码的格式混乱，原因就是使用了8个字符宽度的Tab显示格式，经过一番查找发现可以修改 core.pager 来进行调整，特此记录方便日后查找。 常用配置 设置Tab显示为4个字符的宽度 git config --global core.pager &#39;less -x1,5&#39; 设置Tab显示为8个字符的宽度（默认） git config --global core.pager &#39;less&#39; 设置用户名 git config --global user.name &quot;albert&quot; 设置邮箱 git config --global user.email &quot;albert52190@gmail.com&quot; 设置别名 git config --global alias.st status 配置自动更新子模块的命令 git config --global alias.pullall &#39;!f(){ git pull &quot;$@&quot; &amp;&amp; git submodule update --init --recursive; }; f&#39; 设置自动处理换行符 git config --global core.autocrlf true true 在提交时将CRLF转换为LF，当签出代码时，LF会被转换成CRLFinput 在提交是将CRLF转换为LF，签出时不转换false 不进行转换 总结 git config --global core.pager &#39;less -x1,5&#39; 可将 git show/diff 命令的tab按照4字符宽度显示 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 时间不多了，还有很多事情没有做呢，虽说欲速则不达，但是有些事如果还不做可能就真的没机会了《一荤一素》 2021-12-20 00:09:54]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
        <tag>Git</tag>
        <tag>config</tag>
        <tag>pager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络工具nc的常见使用功能方法]]></title>
    <url>%2Fblog%2F2021%2F12%2F12%2F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7nc%E7%9A%84%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%8A%9F%E8%83%BD%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言nc 是一个Linux环境下常用的工具命令，可以用来帮助开发者查询和解决网路问题，通常被认为是 NetCat 工具的缩写，在网络工具中有“瑞士军刀”的美誉。 nc 在Linux环境下常常是自带的，如果你使用的Linux发行版本没有这个工具也可以自行安装，比如在CentOS上的安装命令就是 yum install nc -y，并且这个工具在Windows上也可以直接下载使用，下载页面地址为 netcat，简约而不简单。 nc netcat ncat这三个名字常常出现在一起，也常常混用，可以简单的认为它们都一样，但是如果要追究细节它们还是有些差异的。原始netcat（也就是nc），在2007年发布1.10稳定版本之后，就不再更新了，原作者是Hobbit。而ncat是nmap项目的作者Fyodor，在原始nc之上进行二次开发的另一款强大工具。另外socat、cryptcat等也属于是原始nc的升级，而原始nc在windows上有时会被杀毒软件查杀，这时可以考虑使用ncat、socat。 关于nc的不同，可以看下我本地的查看情况，第一条是在Ubuntu中的man手册说明，第二条是在CentOS中的man手册说明： 12345678910111213NC(1) BSD General Commands Manual NC(1)NAME nc — arbitrary TCP and UDP connections and listensSYNOPSIS nc [-46bCDdhklnrStUuvZz] [-I length] [-i interval] [-O length] [-P proxy_username] [-p source_port] [-q seconds] [-s source] [-T toskeyword] [-V rtable] [-w timeout] [-X proxy_protocol] [-x proxy_address[:port]] [destination] [port]DESCRIPTION The nc (or netcat) utility is used for just about anything under the sun involving TCP, UDP, or UNIX-domain sockets. It can open TCP connections, send UDP packets, listen on arbitrary TCP and UDP ports, do port scanning, and deal with both IPv4 and IPv6. Unlike telnet(1), nc scripts nicely, and separates error messages onto standard error instead of sending them to standard output, as telnet(1) does with some 123456789101112NCAT(1) Ncat Reference Guide NCAT(1)NAME ncat - Concatenate and redirect socketsSYNOPSIS ncat [OPTIONS...] [hostname] [port]DESCRIPTION Ncat is a feature-packed networking utility which reads and writes data across networks from the command line. Ncat was written for the Nmap Project and is the culmination of the currently splintered family of Netcat incarnations. It is designed to be a reliable back-end tool to instantly provide network connectivity to other applications and users. Ncat will not only work with IPv4 and IPv6 but provides the user with a virtually limitless number of potential uses. ncat 是 nmap 套件的一部分，关于nmap的介绍可以参考下来自百科的以下引用内容： nmap是linux最早的网络扫描工具和嗅探工具包，它可以用来扫描网络上电脑开放的网络连接端，确定哪些服务运行在那些连接端，并且推断出计算机运行的是哪个操作系统（这是亦称 fingerprinting）。它是网络管理员必用的软件之一，用以评估网络系统安全。 正如大多数被用于网络安全的工具，nmap 也是不少黑客及骇客爱用的工具 。系统管理员可以利用nmap来探测工作环境中未经批准使用的服务器，但是黑客会利用nmap来搜集目标电脑的网络设定，从而计划攻击的方法。 nc的用法nc是一个强大的网络工具，下面列举几个常见的用法。 测试udp端口是否可用如果是测试某个IP地址地址是否可以访问，通常会使用 ping 命令，执行之后如果可以到达就会得到数据反馈： 12345678910albert@home-pc:~$ ping 82.156.125.169PING 82.156.125.169 (82.156.125.169) 56(84) bytes of data.64 bytes from 82.156.125.169: icmp_seq=1 ttl=54 time=17.8 ms64 bytes from 82.156.125.169: icmp_seq=2 ttl=54 time=39.9 ms64 bytes from 82.156.125.169: icmp_seq=3 ttl=54 time=12.9 ms64 bytes from 82.156.125.169: icmp_seq=4 ttl=54 time=6.81 ms^C--- 82.156.125.169 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3008msrtt min/avg/max/mdev = 6.816/19.397/39.961/12.502 ms 想要测试一个tcp端口是否可以访问，映入脑海的第一个命令应该是telnet，直接在IP后面加空格和端口就可以了： 12345albert@home-pc:~$ telnet 82.156.125.169 22Trying 82.156.125.169...Connected to 82.156.125.169.Escape character is '^]'.SSH-2.0-OpenSSH_7.4 如果想测试UDP端口是否可达使用telnet就不行了，因为它是一种基于TCP的应用层协议，用来测试UDP端口会因为长时间没有反应而超时： 123albert@home-pc:~$ telnet 82.156.125.169 666Trying 82.156.125.169...telnet: Unable to connect to remote host: Connection refused 这时用 nc 命令就可以解决了，命令格式为 nc -nvuz ip port： 12albert@home-pc:~$ nc -nvuz 82.156.125.169 666Connection to 82.156.125.169 666 port [udp/*] succeeded! 端口扫描这实际实际上是对上一个应用的扩展，使用 nc 命令可以指定一个端口范围，用来扫描多个端口是否可用： 123456albert@home-pc:~$ nc -nvz 82.156.125.169 20-24nc: connect to 82.156.125.169 port 20 (tcp) failed: Connection refusednc: connect to 82.156.125.169 port 21 (tcp) failed: Connection refusedConnection to 82.156.125.169 22 port [tcp/*] succeeded!nc: connect to 82.156.125.169 port 23 (tcp) failed: Connection refusednc: connect to 82.156.125.169 port 24 (tcp) failed: Connection refused 一对一聊天使用 nc 命令可以监听一个端口作为服务器，然后在另一台机器上启动 nc 作为客户端发数据： 12345# 启动服务器[root@VM-0-3-centos ~]# nc -l 1314# 以下为接收到客户端发来的数据12client send msg 12345# 启动客户端albert@home-pc:~$ nc 82.156.125.169 1314# 以下为输入的数据12client send msg 传输文件这个用法是对上一种用法的扩展，通过重定向将文件内容通过网络传输： 12# 服务端将socket内容保存到w文件中[root@VM-0-3-centos ~]# nc -l 1314 &gt; w.txt 123456789albert@home-pc:~$ cat w.txtw-rxd# 客户端将w.txt文件内容发送给服务器albert@home-pc:~$ nc 82.156.125.169 1314 &lt; w.txtalbert@home-pc:~$ 客户端运行完 nc 命令之后就会退出，服务器端的 nc 命令也会结束，w.txt 文件的内容就传送过去了。 端口转发这其实也是传输数据的命令的一个变种，使用 -c 参数可以完成数据转发： 1[root@VM-0-3-centos ~]# nc -l 520 -c "nc 82.156.125.169 1314" 执行命令之后，发送到本机 520 端口的数据就会被转发到 IP 为82.156.125.169的1314端口上了。 总结 nc -nvuz ip port 可以检测指定IP的UDP端口能否访问，如果是TCP端口去掉 -u 选项就可以了 nc -l port 可以启动一个本地服务器，接受发往指定端口的数据，并打印到控制台 nc -l port &gt; filename 可以启动一个本地服务器，接受发往指定端口的数据，并保存到名为 filename 的文件中 nc -l port -c &quot;nc ip new_port&quot; 可以启动一个本地服务器，接受发往指定端口的数据，并转发到ip:new_port的机器上 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生每时每刻都在积攒素材，能否拼凑出高光的时刻，决定了某些事迹有无机会诉说。 2021-12-12 22:11:51]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>nc</tag>
        <tag>NetCat</tag>
        <tag>ncat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次使用Valgrind查找解决内存问题的玄幻旅程]]></title>
    <url>%2Fblog%2F2021%2F12%2F02%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8Valgrind%E6%9F%A5%E6%89%BE%E8%A7%A3%E5%86%B3%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E7%9A%84%E7%8E%84%E5%B9%BB%E6%97%85%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言看题目来说这应该是一篇教程式文章，但为了突出“玄幻”二字，我们不讲细节只讲过程，在过程中体会解决问题的方式和方法，以及避免一些我在这个过程中绕的弯路，如果想找工具的详细使用方法可以去参考文章中翻一翻，有几篇文章写的真不错，下面我们开始扯淡啦。 玄幻旅途 本故事并非虚构，如有雷同，纯属命苦~ 故事背景作为本文主人公的我——小Z，是一个后端C/C++搬运工（这不废话吗，不是C系列谁老倒腾指针和内存？），在一个阳（yue）光（hei）明（feng）媚（gao）的下（wan）午（shang）接到一个完善游戏战斗系统的需求，然后便开始了紧张开发、积极调试的每一天，事情比较顺利，一切都在计划之中，不过平凡的日子总是无趣，没有一点点意外总让人感到有点意外。 初入泥潭好吧，到目前为止一切都很顺利，服务器各个功能模块分批完成，终于完成了最后拼装，启动调试看结果，出现了一点点逻辑问题，这个战斗过程根本停不下来，整个程序一直在递归，最终导致函数调用栈溢出崩溃。不过这都是小问题，简单梳理逻辑后增加必要的出口判断条件，问题很快被解决。 再次启动调试，程序正常运行，符合预期结果。啥？这就完了，幸福来的有点突然啊，整个流程基本符合需求，只是缺少一些细节逻辑需要补充，感觉胜利就在前方了啊！ 一片混沌补充细节的过程中，也需要不断调试来验证结果，咦？怎么连不上服务器了？查看一下进程，果然服务器进程已经不在了，难道是我不小心关掉了，先记一下，解决掉手头上更重要的问题后再来看它。 上次的问题好几天都没有出现了，可能真的是我不小心把服务器进程关掉了，今天还有个小BUG需要修复一下，先搭建好调试环境准备定位一下问题。整个过程比较顺利，没过几分钟BUG就找到了，修复后调试看看结果，Duang！进程挂了，好在这次是在调试状态，能看到是哪里引发的崩溃，查看函数调用栈来看看是谁捣的鬼。 什么玩意，智能指针出作用域时自动析构挂了？这是什么鬼，从上到下看了一遍近百层的函数调用关系，感觉没什么问题啊，真是奇怪。 重新启动进程，开始了疯狂测试，跑了20几次相同的逻辑，没有任何问题啊，那刚刚发生了什么，转过头来继续看刚刚出现崩溃的位置，完全找不到问题。这个问题先放一放，继续补充细节，调试解决发现的BUG，在多次调试之后，Duang！进程又挂了，这次更离谱，在定义lambda表达式的时候崩了，看着函数调用栈依旧一头雾水，看不出是什么问题。 退出调试状态，重启进程，继续跑了10多次相同的逻辑，这次进程真的崩溃了，看来程序真的是有隐藏的BUG。再次重启，继续跑，这次又不崩溃了，这种状况让人有点头大啊。启动调试状态开始测试，跑了几次就崩溃了，原来和调试有关系呀！经过多次测试发现，如果在调试状态下测试几次就会出现崩溃的情况，如果在非调试状态下大概需要跑10多次才会崩。 为了查出问题便开始在调试状态下更加疯狂的测试，这次真的开了眼了，每次崩溃的位置都不太一样，有的在析构函数中，有的设置变量值时，有的在发送函数中，有的在申请内存时，总体来看基本都是围绕着内存出现的问题，但是问题原因未知。 追根溯源虽然经过大量测试仍不能准确给出问题原因，但是几十次的崩溃结果中还是能看到一些规律的，其中有50%左右出现在第二场战斗释放之前战斗对象的时候，40%出现在玩家重新登录释放之前战斗对象的时候，这两种情况加在一起就占了绝大多数，所以要从这里开始入手，查看释放战斗对象的函数是不是存在问题。 因为程序中很少直接使用简单的指针，基本都会用智能指针来代替，所以在战斗对象析构时会有很多小对象自动析构，花了不少时间来看这些代码，结果一无所获，这就怪了，那么多次崩溃都是在这，居然找不到任何问题。 抽丝剥茧因为之前测试时需要完成跑完整个战斗流程，严重影响了测试效率，既然感觉释放战斗对象这部分代码有问题，那就单独跑这一段逻辑呗，单独建个分支，改代码！！！另外还发现一个事情，本来在我机器上需要在调试状态下跑好几次才能重现出的问题，在另一台发布机上两三次就能重现，干脆用它来验证结果。 说干就干，从原来的逻辑中，剥离出创建、释放战斗对象的代码，每次测试重复创建和释放过程几百次，这样就应该很容易就能重现问题了，修改完本地先测试，结果跑了十几次也没出现，部署到发布机上测试多次也没出现问题，和预想的完全不一样，实验失败，这个结果基本说明我的方向错误，并不是这段释放战斗对象的逻辑代码问题，又得重新寻找线索了。 大海捞针上面的验证虽说失败了，但也给我提了醒，既然释放战斗对象的逻辑代码没问题，但是绝大多数奔溃还发生在这里，那肯定是别人把它影响了，结合之前看到的内存问题，应该是有其他的逻辑写错了内存数据，导致释放战斗对象的内存时出现了问题。 这个崩溃在主分支是没有出现过的，在我开发完这个新需求之后才出现了这个问题，那么需要查新加了哪些代码，但是这个版本单单是新的文件就增加了几十个，要想从中找到一个内存问题犹如大海捞针一样。 祭出法宝在大量代码中直接寻找内存问题，非寻常人所能企及，这时可以考虑借助第三方力量——比如检测工具，根据以往经验，我用的最多的内存检测工具是 Valgrind 和 AddressSanitizer，起初 Valgrind 用的比较多的，后来认识了 AddressSanitizer 之后发现使用 Valgrind 后程序运行太慢了，而使用 AddressSanitizer 虽然需要重新编译一次，但是基本不影响原有程序的运行速度，所以渐渐偏向了 ASAN。 但是，这次我先用了 Valgrind，还原代码，重新编译，调整参数后启动服务器程序，果然是半天没反应，测试多次之后居然没崩溃，查看了它的检测报告也没发现什么问题，决定换 ASAN 试试，因为每次用 Valgrind 启动和运行真的太慢了。 修改Makefile重新编译，使用 AddressSanitizer 来进行检测，这次更奇怪，添加了 ASAN 选项的程序编译后，貌似代码逻辑感觉到了它（ASAN）的存在，程序运行逻辑直接变了，原来能完整跑完的战斗逻辑，总是跑到一半因为条件不满足停下了，不过有几次跑到了最后，也出现了崩溃的情况，但是从检测报告中未查到问题的原因，仅仅找到一处内存泄漏问题，修改完崩溃问题依旧存在。 屏蔽无关既然上面的工具没能提供帮助，那么还得依靠我硬啃代码了，还是先来分析之前各种崩溃结果，发现每次析构对象前都给客户端发了消息，而这些消息使用了 protobuf 中 oneof 结构，这个结构之前没用过，会不会因为使用不当，把内存写坏了。 这次我没有直接去看代码的细节，而是采用了屏蔽的方式，将一些不影响战斗逻辑的消息数据精简，不断注释代码，不断发布测试，结果依旧崩溃，最后仅剩一处同步技能的协议，其中也用了 oneof 结构，这时我更加感觉它有问题，但是它不能被注释掉，需要通过它发消息给客户端，然后客户端请求放技能才能将战斗进行下去，测试暂时卡在这了。 移形换位必须想一种办法把这仅剩的一条消息同步去掉，如果不给客户端的同步消息，客户端就不能通知服务器放技能，那只好服务器自己把这些事都做了，修改服务器代码，采用延迟触发的方法，来驱动整个战斗进程能进行下去，最终把仅剩的那一条消息屏蔽掉了，同时把所有的try-catch也屏蔽了。 打包部署发布服，启动测试，问题依旧存在，唉，我麻了！ 再请法宝因为 ASAN 这个工具我一直在观察着输出的报告，并没有发现什么值得注意的问题，所以我打算换为 Valgrind，因为它们两个有点冲突，所以得把Makefile还原回去，重新编译再使用 Valgrind 来测试。 启动程序，依旧卡的像时间静止了一样，启动客户端开始了常规的疯狂测试，Duang！进程挂了，赶紧打开 Valgrind 的输出报告看看，亲人呐，我在里面找到了 Invalid write 的字样。 赶紧去查看这段报告对应的代码问题，其中包含了 std::sort 函数的使用，但是自定义的排序函数不满足严格弱排序规则，感觉这逻辑确实有问题，把它先注释掉来试一下。 风平浪静注释掉 std::sort 之后，在本地机器测试半小时未发生崩溃，重新编译打包发布，几十次测试之后也没有发生崩溃的情况，一切又恢复了平静。 若有所思如果在第一次使用工具时，我给予Valgrind 多一点点宽容就好了。 其实事后看来好像没有多磨曲折，但是真实情况却是，前面的步骤交叉进行，经常会出现反复的情况，前前后后调试了近3天。 为什么如此执着？因为如果类似的问题不再早期发现时解决，后面要想再解决所付出的成本会更大，所以早发现早解决。 参考文章 AddressSanitizerLeakSanitizer 内存错误检测工具-AddressSanitizer（ASAN） 查找内存错误 c++中智能指针使用小结 静态或者全局智能指针使用的注意几点 谈谈如何利用 valgrind 排查内存错误 几个C++内存泄漏和越界检测工具简介 内存泄漏检测工具valgrind神器 使用valgrind检查内存问题 Valgrind学习笔记(一) 关于C#：valgrind-地址是在分配大小为16的块之前的8个字节 c++ seg fault issue: gnu_cxx::exchange_and_add 记一次 TCMalloc Debug 经历 Segmentation fault in gnu_cxx::exchange_and_add () from /usr/lib64/libstdc++.so.6 C++中使用std::sort自定义排序规则时要注意的崩溃问题 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 靠想象打开未来一扇扇大门，靠理性选择其中正确的一扇~ 2021-12-5 01:03:26]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>linux</tag>
        <tag>Valgrind</tag>
        <tag>内存问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下恢复rm误删的文件]]></title>
    <url>%2Fblog%2F2021%2F11%2F25%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%81%A2%E5%A4%8Drm%E8%AF%AF%E5%88%A0%E7%9A%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言一提到在 linux 环境下删除文件，那绝对离不开 sudo rm -rf /* 这个梗，每次看到这个命令，我都想到一幅恶搞的图片： 这个『清理垃圾』的说明真是解释的“恰到好处”，据说有小白在论坛问问题，被人开玩笑的回复了 sudo rm -rf /* 这个命令，结果问题就解决了，人也拜拜了~ 从删库到跑路，一天一个入狱小技巧，所以我们一定要谨慎使用 rm -rf 命令，这相当于我们在 Windows 上使用 Shift+Delete 组合，文件不会放到回收站中，而是直接永久删除了，在 Linux 中执行 rm 命令就相当于永久删除。 虽说 sudo rm -rf /* 危险无比，但是我们很少会直接这样写，但是 rm 命令还是经常用的，这不就在前几天，辛辛苦苦写的Shell脚本就被我直接 rm -rf 删掉了，幸亏我之前将内容打印到了控制台，否则整个脚本就白写了。 rm之后还有救吗尽管 rm 命令表示永久删除，但是不代表文件就一定找不回来，只是找回的几率有大有小。其实删除命令只是在文件节点中作删除标记，并不真正清除文件内容，如果删除后马上进行恢复，那么成功的概率还是很大的，但是如果其他用户一直在用这台机器，或者有一些写盘操作的进程一直在执行，那么这部分数据可能很快就会被覆盖。这时基本上就无法恢复该文件了。 使用foremost找回文件foremost 是一个基于文件头和尾部信息以及文件的内建数据结构恢复文件的命令行工具，接下来是安装和恢复的步骤： 安装 foremost CentOS系统直接运行 sudo yum install https://forensics.cert.org/centos/cert/7/x86_64//foremost-1.5.7-13.1.el7.x86_64.rpm -y 命令就可以完成 如果是Ubuntu系统请尝试命令：sudo apt install foremost 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@VM-0-3-centos ~]# sudo yum install https://forensics.cert.org/centos/cert/7/x86_64//foremost-1.5.7-13.1.el7.x86_64.rpm -yLoaded plugins: fastestmirror, langpacksRepository epel is listed more than once in the configurationforemost-1.5.7-13.1.el7.x86_64.rpm | 46 kB 00:00:01Examining /var/tmp/yum-root-XJIfxM/foremost-1.5.7-13.1.el7.x86_64.rpm: foremost-1.5.7-13.1.el7.x86_64Marking /var/tmp/yum-root-XJIfxM/foremost-1.5.7-13.1.el7.x86_64.rpm to be installedResolving Dependencies--&gt; Running transaction check---&gt; Package foremost.x86_64 0:1.5.7-13.1.el7 will be installed--&gt; Finished Dependency Resolutionepel/7/x86_64 | 4.7 kB 00:00:00epel/7/x86_64/group_gz | 96 kB 00:00:00epel/7/x86_64/updateinfo | 1.0 MB 00:00:00epel/7/x86_64/primary_db | 7.0 MB 00:00:01extras/7/x86_64 | 2.9 kB 00:00:00extras/7/x86_64/primary_db | 243 kB 00:00:00os/7/x86_64 | 3.6 kB 00:00:00updates/7/x86_64 | 2.9 kB 00:00:00updates/7/x86_64/primary_db | 12 MB 00:00:01Dependencies Resolved===================================================================================================================== Package Arch Version Repository Size=====================================================================================================================Installing: foremost x86_64 1.5.7-13.1.el7 /foremost-1.5.7-13.1.el7.x86_64 85 kTransaction Summary=====================================================================================================================Install 1 PackageTotal size: 85 kInstalled size: 85 kDownloading packages:Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : foremost-1.5.7-13.1.el7.x86_64 1/1 Verifying : foremost-1.5.7-13.1.el7.x86_64 1/1Installed: foremost.x86_64 0:1.5.7-13.1.el7Complete![root@VM-0-3-centos ~]# 创建一个测试文件 123456[root@VM-0-3-centos ~]# echo "this is a important file"&gt;important.txt[root@VM-0-3-centos ~]# pwd/root[root@VM-0-3-centos ~]# lsconnecttendis.sh important.txt restore tarlist tendis test.iso[root@VM-0-3-centos ~]# mkdir -p /tmp/restore 删除文件后尝试还原 1234[root@VM-0-3-centos ~]# rm important.txt[root@VM-0-3-centos ~]# foremost -i /dev/vda1 -o /tmp/restore/Processing: /dev/vda1|***********Segmentation fault 执行几分钟之后崩溃，恢复失败，打开目录查看发现： 123[root@VM-0-3-centos ~]# ls /tmp/restore/audit.txt bmp doc exe htm jpg mov mpg pdf ppt rar sdw sxc sxw wav xls zipavi dll docx gif jar mbd mp4 ole png pptx rif sx sxi vis wmv xlsx 看来与需要恢复的文件类型有关，换台机器再换一个png文件试试，先找一个showball.png测试文件，然后确认分区 /dev/vda1 12345678910111213[root@VM-0-3-centos ~]# dfFilesystem 1K-blocks Used Available Use% Mounted ondevtmpfs 930496 0 930496 0% /devtmpfs 941004 24 940980 1% /dev/shmtmpfs 941004 508 940496 1% /runtmpfs 941004 0 941004 0% /sys/fs/cgroup/dev/vda1 51473868 6458344 42692404 14% //dev/loop0 361650 361650 0 100% /mnt/isotmpfs 188204 0 188204 0% /run/user/0[root@VM-0-3-centos ~]# pwd/root[root@VM-0-3-centos ~]# lsrestore showball.png 删除png文件后尝试恢复，进度条结束后即可进入指定的目录 /root/restore/ 查看 1234567[root@VM-0-3-centos ~]# rm showball.pngrm: remove regular file ‘showball.png’? y[root@VM-0-3-centos ~]# pwd/root[root@VM-0-3-centos ~]# foremost -t png -i /dev/vda1 -o /root/restore/Processing: /dev/vda1|*********************************************************************************************| 在指定目录下会有一个 audit.txt 统计文件和一个类型文件夹 png 123456789101112131415[root@VM-0-3-centos ~]# ll restore/total 40-rw-r--r-- 1 root root 24548 Nov 27 22:57 audit.txtdrwxr-xr-- 2 root root 16384 Nov 27 22:56 png[root@VM-0-3-centos ~]# cd restore/[root@VM-0-3-centos restore]# ll pngtotal 43764-rw-r--r-- 1 root root 3500 Nov 27 22:53 00367400.png-rw-r--r-- 1 root root 3578 Nov 27 22:53 00367408.png-rw-r--r-- 1 root root 3445 Nov 27 22:53 00367416.png-rw-r--r-- 1 root root 368 Nov 27 22:53 00367432.png-rw-r--r-- 1 root root 363 Nov 27 22:53 00367456.png-rw-r--r-- 1 root root 392 Nov 27 22:53 00367464.png-rw-r--r-- 1 root root 199 Nov 27 22:53 00367616.png... png目录下的文件名都是一些编号，与原来删除的文件完全不一样了，需要根据 audit.txt 文件确认，打开文件确认一下： 123456789101112131415161718192021222324252627282930313233[root@VM-0-3-centos restore]# head audit.txtForemost version 1.5.7 by Jesse Kornblum, Kris Kendall, and Nick MikusAudit FileForemost started at Sat Nov 27 22:53:48 2021Invocation: foremost -t png -i /dev/vda1 -o /root/restore/Output directory: /root/restoreConfiguration file: /etc/foremost.conf------------------------------------------------------------------File: /dev/vda1Start: Sat Nov 27 22:53:48 2021[root@VM-0-3-centos restore]# head -n 20 audit.txtForemost version 1.5.7 by Jesse Kornblum, Kris Kendall, and Nick MikusAudit FileForemost started at Sat Nov 27 22:53:48 2021Invocation: foremost -t png -i /dev/vda1 -o /root/restore/Output directory: /root/restoreConfiguration file: /etc/foremost.conf------------------------------------------------------------------File: /dev/vda1Start: Sat Nov 27 22:53:48 2021Length: 49 GB (53686025728 bytes)Num Name (bs=512) Size File Offset Comment0: 00367400.png 3 KB 188108800 (16 x 16)1: 00367408.png 3 KB 188112896 (16 x 16)2: 00367416.png 3 KB 188116992 (16 x 16)3: 00367432.png 368 B 188125184 (16 x 16)4: 00367456.png 363 B 188137472 (16 x 16)5: 00367464.png 392 B 188141568 (16 x 16)... audit.txt 文件中记录着恢复文件的简要信息，这需要你知道原来删除文件的相关信息，不然就只能一个个打开查看了，我是通过分辨率查找的 123[root@VM-0-3-centos restore]# grep "1217" audit.txt116: 12888200.png 40 KB 6598758400 (1217 x 690)360: 38088960.png 40 KB 19501547520 (1217 x 690) 根据过滤出的信息把 12888200.png 打开发现就是自己“误删”的文件这就恢复好了 使用extundelete找回文件extundelete 支持ext3、ext4文件系统下的文件恢复，使用 cat /etc/fstab 可以在linux环境下查看文件系统类型 1234567[root@VM-0-3-centos ~]# cat /etc/fstabUUID=21dbe030-aa71-4b3a-8610-3b942dd447fa / ext4 noatime,acl,user_xattr 1 1proc /proc proc defaults 0 0sysfs /sys sysfs noauto 0 0debugfs /sys/kernel/debug debugfs noauto 0 0devpts /dev/pts devpts mode=0620,gid=5 0 0[root@VM-0-3-centos ~]# 安装依赖文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@VM-0-3-centos ~]# yum install e2fsprogs-develLoaded plugins: fastestmirror, langpacksRepository epel is listed more than once in the configurationDetermining fastest mirrorsepel | 4.7 kB 00:00:00extras | 2.9 kB 00:00:00os | 3.6 kB 00:00:00updates | 2.9 kB 00:00:00(1/2): epel/7/x86_64/updateinfo | 1.0 MB 00:00:00(2/2): epel/7/x86_64/primary_db | 7.0 MB 00:00:01Resolving Dependencies--&gt; Running transaction check---&gt; Package e2fsprogs-devel.x86_64 0:1.42.9-19.el7 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved=============================================================================================================== Package Arch Version Repository Size===============================================================================================================Installing: e2fsprogs-devel x86_64 1.42.9-19.el7 os 73 kTransaction Summary===============================================================================================================Install 1 PackageTotal download size: 73 kInstalled size: 162 kIs this ok [y/d/N]: yDownloading packages:e2fsprogs-devel-1.42.9-19.el7.x86_64.rpm | 73 kB 00:00:00Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : e2fsprogs-devel-1.42.9-19.el7.x86_64 1/1 Verifying : e2fsprogs-devel-1.42.9-19.el7.x86_64 1/1Installed: e2fsprogs-devel.x86_64 0:1.42.9-19.el7Complete![root@VM-0-3-centos ~]# 下载extundelete源码 1234567891011121314[root@VM-0-3-centos ~]# wget https://src.fedoraproject.org/repo/pkgs/extundelete/extundelete-0.2.4.tar.bz2/77e626ad31433680c0a222069295d2ca/extundelete-0.2.4.tar.bz2--2021-11-28 18:36:15-- https://src.fedoraproject.org/repo/pkgs/extundelete/extundelete-0.2.4.tar.bz2/77e626ad31433680c0a222069295d2ca/extundelete-0.2.4.tar.bz2Resolving src.fedoraproject.org (src.fedoraproject.org)... 38.145.60.20, 38.145.60.21Connecting to src.fedoraproject.org (src.fedoraproject.org)|38.145.60.20|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 108472 (106K) [application/x-bzip2]Saving to: ‘extundelete-0.2.4.tar.bz2’100%[============================================================================&gt;] 108,472 33.5KB/s in 3.2s2021-11-28 18:36:20 (33.5 KB/s) - ‘extundelete-0.2.4.tar.bz2’ saved [108472/108472][root@VM-0-3-centos ~]# lsextundelete-0.2.4.tar.bz2 解压extundelete源码 1234567891011121314151617181920212223242526272829303132[root@VM-0-3-centos ~]# tar -jxvf extundelete-0.2.4.tar.bz2extundelete-0.2.4/extundelete-0.2.4/acinclude.m4extundelete-0.2.4/missingextundelete-0.2.4/autogen.shextundelete-0.2.4/aclocal.m4extundelete-0.2.4/configureextundelete-0.2.4/LICENSEextundelete-0.2.4/READMEextundelete-0.2.4/install-shextundelete-0.2.4/config.h.inextundelete-0.2.4/src/extundelete-0.2.4/src/extundelete.ccextundelete-0.2.4/src/block.hextundelete-0.2.4/src/kernel-jbd.hextundelete-0.2.4/src/insertionops.ccextundelete-0.2.4/src/block.cextundelete-0.2.4/src/cli.ccextundelete-0.2.4/src/extundelete-priv.hextundelete-0.2.4/src/extundelete.hextundelete-0.2.4/src/jfs_compat.hextundelete-0.2.4/src/Makefile.inextundelete-0.2.4/src/Makefile.amextundelete-0.2.4/configure.acextundelete-0.2.4/depcompextundelete-0.2.4/Makefile.inextundelete-0.2.4/Makefile.am[root@VM-0-3-centos ~]# cd extundelete-0.2.4/[root@VM-0-3-centos extundelete-0.2.4]# lsacinclude.m4 aclocal.m4 autogen.sh config.h.in configure configure.ac depcompinstall-sh LICENSE Makefile.am Makefile.in missing README src[root@VM-0-3-centos extundelete-0.2.4]# 编译xtundelete源码安装 12345678910111213141516[root@VM-0-3-centos extundelete-0.2.4]# ./configure --prefix=/usr/local/extundelete &amp;&amp; make &amp;&amp; make installConfiguring extundelete 0.2.4Writing generated files to diskmake -s all-recursiveMaking all in srcextundelete.cc: In function ‘ext2_ino_t find_inode(ext2_filsys, ext2_filsys, ext2_inode*, std::string, int)’:extundelete.cc:1272:29: warning: narrowing conversion of ‘search_flags’ from ‘int’ to ‘ext2_ino_t &#123;aka unsigned int&#125;’ inside &#123; &#125; [-Wnarrowing] buf, match_name2, priv, 0&#125;; ^Making install in src /usr/bin/install -c extundelete '/usr/local/extundelete/bin'[root@VM-0-3-centos extundelete-0.2.4]# which extundelete/usr/bin/which: no extundelete in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)[root@VM-0-3-centos extundelete-0.2.4]# ll /usr/local/extundelete/bin/total 1296-rwxr-xr-x 1 root root 1323360 Nov 28 18:45 extundelete 如果在这一步报错 configure: error: C++ compiler cannot create executables，可以运行 yum -y install gcc-c++ 命令安装编译环境 准备测试文件 123456789101112[root@VM-0-3-centos examples]# df -TFilesystem Type 1K-blocks Used Available Use% Mounted ondevtmpfs devtmpfs 930496 0 930496 0% /devtmpfs tmpfs 941004 24 940980 1% /dev/shmtmpfs tmpfs 941004 508 940496 1% /runtmpfs tmpfs 941004 0 941004 0% /sys/fs/cgroup/dev/vda1 ext4 51473868 6465732 42685016 14% //dev/loop0 iso9660 361650 361650 0 100% /mnt/isotmpfs tmpfs 188204 0 188204 0% /run/user/0[root@VM-0-3-centos examples]# cp ../extundelete-0.2.4.tar.bz2 .[root@VM-0-3-centos examples]# lsextundelete-0.2.4.tar.bz2 查询文件的inode信息 我们选择刚刚下载的extundelete源码包作为“误删”的文件，先查看一下信息，-li 可以在第一列查看文件的inode信息，examples文件夹的inode值为1311798： 12345678[root@VM-0-3-centos ~]# ls examples/extundelete-0.2.4.tar.bz2[root@VM-0-3-centos ~]# ls -litotal 3616761311798 drwxr-xr-x 2 root root 4096 Nov 28 20:28 examples1310761 drwxr-xr-x 3 1000 1000 4096 Nov 28 18:45 extundelete-0.2.4 918157 drwxr-xr-x 2 root root 4096 Feb 28 2021 tarlist 396057 -rw-r--r-- 1 root root 370329600 Feb 27 2021 test.iso 删除测试文件，并用查询信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@VM-0-3-centos ~]# cd examples/[root@VM-0-3-centos examples]# lsextundelete-0.2.4.tar.bz2[root@VM-0-3-centos examples]# rm extundelete-0.2.4.tar.bz2 -f[root@VM-0-3-centos examples]# ls[root@VM-0-3-centos examples]# /usr/local/extundelete/bin/extundelete /dev/vda1 --inode 1311798NOTICE: Extended attributes are not restored.WARNING: EXT3_FEATURE_INCOMPAT_RECOVER is set.The partition should be unmounted to undelete any files without further data loss.If the partition is not currently mounted, this message indicatesit was improperly unmounted, and you should run fsck before continuing.If you decide to continue, extundelete may overwrite some of the deletedfiles and make recovering those files impossible. You should unmount thefile system and check it with fsck before using extundelete.Would you like to continue? (y/n)yLoading filesystem metadata ... 400 groups loaded.Group: 160Contents of inode 1311798:0000 | ed 41 00 00 00 10 00 00 7a 62 a3 61 1b 7a a3 61 | .A......zb.a.z.a0010 | 1b 7a a3 61 00 00 00 00 00 00 02 00 08 00 00 00 | .z.a............0020 | 00 00 08 00 0b 00 00 00 0a f3 01 00 04 00 00 00 | ................0030 | 00 00 00 00 00 00 00 00 01 00 00 00 79 20 50 00 | ............y P.0040 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0050 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0060 | 00 00 00 00 7c 63 ab ad 00 00 00 00 00 00 00 00 | ....|c..........0070 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0080 | 1c 00 00 00 80 da 0d a3 80 da 0d a3 94 24 04 08 | .............$..0090 | 7a 62 a3 61 94 24 04 08 00 00 00 00 00 00 00 00 | zb.a.$..........00a0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00b0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00c0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00d0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00e0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00f0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................Inode is AllocatedFile mode: 16877Low 16 bits of Owner Uid: 0Size in bytes: 4096Access time: 1638097530Creation time: 1638103579Modification time: 1638103579Deletion Time: 0Low 16 bits of Group Id: 0Links count: 2Blocks count: 8File flags: 524288File version (for NFS): 2913690492File ACL: 0Directory ACL: 0Fragment address: 0Direct blocks: 127754, 4, 0, 0, 1, 5251193, 0, 0, 0, 0, 0, 0Indirect block: 0Double indirect block: 0Triple indirect block: 0File name | Inode number | Deleted status. 1311798.. 393219extundelete-0.2.4.tar.bz2 396764 Deletedconftest.err 1311833 Deleted[root@VM-0-3-centos examples]# 我们发现 extundelete-0.2.4.tar.bz2 文件的状态为 `Deleted` 使用extundelete恢复文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@VM-0-3-centos ~]# /usr/local/extundelete/bin/extundelete /dev/vda1 --restore-directory /tmpNOTICE: Extended attributes are not restored.WARNING: EXT3_FEATURE_INCOMPAT_RECOVER is set.The partition should be unmounted to undelete any files without further data loss.If the partition is not currently mounted, this message indicatesit was improperly unmounted, and you should run fsck before continuing.If you decide to continue, extundelete may overwrite some of the deletedfiles and make recovering those files impossible. You should unmount thefile system and check it with fsck before using extundelete.Would you like to continue? (y/n)yLoading filesystem metadata ... 400 groups loaded.Loading journal descriptors ... 31842 descriptors loaded.*** Error in `/usr/local/extundelete/bin/extundelete': double free or corruption (!prev): 0x00000000014d6020 ***======= Backtrace: =========/lib64/libc.so.6(+0x81299)[0x7f5c08190299]/usr/local/extundelete/bin/extundelete[0x40cdcb]/usr/local/extundelete/bin/extundelete[0x40fee6]/usr/local/extundelete/bin/extundelete[0x4045b4]/lib64/libc.so.6(__libc_start_main+0xf5)[0x7f5c08131555]/usr/local/extundelete/bin/extundelete[0x404aef]======= Memory map: ========00400000-0041c000 r-xp 00000000 fd:01 1311942 /usr/local/extundelete/bin/extundelete0061c000-0061d000 r--p 0001c000 fd:01 1311942 /usr/local/extundelete/bin/extundelete0061d000-0061e000 rw-p 0001d000 fd:01 1311942 /usr/local/extundelete/bin/extundelete0061e000-0061f000 rw-p 00000000 00:00 0014c6000-0176e000 rw-p 00000000 00:00 0 [heap]7f5c00000000-7f5c00021000 rw-p 00000000 00:00 07f5c00021000-7f5c04000000 ---p 00000000 00:00 07f5c07ca1000-7f5c07ef3000 rw-p 00000000 00:00 07f5c07ef3000-7f5c07f0a000 r-xp 00000000 fd:01 265649 /usr/lib64/libpthread-2.17.so7f5c07f0a000-7f5c08109000 ---p 00017000 fd:01 265649 /usr/lib64/libpthread-2.17.so7f5c08109000-7f5c0810a000 r--p 00016000 fd:01 265649 /usr/lib64/libpthread-2.17.so7f5c0810a000-7f5c0810b000 rw-p 00017000 fd:01 265649 /usr/lib64/libpthread-2.17.so7f5c0810b000-7f5c0810f000 rw-p 00000000 00:00 07f5c0810f000-7f5c082d2000 r-xp 00000000 fd:01 265623 /usr/lib64/libc-2.17.so7f5c082d2000-7f5c084d2000 ---p 001c3000 fd:01 265623 /usr/lib64/libc-2.17.so7f5c084d2000-7f5c084d6000 r--p 001c3000 fd:01 265623 /usr/lib64/libc-2.17.so7f5c084d6000-7f5c084d8000 rw-p 001c7000 fd:01 265623 /usr/lib64/libc-2.17.so7f5c084d8000-7f5c084dd000 rw-p 00000000 00:00 07f5c084dd000-7f5c084f2000 r-xp 00000000 fd:01 291206 /usr/lib64/libgcc_s-4.8.5-20150702.so.17f5c084f2000-7f5c086f1000 ---p 00015000 fd:01 291206 /usr/lib64/libgcc_s-4.8.5-20150702.so.17f5c086f1000-7f5c086f2000 r--p 00014000 fd:01 291206 /usr/lib64/libgcc_s-4.8.5-20150702.so.17f5c086f2000-7f5c086f3000 rw-p 00015000 fd:01 291206 /usr/lib64/libgcc_s-4.8.5-20150702.so.17f5c086f3000-7f5c087f4000 r-xp 00000000 fd:01 287349 /usr/lib64/libm-2.17.so7f5c087f4000-7f5c089f3000 ---p 00101000 fd:01 287349 /usr/lib64/libm-2.17.so7f5c089f3000-7f5c089f4000 r--p 00100000 fd:01 287349 /usr/lib64/libm-2.17.so7f5c089f4000-7f5c089f5000 rw-p 00101000 fd:01 287349 /usr/lib64/libm-2.17.so7f5c089f5000-7f5c08ade000 r-xp 00000000 fd:01 266798 /usr/lib64/libstdc++.so.6.0.197f5c08ade000-7f5c08cde000 ---p 000e9000 fd:01 266798 /usr/lib64/libstdc++.so.6.0.197f5c08cde000-7f5c08ce6000 r--p 000e9000 fd:01 266798 /usr/lib64/libstdc++.so.6.0.197f5c08ce6000-7f5c08ce8000 rw-p 000f1000 fd:01 266798 /usr/lib64/libstdc++.so.6.0.197f5c08ce8000-7f5c08cfd000 rw-p 00000000 00:00 07f5c08cfd000-7f5c08d3f000 r-xp 00000000 fd:01 267873 /usr/lib64/libext2fs.so.2.47f5c08d3f000-7f5c08f3f000 ---p 00042000 fd:01 267873 /usr/lib64/libext2fs.so.2.47f5c08f3f000-7f5c08f40000 r--p 00042000 fd:01 267873 /usr/lib64/libext2fs.so.2.47f5c08f40000-7f5c08f42000 rw-p 00043000 fd:01 267873 /usr/lib64/libext2fs.so.2.47f5c08f42000-7f5c08f45000 r-xp 00000000 fd:01 265948 /usr/lib64/libcom_err.so.2.17f5c08f45000-7f5c09144000 ---p 00003000 fd:01 265948 /usr/lib64/libcom_err.so.2.17f5c09144000-7f5c09145000 r--p 00002000 fd:01 265948 /usr/lib64/libcom_err.so.2.17f5c09145000-7f5c09146000 rw-p 00003000 fd:01 265948 /usr/lib64/libcom_err.so.2.17f5c09146000-7f5c09168000 r-xp 00000000 fd:01 265614 /usr/lib64/ld-2.17.so7f5c092b1000-7f5c0935d000 rw-p 00000000 00:00 07f5c09363000-7f5c09367000 rw-p 00000000 00:00 07f5c09367000-7f5c09368000 r--p 00021000 fd:01 265614 /usr/lib64/ld-2.17.so7f5c09368000-7f5c09369000 rw-p 00022000 fd:01 265614 /usr/lib64/ld-2.17.so7f5c09369000-7f5c0936a000 rw-p 00000000 00:00 07ffe581db000-7ffe581fc000 rw-p 00000000 00:00 0 [stack]7ffe581fc000-7ffe581fe000 r-xp 00000000 00:00 0 [vdso]ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall]Aborted[root@VM-0-3-centos ~]# 恢复失败，此路不通！！！ 有其他人成功了，但是我测试失败，释放内存崩溃，有大神给说一下怎么改源码吗？此处存疑，后续再测，先记录一下常用参数 查询inode文件状态：/usr/local/extundelete/bin/extundelete /dev/vda1 --inode 1311798 恢复指定节点数据：/usr/local/extundelete/bin/extundelete /dev/vda1 --restore-inode 1311798 恢复单个文件：/usr/local/extundelete/bin/extundelete /dev/vda1 --restore-file root/examples/extundelete-0.2.4.tar.bz2 恢复一个目录：/usr/local/extundelete/bin/extundelete /dev/vda1 --restore-files root/examples 恢复所有文件：/usr/local/extundelete/bin/extundelete /dev/vda1 --restore-all 预防误删引发的事故 定义别名，提示删除 定义别名 alias rm=&#39;rm -i&#39;， 在删除文件前会出现一个提示，使用 -i 选项来需要逐个确认要删除的文件，只有用户输入 y 才会将文件删除，但是这种做法在加上 -f 选项之后会失效。 禁用rm，使用mv代替 在系统中不允许直接使用rm命令直接删除文件，需要mv文件到指定的回收目录 ~/.delete，然后配合一个定时任务，每周清空~/.delete下文件，相当于手动创建了一个回收站。 总结 使用foremost恢复时的目标目录最好是另外一个磁盘中的目录，把文件恢复到被删除文件所在的磁盘中很可能会在恢复前覆盖被误删的文件 sodu 的全称目前有 substitute user do 和 super user do 两种说法，使用sudo通常是行驶超级用户的权限，但有时也可以其他普通用户，所以翻译成 substitute user do 代替其他用户来做更准确一点 foremost 支持的文件系统比较多，其中包括 ext2、 ext3 、vfat、NTFS、ufs、jfs 等，但是只能恢复特定格式的文件，而 extundelete 只支持ext3、ext4文件系统，不过可恢复的文件类型很多。 除了本文中总结的这两款不怎么好用的恢复软件，还有 testdisk 和 photorec 可以用来恢复，后续可以尝试一下 数据无价，请谨慎删除，可参考别名方法或禁用 rm -rf 来减少事故的发生 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 成年人，结果导向！没有功劳的苦劳是那么的虚弱无力~ 2021-11-28 22:14:54]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>rm</tag>
        <tag>rf</tag>
        <tag>回收站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[完全图与强连通图的那些坑]]></title>
    <url>%2Fblog%2F2021%2F11%2F17%2F%E5%AE%8C%E5%85%A8%E5%9B%BE%E4%B8%8E%E5%BC%BA%E8%BF%9E%E9%80%9A%E5%9B%BE%E7%9A%84%E9%82%A3%E4%BA%9B%E5%9D%91%2F</url>
    <content type="text"><![CDATA[前言图这个数据结构相比队列、栈、树来说算是复杂多了，关于图的问题也多如牛毛，先来看一下常见的问题： 若无向图 G 中含7个顶点，要想保证图 G 在任何情况下都是连通的，则需要的边数最少是几条？ 回答这种问题一定要注意细节，找到关键的点，不然一定会掉到坑里的。这个题关键点有以下几个： 7个顶点 任何条件下连通 最少几条边 其中第1点和第3点不容易出错，比较容易出现问题的是第2点，要想保证任何条件下连通，意思给定边数以后无论怎么连都能通？ 先说下答案是16，至于为什么，我们后面先复习一下图相关的概念再慢慢解释，因为此刻的我连什么是强联通图都忘了~ 一些概念 图：是由顶点V集和边E集构成，边表示了与之相连的两点间的关系，因此图可以表示成G = (V, E) 有向图：是指图中的两个顶点从A到B和从B到A的含义是不同的，我们认为两点的关系是有方向的，则称其为有向图 无向图：是指两点间的连接线无方向无关，这种图叫做无向图 连通性：从图中一个顶点到达另一顶点，若存在至少一条路径，则称这两个顶点是连通着的 连通图：在无向图中，如果任意两个顶点之间都能够连通，则称此无向图为连通图 完全图：在无向图中，如果任意两个顶点之间都边直接相连，则称此无向图为完全图 连通分量：若无向图不是连通图，但图中存在某个子图符合连通图的性质，则称该子图为连通分量 强连通图：在有向图中，若任意两个顶点之间包含至少来回两条通路，则称此有向图为强连通图 有向完全图：在有向图中，如果任意两个顶点之间都有相反的两条弧直接相连，则称此有向图为有向完全图 强连通分量：若有向图不是强连通图，但图中存在某个子图符合强连通图的性质，则称该子图为强连通分量 关于题目的解释这是一个无向图，要想在任何情况下都连通，那考虑极端情况就是孤立一个顶点，让尽可能多的边连接剩余的顶点，那会构成一个 n-1 个顶点的完全图，然后再考虑加一条边把剩下的孤立顶点连起来，这样得到的边数是 N = 5+4+3+2+1 + 1 = 16，用组合数表示就是 $$C^2_{n-1} + 1= (n-1) * (n-2) / 2 + 1$$ 题目变型 若无向图 G 中含7个顶点，要想保证图 G 在是连通的，至少需要几条边？ 答案6条，即 (n-1) 一个包含7个顶点的无向图 G 为完全图，那么它共有几条边？ 答案21条，即 n * (n-1) / 2 若有向图 G 中含7个顶点，要想保证图 G 在是强连通的，至少需要几条弧？ 答案7条，即 n，也就是形成一个环 一个包含7个顶点的有向图 G 为完全图，那么它共有几条弧？ 答案42条，即 n * (n-1) 补充两个图例 完全图，特点是任何两个顶点都有直接的边相连 1234567891011121314graph TB A((A))---B((B)); A((A))---C((C)); A((A))---D((D)); A((A))---E((E)); B((B))---C((C)); B((B))---D((D)); B((B))---E((E)); C((C))---D((D)); C((C))---E((E)); D((D))---E((E)); 强连通图，任意两点间都有路径可达 12345678graph TB A((A))--&gt;B((B)); B((B))--&gt;C((C)); C((C))--&gt;D((D)); D((D))--&gt;E((E)); E((E))--&gt;A((A)); A((A))--&gt;F((F)); F((F))--&gt;A((A));]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>图</tag>
        <tag>完全图</tag>
        <tag>强连通图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树的带权路径长度和哈夫曼树]]></title>
    <url>%2Fblog%2F2021%2F11%2F07%2F%E6%A0%91%E7%9A%84%E5%B8%A6%E6%9D%83%E8%B7%AF%E5%BE%84%E9%95%BF%E5%BA%A6%E5%92%8C%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91%2F</url>
    <content type="text"><![CDATA[前言树的所有叶子结点的带权路径长度之和，称为树的带权路径长度，英文缩写为 WPL，从百度百科中得到的信息为 “树的带权路径长度（weighted path length of tree）是2018年公布的计算机科学技术名词”，这就有点奇怪了，这个词印象中在大学课本里学过啊，怎么会是2018年的名词呢？难道我穿越了？ 我赶紧找来严蔚敏、吴伟民老师编著的《数据结构》翻开来看，在2009年9月第30次印刷的图书的144页中，明确的用加粗字体描述了这样个概念： 树的带权路径长度为树中所有叶子结点的带权路径之和，通常记作 WPL = … 看来我没记错，不是这个百科弄错了，就是2018年重新公布了一次，并不是新的概念，它确实是一个古老的名词了，接下来可以复习一下了。 树的带权路径长度前面虽然已经给出了定义，可什么是路径，为什么要带权，还要一步步来解释。 路径是指从树的一个结点到另一个结点所走过的部分，路径长度也可以理解为两个结点之间的距离，可以简单理解为路过的结点数，那为什么要带权呢？这和我们生活中的路径一样，并不是距离短的路所花费的时间就少，还要考虑路况、成本等多种因素，而权值就是在特定场景下我们赋予每条路的选择概率。 解释了这几个概念之后我们就可以理解文章开头的定义了，把树的每个叶子结点到根结点的带权路径长度加在一起，就是树的带权路径长度。 哈夫曼树当使用已知结点作为叶子结点，用其构成的所有树中，带全路径长度最小的树被称为最优二叉树，也就是哈夫曼树。 我们先来计算一下一颗二叉树的带权路径长度，二叉树形态如下： 1234567graph TB A((ROOT))--&gt;B((P)); A((ROOT))--&gt;C[[2]]; B((P))--&gt;D[[4]]; B((P))--&gt;F((R)); F((P))--&gt;G[[7]]; F((P))--&gt;H[[5]]; 计算二叉树的带权路径长度涉及到树的层数和权值，以上面这个图为例，ROOT 结点所在的层数为0层，往下数字2结点为1层，数字4结点为2层，数字7结点和5结点为3层，方块中的数字代表了该叶子结点的权值，那个这颗树的带权路径长度为： 7 3 + 5 3 + 4 2 + 2 1 = 46 那么这是一颗最优树吗？显然不是，因为它的带权路径长度不是最短，其实从计算公式也可以看出一点门道，计算带权路径长度时会用层数乘以权值，因为权值不会变，那么唯一能减小结果的就是调整层数，一个很直观的贪心思路就是把权值大的放在低层，权值小的放在高层，这样就可以减小最后的值，比如调整成这样： 1234567graph TB A((ROOT))--&gt;B((P)); A((ROOT))--&gt;C((P)); B((P))--&gt;D[[4]]; B((P))--&gt;F[[2]]; C((P))--&gt;G[[7]]; C((P))--&gt;H[[5]]; 这颗树的带权路径长度计算结果为36，比之前的值小了很多： 4 2 + 2 2 + 7 2 + 5 2 = 36 其实这还不是一颗最优的树，最优的结构应该是这样： 1234567graph TB A((ROOT))--&gt;B[[7]]; A((ROOT))--&gt;C((P)); C((P))--&gt;D[[5]]; C((P))--&gt;F((P)); F((P))--&gt;G[[2]]; F((P))--&gt;H[[4]]; 它的带权路径长度计算结果为5，从这可以看出，树的层数高的不一定计算成的带权路径长度就大。 作用前面说了这么多，那么哈夫曼树有什么作用呢？你应该听说过哈夫曼编码吧，这其实就是哈夫曼树的一个应用，用来找到存放一串字符所需的最少的二进制编码。存放二进制还要单独编码吗？也许你想说什么英文字母不都是编好的吗？ 单纯用字母来传递信息有一个问题，那就是会造成浪费，因为每个字母在日常交流中出现的次数并不一样，比如字母 e 是英文中出现频率最高的字母，而字母 z 却出现的很少，所以可以用较短的编码来表示 e 用较长的编码来表示字母 z，这样很直观的就能感觉到同样的信息采取这种方式处理之后会占用更小的空间。 构建哈夫曼树假设有一段英文文件，我们先统计这个文件中每个字母的出现得到次数，统计如下（别问我这个文件写的什么，我胡诌的(#^.^#)）： a:19b:6c:7d:3e:32f:10g:21h:2 因为哈夫曼树使用叶子结点来推导最终的编码，所有我们先用这些数字作为叶子结点： 123456789graph TB A[[19]] B[[6]] C[[7]] D[[3]] E[[32]] F[[10]] G[[21]] H[[2]] 接下来记住一个原则，那就是找当前树的根结点和剩余叶子结点的最小的两个值，然后组成新的树杈。 首先，从19、6、7、3、32、10、21、2 中选择频数最小的两个叶子结点，分别为2和3，计算两个结点的和5作为根： 123456789101112graph TB A[[19]] B[[6]] C[[7]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] F[[10]] G[[21]] 接着，从19、6、7、5、32、10、21 中选择两个最小的结点，分别是根结点5和叶子结点6，计算两个结点的和11作为新的树根： 12345678910111213graph TB A[[19]] C[[7]] R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] F[[10]] G[[21]] 然后，从19、7、11、32、10、21 中选择两个最小的结点，这次都是叶子结点，分别为7和10，计算两个结点的和17形成一颗新的树： 123456789101112131415graph TB A[[19]] R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] G[[21]] 继续，从 19、11、32、17、21 中选择最小的 11 和 17 这两个树的根结点，计算两个结点的和 28 作为组合树的根结点： 123456789101112131415161718graph TB A[[19]] R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] R4((28))--&gt;R2((11)) R4((28))--&gt;R3((17)) G[[21]] 然后，从 19、32、28、21 中选择最小的 19 和 21 这两个叶子结点，计算两个结点的和 40 形成一棵新的树： 123456789101112131415161718graph TB R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] R4((28))--&gt;R2((11)) R4((28))--&gt;R3((17)) R5((40))--&gt;A[[19]] R5((40))--&gt;G[[21]] 接下来，从 32、28、 40 中选择最小的 32 和 28 这两个结点，求和 60 构成一棵树，根结点为60： 12345678910111213141516171819graph TB R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] R4((28))--&gt;R2((11)) R4((28))--&gt;R3((17)) R6((60))--&gt;E[[32]] R6((60))--&gt;R4((28)) R5((40))--&gt;A[[19]] R5((40))--&gt;G[[21]] 最后把剩下的 40 和 60 两个结点连在一起，和为100就得到了一颗哈夫曼树： 12345678910111213141516171819202122graph TB R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] R4((28))--&gt;R2((11)) R4((28))--&gt;R3((17)) R6((60))--&gt;E[[32]] R6((60))--&gt;R4((28)) R5((40))--&gt;A[[19]] R5((40))--&gt;G[[21]] R7((100))--&gt;R6((60)) R7((100))--&gt;R5((40)) 按照上面的定义来算，这颗二叉树的带权路径长度为： WPL = 2 (32 + 19 + 21) + 4 (6 + 7 + 10) + 5 * (3 + 2) = 261 其实还有另一种计算带权路径长度的方法，那就是把除根结点以外的所有数字都加起来： WPL = 60 + 40 + 28 + 32 + 19 + 21 + 11 + 17 + 5 + 6 + 7 + 10 + 3 + 2 = 261 编码我们用统计数量的字母来替换频数，然后在树的左右指针上分别标上数字就可以得到： 123456789101112131415161718192021graph TB R2((11))--0--&gt;R((5)) R2((11))--1--&gt;B[[b]] R((5))--0--&gt;D[[d]] R((5))--1--&gt;H[[h]] R3((17))--0--&gt;C[[c]] R3((17))--1--&gt;F[[f]] R4((28))--0--&gt;R2((11)) R4((28))--1--&gt;R3((17)) R6((60))--1--&gt;E[[e]] R6((60))--0--&gt;R4((28)) R5((40))--0--&gt;A[[a]] R5((40))--1--&gt;G[[g]] R7((100))--0--&gt;R6((60)) R7((100))--1--&gt;R5((40)) 至此我们就可以给出编码了呀，从根结点走到每个叶子结点路径上经过的0和1就是编码内容，编码表如下： a–&gt;10b–&gt;0001c–&gt;0010d–&gt;00000e–&gt;01f–&gt;0011g–&gt;11h–&gt;00001 要想等长编码这8个字母最少需要4个bit，采用哈夫曼编码以后最少用2bit，最多用5bit，这是考虑了出现频率以后的结果，在传输大量数据的时候，采用哈夫曼编码会是一个更优的解决方案。 总结 树的带权路径长度是指树的所有叶子结点的带权路径长度之和，简称WPL 当使用已知结点作为叶子结点，用其构造的所有树中，带全路径长度最小的树被称为最优二叉树，也就是哈夫曼树 哈夫曼树可以用来编码，采用哈夫曼编码后的信息可以可以使空间利用更加高效 哈夫曼树的构造并不是唯一的，相同的权值结点完全可以构造出不同形态的哈夫曼树，甚至连高度都不同 哈夫曼编码还保证了长编码不与短编码冲突的的特点，这个后续有时间我们再聊 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 埋下高昂的头颅，为一飞冲天的壮举积蓄力量，我就在这静静的等，期待你的绽放~ 2021-11-12 00:42:33]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>WPL</tag>
        <tag>哈夫曼树</tag>
        <tag>带权路径长度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MD5是用来加密的吗？BCrypt又是什么呢？]]></title>
    <url>%2Fblog%2F2021%2F10%2F31%2FMD5%E6%98%AF%E7%94%A8%E6%9D%A5%E5%8A%A0%E5%AF%86%E7%9A%84%E5%90%97%EF%BC%9FBCrypt%E5%8F%88%E6%98%AF%E4%BB%80%E4%B9%88%E5%91%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[前言最近经常看到一种关于 MD5 的说法，比如某某系统的登录模块使用了 MD5 加密算法，保证了用户密码的安全。那么 MD5 究竟是不是一种加密算法呢？从概念上来说『加密』对应的是『解密』，我们把数据采用某种方式加密之后，可以在之后的某一时刻进行解密来获得原始数据，照此观点来说 MD5 并不是一种加密算法，它只能把原文加密成密文，而不能将密文解密成原文。但是 MD5 确实把明文变成了不容易被破解的密文，达到了 “加密” 的目的，好像说它可以用来加密勉强也可以。 更准确的说法，MD5 是一种哈希算法，又叫散列算法或者摘要算法，是一类把任意长度数据转换为定长数据的算法统称，它广泛应用于错误检查，经常通过计算 MD5 来检验下载到的文件的完整性，优秀的哈希算法通常需要具有低碰撞概率，MD5 就是其中的一种。 MD5的八卦 可用于处理密码，是不可逆的 刚刚前面已经提到它可以把原始数据变成定长的摘要信息，而不能把摘要信息再还原成原始数据。就比如 110+119=229，通过原始信息 110 和 119 可以转化成摘要信息229，而已知229却无法知道它是由那两个数相加得到的，当然这个例子只是象征性的，它的碰撞率太高了。 既然不可逆，那么怎样才能判断密码信息呢？这可以利用比较hash值来判断，我们在注册时计算密码的 MD5 值入库，当玩家登录时再次使用玩家输入的明文密码再次计算 MD5 值，如果一致就验证成功，这就是为什么哈希算法要有低碰撞率了。 MD5现在不太安全了 因为MD5算法是确定，用一个字符串计算出来的哈希值也是固定的，所以出现了一些针对该算法的破解方法。 暴力枚举法：因为可以不断尝试，并且随着计算机硬件能力的快速提升，使得这种方法来破解短密码称为了可能 字典法：也就是撞库，黑客通过收集互联网已泄露的用户和密码信息，生成对应的字典表，通过撞库来完成破解 彩虹表：在字典法的基础上改进，以时间换空间，使用预计算的哈希链集来降低存储空间，是目前最常用的方法 MD5可以为自己代言（带盐） 对于固定的哈希算法，相同的输入会得到相同的输出，那么针对MD5算法只需要准备一个字典或者一个彩虹表就可以一直沿用，如果在原有的密码上加点料，那么即使两个用户使用相同的密码，因为盐不同，得到的输出值也就不同，那么破解难度大大提高了。 BCrypt加密上面说过单独使用MD5加密不太安全，但是加盐以后可以大大提高破解的难度，为什么BCrypt加密火了起来，大有代替MD5的趋势~ BCrypt 是 Niels Provos 和 DavidMazières 基于 Blowfish 密码设计的，是 OpenBSD 的默认密码哈希算法。 目前有针对 C、C++、Python 、C# 、Java、JS、PHP 等多种编程语言的实现，使用起来非常方便。 它相对于MD5有哪些优势呢？ 自己加盐首先他不用自己来管理用户的“盐”，如果所有的用户使用相同的盐不太安全，每个人生成不同的盐，需要自己单独来存储使用，而BCrypt内部自己实现了随机加盐处理，可以实现每次加密后的密文是不一样的。 对于同一个密码，Bcrypt每次生成的哈希结果都不一样，那么它是如何进行校验的？ 其实BCrypt算法将盐随机生成并混入最终加密后的密码之中，验证时会自动提取，无需单独提供“盐”信息，生成的Hash值通常格式如下： 1$2b$12$ABJPtagiuqTVhnIPvOLoB.hbIlZ3joRkpck3joDsX6xe3O2KShuty 其中 $ 为分隔符，2b是bcrypt加密版本号，12是工作负载，紧接着22位是盐，剩下的字符串就是密码的密文了。 看到这个密码仿佛就是明牌了跟对手打呀，如果你真的获得了加密后的密码，那你就知道了加密版本、工作负载，盐的信息，这样会不会很危险呢？是挺危险的，但是即使你使用MD5加密，那个盐也是要存储的，也会面临同样的问题，另外BCrypt还有其他的法宝。 工作负载BCrypt的工作负载有时也称为加密轮数、成本因子等等（一提到工作负载就想到比特币，数字游戏而已），目的就是提高破解难度，带来的缺点就是速度慢。MD5的Hash值生成通常是微妙级别的，但是Bcrypt一个密码出来的时间比较长，Python环境使用默认12轮负载需要0.25秒生成一个密码（C++环境需要进一步测试）。 所以如果使用Bcrypt，需要考虑它的成本，负责做认证的服务器，可能在原基础上扩容几十倍或者几百倍，它是靠把计算成本提高多个数量级来换取安全的。 使用方便目前在各大主流编程语言中都可以方便的使用BCrypt相关函数，下面以Python为例： 12345678910111213141516#!/usr/bin/env python3# -*- coding: utf-8 -*-import timeimport bcryptpasswd = b'123456nx'start = time.time()salt = bcrypt.gensalt(12)pwsd = bcrypt.hashpw(passwd, salt)cost = time.time() - startprint("[salt]", salt)print("[pwsd]", pwsd)print("[cost]", cost) 运行结果如下： 123[salt] b&apos;$2b$12$lEsQ9dGnRe2vKfFDRUZYAO&apos;[pwsd] b&apos;$2b$12$lEsQ9dGnRe2vKfFDRUZYAOmmmdlgWfHfNO94C/UqCKGGRioruF77u&apos;[cost] 0.24636435508728027 总结 MD5从严格意义上来说并不是一个加密算法，更准确的说法应该是单向散列算法，因为无法逆向进行解密 通过 MD5 计算后的密码，可以使用的破解方法有暴力枚举、字典表、彩虹表等，其中《彩虹表》最常用 BCrypt算法可以随机生成盐，并将盐信息混入最终加密后的密码之中，验证时会自动提取 BCrypt算法引入了工作负载机制，生成Hash值的时间大大延长，相应的破解难度也随之增加 BCrypt算法在Python环境下使用默认参数需0.25秒生成一个密码，选用这种方式需考虑时间成本 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 以史为鉴，理解今天，展望未来~时刻准备着，这次的大团圆无需担心，可能某个清晨的早间新闻，你们已经回来了！ 2021-11-1 00:21:44]]></content>
      <categories>
        <category>加密与认证</category>
      </categories>
      <tags>
        <tag>MD5</tag>
        <tag>BCrypt</tag>
        <tag>Hash</tag>
        <tag>单项散列</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下从路径字符串中截取目录和文件名信息]]></title>
    <url>%2Fblog%2F2021%2F10%2F24%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BB%8E%E8%B7%AF%E5%BE%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E6%88%AA%E5%8F%96%E7%9B%AE%E5%BD%95%E5%92%8C%E6%96%87%E4%BB%B6%E5%90%8D%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言之前在文章《.bat批处理（十）：从路径字符串中截取盘符、文件名、后缀名等信息》中总结了在批处理文件中，也就是Windows环境下怎样从路径字符串中截取文件名、盘符等信息，利用的是Windows的扩展变量，而这种截取路径的需求在linux脚本中也很常见，实现方法相比批处理脚本而言要简单一些，下面列举一下常见的方法。 使用系统命令完成路径截取针对于路径的操作很基础，所以在linux环境下提供了专门的命令来完成路径字符串的截取工作，分别是 basename 和 dirname，从名字就可以很容易知道他们的作用，具体的使用方法如下： 使用 basename 命令获取文件名获取文件名时直接在命令后面添加待截取的路径即可，表现如下： 12albert@home-pc:/mnt/d/data/shell$ basename /mnt/d/data/shell/test.txttest.txt 在shell脚本的写法通常是这样： 12mypath=/mnt/d/data/shell/test.txtecho $(basename $mypath) 运行之后得到的带有后缀的文件名，如果想去掉后缀使用 -s 参数指定要去掉的后缀即可： 12albert@home-pc:/mnt/d/data/shell$ basename -s .txt /mnt/d/data/shell/test.txttest -s 选项也可省略，将后缀名直接放到完整路径后面也可以： 12albert@home-pc:/mnt/d/data/shell$ basename /mnt/d/data/shell/test.txt .txttest 其实 -s 参数后面不一定非得加后缀名，它就是在结果的末尾去掉匹配的字符串，所以可以指定任何结尾字符： 12albert@home-pc:/mnt/d/data/shell$ basename /mnt/d/data/shell/test.txt xttest.t 使用 dirname 命令获取目录名顾名思义 dirname 就是获取目录名的命令，直接在命令后面跟上待截取的路径即可： 12albert@home-pc:/mnt/d/data/shell$ dirname /mnt/d/data/shell/test.txt/mnt/d/data/shell 这个命令不仅可以获取文件所在目录，还可以获取目录的上一级目录，实际上它处理的仅仅是字符串，截取的目标字符也是 /，并不要求目录是有效的 12albert@home-pc:/mnt/d/data/shell$ dirname /mnt/d/data//mnt/d 利用变量提取操作完成截取通过对 ${var} 表达式进行变形可以完成对原变量的部分提取和替换，下面先列举一些主要的截取操作： 表达式 含义 示例 结果 ${var} var字符串原始值 ${var} ^Can you can a can as a canner can can a can$ ${\#var} var字符串的长度 ${\#var} ^43$ ${var:pos} 在字符串var中从位置pos提取子串 ${var:30} ^can can a can$ ${var:pos:len} 在var中从位置pos提取长度为len的子串 ${var:30:5} ^can c$ ${var#substr} 在var左侧删除最短匹配$substr的子串 ${var#*can a} ^ can as a canner can can a can$ ${var##substr} 在var左侧删除最长匹配$substr的子串 ${var##*can a} ^ can$ ${var%substr} 在var右侧删除最短匹配$substr的子串 ${var%can a*} ^Can you can a can as a canner can $ ${var%%substr} 在var右侧删除最长匹配$substr的子串 ${var%%can a*} ^Can you $ 好了，知道了这些规则我们就可以利用这些截取规则，来完成截取目录的需求了，下面假设 var 的值为 ./d/data/shell/test.txt 开始具体操作。 截取根目录要想截取根目录就是要把第一个 / 后面的内容删除就可以了，使用 ${var%%/*} 在var右侧删除最长匹配/*的子串，结果就只剩下 . 了: 123albert@home-pc:/mnt/d/data/shell$ var="./d/data/shell/test.txt"albert@home-pc:/mnt/d/data/shell$ echo $&#123;var%%/*&#125;. 截取文件名截取文件名需要把最后一个 /前面的内容删除，使用 ${var##*/} 在var左侧删除最长匹配 */ 的子串，结果为包含后缀的文件名： 123albert@home-pc:/mnt/d/data/shell$ var="./d/data/shell/test.txt"albert@home-pc:/mnt/d/data/shell$ echo $&#123;var##*/&#125;test.txt 截取文件后缀截取文件名后缀与截取文件名类似，需要包最后一个 . 前面的内容删除，使用 ${var##*.} 在var左侧删除最长匹配 *. 的子串，只保留后缀内容： 123albert@home-pc:/mnt/d/data/shell$ var="./d/data/shell/test.txt"albert@home-pc:/mnt/d/data/shell$ echo $&#123;var##*.&#125;txt 截取文件所在的目录截取文件坐在目录就是只删除文件名，把文件名前面的 / 包括之前的内容进行保留，使用 ${var%/*} 在var右侧删除最短匹配 /* 的子串： 123albert@home-pc:/mnt/d/data/shell$ var="./d/data/shell/test.txt"albert@home-pc:/mnt/d/data/shell$ echo $&#123;var%/*&#125;./d/data/shell 样例假设 var 的值为 ./d/data/shell/test.txt，具体实现的表格总结如下： 需求 表达式 结果 根目录 ${var%%/*} . 文件名 ${var##*/} test.txt 文件后缀 ${var##*.} txt 文件所在目录 ${var%/*} ./d/data/shell 总结 basename 和 dirname 是linux环境下专门截取文件名和目录名的命令工具 处理文件路径截取的通常使用 ${var} 形式的变量提取方法，这种方法很方便，但不仅限于相关目录的处理 常用截取表达式：文件名 ${var##*/}、文件后缀 ${var##*.}、文件所在目录 ${var%/*} ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 今年的1024和去年一样又是周末，凌晨登陆冰封了4、5年的对战平台账号，看着全部灰色的dota好友列表，再想凑齐所有人一起开黑几乎不可能了。打了半宿输多赢少，还是那个手残的我。出了新的英雄，添了新的装备，面对这个曾经挚爱的游戏感觉有些陌生，历史的车轮不会因为你不关注就会停止，相反，这些“平行世界”总是在向前奔跑着，唯一不变的就是变化~ 2021-10-24 19:46:47]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>dirname</tag>
        <tag>basename</tag>
        <tag>路径截取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（十）：从路径字符串中截取盘符、文件名、后缀名等信息]]></title>
    <url>%2Fblog%2F2021%2F10%2F16%2F%E6%89%B9%E5%A4%84%E7%90%86%E4%BB%8E%E8%B7%AF%E5%BE%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E6%88%AA%E5%8F%96%E7%9B%98%E7%AC%A6%E3%80%81%E6%96%87%E4%BB%B6%E5%90%8D%E3%80%81%E5%90%8E%E7%BC%80%E5%90%8D%E7%AD%89%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言又是实际开发中的问题，想要截取一个文件路径中的盘符、文件名等信息，第一反应是正则表达式？或者是 split 函数？这些往往都是“高级”语言中才会有的实现方法，对于批处理来说有点“带不动”啊，那么在bat批处理中要怎样处理类似的请求呢？最近找到了两种方法，接下来会逐一展示一下，不过在展示具体的写法前，我们先来看一下 %~dp0的含义。 %~dp0的含义关于 %~dp0 的作用在之前的总结中 《.bat批处理（四）：路径相关%cd%和%~dp0的区别》 有提到过，它表示当前运行的批处理文件所在的目录，那么它是一个特殊的变量吗？ 可以说算是吧，这个变量特殊在它是从参数变量 %0 扩展而来的，提到 %0 很多人都会想到它是批处理脚本的第一个参数，表示当前运行的脚本全路径，可以写个脚本试一下： 1234567891011# Albert at home-pc in D:\data\bat [0:03:25]% Get-Content showparams.bat@echo offecho %0echo %1# Albert at home-pc in D:\data\bat [0:03:31]% ./showparams.bat good"D:\data\bat\showparams.bat"good 类似的变量还有 1%、2%、3%…. 一直到9%，都依次表示运行批处理脚本时传入的参数，这些变量还有一个本领，那就是支持扩展，写起来花里胡哨的。 扩展字符串扩展字符串是批处理自带的功能，可以实现对表示文件路径的字符串进行特殊的处理，以%0 参数为例，具体功能列举如下： %~0 - 删除路径中的引号 %~f0 - 将 %0 扩展到一个完全合格的路径名 %~d0 - 将 %0 扩展到一个驱动器号 %~p0 - 将 %0 扩展到一个路径 %~n0 - 将 %0 扩展到一个文件名 %~x0 - 将 %0 扩展到一个文件扩展名 %~s0 - 将 %0 扩展的路径只含有短名 %~a0 - 将 %0 扩展到文件的文件属性 %~t0 - 将 %0 扩展到文件的日期/时间 %~z0 - 将 %0 扩展到文件的大小 %~$PATH:0 查找变量0%在路径环境变量$PATH的目录，并将 %0 扩展到找到的第一个完全合格的名称，$PATH未被定义或没找到文件，则结果为空字符串 当然这个写法也可以进行组合，比如 %~d0 和 %~p0 组合后变成 %~dp0 也就是我们常见的那个变量啦~ 可以将这些变量打印出来看一下具体的值： 1234567891011121314151617181920212223242526272829303132# Albert at home-pc in D:\data\bat [0:26:17]% Get-Content showparams.bat@echo offecho %0echo %~0echo %~f0echo %~d0echo %~p0echo %~n0echo %~x0echo %~s0echo %~a0echo %~t0echo %~z0echo %~dp0echo %~nx0# Albert at home-pc in D:\data\bat [0:26:28]% .\showparams.bat"D:\data\bat\showparams.bat"D:\data\bat\showparams.batD:\data\bat\showparams.batD:\data\bat\showparams.batD:\data\bat\showparams.bat--a--------2021/10/17 00:26156D:\data\bat\showparams.bat 从字符串中截取路径、文件名 上面的部分解释了%~dp0，同时也知道了这些脚本参数指出扩展语法，如果是普通变量的话就不能使用扩展语法了，那么对于一个普通的包含字符串怎么才能使用扩展语法，截取到想要的部分呢？目前我知道的有两种方法：一种是传参使其变成脚本参数，也就是 %n的形式，另一种方法就是使用 for 语句，接下来分别看一下。 脚本传参普通的字符串无法进行扩展，如果想把这种变量就需要把它们变成脚本参数，这就需要将参数传递给另一个脚本，这样实现起来会将脚本调用变得复杂一些，实际上可以在一个脚本中完成截取工作，类似于C/C++中的函数调用，可以在批处理中使用 call 命令搭配标签实现，具体代码如下： 123456789101112131415161718192021222324252627# Albert at home-pc in D:\data\bat [17:37:54]% Get-Content extract1.bat@echo offset OriginStr="C:/Demo/myproject/example.txt"echo %OriginStr%call :extract %OriginStr%goto :eof:extractrem 获取到文件路径echo %~dp1rem 获取到文件盘符echo %~d1rem 获取到文件名称echo %~n1rem 获取到文件后缀echo %~x1# Albert at home-pc in D:\data\bat [17:41:25]% .\extract1.bat"C:/Demo/myproject/example.txt"C:\Demo\myproject\C:example.txt 在这段代码中 :eof 标签是一个默认的标签，表示文件结尾，实际需求中需根据具体要求进行调整。 for语法扩展使用 for 循环是另一种实现方式，因为循环变量也可以支持扩展，可以将需要截取的字符串路径放在循环范围中，然后先循环输出测试下： 1234567891011# Albert at home-pc in D:\data\bat [17:46:29]% Get-Content extract2.bat@echo offset OriginStr="C:/Demo/myproject/example.txt"for %%I in (%OriginStr%) do echo %%I# Albert at home-pc in D:\data\bat [17:46:57]% .\extract2.bat"C:/Demo/myproject/example.txt" 在批处理中的循环变量是 %%I的形式，需要两个 % 才可以，后面的变量名可以换成26个字母中的任意一个，并且字母会区分大小写，然后利用这些循环变量就可以进行扩展，然后完成最开始的需求，实现代码如下： 123456789101112131415161718192021222324# Albert at home-pc in D:\data\bat [17:53:53]% Get-Content extract2.bat@echo offset OriginStr="C:/Demo/myproject/example.txt"for %%I in (%OriginStr%) do echo %%Irem 获取到文件路径for %%I in (%OriginStr%) do echo %%~dpIrem 获取到文件盘符for %%I in (%OriginStr%) do echo %%~dIrem 获取到文件名称for %%I in (%OriginStr%) do echo %%~nIrem 获取到文件后缀for %%I in (%OriginStr%) do echo %%~xI# Albert at home-pc in D:\data\bat [17:54:01]% .\extract2.bat"C:/Demo/myproject/example.txt"C:\Demo\myproject\C:example.txt 这种写法的好处就是无需控制标签跳转流程，通过循环命令 for 就可以获取想要的参数，使用起来会方便很多。 总结 在批处理文件中 %~dp0 表示批处理文件所在的目录，而 %cd% 表示执行命令时所在的目录 在批处理文件中想要截取目录操作可以使用变量扩展来实现，而变量必须是 %i 的形式，其中的 i 是可以是 a~zA~Z0~9 for 表达式中的循环变量在cmd命令行中是 %i 的形式，而在批处理文件中需要协程 %%i 的形式 常用的变量扩展有：获取到文件盘符使用 %~d0，获取到文件名称使用 %~n0，获取到文件后缀使用 %~x0 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 反人性需要不断修炼，逆向思维才能战胜人性的弱点，很多时候事情往往不是你想的那样~ 2021-10-17 18:01:07]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11中的std::atomic保证的原子性是什么]]></title>
    <url>%2Fblog%2F2021%2F10%2F13%2FC-11%E4%B8%AD%E7%9A%84std-atomic%E4%BF%9D%E8%AF%81%E7%9A%84%E5%8E%9F%E5%AD%90%E6%80%A7%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[前言提到atomic这个词，你首先想到的是什么呢？作为一个长时间混迹于编程世界的菜鸟，我首先想到的一个词是“原子性”，接着飞入脑海的是 “ACID” 这个缩写词组，既然提到了 ACID 我们就来简单的复习一下。 ACID 是指事务管理的4个特性，常见于数据库操作管理中，它们分别是：原子性，一致性，隔离性和持久性。 原子性（Atomicity）是指事务是一个不可分割的工作单位，事务中的操作要么都执行，要么都不执行。 一致性（Consistency）是指事务前后数据的完整性必须保持一致，完全符合逻辑原运算。 隔离性（Isolation）是指在多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离，无需感知其他事务的存在。 持久性（Durability）是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对数据造成损坏。 C++中的atomic原子（atom）是在化学反应中不可分割基本微粒，而编程世界中的原子性也是取自这里的不可分割的含义，不可分割与事务管理中的原子性含义一致，指的是一个操作或者一系列操作只能全都执行或者都不执行，不会只执行其中一部分，那么C++11中引入atomic有什么用？不使用atomic能不能保证原子性呢？ 其实C++11中引入atomic主要还是降低了编程的复杂度，如果不使用atomic同样可以使用锁机制来保证原子性，接下来我们来看看为什么需要原子性。 一个简单的自增运算i++ 是个再简单不过的语句了，我们可以使用它来做一个计数器，每次自增加1，假设我们有一个工程项目有两条商品生产的流水线，每个流水线生产出一件商品则需要计数器加1，这时我们用两个线程来模拟两条流水线，每个线程函数来调用自增的计数器，来看看有什么问题？ 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;thread&gt;int i = 0;void func(int n)&#123; for (int k = 0; k &lt; n; k++) i++;&#125;int main(int argc, char* argv[])&#123; int n = argc &gt; 1 ? atoi(argv[1]) : 100; std::thread t1(func, n); std::thread t2(func, n); t1.join(); t2.join(); std::cout &lt;&lt; "i=" &lt;&lt; i &lt;&lt; std::endl;&#125; 测试代码如上所示，执行 g++ -std=c++20 -O0 -pthread main.cpp &amp;&amp; ./a.out 10 命令编译并运行得到结果 i=20，貌似很正常，一共两个线程，每个线程执行10次自增操作，结果就应该是20啊，先别太早下结论，增大自增范围试试。 执行 g++ -std=c++20 -O0 -pthread main.cpp &amp;&amp; ./a.out 100000 得到结果 i=112831，多次执行发现每次运行结果都不太一样，但是数据范围在 100000~200000，这就有些奇怪了，每个线程执行循环执行一条语句，那么程序结果应该等于 2n 才对，为什么结果总是小于 2n 呢，难道有些循环没有执行？ 其实不是这样的，i++从C++语言的层面来看确实是一条语句，但是真正再和机器打交道时一般会解释成类似于下面这样3条汇编指令： 1234// x86 msvc v19.latestmov eax, DWORD PTR _i$[ebp]add eax, 1mov DWORD PTR _i$[ebp], eax 3条指令的含义可以理解为读取、自增，设置共三步，既然不是真正的一条语句，那么在多线的环境下就会生语句的交叉执行，比如第一个线程执行读取变量i的值之后，第二个线程也读取了变量i的值，这样两个线程都进行后续的自增和设置指令后，会发现比预期的值少了一个，这种情况在循环次数较多时尤为明显。 通过加锁把自增变为原子操作既然每个自增操作可能会被分解成3条指令，那么我们可以加锁来将3条指令捆绑，当一个线程执行自增操作时加锁来防止其他进程“捣乱”，具体修改如下，可以在自增操作前直接加锁： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;chrono&gt;#include &lt;mutex&gt;int i = 0;std::mutex mt;void inc()&#123; std::lock_guard&lt;std::mutex&gt; l(mt); i++;&#125;void func(int n)&#123; for (int k = 0; k &lt; n; k++) inc();&#125;int main(int argc, char* argv[])&#123; int n = argc &gt; 1 ? atoi(argv[1]) : 100; std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); std::thread t1(func, n); std::thread t2(func, n); t1.join(); t2.join(); std::cout &lt;&lt; "i=" &lt;&lt; i &lt;&lt; std::endl; std::chrono::duration&lt;double&gt; duration_cost = std::chrono::duration_cast&lt; std::chrono::duration&lt;double&gt; &gt;(std::chrono::steady_clock::now() - start); std::cout &lt;&lt; "total cost " &lt;&lt; duration_cost.count() &lt;&lt; " seconds." &lt;&lt; std::endl; return 0;&#125; 执行 g++ -std=c++20 -O0 -pthread main.cpp &amp;&amp; ./a.out 10000000 命令后运行结果如下： 12i=20000000total cost 2.39123 seconds. 通过加锁，我们已经保证了结果的正确性，但是我们知道加锁的额外消耗还是很大的，有没有其他的方式来实现原子操作呢？ 使用atomic来保证自增的原子性其实在C++11之前可以通过嵌入汇编指令来实现，不过自从C++11引入atomic之后，类似的需求变得简单了许多，可以直接使用autmic这个模板类来实现，代码几乎不需要修改，只需将变量 i 改为 atomic&lt;int&gt; 类型，再把锁去掉就可以了，修改后的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;chrono&gt;#include &lt;mutex&gt;#include &lt;atomic&gt;std::atomic&lt;int&gt; i = 0; // int -&gt; atomic&lt;int&gt;std::mutex mt;void inc()&#123; //std::lock_guard&lt;std::mutex&gt; l(mt); //remove lock i++;&#125;void func(int n)&#123; for (int k = 0; k &lt; n; k++) inc();&#125;int main(int argc, char* argv[])&#123; int n = argc &gt; 1 ? atoi(argv[1]) : 100; std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); std::thread t1(func, n); std::thread t2(func, n); t1.join(); t2.join(); std::cout &lt;&lt; "i=" &lt;&lt; i &lt;&lt; std::endl; std::chrono::duration&lt;double&gt; duration_cost = std::chrono::duration_cast&lt; std::chrono::duration&lt;double&gt; &gt;(std::chrono::steady_clock::now() - start); std::cout &lt;&lt; "total cost " &lt;&lt; duration_cost.count() &lt;&lt; " seconds." &lt;&lt; std::endl; return 0;&#125; 执行 g++ -std=c++20 -O0 -pthread main.cpp &amp;&amp; ./a.out 10000000 命令后运行结果如下： 12i=20000000total cost 1.6554 seconds. 通过对比可以发现，使用 std::atomic 模板类之后，在保证了结果正确的同时，相比于加锁实现原子性速度上有了明显的提升。 总结 ACID 是指事务管理中的原子性，一致性，隔离性和持久性4个特性。 加锁（写锁）的目的通常是将可能同时发生的操作串行化，以此来避免对资源的竞争出现问题 操作的并行加快了任务的处理速度，而“加锁”使部分操作回归到串行，两者相互配合是为了在更短的时间内得到正确的结果 std::atomic 降低了原子性操作编程的难度，同时相比于加锁实现原子性还有了性能的提升 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 时光时光慢些吧，不要再让你变老了，我愿用我一切，换你岁月长留~ 时间对于每个人来说，都是公平的，真的是这样吗？我觉得未必吧！ 2021-10-6 00:32:33–]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>atomic</tag>
        <tag>原子性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dijkstra——通过不断松弛来解决单源最短路径问题的算法]]></title>
    <url>%2Fblog%2F2021%2F09%2F11%2FDijkstra%E2%80%94%E2%80%94%E9%80%9A%E8%BF%87%E4%B8%8D%E6%96%AD%E6%9D%BE%E5%BC%9B%E6%9D%A5%E8%A7%A3%E5%86%B3%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98%E7%9A%84%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言Dijkstra算法又称迪杰斯特拉算法，它采用的是一种贪心的策略，使用广度优先搜索的方式解决带权有向图或者无向图的单源最短路径问题，需要注意的是它不能处理带负边权的情况，核心思想就是“守住现有阵地不断攻占底盘”，这一点可以在后面代码实现中慢慢体会，接下来梳理一下算法思路。 Dijkstra思路Dijkstra 算法的思路是维护一个点集合 S 和一个用来保存起点 m 到各个顶点到各个顶点最短距离的数组 dis，用邻接数组来表示带权图信息。 初始情况时，集合 S 中只包括起点m，通过图信息来初始化 dis 数组，将起点 m 可以直接到达的点设置为边的权值，不能到达的点设置为无穷大，比如点 m 到点 n 的距离是d，则 dis[n] = d。 然后从带权图中选择不在集合S 中的到点 m 距离最近的点，假设为 n，把它加到集合 S 中，然后尝试通过点 n “松弛” 那些不在集合 S 中的点到点 m的距离，更新 dis 数组信息，具体操作就是使用点 n 作为中转，如果距离如果点 m 到任意点 x 通过点 n 中转距离变短了，那么就更新 dis[x] 的值。 之后不断重复上面的“松弛”操作，直到集合 S 中包含了所有得到顶点，至此就通过Dijkstra算法求解出了从点 m 到图中任意点的最短距离。 图解实例 看了上面的关于Dijkstra算法的文字描述可能还是有点蒙，这时候需要画个图来解释一下，对于算法问题，特别是图论方面的算法题，有时候真的是一图胜千言，奈何我真的是不想画图，一方面因为“懒”，另一方面就是图片的搬运比较麻烦，所以对于大部分问题我都是文字描述，但是为了解释这个Dijkstra我还是决定画一画，假如求解从点a 到各个顶点的最短距离，初始图信息如下： 第一步，我们把点 a 添加到集合 S 中变为 S = {a}，然后初始化dis数组为 dis = {0, 1, 12, ∞, ∞, ∞}，加入集合的点用红色表示，操作之后更新如下： 第二步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 b，将点 b 添加到集合 S 中，通过点 b 中转更新 dis 数组，dis[c]变为8，dis[d]变为4，更新后集合为 S = {a, b}， 距离数组为 dis = {0, 1, 8, 4, ∞, ∞}, 图信息如下： 第三步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 d，将点 d 添加到集合 S 中，通过点 d 中转更新 dis 数组，dis[e]变为14，dis[f]变为17，更新后集合为 S = {a, b, d}， 距离数组为 dis = {0, 1, 8, 4, 14, 17}, 图信息如下： 第四步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 c，将点 c 添加到集合 S 中，通过点 c 中转更新 dis 数组，dis[e]变为13，更新后集合为 S = {a, b, d, c}， 距离数组为 dis = {0, 1, 8, 4, 13, 17}, 图信息如下： 第五步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 e，将点 e 添加到集合 S 中，通过点 e 中转更新 dis 数组，通过距离判断发现此次不需要更新dis数组，更新后集合为 S = {a, b, d, c, e}， 距离数组为 dis = {0, 1, 8, 4, 13, 17}, 图信息如下： 第六步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 f，将点 f 添加到集合 S 中，至此集合 S 中包含了所有的顶点，Dijkstra算法执行结束，集合信息为 S = {a, b, d, c, e, f}， 距离数组为 dis = {0, 1, 8, 4, 13, 17}, 图信息如下： 代码实现通过上面的图解实例对于Dijkstra的实现应该有了一些思路，那么接下来我们把它转化成代码： 1234567891011121314151617void Dijkstra(vector&lt;vector&lt;int&gt;&gt;&amp; graph)&#123; vector&lt;int&gt; dis = graph[0]; set&lt;int&gt; S; const int n = dis.size(); for (int i = 0, x = 0; i &lt; n; i++, x = 0) &#123; // find minimum weight for (int j = 0; j &lt; n; j++) if (!S.count(j) &amp;&amp; (x == 0 || dis[j] &lt; dis[x])) x = j; S.insert(x); // relax for (int j = 0; j &lt; n; j++) if (!S.count(j) &amp;&amp; dis[x] + graph[x][j] &lt; dis[j]) dis[j] = dis[x] + graph[x][j]; &#125;&#125; 运行上述代码之后我们便得到了节点0到任意点的最短路径长度数组 dis。 从上面的分析我们可以知道从点 a 到点 f 的最短路径长度是 17，那么最短路径怎样求呢？ 其实只要在做松弛操作时记录每个节点是从哪个节点松弛得到的就可以了，比如可以使用一个pre数组来记录这个信息，当计算 dis 结束时通过pre数组反推就可以得到最短路径，简单实现如下： 12345678910111213141516171819202122232425262728293031void Dijkstra(vector&lt;vector&lt;int&gt;&gt;&amp; graph)&#123; vector&lt;int&gt; dis = graph[0]; set&lt;int&gt; S; const int n = dis.size(); vector&lt;int&gt; pre(n, 0); // save previous point index for (int i = 0, x = 0; i &lt; n; i++, x = 0) &#123; // find minimum weight for (int j = 0; j &lt; n; j++) if (!S.count(j) &amp;&amp; (x == 0 || dis[j] &lt; dis[x])) x = j; S.insert(x); // relax for (int j = 0; j &lt; n; j++) if (!S.count(j) &amp;&amp; dis[x] + graph[x][j] &lt; dis[j]) &#123; dis[j] = dis[x] + graph[x][j]; pre[j] = x; &#125; &#125; // output path info vector&lt;int&gt; path&#123;5&#125;; while(path.back() != 0) &#123; path.push_back(pre[path.back()]); &#125; for (auto it = path.rbegin(); it != path.rend(); it++) cout &lt;&lt; *it &lt;&lt; " ";&#125; 总结 Dijkstra算法的时间复杂度为O(N^2)，空间复杂度为 O(N)，如果对时间复杂度有更高要求可以使用堆结构进行优化 Dijkstra是一种求解单源最短路径的算法，在时间复杂度这一项要优于之前所说的 Floyd 算法 Dijkstra不能处理带负边权的情况，不过实际生活中类似于行车路线、管道铺设等问题都不会有负边权，应用还是比较广泛的 该算法仔细分析之后还是比较好理解的，不过还是有一些变型和编程技巧，需要在实际问题中灵活变通 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 历史总是惊人的相似，却不会简单的重复。在柯立芝实行了以放任自流的经济政策之后，紧接着便迎来了1929年的大萧条；而在克林顿到小布什任期内采取的经济自由化的政策，引发了之后2008年的国际金融危机；如今我们抬头看看大洋彼岸那疯狂运转的印钞机，这次的泡泡或许很快就能迎来炸裂的时刻~ 2021-9-12 23:29:46]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>算法</tag>
        <tag>最短路径</tag>
        <tag>Dijkstra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Floyd-Warshall——仅用4行代码就能解决多源最短路径问题的算法]]></title>
    <url>%2Fblog%2F2021%2F09%2F05%2FFloyd-Warshall%E2%80%94%E2%80%94%E4%BB%85%E7%94%A85%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%B0%B1%E8%83%BD%E8%A7%A3%E5%86%B3%E5%A4%9A%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98%E7%9A%84%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言Floyd-Warshall算法简记Floyd算法，又称弗洛伊德算法，是解决任意两点间的最短路径问题的一种常用算法，核心思想就是“不断利用第三者影响原配关系”，这一点在4行核心代码中表现的淋漓尽致，接下来梳理一下算法思路。 Floyd思路从A点走到B点要想路径最短只有两种可能，一种就是直接从A到B，另一种就是通过其他点来中转，Floyd的思路就是先把直接能到达的点固定下来，然后不断的尝试从其他点来中转来降低路程。 Floyd算法实现通常使用一个二维数组来表示任意两点之间的初始距离，每个点到自身的距离为0，若两个点之间没有直接连通，则赋值为 +∞，我们假设这个二维数组是 v，则 v[i][j] 代表了从点 i 到点 j 的初始距离。 假设不允许中转，那么二维数组 v 中的数据就代表了任意两点间的距离。 如果允许中转一次，我们假设只允许从节点1进行中转，那么点 i 到点 j 的最近距离最小为 v[i][j] 或者 v[i][1] + v[1][j]，如果 v[i][1] + v[1][j] 的值更小，我们可以使用它来更新 v[i][j] 的值，这时 v[i][j] 就不仅仅是一个值了，而是隐含着 i-&gt;1-&gt;j 这样一条路径，这个过程实际上翻译成代码就是： 123for(int i = 0; i &lt; n; i++) for(int j = 0; j &lt; n; j++) v[i][j] = min(v[i][j], v[i][1] + v[1][j]); 那么这条路径怎样才能更短呢？ 答案就是引入另一个点，比如我们不仅允许从节点1中转，也允许从节点2中转，从上一步我们知道从从点 i 到点 j 的最短距离是从 i-&gt;1-&gt;j 得到的，实际上经过上面一步，任意两点的距离都是允许从节点1中转条件下的最小值， 那么引入节点2之后就是要看看 v[i][j] 和 v[i][2] + v[2][j] 谁更小一点，然后遍历更新即可，类似的代码可以写成： 123for(int i = 0; i &lt; n; i++) for(int j = 0; j &lt; n; j++) v[i][j] = min(v[i][j], v[i][2] + v[2][j]); 看到套路了没有，就是每个点都作为一个可能中转的点来试一下，整个算法就结束了，好神奇~ 完整4行代码如下： 1234for(int k = 0; k &lt; n; k++) for(int i = 0; i &lt; n; i++) for(int j = 0; j &lt; n; j++) v[i][j] = min(v[i][j], v[i][k] + v[k][j]); 简单粗暴又不失美感！ 示例初始路径及每条边的距离如图： 翻译成二维数组如下： 0 2 7 4 ∞ 0 4 ∞ 9 1 0 ∞ 6 ∞ 15 0 仅通过节点0作为中转，二维数组更新如下： 0 2 7 4 ∞ 0 4 ∞ 9 1 0 13 6 8 13 0 增加节点1作为中转，二维数组更新如下： 0 2 6 4 ∞ 0 4 ∞ 9 1 0 13 6 8 12 0 再增加节点2作为中转，二维数组更新如下： 0 2 6 4 13 0 4 17 9 1 0 13 6 8 12 0 最后增加节点3作为中转，二维数组更新如下： 0 2 6 4 13 0 4 17 9 1 0 13 6 8 12 0 至此我们就求解出了任意两点间的最小距离。 总结 Floyd算法的时间复杂度为O(N^3)，空间复杂度为 O(N^2) Floyd是一种求解多源最短路径的算法，如果是求解单源最短路径这 N^3 的时间复杂度确实有点伤 Floyd可以正确处理有向图或存在负权边的图，但不能处理存在负权回路的图的最短路径问题 4行代码3层循环或许可以助它称为最容易让人理解的最短路径算法 这4行代码只是一个理想化的模型，实际在编码时要注意加法的越界问题，因为两个无穷大相加理论上是无穷大，但在代码里可能就崩溃了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生到底是追求到达目的地还是准备欣赏沿途的风景，一味地向前奔跑忽略了周围的一切，很多美好的事物就在身边却不自知，我们已经被世俗蒙蔽了双眼，什么时候可以慢下来呢？ 2021-9-7 01:14:05]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Floyd</tag>
        <tag>算法</tag>
        <tag>最短路径</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从一个小题中的应用来体会下std::tie的便利之处]]></title>
    <url>%2Fblog%2F2021%2F08%2F15%2F%E4%BB%8E%E4%B8%80%E4%B8%AA%E5%B0%8F%E9%A2%98%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E6%9D%A5%E4%BD%93%E4%BC%9A%E4%B8%8Bstd-tie%E7%9A%84%E4%BE%BF%E5%88%A9%E4%B9%8B%E5%A4%84%2F</url>
    <content type="text"><![CDATA[前言今天主要学习一下 std::tie 函数的使用方法，之前看到 tie 函数是和 IO 绑定的，最近发现它是和 std::tuple 绑定的，查询资料后发现两个函数虽然名字相同，但是在不同的作用域下，今天学一下和 tuple 有关的这个 tie 函数，不过在学习之前先看一道小题。 解题过程爬楼梯的最少成本这是 LeetCode 上的一道题，题目描述如下： 数组的每个下标作为一个阶梯，第 i 个阶梯对应着一个非负数的体力花费值 cost[i]（下标从 0 开始）。 每当爬上一个阶梯都要花费对应的体力值，一旦支付了相应的体力值，就可以选择向上爬一个阶梯或者爬两个阶梯。 请找出达到楼层顶部的最低花费。在开始时，你可以选择从下标为 0 或 1 的元素作为初始阶梯。 示例 1：123输入：cost = [10, 15, 20]输出：15解释：最低花费是从 cost[1] 开始，然后走两步即可到阶梯顶，一共花费 15 。 示例 2：123输入：cost = [1, 100, 1, 1, 1, 100, 1, 1, 100, 1]输出：6解释：最低花费方式是从 cost[0] 开始，逐个经过那些 1 ，跳过 cost[3] ，一共花费 6 。 提示： 2 &lt;= cost.length &lt;= 1000 0 &lt;= cost[i] &lt;= 999 题目分析这种求解最小花费、最大方案数，最大价值的题目是典型的动态规划题目，这道题也可以使用动态规划的方式来解，既然每次可以选择爬一个或者两个阶梯，那么到达某一个阶梯的花费就等于这个阶梯的花费加上前一个阶梯花费和前两个花费的之间最小值即可，最终的结果取最后一个阶梯和倒数第二个阶梯中的最小值，代码比较简单，实现如下： 12345678910111213class Solution &#123;public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123; int n = cost.size(); vector&lt;int&gt; ans(n); ans[0] = cost[0]; ans[1] = cost[1]; for (int i = 2; i &lt; n; i++) ans[i] = min(ans[i-1], ans[i-2]) + cost[i]; return min(ans[n-1], ans[n-2]); &#125;&#125;; DP优化虽然使用dp数组求解起来很方便，但是从实现上可以看出，每个阶梯的花费只与它前两个阶梯的花费有关，所以使用一个长度为N的数组在空间上有些浪费，其实只要两个变量就可以了，我们用 first 和 second 两个变量分别表示某个阶梯前两个阶梯的花费，可以实现如下代码： 1234567891011121314class Solution &#123;public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123; int n = cost.size(), first = cost[0], second = cost[1]; for (int i = 2; i &lt; n; i++) &#123; int temp = min(first, second) + cost[i]; first = second; second = temp; &#125; return min(first, second); &#125;&#125;; 利用tie进行写法优化使用两个变量优化之后这个算法变成了 O(1) 的空间复杂度，但是在 for 循环中的写法还是有些啰嗦，其实这种写法和交换两个变量值过程非常相似，在GO语言中可以写成 a,b = b,a 来完成交换，但是在C++中这样的写法是错误的，不管是引入第三个变量，还是通过异或解决都需要写三条语句，但是这种情况在遇到 std::tie 函数之后有望得到改变，上面的写法利用 std::tie 可以改写如下： 12345678class Solution &#123;public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123; int n = cost.size(), first = cost[0], second = cost[1]; for (int i = 2; i &lt; n; i++) tie(first, second) = make_tuple(second, min(first, second) + cost[i]); return min(first, second); &#125;&#125;; std::tuple在学习 std::tie 的作用方式之前，先来看一下 std::tuple 是什么。如果你对这个结构有些陌生，可以先想想 std::pair 这个结构。首先 std::tuple 是一个类模板，同时他也是一个固定大小的由各种类型的值组成集合，是 std::pair 的一种泛化实现。 std::pair 中包含两个元素，而 std::tuple 可以同时包含多个元素，它拥有 struct 的表现，但是无需定义实际的 struct，在函数返回多个值时拥有良好的表现。 std::tuple的访问 利用 std::get 函数通过下标访问（C++11） 123auto t = std::make_tuple(110, "excellent", 3.14);std::cout &lt;&lt; "(" &lt;&lt; std::get&lt;0&gt;(t) &lt;&lt; ", " &lt;&lt; std::get&lt;1&gt;(t) &lt;&lt; ", " &lt;&lt; std::get&lt;2&gt;(t) &lt;&lt; ")" &lt;&lt; std::endl; 利用 std::tie 函数进行参数解绑（C++11) 123456auto t = std::make_tuple(110, "excellent", 3.14);int n;std::string s;float d;std::tie(n, s, d) = t;std::cout &lt;&lt; "(" &lt;&lt; n &lt;&lt; ", " &lt;&lt; s &lt;&lt; ", " &lt;&lt; d &lt;&lt; ")" &lt;&lt; std::endl; 利用 std::get 函数通过类型访问（C++14），这种使用方式如果每种类型不唯一会出现编译错误 123auto t = std::make_tuple(110, "excellent", 3.14);std::cout &lt;&lt; "(" &lt;&lt; std::get&lt;int&gt;(t) &lt;&lt; ", " &lt;&lt; std::get&lt;const char*&gt;(t) &lt;&lt; ", " &lt;&lt; std::get&lt;double&gt;(t) &lt;&lt; ")" &lt;&lt; std::endl; 利用结构化绑定的方式来访问（C++17） 123auto t = std::make_tuple(110, "excellent", 3.14);auto [n, s, d] = t;std::cout &lt;&lt; "(" &lt;&lt; n &lt;&lt; ", " &lt;&lt; s &lt;&lt; ", " &lt;&lt; d &lt;&lt; ")" &lt;&lt; std::endl; 以上的几个例子的输出结果都是 (110, excellent, 3.14) std::tie函数中使用std::ignore占位使用 std::tie 函数来获取 std::tuple 参数时，有时不需要所有的参数，这种情况下可以使用 td::ignore 来占位，代替那些不关心的参数，比如 std::set 结构中 insert 函数的返回值。 1234567std::set&lt;int&gt; st;std::pair&lt;std::set&lt;int&gt;::iterator, bool&gt; sp1 = st.insert(4);std::cout &lt;&lt; *sp1.first &lt;&lt; " " &lt;&lt; sp1.second &lt;&lt; std::endl;std::pair&lt;std::set&lt;int&gt;::iterator, bool&gt; sp2 = st.insert(4);std::cout &lt;&lt; *sp2.first &lt;&lt; " " &lt;&lt; sp2.second &lt;&lt; std::endl; 运行结果如下： 124 14 0 如果我们不关心插入的元素是什么，只想知道此次插入操作是否成功，可以利用 std::tie 和 std::ignore 来实现： 12345678std::set&lt;int&gt; st;bool inserted = false;std::tie(std::ignore, inserted) = st.insert(4);std::cout &lt;&lt; inserted &lt;&lt; std::endl;std::tie(std::ignore, inserted) = st.insert(4);std::cout &lt;&lt; inserted &lt;&lt; std::endl; 运行结果如下： 1210 总结 std::tuple 是 std::pair 一种更加通用的实现，std::pair 只能包含两个元素，而 std::tuple 可以包含多个任意类型的元素 tie 本意是系牢、约束、连接、束缚的意思，用在 std::tuple 上却是用来解绑参数的，含义恰好相反了，很有趣是不是 实际上 std::tie 这个函数的作用是把一些左值绑定到 std::tuple 来达到解析参数的目的，函数作用还是 “tie” std::ignore 可以用在 std::tie 函数中作为占位符，用来替代一些不关心的参数 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有些事情反过来想一想，问题可能很快就解决了——记一次拼图游戏中一个对手的高谈阔论 2021-8-15 23:48:37]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>tie</tag>
        <tag>动态规划</tag>
        <tag>DP</tag>
        <tag>tuple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中使用std::sort自定义排序规则时要注意的崩溃问题]]></title>
    <url>%2Fblog%2F2021%2F08%2F07%2FC-%E4%B8%AD%E4%BD%BF%E7%94%A8std-sort%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F%E8%A7%84%E5%88%99%E6%97%B6%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%B4%A9%E6%BA%83%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言看到这个标题应该会有很多人一下子就懂了，也会有些人感到迷惑，简简单单排序怎么会奔溃呢？我第一次接触这个问题还是很久以前刚刚参加工作的时候，当时也是写出了导致程序崩溃的代码，通过上网查询解决了问题，至此以后就对这个 sort 函数警惕了一些，一直记得就是在sort的自定义函数中判断条件不要加等号，至于本质的原因一直没有去探究，正好最近又改了一个相关的问题，所以决定从源码和定义的角度来看看为什么会出现这个问题。 sort的使用sort函数真的挺好用，比如像下面这样： 12345678910111213#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;int main()&#123; std::vector&lt;int&gt; values&#123;3, 5, 4, 4, 5, 1&#125;; std::sort(values.begin(), values.end()); for (auto v : values) std::cout &lt;&lt; v &lt;&lt; std::endl; return 0;&#125; 只是 std::sort(values.begin(), values.end()); 这样简简单单的一句就完成了vector数据从小到达的排序，运行结果如下： 12345678albert@home-pc:/data/cpp$ g++ testsort.cpp --std=c++11albert@home-pc:/data/cpp$ ./a.out134455 自定义比较函数上面举的例子是从小到大排序，这是 sort 函数的默认行为，所以不需要额外的参数，如果是想从大到小排序，那么就需要定义一个比较函数了，方法也比较简单，写一个lambda表达式就可以了，比如像下面这样： 1234567891011int main()&#123; std::vector&lt;int&gt; values&#123;3, 5, 4, 4, 5, 1&#125;; std::sort(values.begin(), values.end(), [](int v1, int v2)&#123; return v1 &gt;= v2; &#125;); for (auto v : values) std::cout &lt;&lt; v &lt;&lt; std::endl; return 0;&#125; 按照比较函数定义，我们把数据按照前面大于等于后面的方式排序就完成了从大到小的排序的要求，看看这样写有没有什么问题？如果这里的等号 = 已经引起了你的不适，说明你可能踩过这里的坑，是的，这样写容易造成崩溃，我们来运行一下。 12345678albert@home-pc:/data/cpp$ g++ testsort.cpp --std=c++11albert@home-pc:/data/cpp$ ./a.out554431 咦？怎么没事，我之前用MSVC测试还会崩溃的，难道和编译器有关？ 当我们增大数据量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899std::vector&lt;int&gt; values&#123;3,5,4,4,1,5,4,5,4,5,4,5,4,5,4,5,4,4,5,4,4,4,5,4,5,5,4,5,4,4,5,4,5,4,5,5,5&#125;;// 运行结果如下albert@home-pc:/data/cpp$ g++ testsort.cpp --std=c++11 -galbert@home-pc:/data/cpp$ ./a.out0555555555555555544444444444444444431*** Error in `./a.out': double free or corruption (out): 0x0000000002016c20 ***======= Backtrace: =========/lib/x86_64-linux-gnu/libc.so.6(+0x777f5)[0x7ff5ffef77f5]/lib/x86_64-linux-gnu/libc.so.6(+0x8038a)[0x7ff5fff0038a]/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7ff5fff0458c]./a.out[0x4024e2]./a.out[0x4023ab]./a.out[0x402226]./a.out[0x4020a1]./a.out[0x401edb]./a.out[0x400c67]/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7ff5ffea0840]./a.out[0x400a39]======= Memory map: ========00400000-00403000 r-xp 00000000 00:00 212044 /mnt/d/data/cpp/testsort/a.out00403000-00404000 r-xp 00003000 00:00 212044 /mnt/d/data/cpp/testsort/a.out00603000-00604000 r--p 00003000 00:00 212044 /mnt/d/data/cpp/testsort/a.out00604000-00605000 rw-p 00004000 00:00 212044 /mnt/d/data/cpp/testsort/a.out02005000-02037000 rw-p 00000000 00:00 0 [heap]7ff5f8000000-7ff5f8021000 rw-p 00000000 00:00 07ff5f8021000-7ff5fc000000 ---p 00000000 00:00 07ff5ffb70000-7ff5ffc78000 r-xp 00000000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffc78000-7ff5ffc7a000 ---p 00108000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffc7a000-7ff5ffe77000 ---p 0010a000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffe77000-7ff5ffe78000 r--p 00107000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffe78000-7ff5ffe79000 rw-p 00108000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffe80000-7ff600040000 r-xp 00000000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600040000-7ff600049000 ---p 001c0000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600049000-7ff600240000 ---p 001c9000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600240000-7ff600244000 r--p 001c0000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600244000-7ff600246000 rw-p 001c4000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600246000-7ff60024a000 rw-p 00000000 00:00 07ff600250000-7ff600266000 r-xp 00000000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17ff600266000-7ff600465000 ---p 00016000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17ff600465000-7ff600466000 rw-p 00015000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17ff600470000-7ff6005e2000 r-xp 00000000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6005e2000-7ff6005ef000 ---p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6005ef000-7ff6007e2000 ---p 0017f000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6007e2000-7ff6007ec000 r--p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6007ec000-7ff6007ee000 rw-p 0017c000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6007ee000-7ff6007f2000 rw-p 00000000 00:00 07ff600800000-7ff600825000 r-xp 00000000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7ff600825000-7ff600826000 r-xp 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7ff600a25000-7ff600a26000 r--p 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7ff600a26000-7ff600a27000 rw-p 00026000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7ff600a27000-7ff600a28000 rw-p 00000000 00:00 07ff600b70000-7ff600b71000 rw-p 00000000 00:00 07ff600b80000-7ff600b82000 rw-p 00000000 00:00 07ff600b90000-7ff600b91000 rw-p 00000000 00:00 07ff600ba0000-7ff600ba1000 rw-p 00000000 00:00 07ff600bb0000-7ff600bb1000 rw-p 00000000 00:00 07ff600bc0000-7ff600bc1000 rw-p 00000000 00:00 07fffc026e000-7fffc0a6e000 rw-p 00000000 00:00 0 [stack]7fffc10b8000-7fffc10b9000 r-xp 00000000 00:00 0 [vdso]Aborted (core dumped) 这次终于崩溃了，但显示确实内存越界问题，并且排序后第一个元素是0，这不是我们vector中的元素啊，看来肯定是出问题了 反复尝试几次又找到一个测试用例： 1std::vector&lt;int&gt;values&#123;3,5,4,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5&#125;; 运行之后直接得到了 Segmentation fault (core dumped) 错误，没错，这就是我想要的，来从 sort 源码中看看为什么加了 = 就会出现崩溃 sort源码崩溃分析sort 函数的源码还不算太长，我就一点点来看了 1234567891011121314template&lt;typename _RandomAccessIterator, typename _Compare&gt; inline void sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) &#123; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt; _RandomAccessIterator&gt;) __glibcxx_function_requires(_BinaryPredicateConcept&lt;_Compare, typename iterator_traits&lt;_RandomAccessIterator&gt;::value_type, typename iterator_traits&lt;_RandomAccessIterator&gt;::value_type&gt;) __glibcxx_requires_valid_range(__first, __last); std::__sort(__first, __last, __gnu_cxx::__ops::__iter_comp_iter(__comp)); 这算是个入口函数，做了一些类型检查，然后就调用了内部的 std::__sort 函数 12345678910111213template&lt;typename _RandomAccessIterator, typename _Compare&gt; inline void __sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) &#123; if (__first != __last) &#123; std::__introsort_loop(__first, __last, std::__lg(__last - __first) * 2, __comp); std::__final_insertion_sort(__first, __last, __comp); &#125; &#125; 当排序范围不为空时，函数会对传入的范围进行排序，为了最大程度的提高效率，结合了快排、堆排和插入排序等多种排序方法，分为 std::__introsort_loop 和 std::__final_insertion_sort 两个阶段。 第一阶段使用“快排+堆排”的方法，当元素个数小于等于 _S_threshold（enum { _S_threshold = 16 }）时，不做处理，交给第二阶段来做，对于元素个数大于_S_threshold的序列，执行快排，当快排的递归深入到一定深度 __depth_limit（通过元素个数计算出来的）时，不再递归深入，对待排序元素执行堆排序，代码如下： 123456789101112131415161718192021/// This is a helper function for the sort routine.template&lt;typename _RandomAccessIterator, typename _Size, typename _Compare&gt; void __introsort_loop(_RandomAccessIterator __first, _RandomAccessIterator __last, _Size __depth_limit, _Compare __comp) &#123; while (__last - __first &gt; int(_S_threshold)) &#123; if (__depth_limit == 0) &#123; std::__partial_sort(__first, __last, __last, __comp); return; &#125; --__depth_limit; _RandomAccessIterator __cut = std::__unguarded_partition_pivot(__first, __last, __comp); std::__introsort_loop(__cut, __last, __depth_limit, __comp); __last = __cut; &#125; &#125; 第二阶段使用“插入排序”，当元素个数小于等于 _S_threshold（enum { _S_threshold = 16 }）时，执行普通的插入排序，当大于 _S_threshold 时，执行两次的“插入”排序操作，首先使用普通的插入排序来排 [first, _S_threshold) 这个范围的元素，然后使用无保护的插入排序，完成 [_S_threshold, last) 这个范围的排序。 1234567891011121314template&lt;typename _RandomAccessIterator, typename _Compare&gt; void __final_insertion_sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) &#123; if (__last - __first &gt; int(_S_threshold)) &#123; std::__insertion_sort(__first, __first + int(_S_threshold), __comp); std::__unguarded_insertion_sort(__first + int(_S_threshold), __last, __comp); &#125; else std::__insertion_sort(__first, __last, __comp); &#125; 其中的普通插入排序没有什么特别的地方，就是遍历前边小于等于_S_threshold个元素进行普通的插入排序，而后面这个无保护的插入排序 std::__unguarded_insertion_sort 往往就是出现问题的地方，代码如下： 123456789101112131415161718192021222324252627template&lt;typename _RandomAccessIterator, typename _Compare&gt; inline void __unguarded_insertion_sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) &#123; for (_RandomAccessIterator __i = __first; __i != __last; ++__i) std::__unguarded_linear_insert(__i, __gnu_cxx::__ops::__val_comp_iter(__comp)); &#125;template&lt;typename _RandomAccessIterator, typename _Compare&gt; void __unguarded_linear_insert(_RandomAccessIterator __last, _Compare __comp) &#123; typename iterator_traits&lt;_RandomAccessIterator&gt;::value_type __val = _GLIBCXX_MOVE(*__last); _RandomAccessIterator __next = __last; --__next; while (__comp(__val, __next)) &#123; *__last = _GLIBCXX_MOVE(*__next); __last = __next; --__next; &#125; *__last = _GLIBCXX_MOVE(__val); &#125; 这段代码看 __unguarded_insertion_sort 还没有什么问题，但是 __unguarded_linear_insert 中的逻辑就比较迷幻了，只有当 __comp(__val, __next) 的值为false时才会停止。 其中 __comp 就是我们之前自定义的lambda表达式，我们当时写的是 return v1 &gt;= v2;，翻译过来也就是当!(val &gt;= __next) 时，即后一个元素小于前一个元素的时候停止，那么为什么会出问题呢？ 我们知道前_S_threshold个元素我们之前已经按照从大到小排好序了，那么按道理遍历到这个区域就会找到后一个元素小于前一个元素的情况，也就是插入排序遍历到这就会停止，等等！好像有什么不对劲，如果这里的元素都相等就找不到停止的情况了，这就会造成访问的越界，这就是程序崩溃的本质原因了。 那么去掉等号会是个什么情况呢？运行到这里就是要找到满足条件的 !(val &gt; __next)元素时停止，也就是找到后一个元素小于等于前一个元素的时候停止，因为前_S_threshold个元素已经排好序，这个条件是肯定满足的，所以不会出现越界情况，这就是为什么自定义比较函数中，两个元素相等时一定要返回false了。 为什么使用无保护的插入排序既然这里这么容易越界，为什么不判断一下边界条件来防止越界，而是用这种无保护的插入排序呢？ 这里使用无保护的插入排序原因很简单，就是为了提升效率，因为省略掉越界的检查，少了很多次的比较操作，效率肯定有了提升，它的前提是左边必须有已经排好序的元素，所以在函数 __unguarded_insertion_sort 函数之前先调用 __insertion_sort 来完成了[0, _S_threshold) 这个范围的元素排序，便是为了后面这个无保护插入排序的使用。 C++标准要求说到这里sort函数的自定义比较函数还是太容易出错了，有没有什么实现标准呢？其实标准中对这个比较函数的要求写的很详细，具体可以参考 Compare的实现要求。 Compare 是一些标准库函数针对用户提供的函数对象类型所期待的一组要求，其实就是要满足严格若排序关系，翻译成人话就是自定义的比较函数 comp 需要下面三条要求： 对于任意元素a，需满足 comp(a, a) == true 对于任意两个元素a和b，若 comp(a, b)==true 则要满足 comp(b, a)==false 对于任意三个元素a、b和c，若 comp(a, b)==true 且 comp(b, c)==true 则需要满足 comp(a, c)==true 从这条规则也能看出我们之前定义的问题： 123std::sort(values.begin(), values.end(), [](int v1, int v2)&#123; return v1 &gt;= v2;&#125;); 这个自定义的比较函数，当 v1 和 v2 相等时，comp(v1, v2)==true， 但是 comp(v2, v1)的值也是 true，当我们把代码中的等号 = 去掉以后，也就满足了条件2，另外在复杂的比价逻辑中，条件3的传递性问题也是需要注意的问题。 构造一个崩溃的示例理解了前面崩溃的原因，我们就不需要猜了，可以直接构造一个百分之百奔溃的测试用例，因为前16（_S_threshold）个元素会使用正常的插入排序，后面的元素才会使用无保护的插入排序，我们其实构造一个17个相同元素的vector就可以了，下面我们来试一下： 1234567891011int main()&#123; std::vector&lt;int&gt; values&#123;6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6&#125;; std::sort(values.begin(), values.end(), [](int v1, int v2)&#123; return v1 &gt;= v2; &#125;); for (auto v : values) std::cout &lt;&lt; v &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374albert@home-pc:/data/cpp$ g++ testsort.cpp --std=c++11 -galbert@home-pc:/data/cpp$ ./a.out06666666666666666*** Error in `./a.out': double free or corruption (out): 0x0000000001fd9c20 ***======= Backtrace: =========/lib/x86_64-linux-gnu/libc.so.6(+0x777f5)[0x7feaf8ef77f5]/lib/x86_64-linux-gnu/libc.so.6(+0x8038a)[0x7feaf8f0038a]/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7feaf8f0458c]./a.out[0x402446]./a.out[0x40230f]./a.out[0x40218a]./a.out[0x402005]./a.out[0x401e65]./a.out[0x400bf1]/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7feaf8ea0840]./a.out[0x4009e9]======= Memory map: ========00400000-00403000 r-xp 00000000 00:00 211636 /mnt/d/data/cpp/testsort/a.out00403000-00404000 r-xp 00003000 00:00 211636 /mnt/d/data/cpp/testsort/a.out00603000-00604000 r--p 00003000 00:00 211636 /mnt/d/data/cpp/testsort/a.out00604000-00605000 rw-p 00004000 00:00 211636 /mnt/d/data/cpp/testsort/a.out01fc8000-01ffa000 rw-p 00000000 00:00 0 [heap]7feaf4000000-7feaf4021000 rw-p 00000000 00:00 07feaf4021000-7feaf8000000 ---p 00000000 00:00 07feaf8b70000-7feaf8c78000 r-xp 00000000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8c78000-7feaf8c7a000 ---p 00108000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8c7a000-7feaf8e77000 ---p 0010a000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8e77000-7feaf8e78000 r--p 00107000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8e78000-7feaf8e79000 rw-p 00108000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8e80000-7feaf9040000 r-xp 00000000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9040000-7feaf9049000 ---p 001c0000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9049000-7feaf9240000 ---p 001c9000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9240000-7feaf9244000 r--p 001c0000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9244000-7feaf9246000 rw-p 001c4000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9246000-7feaf924a000 rw-p 00000000 00:00 07feaf9250000-7feaf9266000 r-xp 00000000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17feaf9266000-7feaf9465000 ---p 00016000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17feaf9465000-7feaf9466000 rw-p 00015000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17feaf9470000-7feaf95e2000 r-xp 00000000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf95e2000-7feaf95ef000 ---p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf95ef000-7feaf97e2000 ---p 0017f000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf97e2000-7feaf97ec000 r--p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf97ec000-7feaf97ee000 rw-p 0017c000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf97ee000-7feaf97f2000 rw-p 00000000 00:00 07feaf9800000-7feaf9825000 r-xp 00000000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7feaf9825000-7feaf9826000 r-xp 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7feaf9a25000-7feaf9a26000 r--p 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7feaf9a26000-7feaf9a27000 rw-p 00026000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7feaf9a27000-7feaf9a28000 rw-p 00000000 00:00 07feaf9bc0000-7feaf9bc1000 rw-p 00000000 00:00 07feaf9bd0000-7feaf9bd2000 rw-p 00000000 00:00 07feaf9be0000-7feaf9be1000 rw-p 00000000 00:00 07feaf9bf0000-7feaf9bf1000 rw-p 00000000 00:00 07feaf9c00000-7feaf9c01000 rw-p 00000000 00:00 07feaf9c10000-7feaf9c11000 rw-p 00000000 00:00 07ffffb85e000-7ffffc05e000 rw-p 00000000 00:00 0 [stack]7ffffc61d000-7ffffc61e000 r-xp 00000000 00:00 0 [vdso]Aborted (core dumped) 完全符合预期，如果再删除vector中的一个元素就不会崩溃了。 平台差异这篇文章的代码编译和运行都是在Linux下完成的，但是我之前在Windows上测试时，可不需要最少17个元素的前提，这是为什么呢？因为在微软这一套编译环境下，直接检测了Compare中的条件2，并且是以断言的方式给出提示的，所以与Linux上的运行表现还有一些差异。 总结 使用 std::sort 函数自定义比较函数时，需要满足严格弱排序性，若 comp(a, b)==true 则 comp(b, a)==false，那么在比较函数中两个元素相等的情况要返回false 使用 std::sort 函数出现崩溃是往往是不满足严格若排序性，但是在复杂的比较函数中也可能不满足传递性 std::sort 为了把排序效率提高到极致，综合使用了快排、堆排、插入排序等多种排序方法 std::sort 在不同的平台实现不同，当比较函数不满足严格若排序时，gcc环境下至少有17个元素才会崩溃，而 MSVC 则在Debug时没有元素个数限制，会通过断言直接判断这个条件是否满足 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 可是啊 总有那风吹不散的认真 总有大雨也不能抹去的泪痕~ 2021-8-8 23:57:53]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>sort</tag>
        <tag>lambda</tag>
        <tag>crash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用box2dweb做一个下落的小球，宝宝玩的不亦乐乎]]></title>
    <url>%2Fblog%2F2021%2F07%2F21%2F%E4%BD%BF%E7%94%A8box2dweb%E5%81%9A%E4%B8%80%E4%B8%AA%E4%B8%8B%E8%90%BD%E7%9A%84%E5%B0%8F%E7%90%83%EF%BC%8C%E5%AE%9D%E5%AE%9D%E7%8E%A9%E7%9A%84%E4%B8%8D%E4%BA%A6%E4%B9%90%E4%B9%8E%2F</url>
    <content type="text"><![CDATA[前言红球球~ 蓝球球~ 家里的宝宝就喜欢玩球球，特别是蓝色的，而最近正好找到了一个优秀的物理引擎，就拿它来做了一个下落的小球，宝宝玩的不亦乐乎~ 起初我是想模拟一个小球在不同曲线上的运动状态，一直在找一个简易的重力引擎，找了一段时间没有什么收获，也想过自己写个重力系统，可是仔细想想觉得有点太麻烦，后来玩了一些flash游戏，发现这类依靠重力的作为主要玩法的游戏还真不少，肯定有一个好的引擎可以直接拿来用，然后就发现了 Box2D 这个物理引擎，不但带有重力系统，还支持碰撞。 box2dweb其实 cocos2dx 和 unity 中都有使用 Box2D 引擎，只不过自己进行了封装，如果使用这些客户端引擎做出来的东西依赖性比较强，还好顺着 Box2D 这条线发现了一个 box2dweb 引擎，是一个 Box2D 的 js 版本，这就非常方便了，有个浏览器就可以直接运行了，还免去了编译安装的麻烦。 下落的小球开局一张图，后面接着编。。。 把这个下落的小球叫做一个游戏确实有些勉强，但是宝宝就是很喜欢玩啊，游戏本来就是让人开心的，开心就好咯！接下来记录一下这个小球是怎么实现的吧~ 具体实现下载引擎box2dweb 引擎只有一个js文件，可以在github上搜索下载 hecht-software/box2dweb ，不过最近访问缓慢，也可以下载这个我备份的版本 Box2dWeb-2.1.a.3.min.js，下载之后直接引用即可。 引入js引擎新建一个 html 文件命名为 fallball-game.html，编写如下内容，引入 Box2dWeb-2.1.a.3.min.js 文件 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;fall ball game&lt;/title&gt; &lt;script type="text/javascript" src="./Box2dWeb-2.1.a.3.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="./fallball.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;canvas id="mycanvas" width="1600" height="900"&gt;&lt;/canvas&gt;&lt;/body&gt;&lt;/html&gt; 编写小球逻辑在上面的 html 文件中不仅引入了 Box2dWeb-2.1.a.3.min.js 这个物理引擎，还引入了一个 fallball.js 文件，这是一个新建的自定义的js文件，需要我们在这个文件中编写小球的逻辑。 引入常用类型并简写1234567891011121314var b2Vec2 = Box2D.Common.Math.b2Vec2, b2AABB = Box2D.Collision.b2AABB, b2BodyDef = Box2D.Dynamics.b2BodyDef, b2Body = Box2D.Dynamics.b2Body, b2FixtureDef = Box2D.Dynamics.b2FixtureDef, b2Fixture = Box2D.Dynamics.b2Fixture, b2World = Box2D.Dynamics.b2World, b2MassData = Box2D.Collision.Shapes.b2MassData, b2PolygonShape = Box2D.Collision.Shapes.b2PolygonShape, b2CircleShape = Box2D.Collision.Shapes.b2CircleShape, b2DebugDraw = Box2D.Dynamics.b2DebugDraw, b2MouseJointDef = Box2D.Dynamics.Joints.b2MouseJointDef, b2RayCastInput = Box2D.Collision.b2RayCastInput, b2RayCastOutput = Box2D.Collision.b2RayCastOutput 以上这些都是 Box2D 引擎中常用的类，使用简写的变量来引用这些类，这样在后面使用时会方便很多。 创建Box2D世界b2World 是 Box2D 系统模拟物理世界的核心，可以想象成我们生活中的地球，在地球上有各种各样的物理环境，比如从空中自由落体的小球，在水中航行的轮船等等，使用 b2World 就可以创建这样一个世界。 在物理世界中首先需要的是重力，那么先定义一个有方向的力： 1var gravity = new b2Vec2(0, 9.8); 作为一个模拟物理环境的引擎，效率使我们需要考虑的问题，对于静止不动的对象，最好不进行模拟计算来节省CPU运算，这种静止的对象可以让他们在 Box2D 环境中睡觉。12var doSleep = true; 需要的参数都准备好了，这样可以new出一个Box2D世界了。 1var world = new b2World(gravity, doSleep); 创建一个小球 先创建一个物体 12345var bodydef = new b2BodyDef();// 物体类型定义，基本上常用的有两种定义：b2_staticBody 静态物体; b2_dynimacBod动态物体bodydef.type= b2Body.b2_staticBody;// 定义物体位置bodydef.position.Set(0, 0); 再创建一个材质 1234var fixDef = new b2FixtureDef();fixDef.density = 1.0; // desity 密度，如果密度为0或者null，该物体则为一个静止对象fixDef.friction = 0.5; //摩擦力（0~1）fixDef.restitution = 0.2;// 弹性（0~1） 为材质添加一个形状并创建小球 12345678fixDef.shape = new b2CircleShape(0.5);bodyDef.type = b2Body.b2_dynamicBody;bodyDef.position.Set(3, 0);var bomb = world.CreateBody(bodyDef);bomb.userData = "iambomb";bomb.CreateFixture(fixDef) b2PolygonShape为多边形，设置形状大小时对应着SetAsBox(halfWidth, halfHeight)函数，参数半长和半宽，如果自定义多边形可以使用一个SetAsArray(vertexArray,vertexCount)，其中vertexArray为顶点矢量（b2Vec2）数组，vertexCount为顶点数，最多8个 b2CircleSharp为圆形，对应的设置属性为半径，函数为SetRadius(radius) 需要注意的是Box2d中的单位是米，1米是30像素，在绘制材质图片时需注意单位换算 使用 bomb.ApplyForce(force, point); 可以添加一个外力，force是一个b2Vec2的向量代表外力，point一个b2Vec2的向量代表物体的着力点。使用 bomb.SetMassFromShapes(); 可以根据形状计算质量 创建背景刚体物理12345678910111213141516171819202122232425var vertices = [ new b2Vec2(0, 0), new b2Vec2(20, 10), new b2Vec2(0, 10),]fixDef.shape = new b2PolygonShape.AsArray(vertices, 3);//fixDef.shape = new b2PolygonShape();//fixDef.shape.SetAsBox(30, 1);bodyDef.type = b2Body.b2_staticBody;bodyDef.position.Set(0, 0);world.CreateBody(bodyDef).CreateFixture(fixDef);var vertices2 = [ new b2Vec2(19, 10), new b2Vec2(40, 0), new b2Vec2(40, 10),]fixDef.shape = new b2PolygonShape.AsArray(vertices2, 3);bodyDef.type = b2Body.b2_staticBody;bodyDef.position.Set(0, 5);world.CreateBody(bodyDef).CreateFixture(fixDef); 参照小球创建的方式创建两个三角形物体，这里注意一个问题，三角形的坐标需要顺时针给出，否则两个刚体的碰撞会出现问题。 创建调试环境实际上，Box2D只是集成了各种算法，对b2Body对象进行物理模拟计算，并将计算结果存储到这个对象中，但是它并不是DisplayObject的子类，也就意味着我们无法通过addChild()将它添加到舞台上，这时可以借助b2DebugDraw类，绑定一个Sprite对象，Box2D就能帮我们在这个Sprite中，用绘图API绘制出模拟图形，方便我们进行调试。 12345678var debugDraw = new b2DebugDraw();debugDraw.SetSprite(document.getElementById("mycanvas").getContext("2d"));debugDraw.SetDrawScale(30.0);debugDraw.SetFillAlpha(0.5);debugDraw.SetLineThickness(1.0);debugDraw.SetFlags( b2DebugDraw.e_shapeBit | b2DebugDraw.e_jointBit | b2DebugDraw.e_controllerBit | b2DebugDraw.e_pairBit);world.SetDebugDraw(debugDraw); 世界更新12345678910111213141516171819function update () &#123; world.Step(1 / 60, //frame-rate 10, //velocity iterations 10); //position iterations world.DrawDebugData(); for (var b = world.m_bodyList; b != null; b = b.m_next) &#123; if (b.GetUserData()) &#123; context.save(); context.translate(b.GetPosition().x * 30, b.GetPosition().y * 30); context.rotate(b.GetAngle()); context.drawImage(b.GetUserData(), -b.GetUserData().width / 2, -b.GetUserData().height / 2); context.restore(); &#125; &#125; world.ClearForces();&#125;setInterval(update, 1000 / 60); 定义更新函数，注册定时器，用来定时更新。物理引擎的 world.Step 函数是整个 Box2D 引擎的核心, 它驱动了物理世界的运行。而上述代码中 for 循环的部分处理了材质跟随刚体运动的逻辑，可以自定义显示图片，这个已经实验成功，改天可以接着写一写。 总结 Box2D 是一个优秀易用的物理引擎，而 box2dweb 是一个js版本，可以很方便的在html引入使用 box2dweb 不仅能模拟重力环境，还能模拟物体之间的碰撞，可以通过参数调整物体的密度、摩擦力和弹力 物体的形状可以多边形，可以通过给出顶点坐标的形式设置，不过要注意按照顺时针方向给出，否则碰撞失败，具体原因还不太清楚 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 这个世界上，还有很多不被认可的梦想，不被祝福的感情，不被眷顾的孩子，他们不曾犯错，却只能颤颤巍巍，单薄地行走在路上。你我都一样 要承认 我们都很平凡 并且在负重~ 2021-7-25 18:59:01]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>Game</tag>
        <tag>JS</tag>
        <tag>物理引擎</tag>
        <tag>box2dweb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++中的数据类型转换static_cast/dynamic_cast/const_cast/reinterpret_cast]]></title>
    <url>%2Fblog%2F2021%2F07%2F10%2FC-C-%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[前言C/C++属于静态语言，也就是在编译时变量的数据类型即可确定的强类型语言。当不同的数据类型在一起运算或者相互赋值的时候，就需要进行数据类型转换。不同数据类型占用的内存空间不同，而各种数据类型的转换时有规则的，一种通用的规则就是“小转大”自动进行，“大转小”需要强制执行。这里的“大”和“小”指的是数据范围。 为什么会有数据范围大小的区别呢？这就和饭店里的盘子一样，不同的菜肴通常使用不同的盘子，盘子有大有小，如果把小盘子里的菜装到大盘子里没有问题，但是把大盘子里的菜放到小盘子中就会溢出来，假设都使用大盘子就不会产生溢出的问题，但是这样会产生空间的浪费。而C/C++中不同类型的变量占用的内存空间不同与这些盘子非常相似，当范围小的变量赋值给范围大的变量时没有问题，但是反过来也会出现溢出。 数据类型自动转换当不同类型的变量同时运算时就会发生数据类型的自动转换，以常见的 char、short、int、long、float、double 这些类型为例，如果 char 和 int 两个类型的变量相加时，就会把 char 先转换成 int 再进行加法运算，如果是 int 和 double 类型的变量相乘就会把 int 转换成 double 再进行运算。 自动转换的行为如下图所示，方向是从左往右自动进行： 123456graph LR A((char))--&gt;C(unsigned/int) B((short))--&gt;C(unsigned/int) C(unsigned/int)--&gt;D(unsigned/long) D(unsigned/long)--&gt;F[double] E(float)--&gt;F[double] C语言中的强制类型转换前面说了自动转换，从这里开始聊聊强制类型转换，需要强制类型转换往往程序不那么智能了，需要人工进行干预。比如把一个int 类型的变量赋值给 char 类型的变量，或者说把两个 int 相乘时可能会得到一个很大的数，所以需要先把 int 强制转换成 double 计算防止溢出。 强制类型转换的格式为：(new_type_name) expression，其中 new_type_name 为新类型名称，expression为表达式。例如： 123int val = 65535;char ch = (char)val; 或者 12int m = 2147483647, n = 100;double result = (double)m * n; 无论是自动的类型转换还是强制类型转换，都只是为了本次操作或运算而进行的临时转换，转换的结果也会保存到临时的内存空间内，不会改变数据本来的类型或者具体的值。 有些强制类型转换是对原有数据的重新解释，比如： 12345void test(void* p)&#123; char* buffer = (char*)p; // ...&#125; void* 类型的变量p，经过强制类型转换以后变成了char类型的指针，此后就可以把这段内存空间当成字符数组来处理了。 C++中的强制类型转换在C++语言中新增了四个用于强制类型转换的关键字，分别是 static_cast、 dynamic_cast, const_cast、 和 reinterpret_cast，使用语法为 xxxx_cast&lt;new_type_name&gt;(expression)。 相比于C语言中使用小括号()来完成强制类型转换，C++中这几个关键字的引入能更清晰的表明它要完成强制类型转换的意图，容易暴露出隐藏的问题。 其实很长一段时间以来，我对于这四种强转方式区分的不是很清晰，其中 const_cast 的功能还比较容易辨别，但是另外3种经常混作一团，所以才有了这篇总结，而仔细学习后才发现，这4种强转关键字的区别就在他们的名字上，下面逐个来看一下。 static_cast这个关键字的作用主要表现在 static 上，是一种静态的转换，在编译期就能确定的转换，可以完成C语言中的强制类型转换中的大部分工作，但需要注意的是，它不能转换掉表达式的 const、volitale 或者 __unaligned 属性。 它主要有以下几种用法： 用于基本数据类型之间的转换，如把int转换成char，把int转换成double等。 123int val = 110119;char c = static_cast&lt;char&gt;(val);double d = static_cast&lt;double&gt;(val); 将表达式转换成void类型，并将转换后的结果丢弃 12int val = 110119;static_cast&lt;void&gt;(val); 可以用于void* 和其他指针类类型之间的转换，但是不能用于两个无关指针类型的直接转换 1234567// 正常转换int *p = new int;void* p1 = static_cast&lt;void*&gt;(p);char* p2 = static_cast&lt;char*&gt;(p1);// 编译失败 //error: invalid static_cast from type ‘int*’ to type ‘char*’char* p3 = static_cast&lt;char*&gt;(p); 可以用于类继承结构中基类和派生类之间指针或引用的转换，向上转型安全，向下转型由于没有动态类型检查，是不安全的。 123456struct B &#123; &#125;;struct D : B &#123; &#125;;D d;B&amp; rb = d;D&amp; rd = static_cast&lt;D&amp;&gt;(rb); 如果涉及左值到右值、数组到指针或函数到指针的转换，也可以通过static_cast显式执行。 1234template&lt;typename _Tp&gt;inline typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp;move(_Tp&amp;&amp; __t)&#123; return static_cast&lt;typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp;&gt;(__t); &#125; dynamic_cast从名字上看，这个关键字与 static_cast 的静态转换是对立的，这是一个“动态”转换函数，只能对指针和引用的进行转换，并且只用于类继承结构中基类和派生类之间指针或引用的转换，可以进行向上、向下，或者横向的转换。 相比于 static_cast 的编译时转换， dynamic_cast 的转换还会在运行时进行类型检查，转换的条件也比较苛刻，必须有继承关系的类之间才能转换，并且在基类中有虚函数才可以，有一种特殊的情况就是可以把类指针转换成 void* 类型。 关于使用中的常见问题，参考以下几种情况： 普通类型的指针无法转换 12345int val = 100;int *p = &amp;val;// 编译失败 //error: cannot dynamic_cast ‘p’ (of type ‘int*’) to type ‘char*’ (target is not pointer or reference to class)char* pc = dynamic_cast&lt;char*&gt;(p); 继承结构中基类里面没有虚函数无法转换 12345678struct B &#123; &#125;;struct D : B &#123; &#125;;D d;B* pb = &amp;d;// 编译失败 //error: cannot dynamic_cast ‘pb’ (of type ‘struct test1()::B*’) to type ‘struct test1()::D*’ (source type is not polymorphic)D* pd = dynamic_cast&lt;D*&gt;(pb) 指针或引用转换的类型不是正确的类型，如果参数类型是指针会返回目标类型空指针，如果参数类型是引用则会抛出 std::bad_cast 异常。 123456789struct B &#123; virtual void test() &#123;&#125; &#125;;struct D : B &#123; &#125;;B d;B* pb = &amp;d;D* pd = dynamic_cast&lt;D*&gt;(pb);// 编译成功，但是pb指针指向的类型是 B，向下转型失败，输出结果是0，也就是空指针std::cout &lt;&lt; pd &lt;&lt; std::endl; 一个正常转换的例子，包含向上、向下、横向转换 1234567891011121314151617181920struct B &#123; virtual void test() &#123;&#125; &#125;;struct D1 : virtual B &#123; &#125;;struct D2 : virtual B &#123; &#125;;struct MD : D1, D2 &#123; &#125;;D1* pd1 = new MD();std::cout &lt;&lt; pd1 &lt;&lt; std::endl;// 向上转型B* pb = dynamic_cast&lt;B*&gt;(pd1);std::cout &lt;&lt; pb &lt;&lt; std::endl;// 向下转型MD* pmd = dynamic_cast&lt;MD*&gt;(pd1);std::cout &lt;&lt; pmd &lt;&lt; std::endl;// 横向转型D2* pd2 = dynamic_cast&lt;D2*&gt;(pd1);std::cout &lt;&lt; pd2 &lt;&lt; std::endl; 运行结果如下，在横向转换时指针发生了变化，可以看出 dynamic_cast 不是简单的数据强转，还进行了指针的偏移： 123456albert@home-pc:/mnt/d/testconvert$ g++ cppconvert.cppalbert@home-pc:/mnt/d/testconvert$ ./a.out0x15c0c400x15c0c400x15c0c400x15c0c48 const_cast在C/C++中，const限定符通常被用来限定变量，用于表示该变量的值不能被修改，这种限定可以避免程序员犯一些初级错误，但同时也造成了一些不便，比如一些已有函数要求非常量指针，但是掉用这些函数的接口函数中都传递了常量指针，这时候就要对指针类型去常量化。 但需要特别注意的是 const_cast 不能去除变量的常量性，只能用来去除指向常数对象的指针或引用的常量性，且去除常量性的对象必须为指针或引用。 常量指针被转化成非常量指针，并且仍然指向原来的对象，常量引用被转换成非常量引用，并且仍然指向原来的对象；常量对象可能被转换成非常量对象。 尝试去除非指针和引用的类型的常量性会编译失败 1234const int i = 6;// 编译错误 //int j = const_cast&lt;int&gt;(i); 企图用一个指针来修改常量： 1234const int val = 6;//编译错误 //error: invalid conversion from ‘const int*’ to ‘int*’ [-fpermissive]int* cp = &amp;val; 修改一个指针的常量性： 12345678910const int val = 6;std::cout &lt;&lt; "&amp;val=" &lt;&lt; &amp;val &lt;&lt; ", val=" &lt;&lt; val &lt;&lt; std::endl;const int* cp = &amp;val;int *p = const_cast&lt;int*&gt;(cp);*p = 2;std::cout &lt;&lt; "&amp;val=" &lt;&lt; &amp;val &lt;&lt; ", val=" &lt;&lt; val &lt;&lt; std::endl;std::cout &lt;&lt; "p=" &lt;&lt; p &lt;&lt; ", *p=" &lt;&lt; *p &lt;&lt; std::endl; 运行结果如下： 123&amp;val=0x7ffff7446bd4, val=6&amp;val=0x7ffff7446bd4, val=6p=0x7ffff7446bd4, *p=2 运行之后，变量 p 指向了变量val地址，并改变了地址所指向的内存数据，但是打印 val 的值并没有发生变化，这是因为 val 作为常量在编译期使用它的地方就进行了替换，接下来再看另一种情况。 1234567891011int init = 6;const int val = init;std::cout &lt;&lt; "&amp;val=" &lt;&lt; &amp;val &lt;&lt; ", val=" &lt;&lt; val &lt;&lt; std::endl;const int* cp = &amp;val;int *p = const_cast&lt;int*&gt;(cp);*p = 2;std::cout &lt;&lt; "&amp;val=" &lt;&lt; &amp;val &lt;&lt; ", val=" &lt;&lt; val &lt;&lt; std::endl;std::cout &lt;&lt; "p=" &lt;&lt; p &lt;&lt; ", *p=" &lt;&lt; *p &lt;&lt; std::endl; 代码逻辑不变，只在开始的位置使用 init 这个变量来代替 6 这个常数，运行结果如下： 123val=0x7fffe8c71fa0, val=6&amp;val=0x7fffe8c71fa0, val=2p=0x7fffe8c71fa0, *p=2 运行之后 val 本身的变化也应用到了使用它的地方，这里的编译器替换已经不起作用了。 实际上，使用const_cast通常是一种无奈之举，利用const_cast去掉指针或引用的常量性并且去修改原始变量的数值，这是一种非常不好的行为，如果可以的话，尽可能在程序设计阶段就规避这种情况。 reinterpret_cast它被用于不同类型指针或引用之间的转换，或者指针和整数之间的转换，是对比特位的简单拷贝并重新解释，因此在使用过程中需要特别谨慎，比如前面提到的一个例子，static_cast 不能将 int* 直接强转成 char*，使用reinterpret_cast就可以办到。 不同基础类型指针类型之间转换： 1234567int *p = new int;// 编译失败 //error: invalid static_cast from type ‘int*’ to type ‘char*’char* p1 = static_cast&lt;char*&gt;(p);// 编译成功char* p2 = reinterpret_cast&lt;char*&gt;(p1); 基础类型指针与类对象指针之间的转换： 123456789struct B &#123; int val;&#125;;B b&#123;100&#125;;std::cout &lt;&lt; "b.val=" &lt;&lt; b.val &lt;&lt; std::endl;int* p = reinterpret_cast&lt;int*&gt;(&amp;b);std::cout &lt;&lt; "*p=" &lt;&lt; *p &lt;&lt; std::endl; 运行之后可以得到 *p 的值为100，也就是重新解释了变量 b 的地址为整型指针。 将地址值转换成整数 123456789struct B &#123; int val;&#125;;B b&#123;101&#125;;std::cout &lt;&lt; "&amp;b=" &lt;&lt; &amp;b &lt;&lt; std::endl;long addr = reinterpret_cast&lt;long&gt;(&amp;b);std::cout &lt;&lt; "addr=" &lt;&lt; addr &lt;&lt; std::endl; 运行结果如下： 12&amp;b=0x7ffffdc4f270addr=140737450930800 这里的地址 0x7ffffdc4f270 被解释成了整数 140737450930800，因为涉及到字节序，这也是很多文章提到的 reinterpret_cast 不具备一致性的问题，我们需要知道这一个点，只要代码不依赖主机字节序就没有问题。 强转关键字的选择好几个关键字，并且有些功能还是重复的，那么究竟该选哪一个呢？这个真得按照经验来选，我建议使用排除法，按照 const_cast -&gt; dynamic_cast -&gt; reinterpret_cast -&gt; static_cast 的顺序带入选择。 先看是不是要去掉指针或引用的常量属性，如果是只能选择 const_cast 再看转换的是不是继承体系下的多态结构，如果是这种结构下的指针和引用的转换最好使用 dynamic_cast 接着看是不是偏底层的代码，需要将无关类型指针进行转换，或者指针与整数之间进行转换，如果是则选择 reinterpret_cast 前三种情况都不满足，那就只能使用 static_cast 了 总结 C/C++中不同数据类型进行运算或者赋值的时候会发生数据转换，这种转换有些是自动进行的，有些需要进行显示的强制类型转换 在C语言中强制类型转换写成(new_type_name) expression的形式，new_type_name 是要转换的目标类型，expression 是待转换的表达式 在C++中强制类型转换通过更明显的关键字来完成，分别是static_cast、 dynamic_cast, const_cast、 和 reinterpret_cast static_cast 是静态转换，在编译期完成完成转换，与C语言中的强制类型转换重合度最高 dynamic_cast 是动态转换，在运行时转换会进行检查，必须用在有继承关系的多态结构中 const_cast 是常量转换，用于取出指针或引用的常量属性，但是尽量通过设计杜绝它的使用场景 reinterpret_cast 是一种内存数据的重新解释，比较原始，开发者使用它的时候应该明确的知道自己在做什么 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 今夜的雨，好美~ 2021-7-12 00:18:13]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>数据转换</tag>
        <tag>自动</tag>
        <tag>强制类型转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[换个角度来看看C++中的左值、右值、左值引用、右值引用]]></title>
    <url>%2Fblog%2F2021%2F07%2F04%2F%E6%8D%A2%E4%B8%AA%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%9C%8B%E7%9C%8BC-%E4%B8%AD%E7%9A%84%E5%B7%A6%E5%80%BC%E3%80%81%E5%8F%B3%E5%80%BC%E3%80%81%E5%B7%A6%E5%80%BC%E5%BC%95%E7%94%A8%E3%80%81%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言对于左值和右值有一个不太严谨的定义——在赋值表达式 = 左侧是的左值，而在 = 右侧的是右值。通过不断学习和尝试，最近我发现一个新的说法更加贴切，那就是“左值是容器，右值是东西”。对于这个定义我们可以类比一下水杯和水，通过水杯可以操作水杯中的水，操作过程中的中间结果如果想要进一步操作，可以将其放入其他的水杯，如果没有水杯就无法找到曾经操作过的水了，也就无法继续操作了。 123int a = 2;int b = 6;int c = a + b; 在这个例子中，变量 a，b, c 都是水杯，而 2、6、a + b 都是被用来操作的水，只有把这些“水”放到“水杯”中才能被找到，才可以进行下一步操作。 关于左值、右值、左值引用和右值引用的概念可以看看之前的总结： 简单聊聊C/C++中的左值和右值 C++11在左值引用的基础上增加右值引用 虽然温故不一定知新，但绝对可以增强记忆，参照着之前的理解，今天来换一种窥探本质的方式。 汇编代码初探为了熟悉一下汇编代码，我们先写个简单的例子，内容就是上述提到的那一段，新建一个文件 main.cpp，然后编写如下代码： 12345678int main()&#123; int a = 6; int b = 2; int c = a + b; return 0;&#125; 运行 g++ main.cpp --std=c++11 -S -o main.s 编译这段代码，生成汇编文件 main.s，打开文件内容如下： 123456789101112131415161718192021222324252627 .file "main.cpp" .text .globl main .type main, @functionmain:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl $6, -12(%rbp) movl $2, -8(%rbp) movl -12(%rbp), %edx movl -8(%rbp), %eax addl %edx, %eax movl %eax, -4(%rbp) movl $0, %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size main, .-main .ident "GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609" .section .note.GNU-stack,"",@progbits 其中代表定义变量和做加法的语句转换成汇编代码如下： 123456movl $6, -12(%rbp) // 把立即数6放到内存地址为-12(%rbp)的位置，也就是变量a中movl $2, -8(%rbp) // 把立即数2放到内存地址为-8(%rbp)的位置，也就是变量b中movl -12(%rbp), %edx // 把内存地址为-12(%rbp)的位置（变量a）的数据放到寄存器%edx中movl -8(%rbp), %eax // 把内存地址为-8(%rbp)的位置（变量b）的数据放到寄存器%eax中addl %edx, %eax // 把寄存器%edx中的数据加到寄存器%eax中movl %eax, -4(%rbp) // 把寄存器%eax中的计算所得结果数据放到内存地址为-4(%rbp)的位置，也就是变量c中 指针变量首先来看看通过指针来修改变量值的过程，测试代码如下： 123int a = 6;int* p = &amp;a;*p = 2; 转换成汇编代码如下： 12345movl $6, -20(%rbp) // 把立即数6放到内存地址为-20(%rbp)的位置，也就是变量a中leaq -20(%rbp), %rax // 把这个内存地址-20(%rbp)，也就是变量a的地址保存在寄存器%rax中movq %rax, -16(%rbp) // 把寄存器%rax中的保存的变量a的地址，放到内存地址为-16(%rbp)的位置，也就是变量p中movq -16(%rbp), %rax // 把内存地址为-16(%rbp)的位置（变量p）的数据放到寄存器%rax中movl $2, (%rax) // 把立即数2放在寄存器%rax中保存的地址位置中，也就是p所指向的地址，即变量a中 通过汇编代码可以发现，通过指针修改变量的值实际上是在指针变量中保存变量的地址值，修改变量时是通过指针变量直接找到变量所在内存，然后直接修改完成的。 左值引用接着来看下通过引用来修改变量值的过程，测试代码如下： 123int a = 6;int&amp; r = a;r = 2; 转换成汇编代码如下： 12345movl $6, -20(%rbp)leaq -20(%rbp), %raxmovq %rax, -16(%rbp)movq -16(%rbp), %raxmovl $2, (%rax) 看到这里是不是有点意思了，这几行通过引用修改变量值的代码转换成汇编代码以后，居然和之前通过指针修改变量值的汇编代码一模一样。咦？仿佛发现了引用的本质呀！ 常量引用在传统C++中我们知道，引用变量不能引用一个右值，但是常引用可以办到这一点，测试代码如下： 1const int&amp; a = 6; 转换成汇编代码如下： 1234movl $6, %eax //把立即数放到寄存器%eax中movl %eax, -20(%rbp) //把寄存器%eax中的数字6放到内存地址为-20(%rbp)的位置，一个临时变量中leaq -20(%rbp), %rax //把临时变量的内存地址-20(%rbp)放到寄存器%rax中movq %rax, -16(%rbp) //把寄存器%rax中存储的临时变量的内存地址-20(%rbp)放到内存地址为-16(%rbp)的位置 这段代码的翻译结果与前面指针变量的例子很像，首先有一个变量（匿名变量）来存储值，然后是一个新的内存地址来保存之前变量的地址。 右值引用右值引用需要C++11才能使用，与常引用对比的优点就是可以修改右值，实际上我认为还是修改的左值！测试代码如下： 12int&amp;&amp; a = 6;a = 2 转换成汇编代码如下： 123456movl $6, %eax //把立即数放到寄存器%eax中movl %eax, -20(%rbp) //把寄存器%eax中的数字6放到内存地址为-20(%rbp)的位置，一个临时变量中leaq -20(%rbp), %rax //把临时变量的内存地址-20(%rbp)放到寄存器%rax中movq %rax, -16(%rbp) //把寄存器%rax中存储的临时变量的内存地址-20(%rbp)放到内存地址为-16(%rbp)的位置movq -16(%rbp), %rax // 把内存地址为-16(%rbp)的位置（变量p）的数据放到寄存器%rax中movl $2, (%rax) // 把立即数2放在寄存器%rax中保存的地址位置中，也就是p所指向的地址，即变量a中 这段汇编代码与常量引用相比只缺少赋值的部分，与左值引用相比几乎一样，只有在最开始立即数6的处理上有一点点差异，是不是感觉很神奇？ 一点点惊奇对比了前面这些代码的汇编指令后有没有什么想法？什么常量引用，什么右值引用，这些不过都是“愚弄”程序员的把戏，但这些概念的出现并不是为了给程序员们带来麻烦，相反它们的出现使得程序编写更加可控，通过编译器帮助“粗心”的开发者们先暴露了一波问题。 通过汇编代码来看，常量引用其实引用的并非常量，而是引用了一个变量；右值引用引用的也并非右值，同样是一个保存了右值的变量。这年头常量都能变，还有什么不能变的呢？ 来看看下面这段代码，仔细想想常量真的变了吗？运行之后各个变量的值是多少呢？ 123456const int a = 6;int *p = const_cast&lt;int*&gt;(&amp;a);*p = 2;int b = *p;int c = a; 这段代码运行之后的打印结果：a=6, b=2, c=6，变量a作为一个常量没有被改变，貌似常量还是有点用的，哈哈~ 这段代码转换成汇编代码如下： 123456789movl $6, -28(%rbp)leaq -28(%rbp), %raxmovq %rax, -16(%rbp)movq -16(%rbp), %raxmovl $2, (%rax)movq -16(%rbp), %raxmovl (%rax), %eaxmovl %eax, -24(%rbp)movl $6, -20(%rbp) 通过汇编来看你会发现，其实变量a的值已经通过指针 p 修改过了，只不过后面引用a变量的地方，因为它是常量，直接使用立即数6替换了。 改写一下代码，将常量6换成一个变量： 1234567int i = 3;const int a = i;int *p = const_cast&lt;int*&gt;(&amp;a);*p = 2;int b = *p;int c = a; 转换成汇编代码为： 123456789101112movl $3, -28(%rbp)movl -28(%rbp), %eaxmovl %eax, -32(%rbp)leaq -32(%rbp), %raxmovq %rax, -16(%rbp)movq -16(%rbp), %raxmovl $2, (%rax)movq -16(%rbp), %raxmovl (%rax), %eaxmovl %eax, -24(%rbp)movl -32(%rbp), %eaxmovl %eax, -20(%rbp) 这段代码运行的结果为：i=3, a=2, b=2, c=2，看来常量也禁不住我们这么折腾啊 所以从这一点可以看出C++代码中无常量，只要是定义出的变量都可以修改，而常量只是给编译器优化提供一份指导，比如可以把一些字面量在编译期间替换，但是运行时的常量还是能改的。 总结 左值和右值更像是容器与数据的关系，不过C++11提出的将亡值的概念又模糊这两者的界限，将亡值可以看成是即将失去容器的数据 在Ubuntu16.04、GCC5.4.0的环境下，通过左值引用和指针修改一个变量值生成的汇编代码完全一致 C++11中右值引用与常量引用生成的汇编代码一致，与左值引用生成的代码只在初始化时有一点差异 常量并非不可修改，它只是一种“君子协定”，你要知道什么情况下可以改，什么情况下绝对不可以改 const_cast 目的并不是让你去修改一个本身被定义为const的值，这样修改后果是可能是无法预期的，它存在的目的是调整一些指针、引用的权限，比如在函数传递参数的时候 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 身上若无千斤担，谁拿生命赌明天~世间唯一不变的就是变化 2021-7-5 00:36:29]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>左值</tag>
        <tag>右值</tag>
        <tag>右值引用</tag>
        <tag>C++</tag>
        <tag>C++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11中std::move和std::forward到底干了啥]]></title>
    <url>%2Fblog%2F2021%2F07%2F04%2FC-11%E4%B8%ADstd-move%E5%92%8Cstd-forward%E5%88%B0%E5%BA%95%E5%B9%B2%E4%BA%86%E5%95%A5%2F</url>
    <content type="text"><![CDATA[前言C++11中的右值引用的出现，在特定情况下减少了对象的拷贝，提升了C++程序的效率，伴随而来的 std::move 和 std::forward 也大量出现在程序代码中，但是这两个函数究竟干了啥呢？其实他们的本质都是转换函数，也就是完成左值和右值之间的转换，需要注意的是左值可以转换成右值，但是右值无法转换成左值。 关于左值、右值、左值引用和右值引用的概念可以看看之前的总结： 简单聊聊C/C++中的左值和右值 C++11在左值引用的基础上增加右值引用 虽然温故不一定知新，但绝对可以增强记忆，本章的内容说起来很绕，我也是边学边总结，有不对的地方还请大佬们指出来。 左值引用和右值引用了解过基础的引用知识之后我们都知道左值引用的形式为 T&amp; t，一般会像成下面这样使用： 123456789101112class A&#123;private: int n;&#125;;void test(A&amp; obj) &#123; //...&#125;A obj;test(obj); 而右值引用是在左值引用的基础上多加一个&amp;，形式变为 T&amp;&amp; t，使用方式如下： 1234567891011class A&#123;private: int n;&#125;;void test(A&amp;&amp; obj) &#123; //...&#125;test(A()); 这种通过 &amp; 的个数区分左值引用和右值引用的方法，在大多数的普通函数中没有问题，但是放到模板参数或者 auto 关键字之后的位置就不太灵了，因为这些地方会推导实际的类型，正是有了参数推导，才使得模板中出现了“万能引用”的说法，也就是下面这样： 12345678910111213141516#include &lt;iostream&gt;using namespace std;template&lt;typename T&gt;void func(T&amp;&amp; val)&#123; cout &lt;&lt; val &lt;&lt; endl;&#125;int main()&#123; int year = 2020; func(year); func(2020); return 0;&#125; 函数 func 即能接受变量 year 这样的左值作为参数，也能接受 2020 这样的常数作为右值，简直太完美。那么这里是怎样推导的呢？这就要请出一个引用的“折叠”规则了，描述如下： A&amp; &amp; 折叠成 A&amp; A&amp; &amp;&amp; 折叠成 A&amp; A&amp;&amp; &amp; 折叠成 A&amp; A&amp;&amp; &amp;&amp; 折叠成 A&amp;&amp; 根据这个规则，func 函数在接受 year 作为参数时应该是一个左值引用，那么模板参数 T 会被推到为 A&amp; 与后面的 &amp;&amp; 折叠为 A&amp;，接受 year 没问题。而这个函数在接受 2020 作为参数时应该是一个右值引用，那么模板参数 T 会被推导成 A，与后面的 &amp;&amp; 形成 A&amp;&amp;，可以接受右值，知道了这些基础知识我们接着往后看。 std::move这个函数听起来好像是一个小人移动时调用的函数，但它却是一个把左值转化成右值的转化函数，我们看一下 std::move 函数的实现： 123456789/** * @brief Convert a value to an rvalue. * @param __t A thing of arbitrary type. * @return The parameter cast to an rvalue-reference to allow moving it.*/template&lt;typename _Tp&gt; constexpr typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; move(_Tp&amp;&amp; __t) noexcept &#123; return static_cast&lt;typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp;&gt;(__t); &#125; 这是一个模板函数，一共才4行，好像最麻烦的就是这个 std::remove_reference&lt;_Tp&gt;::type&amp;&amp; 了，先来看看它是什么，其实它的作用就是，移除类型的引用，返回原始类型。 std::remove_reference它的可能实现如下： 1234567891011121314template &lt;typename T&gt;struct remove_reference &#123; using type = T;&#125;;template &lt;typename T&gt; // 模板特化struct remove_reference&lt;T&amp;&gt; &#123; using type = T;&#125;;template &lt;typename T&gt; // 模板特化struct remove_reference&lt;T&amp;&amp;&gt; &#123; using type = T;&#125;; 它的作用可以参考 cppreference.com - remove_reference，示例如下： 12345678910111213141516171819#include &lt;iostream&gt; // std::cout#include &lt;type_traits&gt; // std::is_sametemplate&lt;class T1, class T2&gt;void print_is_same() &#123; std::cout &lt;&lt; std::is_same&lt;T1, T2&gt;() &lt;&lt; '\n';&#125;int main() &#123; std::cout &lt;&lt; std::boolalpha; print_is_same&lt;int, int&gt;(); print_is_same&lt;int, int &amp;&gt;(); print_is_same&lt;int, int &amp;&amp;&gt;(); print_is_same&lt;int, std::remove_reference&lt;int&gt;::type&gt;(); print_is_same&lt;int, std::remove_reference&lt;int &amp;&gt;::type&gt;(); print_is_same&lt;int, std::remove_reference&lt;int &amp;&amp;&gt;::type&gt;();&#125; 运行结果 123456truefalsefalsetruetruetrue 从这个例子可以清晰的看出 std::remove_reference 就是返回去掉引用的原始类型。 static_cast明白了上面 std::remove_reference 的作用，整个 std::move 函数就剩下一个 static_cast 函数了，其实到这里也就清晰了，std::move 函数的作用就先通过 std::remove_reference 函数得到传入参数的原始类型 X，然后再把参数强转成 X&amp;&amp; 返回即可，参数的 _Tp 的推导参考引用折叠规则。 std::move 到底干了啥通过前面的一通分析我们发现，std::move 的内部只做了一个强制类型转换，除此之外啥也没干，其实就是对传入的参数重新解释了一下，并没有实质性的动作。 那么为什么要使用 std::move 这个名字呢？这个名字更多的是起到提醒的作用，告诉使用者这里可能进行了到右值的转化，相关的对象后续可能发生移动，“被掏空”了，如果你继续使用这个对象，行为是未定义的，后果自负。 std::forwardstd::forward 被称为完美转发，听起来和 “万能引用”一样厉害，使用的头文件为 &lt;utility&gt;，在 /usr/include/c++/5/bits/move.h 文件中的定义如下： 12345678910111213141516171819202122232425/** * @brief Forward an lvalue. * @return The parameter cast to the specified type. * * This function is used to implement "perfect forwarding". */template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp; __t) noexcept &#123; return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125;/** * @brief Forward an rvalue. * @return The parameter cast to the specified type. * * This function is used to implement "perfect forwarding". */template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept &#123; static_assert(!std::is_lvalue_reference&lt;_Tp&gt;::value, "template argument" " substituting _Tp is an lvalue reference type"); return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125; std::forward 用于函数模板中完成参数转发任务，我们必须在相应实参为左值，该形参成为左值引用时把它转发成左值，在相应实参为右值，该形参成为右值引用时把它转发成右值。 有了前面的铺垫我们直接来分析代码吧，第一个版本接受参数苏为左值引用的情况，因为 std::remove_reference&lt;_Tp&gt;::type 是 _Tp 的原始类型，所以 t 就是左值引用类型，调用这个函数时，_Tp 为 X&amp; 类型，经过引用这的 _Tp&amp;&amp; =&gt; X&amp; &amp;&amp; =&gt; X&amp;，所以返回值也是左值引用。 同理，第二个版本接受右值引用参数，返回值也是一个右值引用。 从目前的情况来看，std::forward 好像什么也没做，只是将参数强转以后返回，如果不使用这个函数会有什么问题呢？ 必要性为什么要使用 std::forward 我们可以通过一个例子来看一下： 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;utility&gt;void Print(int&amp; val) &#123; std::cout &lt;&lt; "lvalue refrence: val=" &lt;&lt; val &lt;&lt; std::endl;&#125;void Print(int&amp;&amp; val) &#123; std::cout &lt;&lt; "rvalue refrence: val=" &lt;&lt; val &lt;&lt; std::endl;&#125;template&lt;typename T&gt;void TPrint(T &amp;&amp;t) &#123; return Print(t);&#125;int main() &#123; int date = 1021; TPrint(date); TPrint(501); return 0;&#125; 看到这个例子可以先思考一下，运行结果会是什么呢？可能和你想的有点不一样哦，看看下面的答案： 12lvalue refrence: val=1021lvalue refrence: val=501 有点出乎意料啊，为什么 Print(int&amp;&amp; val) 这个函数没有被调用呢？原因在于“右值引用是一个左值”，很懵对不对，接着往下看： 1234int i = 101;int&amp; li = i;int&amp;&amp; ri = 120; 这段代码中哪些是左值，哪些是右值呢？可以肯定的是 i、li 是左值， 101、120 是右值，而ri也是左值，因为它也一个可以取地址并长期有效的变量啊，只不过这个左值引用了一个右值而已。 接着回到刚才的例子，TPrint(501); 调用模板函数时，T被推导为 int，所以模板被实例化为： 123void TPrint(int&amp;&amp; t) &#123; return Print(t);&#125; 运行到这里，t 实际上是一个左值，所以调用了 void Print(int&amp; val) 这个函数，那么怎样才能调用 void Print(int&amp;&amp; val) 这个版本呢？是时候请出 std::forward 函数了，将模板函数进行如下修改： 1234template&lt;typename T&gt;void TPrint(T&amp;&amp; t) &#123; return Print(std::forward&lt;T&gt;(t));&#125; 修改之后再来分析一下，TPrint(501); 调用模板函数时，T被推导为 int，所以模板被实例化为： 123void TPrint(int&amp;&amp; t) &#123; return Print(std::forward&lt;int&gt;(t));&#125; 这里会调用 std::forward 的这个版本： 12345678template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept &#123; static_assert(!std::is_lvalue_reference&lt;_Tp&gt;::value, "template argument" " substituting _Tp is an lvalue reference type"); return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125; 函数的返回类型为 int&amp;&amp;，然后就调用了 void Print(int&amp;&amp; val) 这个版本的打印函数。 疑惑可能有人会说，这不对啊，使用 std::forward 修改之前函数参数就是 int&amp;&amp; 类型，修改之后得到的返回值还是 int&amp;&amp; 类型，这有什么区别吗？ 这里的区别就在于，使用 std::forward 之前的 int&amp;&amp; 是有名字的变量 t，它是一个左值，而使用 std::forward 之后的 int&amp;&amp; 是有个匿名变量，它是一个右值，真正的差距就在这里。 std::forward 到底干了啥它和 std::move 一样，std::forward 也是做了一个强制类型转换，当形参成为左值引用时把它转换成左值引用返回，当形参成为右值引用时把它转换成右值引用返回。 总结 std::move 并没有实际的“移动”操作，只是在内部进行了强制类型转换，返回一个相关类型的右值引用 std::move 的名字主要标识它后续可能会被其他人“掏空”，调用它之后如果继续使用，行为未定义，后果自负 std::forward 的本质也是进行强制类型转换，形参为左值时返回左值引用，形参为右值时返回右值引用 从定义入手可以理解很多花里胡哨的东西，透过现象看其本质。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 日拱一卒无有尽，功不唐捐终入海~ 我们追求的样子：十分沉静，九分气质，八分资产，七分现实，三分颜值，二分糊涂，一份自知之明。 2021-7-18 21:23:01]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>move</tag>
        <tag>forward</tag>
        <tag>完美转发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手搭建一个redis集群]]></title>
    <url>%2Fblog%2F2021%2F06%2F26%2F%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AAredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[前言所谓“难者不会，会者不难”，这不只是一句简简单单的俗语，而是历经人类进化史而得到的历史经验，只有亲身体会过才会越发了解这句话的意义。当你苦思冥想几天几夜难以解决的问题，一句过来人的点拨便可瞬间化解，这是人生路上的需要经历的，愿每个人在关键时刻都能得到善良人的点拨。 人生有四个境界：①不知道自己不知道；②知道自己不知道；③不知道自己知道，④知道自己知道。不同的境界对应着不同的人生，仔细想想这四种境界并没有哪一种是绝对的好与坏，具体怎么选择取决于你想要什么样的人生，想想坐井观天的那只青蛙，属于“不知道自己不知道”的典型，如果它每天逍遥快活，完全没必要非得跳出井来卷入井外的纷争。 长久以来自给自足的小农经济，如果没有战乱，没有暴政，没有入侵，耕田织布的生活不也挺好，但是人性是贪婪的，欲望促使人们侵略、扩张，使得一些人进入了“知道自己不知道”的境界，于是加速了人与人之间的交流。进入现代社会以后，信息爆炸，越来越多的人“觉醒”了，但只是“觉醒”而不付出努力就会使自己陷入焦虑，其实焦虑不可怕，重要的是你要采取什么措施来缓解焦虑。 题外话扯多了，回归正题，写这篇总结的主要还是一个学习记录，也希望这样的记录可以解决一些人的困惑，这些东西并不复杂，也许只是我这个初学者才会关心的问题，但是现在有一种现象，就是大佬们不愿写基础的东西，而小白们想成为大佬从基础学习，却发现路上的坑让他们头破血流。一句学神们口中的“不难证明”，可能你几个月都证明不出来，这个问题是双方面的，一方面小白认为这东西很难，不知道怎么入手，另一方面大神感觉这东西这么简单还需要学？ 屠龙勇士中成恶龙，大神们经过时间的洗礼，很多人已经忘记了自己曾经的头破血流，忘记了自己怎么从小白一步步走来，所以遇到困惑的人不要先入为主，不要认为这个东西太简单了怎么可能有人不会？耐心聆听给出建议，或许可以瞬间打通他的任督二脉。 好了，好了，又跑题了，拉回来，我们从头来搭建一个redis集群，不过搭建之前我们先来看看“安装”这个词。 安装的含义安装软件、安装程序、安装APP，自从电子设备飞入寻常百姓家，我们每天都在安装、安装、安装。但是这个词并不是电子设备出现后才诞生的。这个词自古就有，安装——本意是：把机件、器材等固定在一定的位置。那么软件、APP的安装就是把它们放在一定的位置。 软件是什么？至今我都记得初中的一道计算机的题目，我是04年在学校机房第一次接触到真的计算机，当时有一道题是问：Windows是系统软件，还是应用软件？当时这种题目真的是死记硬背啊，脑子里完全没有概念，每周只有一节课，还不一定每个人都有机器，那时的软件对我来说就是一个迷。 现在接触计算机也这么多年了，对软件也有了一些认识。软件可以说是用来向发出计算机发出任务的东西，当我们对一个人发号任务指令时，能通过声音传递给人，然后接到指令的人可以完成相应的任务，当我们给计算机发送指令时，可以通过键盘、鼠标等把指令输进去，也可以运行软件来完成某些计算机任务，其实软件就是一些提前编好的任务指令的集合。 那么安装软件就是把软件放到对应的地方，为什么要安装？为什么要放到指定的地方？随便放有什么问题？其实还是为了方便。就像生活中我们把床安装在卧室，把马桶安装在厕所一样，大家都是这样，已经成了习惯。如果你把床安装在厕所，并不是不能用来睡觉，只是其他人在来到厕所睡觉这件事上有些不太习惯。 因为软件并不只是一个东西，他还有数据啊、配置啊、服务啊等等，所以这些东西都要放在相应的位置，并且做一个登记，就像你把床放到厕所一样，这种不一样的习惯更要登记下来，否则对于一个普通人来说，肯定会去卧室找床，而不是厕所。 windows中软件的傻瓜式安装降低了软件的安装难度，它有一个注册表，里面记录了很多奇奇怪怪的东西，在windows上安装软件也就是把软件的各部分发到对应的位置，然后通过注册表这个总管登记入册，但是有些软件不太友好，安装的时候一顿乱放，卸载的时候不自己清理干净，导致出现很多问题，比如当年的SQLServer和3DMax，当时为了再次安装不得不重做了好多次系统。 在linux中安装软件就没有注册表这个东西，只要把软件各部分按照类型分别放好就行，比如可执行程序放/usr/bin/，库文件放/usr/lib，配置文件放 /etc/等等，卸载就是把这些新加的东西删除就可以了，相比windows的注册表要简单一些。 好了，接下来我们就来安装配置一下redis-cluster，看了一下，本地机器真的很干净，什么都没有，现在从安装redis开始吧。 安装redis 系统是ubuntu16.04，偷个懒，直接通过命令 sudo apt-get install redis-server 安装吧，当然你也可以通过源码编译安装，这都是可以的，输出信息如下： 12345678910111213141516171819202122232425262728293031323334353637383940albert@home-pc:/usr/src$$ sudo apt-get install redis-serverReading package lists... DoneBuilding dependency treeReading state information... DoneThe following additional packages will be installed: libjemalloc1 redis-toolsSuggested packages: ruby-redisThe following NEW packages will be installed: libjemalloc1 redis-server redis-tools0 upgraded, 3 newly installed, 0 to remove and 125 not upgraded.Need to get 519 kB of archives.After this operation, 1,507 kB of additional disk space will be used.Do you want to continue? [Y/n] YGet:1 http://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial/universe amd64 libjemalloc1 amd64 3.6.0-9ubuntu1 [78.9 kB]Get:2 http://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial-updates/universe amd64 redis-tools amd64 2:3.0.6-1ubuntu0.4 [95.5 kB]Get:3 http://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial-updates/universe amd64 redis-server amd64 2:3.0.6-1ubuntu0.4 [344 kB]Fetched 519 kB in 13s (38.0 kB/s)Selecting previously unselected package libjemalloc1.(Reading database ... 33895 files and directories currently installed.)Preparing to unpack .../libjemalloc1_3.6.0-9ubuntu1_amd64.deb ...Unpacking libjemalloc1 (3.6.0-9ubuntu1) ...Selecting previously unselected package redis-tools.Preparing to unpack .../redis-tools_2%3a3.0.6-1ubuntu0.4_amd64.deb ...Unpacking redis-tools (2:3.0.6-1ubuntu0.4) ...Selecting previously unselected package redis-server.Preparing to unpack .../redis-server_2%3a3.0.6-1ubuntu0.4_amd64.deb ...Unpacking redis-server (2:3.0.6-1ubuntu0.4) ...Processing triggers for libc-bin (2.23-0ubuntu11) ...Processing triggers for man-db (2.7.5-1) ...Processing triggers for ureadahead (0.100.0-19.1) ...Processing triggers for systemd (229-4ubuntu21.27) ...Setting up libjemalloc1 (3.6.0-9ubuntu1) ...Setting up redis-tools (2:3.0.6-1ubuntu0.4) ...Setting up redis-server (2:3.0.6-1ubuntu0.4) ...invoke-rc.d: could not determine current runlevelProcessing triggers for libc-bin (2.23-0ubuntu11) ...Processing triggers for ureadahead (0.100.0-19.1) ...Processing triggers for systemd (229-4ubuntu21.27) ...albert@home-pc:/usr/src$ 安装完查找一下命令安装的位置，redis服务器和客户端应该都在 /usr/bin/ 1234albert@home-pc:/usr/src$ which redis-server/usr/bin/redis-serveralbert@home-pc:/usr/src$ which redis-cli/usr/bin/redis-cli 修改配置文件，设置密码，通过 sudo vim /etc/redis/redis.conf 命令打开文件搜索 requirepass 找到下面这一行，把前面的#去掉，requirepass 后面的就是密码，可以自己修改。 1234567# Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared #把这一行前面的&apos;#&apos;去掉就行，密码我就不改了# Command renaming. 启动redis，直接运行 sudo redis-server /etc/redis/redis.conf，redis服务器就起来了 123albert@home-pc:/mnt/c/Users/Albert$ sudo redis-server /etc/redis/redis.confalbert@home-pc:/mnt/c/Users/Albert$ ps -ef | grep redisroot 663 1 0 13:52 ? 00:00:00 redis-server 127.0.0.1:6379 启动客户端使用redis，连接默认的6379端口，密码foobared 12345678910albert@home-pc:/mnt/c/Users/Albert$ redis-cli -a foobared -p 6379127.0.0.1:6379&gt; set name redis-testOK127.0.0.1:6379&gt; get name"redis-test"127.0.0.1:6379&gt; set age 18OK127.0.0.1:6379&gt; get age"18"127.0.0.1:6379&gt; 至此redis就可以使用了，测试了set和get两个命令如上 redis编译安装忽然发现Ubuntu16.04自带的 redis3.0.6 版本太低了，无法自动配置集群，所以决定编译安装，如果自动安装完redis版本在5以上，那就不需要编译安装了。 新建/usr/local/redis/目录并进入，然后下载redis源码redis-6.2.4.tar.gz 12345678910111213141516albert@home-pc:/usr/local$ mkdir redisalbert@home-pc:/usr/local$ sudo mkdir redisalbert@home-pc:/usr/local$ cd redis/albert@home-pc:/usr/local/redis$ sudo wget https://download.redis.io/releases/redis-6.2.4.tar.gz--2021-06-26 14:10:42-- https://download.redis.io/releases/redis-6.2.4.tar.gzResolving download.redis.io (download.redis.io)... 45.60.125.1Connecting to download.redis.io (download.redis.io)|45.60.125.1|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 2457940 (2.3M) [application/octet-stream]Saving to: ‘redis-6.2.4.tar.gz’redis-6.2.4.tar.gz 100%[========================================&gt;] 2.34M 45.4KB/s in 54s2021-06-26 14:11:37 (44.3 KB/s) - ‘redis-6.2.4.tar.gz’ saved [2457940/2457940]albert@home-pc:/usr/local/redis$ 运行 sudo tar -zxvf redis-6.2.4.tar.gz 命令解压文件 运行 cd redis-6.2.4 进入 redis-6.2.4 目录 直接运行 sudo make &amp;&amp; make install 即可，如果遇到了 ‘/bin/sh: 1: pkg-config: not found’ 错误，先通过 sudo apt-get install pkg-config 安装 pkg-config，再运行 make 即可。 这次 redis-server 和 redis-cli 都被安装到了 /usr/local/bin/ 目录，版本是 6.2.4，接下来我们就用这个版本来搭建redis集群 redis集群为什么要用redis集群？很直接的原因就是一个redis不够用了，需要使用好几个来组合分摊数据量和压力，那么搭建redis集群其实就是启动多个redis程序实例，让他们配合工作就好了，他们之间的配合是redis本身实现的，我们只需要配置启动多个实例就行了，因为redis集群最少需要三主三从，所以我们启动6个实例，： 新建 /usr/local/redis-cluster 目录并进入 12albert@home-pc:/usr/local$ sudo mkdir redis-clusteralbert@home-pc:/usr/local$ cd redis-cluster/ 为了彼此的配置、数据、日志不相互影响，6个实例需要分别配置，我们建立如下的目录结构，按照端口号配置6个配置文件 12345678910111213141516.├── conf│ ├── redis-6301.conf│ ├── redis-6302.conf│ ├── redis-6303.conf│ ├── redis-6304.conf│ ├── redis-6305.conf│ └── redis-6306.conf├── data│ ├── redis-6301│ ├── redis-6302│ ├── redis-6303│ ├── redis-6304│ ├── redis-6305│ └── redis-6306└── log 修改redis配置文件，需要修改的配置文件中的内容如下，首先把下面几项前面的#去掉，然后按照端口号命名相互影响的目录和文件名，以第一个实例端口6301为例： 12345678910pidfile /var/run/redis/redis-server.pidport 6379logfile /var/log/redis/redis-server.logdir /var/ilib/redisdbfilename dump.rdb#requirepass foobared#cluster-enabled yes#cluster-config-file nodes-6379.conf#cluster-node-timeout 15000 文件 redis-6301.conf 修改后的文件内容如下，接下来5个配置文件分别按照6302, 6303, 6304, 6305, 6306来配置： 12345678910pidfile /var/run/redis/redis-6301-server.pidport 6301logfile /usr/local/redis-cluster/log/redis-6301-server.logdir /usr/local/redis-cluster/data/redis-6301dbfilename dump-6301.rdbrequirepass foobaredcluster-enabled yescluster-config-file nodes-6301.confcluster-node-timeout 15000 进入 /usr/local/redis-cluster 目录并启动实例： 1234567891011121314albert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6301.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6302.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6303.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6304.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6305.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6306.confalbert@home-pc:/usr/local/redis-cluster$ ps -ef | grep redisroot 663 1 0 13:52 ? 00:00:00 redis-server 127.0.0.1:6379root 5928 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6301 [cluster]root 5935 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6302 [cluster]root 5942 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6303 [cluster]root 5949 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6304 [cluster]root 5956 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6305 [cluster]root 5963 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6306 [cluster] 此时查看目录已经生成了默认的节点文件： 12345678910111213141516171819202122232425262728293031albert@home-pc:/usr/local/redis-cluster$ tree.├── conf│ ├── redis-6301.conf│ ├── redis-6302.conf│ ├── redis-6303.conf│ ├── redis-6304.conf│ ├── redis-6305.conf│ └── redis-6306.conf├── data│ ├── redis-6301│ │ └── nodes-6301.conf│ ├── redis-6302│ │ └── nodes-6302.conf│ ├── redis-6303│ │ └── nodes-6303.conf│ ├── redis-6304│ │ └── nodes-6304.conf│ ├── redis-6305│ │ └── nodes-6305.conf│ └── redis-6306│ └── nodes-6306.conf└── log ├── redis-6301-server.log ├── redis-6302-server.log ├── redis-6303-server.log ├── redis-6304-server.log ├── redis-6305-server.log └── redis-6306-server.log9 directories, 18 files 虽然实例都启动起来了，但是现在每个实例是单独的，需要把它们连接到一起，运行如下命令 redis-cli -a foobared --cluster create 127.0.0.1:6301 127.0.0.1:6302 127.0.0.1:6303 127.0.0.1:6304 127.0.0.1:6305 127.0.0.1:6306 --cluster-replicas 1 即可，命令中的 --cluster-replicas 1 表示为每个主节点都提供一个从节点，中间还需要输入 yes，这样运行完就形成了一个三主三从的redis集群： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152albert@home-pc:/usr/local/redis-cluster$ redis-cli -a foobared --cluster create 127.0.0.1:6301 127.0.0.1:6302 127.0.0.1:6303 127.0.0.1:6304 127.0.0.1:6305 127.0.0.1:6306 --cluster-replicas 1Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 127.0.0.1:6305 to 127.0.0.1:6301Adding replica 127.0.0.1:6306 to 127.0.0.1:6302Adding replica 127.0.0.1:6304 to 127.0.0.1:6303&gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity[WARNING] Some slaves are in the same host as their masterM: 5d60bdad15a6f8ceec188a1081e9381f181a5c5e 127.0.0.1:6301 slots:[0-5460] (5461 slots) masterM: 8c814d4a32763d47723398fcf8f596d7b6340afc 127.0.0.1:6302 slots:[5461-10922] (5462 slots) masterM: 7bc53b512772c3a1df3217facca283ff9564d32d 127.0.0.1:6303 slots:[10923-16383] (5461 slots) masterS: b39bdb5b6e720e9fbedd43e58b57661910dcc3d7 127.0.0.1:6304 replicates 5d60bdad15a6f8ceec188a1081e9381f181a5c5eS: 5fef2bedd430bf86cdff63cb2f852aeb21e1b18f 127.0.0.1:6305 replicates 8c814d4a32763d47723398fcf8f596d7b6340afcS: 0123b160087743a5296807145b426d9b9cefcf21 127.0.0.1:6306 replicates 7bc53b512772c3a1df3217facca283ff9564d32dCan I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join.&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6301)M: 5d60bdad15a6f8ceec188a1081e9381f181a5c5e 127.0.0.1:6301 slots:[0-5460] (5461 slots) master 1 additional replica(s)S: b39bdb5b6e720e9fbedd43e58b57661910dcc3d7 127.0.0.1:6304 slots: (0 slots) slave replicates 5d60bdad15a6f8ceec188a1081e9381f181a5c5eM: 7bc53b512772c3a1df3217facca283ff9564d32d 127.0.0.1:6303 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: 5fef2bedd430bf86cdff63cb2f852aeb21e1b18f 127.0.0.1:6305 slots: (0 slots) slave replicates 8c814d4a32763d47723398fcf8f596d7b6340afcM: 8c814d4a32763d47723398fcf8f596d7b6340afc 127.0.0.1:6302 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: 0123b160087743a5296807145b426d9b9cefcf21 127.0.0.1:6306 slots: (0 slots) slave replicates 7bc53b512772c3a1df3217facca283ff9564d32d[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. redis集群的使用当使用redis-cli连接集群的时候，直接连3个主节点的任意一个就可以，把cluster当成一个整理来看待，测试如下： 1234567albert@home-pc:/usr/local/redis-cluster$ redis-cli -a foobared -p 6301Warning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.127.0.0.1:6301&gt; get name(error) MOVED 5798 127.0.0.1:6302127.0.0.1:6301&gt; set name cluster-test(error) MOVED 5798 127.0.0.1:6302127.0.0.1:6301&gt; 我们发现测试Redis命令的时候报错了，显示 (error) MOVED 5798 127.0.0.1:6302，这说明 name 这个键不在端口为 6301 这个实例上，而在端口为 6302 这个实例上，使用 cluster nodes 命令可以查询集群的节点信息 1234567127.0.0.1:6301&gt; cluster nodesb39bdb5b6e720e9fbedd43e58b57661910dcc3d7 127.0.0.1:6304@16304 slave 5d60bdad15a6f8ceec188a1081e9381f181a5c5e 0 1624711999000 1 connected7bc53b512772c3a1df3217facca283ff9564d32d 127.0.0.1:6303@16303 master - 0 1624712000000 3 connected 10923-163835fef2bedd430bf86cdff63cb2f852aeb21e1b18f 127.0.0.1:6305@16305 slave 8c814d4a32763d47723398fcf8f596d7b6340afc 0 1624711998971 2 connected5d60bdad15a6f8ceec188a1081e9381f181a5c5e 127.0.0.1:6301@16301 myself,master - 0 1624712000000 1 connected 0-54608c814d4a32763d47723398fcf8f596d7b6340afc 127.0.0.1:6302@16302 master - 0 1624711999978 2 connected 5461-109220123b160087743a5296807145b426d9b9cefcf21 127.0.0.1:6306@16306 slave 7bc53b512772c3a1df3217facca283ff9564d32d 0 1624712000991 3 connected 这种情况下可以手动连接端口为6302的节点，也可以在连接 6301 时添加一个 -c 参数，这样在当前实例找不到指定的键时会自动切换，下面测试一下： 12345678albert@home-pc:/usr/local/redis-cluster$ redis-cli -a foobared -p 6301 -cWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.127.0.0.1:6301&gt; set name cluster666-&gt; Redirected to slot [5798] located at 127.0.0.1:6302OK127.0.0.1:6302&gt; get name&quot;cluster666&quot;127.0.0.1:6302&gt; 自动跳转到了端口为6302的实例上，这波操作挺666 C++工程连接使用redis-clusterredis-cli 可以在集群的各个节点之间自动跳转，那是人家的本事，如果是你想自己连接集群，首先看看你使用的是那个库了。 如果使用的语言是C/C++，并且用的是最常见的hiredis，那么很抱歉它本身不提供集群的便利访问，像 MGET，MSET 这种操作多个键的命令得自己花点力气了，不过如果你考虑换一个库那么这些问题就解决了。 hiredis-vip 是第一个选择，这个名字听起来好像充了前才能用，实际上是“唯品会 一家专门做特卖的网站”，它先搞出来开源的，可以把集群看成一个整体，连接任一节点就可以操作，但是这个库有个问题，不支持认证，所以后来人在这个基础上进行了二次开发。hiredis-vip源码 hiredis-cluster 就是这个二次创作的产物，现在算是官方推荐产品了，如果你想方便的操作集群，不妨下载这个库hiredis-cluster源码试试，另外redis-plus-plus也属于官方推荐产品，有时间可以了解一下。 hiredis-cluster今天拿 hiredis-cluster 开刀，看看它怎么用，改天再研究那个 redis-plus-plus。 首先创建一个目录 /usr/local/redis-cluster-cli，进入目录下载源码： 123456789101112131415161718albert@home-pc:/usr/local/redis-cluster$ sudo mkdir -p /usr/local/redis-cluster-clialbert@home-pc:/usr/local/redis-cluster$ cd /usr/local/redis-cluster-clialbert@home-pc:/usr/local/redis-cluster-cli$ sudo wget https://github.com/Nordix/hiredis-cluster/archive/refs/tags/0.6.0.tar.gz--2021-06-26 22:01:15-- https://github.com/Nordix/hiredis-cluster/archive/refs/tags/0.6.0.tar.gzResolving github.com (github.com)... 140.82.114.3Connecting to github.com (github.com)|140.82.114.3|:443... connected.HTTP request sent, awaiting response... 302 FoundLocation: https://codeload.github.com/Nordix/hiredis-cluster/tar.gz/refs/tags/0.6.0 [following]--2021-06-26 22:01:16-- https://codeload.github.com/Nordix/hiredis-cluster/tar.gz/refs/tags/0.6.0Resolving codeload.github.com (codeload.github.com)... 13.229.189.0Connecting to codeload.github.com (codeload.github.com)|13.229.189.0|:443... connected.HTTP request sent, awaiting response... 200 OKLength: unspecified [application/x-gzip]Saving to: ‘0.6.0.tar.gz’0.6.0.tar.gz [ &lt;=&gt; ] 69.58K 19.4KB/s in 3.6s2021-06-26 22:01:21 (19.4 KB/s) - ‘0.6.0.tar.gz’ saved [71249] 解压源码，进入目录hiredis-cluster-0.6.0 1234567891011albert@home-pc:/usr/local/redis-cluster-cli$ sudo tar -zxvf 0.6.0.tar.gzhiredis-cluster-0.6.0/hiredis-cluster-0.6.0/.clang-format......hiredis-cluster-0.6.0/tests/test_utils.hhiredis-cluster-0.6.0/win32.halbert@home-pc:/usr/local/redis-cluster-cli$ ls0.6.0.tar.gz hiredis-cluster-0.6.0albert@home-pc:/usr/local/redis-cluster-cli$ cd hiredis-cluster-0.6.0/albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0$ 按照README文档编译库文件 1234567albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0$ sudo mkdir build &amp;&amp; cd buildalbert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0/build$ sudo cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo -DDISABLE_TESTS=ON ..CMake Error at CMakeLists.txt:1 (cmake_minimum_required): CMake 3.11 or higher is required. You are running version 3.5.1-- Configuring incomplete, errors occurred! oh no！要求cmake版本最低3.11，而我的ubuntu16.04自带的cmake是3.5.1，好吧我先去升个级 123456$ wget http://www.cmake.org/files/v3.11/cmake-3.11.3.tar.gz$ tar xf cmake-3.11.3.tar.gz$ cd cmake-3.11.3$ ./configure$ make$ sudo make install 继续编译 hiredis-cluster 库，先运行cmake生成Makefile文件，然后运行 make 命令开始编译： 1234567891011121314151617181920212223242526272829303132333435363738394041albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0/build$ sudo cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo -DDISABLE_TESTS=ON ..-- The C compiler identification is GNU 5.4.0-- The CXX compiler identification is GNU 5.4.0-- Check for working C compiler: /usr/bin/cc-- Check for working C compiler: /usr/bin/cc -- works-- Detecting C compiler ABI info-- Detecting C compiler ABI info - done-- Detecting C compile features-- Detecting C compile features - done-- Check for working CXX compiler: /usr/bin/c++-- Check for working CXX compiler: /usr/bin/c++ -- works-- Detecting CXX compiler ABI info-- Detecting CXX compiler ABI info - done-- Detecting CXX compile features-- Detecting CXX compile features - doneDetected version: 0.5.0Downloading dependency 'hiredis'.....albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0/build$ sudo makeScanning dependencies of target hiredis_cluster[ 5%] Building C object CMakeFiles/hiredis_cluster.dir/adlist.c.o[ 11%] Building C object CMakeFiles/hiredis_cluster.dir/command.c.o[ 17%] Building C object CMakeFiles/hiredis_cluster.dir/crc16.c.o[ 23%] Building C object CMakeFiles/hiredis_cluster.dir/dict.c.o[ 29%] Building C object CMakeFiles/hiredis_cluster.dir/hiarray.c.o[ 35%] Building C object CMakeFiles/hiredis_cluster.dir/hircluster.c.o[ 41%] Building C object CMakeFiles/hiredis_cluster.dir/hiutil.c.o[ 47%] Linking C shared library libhiredis_cluster.so[ 47%] Built target hiredis_clusterScanning dependencies of target hiredis[ 52%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/alloc.c.o[ 58%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/async.c.o[ 64%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/dict.c.o[ 70%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/hiredis.c.o[ 76%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/net.c.o[ 82%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/read.c.o[ 88%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/sds.c.o[ 94%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/sockcompat.c.o[100%] Linking C shared library libhiredis.so[100%] Built target hiredis 进入 /usr/local/redis-cluster-cli/hiredis-cluster-0.6.0 目录，新建 cluster-cli.cpp 文件，编写测试代码如下： 123456789101112131415161718192021222324252627282930#include "hircluster.h"#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main(int argc, char **argv) &#123; struct timeval timeout = &#123;1, 500000&#125;; // 1.5s redisClusterContext *cc = redisClusterContextInit(); redisClusterSetOptionAddNodes(cc, "127.0.0.1:6301"); redisClusterSetOptionConnectTimeout(cc, timeout); redisClusterSetOptionPassword(cc, "foobared"); redisClusterSetOptionRouteUseSlots(cc); redisClusterConnect2(cc); if (cc &amp;&amp; cc-&gt;err) &#123; printf("Error: %s\n", cc-&gt;errstr); exit(-1); &#125; redisReply *reply = (redisReply *)redisClusterCommand(cc, "SET %s %s", "name", "cluster999"); printf("SET: %s\n", reply-&gt;str); freeReplyObject(reply); redisReply *reply2 = (redisReply *)redisClusterCommand(cc, "GET %s", "name"); printf("GET: %s\n", reply2-&gt;str); freeReplyObject(reply2); redisClusterFree(cc); return 0;&#125; 运行如下命令，编译代码，然后运行，结果和预期一致，虽然连接的是端口为6301的实例，也成功取到了端口为6302实例上的数据： 1234albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0$ sudo gcc cluster-cli.cpp -I ./build/_deps/ -L ./build/ -Wl,-rpath=./build -lhiredis_cluster -L ./build/_deps/hiredis-build/ -Wl,-rpath=./build/_deps/hiredis-build/ -lhiredis -o cluster-clialbert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0$ ./cluster-cliSET: OKGET: cluster999 总结 安装软件其实就是把软件的各部分数据放到它该放的地方而已，卸载软件就是把这些数据删掉了 redis也是一款软件，与其他软件并无不同，想要使用多个同时使用，启动多个实例就好了，需要注意配置别冲突 单独运行的redis实例之间不会有沟通，想要这些实例联合起来工作，需要把它们组成一个集群 hiredis-cluster 和 cluster-plus-plus 是使用的C/C++连接redis集的官方推荐库 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 一念动山海，一念山海平~ 2021-6-27 00:35:13]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>集群</tag>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cpplint中filter参数的每个可选项的含义]]></title>
    <url>%2Fblog%2F2021%2F06%2F17%2Fcpplint%E4%B8%ADfilter%E5%8F%82%E6%95%B0%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%8F%AF%E9%80%89%E9%A1%B9%E7%9A%84%E5%90%AB%E4%B9%89%2F</url>
    <content type="text"><![CDATA[前言cpplint 是一款优秀的代码格式检查工具，有了它可以统一整个团队的代码风格，完整的工具就是一个Python脚本，如果安装了Python环境，直接使用 pip install cpplint 命令就可以安装了，非常的方便。 具体的使用方法可以通过 cpplint --help 查询，语法如下： 123456789101112131415Syntax: cpplint.py [--verbose=#] [--output=emacs|eclipse|vs7|junit|sed|gsed] [--filter=-x,+y,...] [--counting=total|toplevel|detailed] [--root=subdir] [--repository=path] [--linelength=digits] [--headers=x,y,...] [--recursive] [--exclude=path] [--extensions=hpp,cpp,...] [--includeorder=default|standardcfirst] [--quiet] [--version] &lt;file&gt; [file] ... Style checker for C/C++ source files. This is a fork of the Google style checker with minor extensions. 其中有一句 [--filter=-x,+y,...] 就是本文总结的重点。 filter是什么这个filter究竟是什么呢？我将它强行解释成代码的“过滤器”，cpplint 是一款检查C++源代码风格的工具，遵循的是Google的编码风格，但是这些规则并不是对于所有人都合适，我们应该有目的进行选择，这个filter参数就是用来屏蔽或者启用一些规则的，我们还是从帮助文档里来看，其中有一段 1234567891011121314filter=-x,+y,... Specify a comma-separated list of category-filters to apply: only error messages whose category names pass the filters will be printed. (Category names are printed with the message and look like "[whitespace/indent]".) Filters are evaluated left to right. "-FOO" means "do not print categories that start with FOO". "+FOO" means "do print categories that start with FOO". Examples: --filter=-whitespace,+whitespace/braces --filter=-whitespace,-runtime/printf,+runtime/printf_format --filter=-,+build/include_what_you_use To see a list of all the categories used in cpplint, pass no arg: --filter= 这一段说明了filter参数的用法，就是以+或者 - 开头接着写规则名，就表示启用或者屏蔽这些规则，使用 --filter= 参数会列举出所有规则，我们来看一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970C:\Users\Albert&gt;cpplint --filter= build/class build/c++11 build/c++14 build/c++tr1 build/deprecated build/endif_comment build/explicit_make_pair build/forward_decl build/header_guard build/include build/include_subdir build/include_alpha build/include_order build/include_what_you_use build/namespaces_headers build/namespaces_literals build/namespaces build/printf_format build/storage_class legal/copyright readability/alt_tokens readability/braces readability/casting readability/check readability/constructors readability/fn_size readability/inheritance readability/multiline_comment readability/multiline_string readability/namespace readability/nolint readability/nul readability/strings readability/todo readability/utf8 runtime/arrays runtime/casting runtime/explicit runtime/int runtime/init runtime/invalid_increment runtime/member_string_references runtime/memset runtime/indentation_namespace runtime/operator runtime/printf runtime/printf_format runtime/references runtime/string runtime/threadsafe_fn runtime/vlog whitespace/blank_line whitespace/braces whitespace/comma whitespace/comments whitespace/empty_conditional_body whitespace/empty_if_body whitespace/empty_loop_body whitespace/end_of_line whitespace/ending_newline whitespace/forcolon whitespace/indent whitespace/line_length whitespace/newline whitespace/operators whitespace/parens whitespace/semicolon whitespace/tab whitespace/todo 这些选项还挺多的，一共有69项，但是现在有一个问题，我就一直没找到这些选项都代表什么含义，有些从名字可以推断出来，比如 whitespace/line_length 应该是指每行的长度限制，但是 whitespace/comma 单单从名字你知道他们是什么意思吗？所以我想简单总结一下。 一个小实验都说cpplint非常好用，那么接下来我们看看这个工具要怎么用，先新建一个文件teststyle，在里面随便写一些C++代码，如下： 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;map&gt;using namespace std;class Style&#123; public: void test() &#123; cout &lt;&lt; "This is style class" &lt;&lt; endl; &#125; void showName(string&amp; extraMsg) &#123; cout &lt;&lt; extraMsg &lt;&lt; className &lt;&lt; endl; &#125;public: string className;&#125;;int main()&#123; Style s; string msg_fjakdjfkadjfkadjffjadfkasdjffajsdfkadvljakdjfakdfjkadfjkasdjfkasdfj="class_name:"; s.showName( msg_fjakdjfkadjfkadjffjadfkasdjffajsdfkadvljakdjfakdfjkadfjkasdjfkasdfj ); if(s.className == "") &#123; cout &lt;&lt; "class name for s is empty." &lt;&lt; endl; &#125; return 0;&#125; 这段临时“发挥”的代码可以正常编译运行，然后用cpplint工具检测一下： 1234567891011121314151617181920&gt; cpplint .\teststyle.cpp.\teststyle.cpp:0: No copyright message found. You should have a line: &quot;Copyright [year] &lt;Copyright Owner&gt;&quot; [legal/copyright] [5].\teststyle.cpp:3: Do not use namespace using-directives. Use using-declarations instead. [build/namespaces] [5].\teststyle.cpp:6: &#123; should almost always be at the end of the previous line [whitespace/braces] [4].\teststyle.cpp:7: public: should be indented +1 space inside class Style [whitespace/indent] [3].\teststyle.cpp:8: Tab found; better to use spaces [whitespace/tab] [1].\teststyle.cpp:12: Is this a non-const reference? If so, make const or use a pointer: string&amp; extraMsg [runtime/references] [2].\teststyle.cpp:13: &#123; should almost always be at the end of the previous line [whitespace/braces] [4].\teststyle.cpp:16: public: should be indented +1 space inside class Style [whitespace/indent] [3].\teststyle.cpp:21: &#123; should almost always be at the end of the previous line [whitespace/braces] [4].\teststyle.cpp:23: Lines should be &lt;= 80 characters long [whitespace/line_length] [2].\teststyle.cpp:23: Missing spaces around = [whitespace/operators] [4].\teststyle.cpp:24: Lines should be &lt;= 80 characters long [whitespace/line_length] [2].\teststyle.cpp:24: Extra space after ( in function call [whitespace/parens] [4].\teststyle.cpp:24: Extra space before ) [whitespace/parens] [2].\teststyle.cpp:26: Missing space before ( in if( [whitespace/parens] [5].\teststyle.cpp:27: &#123; should almost always be at the end of the previous line [whitespace/braces] [4].\teststyle.cpp:32: Could not find a newline character at the end of the file. [whitespace/ending_newline] [5]Done processing .\teststyle.cppTotal errors found: 17 这么一小段代码居然报出了17个错误，厉不厉害？刺不刺激？下面来逐个解释一下： .\teststyle.cpp:0: No copyright message found. You should have a line: “Copyright [year] “ [legal/copyright] [5] [legal/copyright] 表示文件中应该有形如 Copyright [year] &lt;Copyright Owner&gt; 版权信息 \teststyle.cpp:3: Do not use namespace using-directives. Use using-declarations instead. [build/namespaces] [5] [legal/copyright] 表示第3行 using namespace std; 应该使用 using-declarations 而不要使用 using-directives，这个规则可以简单的理解为使用命名空间，每次只引用其中的成员，而不要把整个命名空间都引入。 .\teststyle.cpp:6: { should almost always be at the end of the previous line [whitespace/braces] [4] [whitespace/braces] 表示第6行的大括号应该放在上一行末尾 .\teststyle.cpp:7: public: should be indented +1 space inside class Style [whitespace/indent] [3] [whitespace/indent] 表示第7行 public: 应该在行首只保留一个空格 .\teststyle.cpp:8: Tab found; better to use spaces [whitespace/tab] [1] [whitespace/tab] 表示代码中第8行出现了Tab字符，应该使用空格代替 .\teststyle.cpp:12: Is this a non-const reference? If so, make const or use a pointer: string&amp; extraMsg [runtime/references] [2] [runtime/references] 表示代码第12行建议使用常引用 .\teststyle.cpp:23: Lines should be &lt;= 80 characters long [whitespace/line_length] [2] [whitespace/line_length] 表示代码第23行长度超过了80个字符 .\teststyle.cpp:23: Missing spaces around = [whitespace/operators] [4] [whitespace/operators] 表示代码第23行在赋值符号 = 前后应该有一个空格 .\teststyle.cpp:24: Extra space after ( in function call [whitespace/parens] [4] [whitespace/parens] 表示代码第24行在小括号后面出现了多余的空格 .\teststyle.cpp:26: Missing space before ( in if( [whitespace/parens] [5] [whitespace/parens] 表示代码第26行if后面缺少空格 .\teststyle.cpp:32: Could not find a newline character at the end of the file. [whitespace/ending_newline] [5] [whitespace/ending_newline] 表示32行，文件末尾应该是一个空行 按照上面cpplint提示修改代码如下： 12345678910111213141516171819202122232425262728293031// Copyright [2021] &lt;Copyright albert&gt;#include &lt;iostream&gt;#include &lt;map&gt;using std::cout;using std::endl;using std::string;class Style &#123; public: void test() &#123; cout &lt;&lt; "This is style class" &lt;&lt; endl; &#125; void showName(const string&amp; extraMsg) &#123; cout &lt;&lt; extraMsg &lt;&lt; className &lt;&lt; endl; &#125; public: string className;&#125;;int main() &#123; Style s; string msg = "class_name:"; s.showName(msg); if (s.className == "") &#123; cout &lt;&lt; "class name for s is empty." &lt;&lt; endl; &#125; return 0;&#125; 自己指定筛选规则有些人按照上面默认的规则修改代码之后感觉清爽了不少，而有些人却更加郁闷了，因为这些规则是google内部自己根据需要制定的，并不能满足所有人的需求，所以自己需要有目的的做出选择，比如我就决定项目中不写版权信息，那么再使用cpplint 时可以把检测版权信息的规则过滤掉：cpplint --filter=&quot;-legal/copyright&quot; .\teststyle.cpp。 对照表格总体来说规则还是很多的，想要在一段代码中展示出所有的情况不太容易，所以整理了下面的表格，对一些规则做了解释，因为有些情况我也没有遇到，所以先空着，后面再慢慢补充，这也是做这篇总结的目的，当有一种规则需求时先来查一下，越来越完整。 filter 解释 build/class build/c++11 build/c++14 build/c++tr1 build/deprecated build/endif_comment build/explicit_make_pair build/forward_decl build/header_guard ①头文件需要添加只被包含一次的宏，#ifndef、#define build/include build/include_subdir build/include_alpha build/include_order build/include_what_you_use build/namespaces_headers build/namespaces_literals build/namespaces ①不要引入整个命名空间，仅引入需要使用的成员 build/printf_format build/storage_class legal/copyright ①文件中缺少版权信息 readability/alt_tokens readability/braces ①如果if一个分支包含大括号，那么其他分支也应该包括大括号 readability/casting readability/check readability/constructors readability/fn_size readability/inheritance readability/multiline_comment readability/multiline_string readability/namespace readability/nolint readability/nul readability/strings readability/todo ①TODO注释中应包括用户名 readability/utf8 ①文件应该使用utf8编码 runtime/arrays runtime/casting runtime/explicit runtime/int runtime/init runtime/invalid_increment runtime/member_string_references runtime/memset runtime/indentation_namespace runtime/operator runtime/printf ①使用sprintf替换strcpy、strcat runtime/printf_format runtime/references ①确认是否要使用常引用 runtime/string runtime/threadsafe_fn runtime/vlog whitespace/blank_line whitespace/braces ①左大括号应该放在上一行末尾 whitespace/comma ①逗号后面应该有空格 whitespace/comments ①//后应该紧跟着一个空格 whitespace/empty_conditional_body whitespace/empty_if_body whitespace/empty_loop_body whitespace/end_of_line whitespace/ending_newline ①文件末尾需要空行 whitespace/forcolon whitespace/indent ①public、protected、private前需要1个空格 whitespace/line_length ①代码行长度有限制 whitespace/newline whitespace/operators ①操作符前后需要有空格 whitespace/parens ①if、while、for、switch后的小括号前需要有空格。②小括号中的首个参数前和最后参数尾不应有空格 whitespace/semicolon ①分号后缺少空格，比如{ return 1;} whitespace/tab ①使用空格代替tab whitespace/todo ①TODO注释前空格太多。②TODO注释中用户名后应该有一个空格 总结 cpplint 是一个检查c++代码风格的小工具 cpplint.py 其实是一个Python脚本文件，使用前可以先安装Python环境 使用 cpplint 时默认遵循的是Google的代码风格 为了让代码检测符合自己的习惯，需要使用--filter=参数选项，有多种规则可以选择或者忽略 --filter=中的规则是一个大类，比如 whitespace/parens 既检查小括号前缺少空格的情况，也会检查小括号中多空格的情况 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 生活中会有一些感悟的瞬间，娃娃哭闹时大人们总是按照自己的经验来出处理，碰上倔脾气小孩往往毫无作用。其实孩子是最单纯的，想要什么不想要什么都摆在脸上，愿望一旦被满足立马就不哭了，而大人才世界是难处理的，长大的人类善于隐藏和伪装，想要的不一定说出来，说出来不一定是想要的，所以很多人才会羡慕小孩子的天真和无邪~ 努力吧！哪管什么真理无穷，进一步有进一步的欢喜 2021-6-20 18:46:45]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>Python</tag>
        <tag>cpplint</tag>
        <tag>filter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++对我来说简直就是星辰大海，为了避免翻船，我选择从小河沟出发]]></title>
    <url>%2Fblog%2F2021%2F05%2F24%2FC-%E5%AF%B9%E6%88%91%E6%9D%A5%E8%AF%B4%E7%AE%80%E7%9B%B4%E5%B0%B1%E6%98%AF%E6%98%9F%E8%BE%B0%E5%A4%A7%E6%B5%B7%2F</url>
    <content type="text"><![CDATA[你学的越多，不懂的东西反而越多~ 前言以前觉得 C++ 并没有什么复杂的，不就是 C 语言加上类定义、模板、容器、算法函数这些就可以了吗，只要我不用，它就难不倒我，用到了查查文档也就搞定了，真的是年少轻狂啊。 随着学习的深入渐渐发现，即使抛开那些算法函数、那些冗长的模板，单单是 C++ 核心的概念和类型就够喝上好几壶的，随便罗列几个，像 std::furnitrue、std::memory_order、std::packaged_task 等等这些，之前都没听说过，特别是C++20的协程，到现在还是一头雾水。 C++ 缺少了 C 语言的纯粹，总是喜欢在编译时加点料，但是这个协程加的料超多，一时间还有点接受不了。 不过第一次听说协程这个词是在 Lua 中，全称被叫做协同程序，记得没错是在 《Lua程序设计》这本书中看到的，里面专门有一章是讲coroutine的，并且在 Lua 中定义和使用协程很方便，所以决定先复习一下 Lua 中的协程，然后对比着 C++的协程来进行拓展学习。 进程 vs 线程 vs 协程这三者常常被拿来比较，而引入多进程、多线程、多协程有一个简单而纯粹的目的，那就是榨干CPU，不过这三者侧重还有所不同。 进程是资源分配最小单位，每个进程都有独立的地址空间，来维护代码段、堆栈段和数据段，但是创建和切换进程的开销较大，可以在多台物理机和多核CPU上提高效率，依靠管道（pipe）、命名管道（named pipe/FIFO）、信号量（semophore）、消息队列（message queue）、信号（sinal）、共享内存（shared memory）、套接字（socket）、全双工管道等途径来进行通信。 线程是任务调度和执行的最小单位，没有独立的地址空间，但有独立的运行栈和程序计数器（PC），创建和切换线程的开销相比进程来说要小得多，线程之间通信更加方便，除了可以使用进程间通信的方式，还可以简单地通过共享全局变量，静态变量等进行通信，但是需要锁机制、信号量机制、信号机制来控制线程间互斥。 协程这个概念就比较迷了，其实它不像多进程和多线程那样可以在多核机器上提供并行的能力，而是侧重于相互协作共同完成某个任务，同一个线程中可以启动多个协程，但这些协程同一时刻只能有一个在运行。 协程其实可以看成是一个可以被随时停止和唤醒的函数，使用协程是为了在用户层面来控制调用逻辑，对比于多线程程序的线程调度完全看操作系统的心情的处境，多协程的程序就比较自主了，可以由开发者来控制函数执行顺序。 还有一个特性很重要，就是使用协程可以实现用“同步”的方式来写“异步”的代码，这一点不理解没关系，以后会慢慢明白的。说到这，不得不说一下关于同步和异步、阻塞和非阻塞这几个概念，它们常常被大家混在一起来说，实际上只是从不同维度来描述了一件事情，下面简单叙述下。 同步 vs 异步同步和异步指的是消息通信的机制，或者说得到结果的方式。 同步：调用函数后就能返回想要的结果，有点像去食堂买饭，自己去食堂付完钱（调用），饭（结果）就可以被拿回来了，这就是同步调用的方式，与返回结果的时间长短无关，得到结果之后直接执行后面的逻辑（吃饭）就可以了，所以同步的逻辑是最好写的。 异步：调用函数后并不能直接得到想要的结果，需要通过回调或者其他消息来通知，这就有点像定外卖了，打开APP选好饭菜输入地址（注册回调），开始付钱（调用），此时并不能直接得到饭（结果），而是一段时间之后，有外卖小哥将饭（结果）给你送来，这时才能执行后面的逻辑（吃饭）。 总结来说，需要自己取结果的就是同步，依靠别人送结果的就是异步。 阻塞 vs 非阻塞阻塞和非阻塞指的是程序在等待调用结果时的状态，强调在获得结果之前的表现。 阻塞：调用函数后由于不满足某种条件（比如读socket但是没有数据）被挂起，当条件满足（socket来数据了）时被唤醒，并将结果返回。 非阻塞：调用函数后如果不满足指定条件（比如读socket但是没有数据）不挂起，而是返回一个表示没有取到结果的值，你可以按照某种间隔再次调用函数，直到取到结果为止，当然你也可以调用一次就结束了。 总结来说，不满足条件时调用方被挂起就是阻塞调用，否则就是非阻塞调用。 协程学习C++的协程是暂时学不明白了，为了不翻车，我还是从熟悉的 Lua 入手，来举例说明什么是协程？有什么用？为什么这样用？弄明白以后再慢慢用 C++ 来实现相同的目的，毕竟 C++ 这一块需要实现的内容也有点多。 消费者-生产者提到 Lua 的协程就会想到 “消费者-生产者”的例子，网上关于这个的实现有特别多的版本，整体上来说大同小异，基本上都是 《C++程序设计》这本书中的内容，但是这一部分我看了很多遍，感觉这个例子并不太好。 123456789101112131415161718192021222324252627282930313233343536373839404142function receive(prod) -- 激活协同程序 local status,value = coroutine.resume(prod) return valueendfunction send(x) -- 挂起协同程序 coroutine.yield(x)endfunction producer() -- 生产者 return coroutine.create( -- 创建协同程序 function() while true do local x = io.read() -- 产生新值 send(x) end end )endfunction filter(prod) -- 过滤器 return coroutine.create( -- 创建协同程序 function() for line = 1, math.huge do local x = receive(prod) -- 激活协同程序来获取新值 x = string.format("%5d %s",line , x ) -- 过滤规则 send(x) -- 挂起激活程序 end end )endfunction consumer(prod) while true do local x = receive(prod) -- 获取新值 io.write(x, "\n") -- 消费新值 endendp= producer() -- 初始化生产者f = filter(p) -- 初始化过滤器consumer(f) -- 初始化消费者并启动程序 这就是一个消费者驱动的模型，首先由启动消费者，然后调用生产者来生产资源，接着消费者消耗掉新的资源，再控制生产者生产新的资源，以此方式循环往复，其实就是下面代码的复杂化： 12345678function consumer_producer() while true do local x = io.read() -- 产生新值 io.write(x, "\n") -- 消费新值 endendconsumer_producer() -- 启动生产者消费者 这个例子以我现在的菜鸟水平来看没啥用，但是有一点比较好，就是展示了可以用协程来控制程序执行顺序的强大功能，只是这个消费者和生产者强耦合的设计实在是看不明白。 自己想个例子既然他们的例子我都不喜欢，那我就自己想一个，叮铃铃！下面我收到了一个新的需求： 计算1+2+3+4+5+6+7+8+9+10的和，然后等待5秒钟后，将结果显示在控制台上。 乍一听，这个需求太简单了吧，没有一点难度，其实不然，其中蕴含着大量玄机，简直就是一个万能句式： 做一件事情A，然后等待某件事发生，再做一件事情B（可能与A相关） 仔细想想，这样的“句式”在开发中，生活中是不是经常出现？ 下载电影，下载完成后，播放电影 开始加载场景，加载完成后，隐藏加载进度条 发送一个请求，收到回复时，将回复结果显示出来 … 看了吧，现实中有很多这类需求，我们接下来尝试着实现一下 常规写法123456789101112131415161718192021222324252627-- lua 没有 sleep 函数，使用while循环模拟function sleep(n) local t = os.clock() while os.clock() - t &lt;= n do endendfunction task_method_1() print(string.format("program start at %s", os.date("%H:%M:%S"))) -- 求和 local sum = 0; for i=1,10 do sum = sum + i; end -- 等待 sleep(5); -- 展示 print(string.format("program end at %s and sum = %d", os.date("%H:%M:%S"), sum))endfunction main1() task_method_1()endmain1() 代码很简单，为了看起来更连贯这里就不分段展示了，首先模拟一个 sleep 函数，然后实现 task_method_1 函数来完成原始需求——求和、等待、展示，最后通过主函数来调用就可以了。 运行结果如下: program start at 01:30:27program end at 01:30:32 and sum = 55 进阶写法看了上面的代码有没有发现什么问题？这是一种同步的实现方式，整个程序在中间等待的5秒钟什么都不能做，必须等倒计时结束才能做后面的事情，这要是购物APP点了5秒没反应就直接X掉了，这可是赤果果的金钱损失啊，绝不能让这种事情发生。 怎么办呢？我确实需要5秒钟的处理时间，但是又不能让用户卡在那，我可以显示一个进度条，进度一直再变化，用户就不会以为程序卡死了，如果进度走的比较慢，他可能以为手机老旧该换了，没准还促进了手机的销量呢！ 顺着这个思路写出了下面这种实现，这是一种异步的实现方式，通过回调函数来通知最终要显示的结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445function task_method_2() print(string.format("program start at %s", os.date("%H:%M:%S"))) -- 求和 local sum = 0; for i=1,10 do sum = sum + i; end -- 注册回调函数，进行等待 add_callback(5, call_back_print, sum)endfunction call_back_print(data) --展示结果 print(string.format("program end at %s and sum = %d", os.date("%H:%M:%S"), data))endfunction add_callback(inteval, func, data) interval_time = inteval call_back = func msg_data = dataendfunction main2() local t0 = os.clock(); local t = t0; task_method_2() while true do local now = os.clock() if now - t &gt;= 1 then print(string.format("program run %f seconds", now - t0)) t = now; if interval_time and call_back and now - t0 &gt;= interval_time then call_back(msg_data) break; end end endendmain2() 在函数 task_method_2 中计算完求和的结果，并没有等待，而是通过 add_callback 函数注册了等待时间、回调函数、以及回调展示的结果，然后直接返回了调用方，调用主函数 main2 中计算这时间差并展示进度，等倒计时一结束就执行回调函数，进而展示出结果。 运行结果如下，通过打印信息展示处理进度条: program start at 01:44:56program run 1.000000 secondsprogram run 2.001000 secondsprogram run 3.001000 secondsprogram run 4.001000 secondsprogram run 5.001000 secondsprogram end at 01:45:01 and sum = 55 协程写法卡顿的问题解决了，但是添加了一大堆额外的注册和回调函数，有些麻烦啊，怎么把它们去掉呢？ 终于等到协程出场了，同步调用很卡、异步回调很烦，那么协程可以实现用“同步”的方式来写“异步”的代码，既不卡也不烦，下面来看一下实现。 123456789101112131415161718192021222324252627282930313233343536373839function task_method_3() print(string.format("program start at %s", os.date("%H:%M:%S"))) -- 求和 local sum = 0; for i=1,10 do sum = sum + i; end -- 等待 coroutine.yield(5); -- 展示 print(string.format("program end at %s and sum = %d", os.date("%H:%M:%S"), sum))endfunction main3() local t0 = os.clock(); local t = t0; local co = coroutine.create(task_method_3) local status, interval = coroutine.resume(co) while true do local now = os.clock() if now - t &gt;= 1 then print(string.format("program run %f seconds", now - t0)) t = now; if now - t0 &gt;= interval then coroutine.resume(co) break; end end endendmain3() 对比 task_method_3 和 task_method_1 函数，只是将 sleep 函数换成了 coroutine.yield(5)，整个需求函数很紧凑。 程序运行逻辑是这样的，先将 task_method_3 函数包装成协程 co，然后启动 co 执行求和逻辑，执行到 coroutine.yield(5); 这句，协程被暂停并将5返回，主函数 main3 中收到返回值5后开始计时并展示进度值，直到5秒等待期结束再次唤醒协程 co，coroutine.yield(5); 后面的代码继续执行，完成最后的展示需求。 运行结果如下： program start at 01:50:59program run 1.000000 secondsprogram run 2.000000 secondsprogram run 3.000000 secondsprogram run 4.000000 secondsprogram run 5.000000 secondsprogram end at 01:51:04 and sum = 55 总结 多进程/多线程的引入并不是总能降低任务消耗的时间，还要考虑到进程/线程切换的消耗问题，参考Redis实现 多协程的引入本质上是为了更好的控制程序运行的逻辑，虽然它往往也能带来效率上的提升 coroutine.yield 是协程的中核心函数，主动让出CPU，如果协程不自己挂起，外部无法干预 知识的迁移是一项重要的技能，下一步要用C++协程来实现这个需求啦，边学边写喽 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 拨开那一片云，是你未曾实现的梦想，岁月流转，梦想在变，有些事不得不放弃坚守（固执），珍惜眼前的一切，迎接明天的朝阳~ 2021-5-28 00:27:18]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>C++20</tag>
        <tag>coruntine</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用AddressSanitizer搭配addr2line查找C/C++内存泄漏问题]]></title>
    <url>%2Fblog%2F2021%2F05%2F15%2F%E4%BD%BF%E7%94%A8AddressSanitizer%E6%90%AD%E9%85%8Daddr2line%E6%9F%A5%E6%89%BEC-C-%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言指针是C/C++程序中的利器，同时也引入了风险，现代C++中增加了智能指针来降低使用“裸”指针带来的风险，但是智能指针不是一颗银弹，它不能解决所有的指针问题，内存泄漏在C/C++程序开发中依旧是值得注意的，学会合理、合适的方法来查找内存泄漏问题也是一项有用的技能。 通常内存泄漏问题会在开发到一定程度时集中检查，一些检测方法长时间不去使用难免会忘记，所以本文记录一种自己常用的检测方法，方便日后查阅。 AddressSanitizerAddressSanitizer 是什么东西呢？从名字上直接翻译叫“地址消毒剂”，其实就是用来检查地址问题的。 它是一款地址问题检测工具，简称 ASAN，开源项目主地址为 google/sanitizers，是众多检测工具AddressSanitizer, MemorySanitizer, ThreadSanitizer, LeakSanitizer 中的一款，功能非常强大，可以检测出栈上缓冲区溢出、堆上缓冲区溢出、引用已释放内存、内存泄漏等多种地址问题。 今天想记录的是使用 AddressSanitizer 检测内存泄漏的步骤，其实检测内存泄漏的功能目前已经被基本独立成了 LeakSanitizer，不过仍可以通过在 AddressSanitizer 工具中通过参数来开启和关闭使用。 检测步骤其实使用 ASAN 检测内存泄漏还是比较简单的，g++4.8 以上的版本自带了 ASAN 工具，只要编译时指定好参数，编译完成后正常启动运行程序就可以了，只不过有些情况下只从检测报告中无法准确定位问题，需要借助一些工具进一步缩小检测范围。 泄漏发生在可执行程序本身这种情况检测起来比较容易，编写如下测试代码： 12345678910111213141516//test.cpp#include &lt;iostream&gt;void func()&#123; int* p = new int(); // 内存泄漏的位置 p = nullptr;&#125;int main()&#123; func(); std::cout &lt;&lt; "test leak" &lt;&lt; std::endl; return 0;&#125; 使用g++进行编译，编译时添加参数 -fsanitize=leak 就可以了，启动后可以清晰的展示出内存泄漏的位置 test.cpp:5，也就是 test.cpp 文件的第5行。 123456789101112131415albert@home-pc:/mnt/d/data/cpp/testleak$ g++ test.cpp -g -o test --std=c++11 -fsanitize=leakalbert@home-pc:/mnt/d/data/cpp/testleak$ ./testtest leak===================================================================344==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4 byte(s) in 1 object(s) allocated from: #0 0x7fc7d796d815 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd815) #1 0x400967 in func() /mnt/d/data/cpp/testleak/test.cpp:5 #2 0x400985 in main /mnt/d/data/cpp/testleak/test.cpp:11 #3 0x7fc7d722083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 4 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 这里有个小意外，将 int* p = new int(); 这句代码改成 int* p = new int[10]; 可以检测出内存泄漏如下： 123456789101112131415albert@home-pc:/mnt/d/data/cpp/testleak$ g++ test.cpp -fsanitize=leak -g -o test --std=c++11albert@home-pc:/mnt/d/data/cpp/testleak$ ./testtest leak===================================================================416==ERROR: LeakSanitizer: detected memory leaksDirect leak of 400 byte(s) in 1 object(s) allocated from: #0 0x7fdc0c16d975 in operator new[](unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd975) #1 0x400967 in func() /mnt/d/data/cpp/testleak/test.cpp:5 #2 0x40097f in main /mnt/d/data/cpp/testleak/test.cpp:11 #3 0x7fdc0ba2083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 400 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 但是将 int* p = new int(); 这句代码改成 int* p = new int[1024]; 就无法检测是内存泄漏了，只能修改编译选项为 -fsanitize=address 才能检测出泄漏，目前还不知道真正的原因是什么。 123456789101112131415albert@home-pc:/mnt/d/data/cpp/testleak$ g++ test.cpp -fsanitize=address -g -o test --std=c++11albert@home-pc:/mnt/d/data/cpp/testleak$ ./testtest leak===================================================================432==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4096 byte(s) in 1 object(s) allocated from: #0 0x7f1d42b296b2 in operator new[](unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x996b2) #1 0x400b27 in func() /mnt/d/data/cpp/testleak/test.cpp:5 #2 0x400b3f in main /mnt/d/data/cpp/testleak/test.cpp:11 #3 0x7f1d4235083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: AddressSanitizer: 4096 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 泄漏发生在编译所需动态库中如果内存泄漏发生在编译时使用的动态库中，那么这和上一种情况基本一致，可以直接编译后运行就能发现，测试代码如下 123// myadd.hint add(int a, int b); 123456789// myadd.cpp#include "myadd.h"int add(int a, int b)&#123; int* p = new int(); // 内存泄漏的位置 return a + b;&#125; 12345678910// test.cpp#include "myadd.h"#include &lt;iostream&gt;int main()&#123; std::cout &lt;&lt; "519 + 1 = " &lt;&lt; add(519, 1) &lt;&lt; std::endl; return 0;&#125; 添加编译选项 -fsanitize=leak 编译后运行，也可以直接显示出内存泄漏的位置，内存泄漏在 libmyadd.so 动态库中的 add 函数中。 12345678910111213141516albert@home-pc:/mnt/d/data/cpp/testleak$ g++ -shared -fPIC -o libmyadd.so myadd.cppalbert@home-pc:/mnt/d/data/cpp/testleak$ g++ test3.cpp -L. -lmyadd -o test -Wl,-rpath=. -fsanitize=leakalbert@home-pc:/mnt/d/data/cpp/testleak$ ./test519 + 1 = 520===================================================================493==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4 byte(s) in 1 object(s) allocated from: #0 0x7ff1aff6d815 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd815) #1 0x7ff1afd506b7 in add(int, int) (libmyadd.so+0x6b7) #2 0x4009cd in main (/mnt/d/data/cpp/testleak/test+0x4009cd) #3 0x7ff1af61083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 4 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 泄漏发生在自定义加载的动态库中这种情况要想精确定位问题就麻烦一些了，下面是用来测试的代码 123// myadd.hextern "C" int add(int a, int b); 123456789// myadd.cpp#include "myadd.h"extern "C" int add(int a, int b)&#123; int* p = new int(); // 内存泄漏的位置 return a + b;&#125; 12345678910111213141516171819// test.cpp#include "myadd.h"#include &lt;dlfcn.h&gt;#include &lt;iostream&gt;typedef int (*FUNC)(int a,int b);int main() &#123; void* handle = dlopen("./libmyadd.so", RTLD_LAZY); FUNC myadd = (FUNC)dlsym(handle,"add"); int nVal = 0; std::cin &gt;&gt; nVal; std::cout &lt;&lt; "519 + 1 = " &lt;&lt; myadd(519, 1) &lt;&lt; ", input:" &lt;&lt; nVal &lt;&lt; std::endl; dlclose(handle); return 0;&#125; 添加编译选项 -fsanitize=leak 编译后运行，输入数字618，程序运行结束，显示内存泄漏出现在 0x7fc88f0f06b7 (&lt;unknown module&gt;) 1234567891011121314151617albert@home-pc:/mnt/d/data/cpp/testleak$ g++ -shared -fPIC -o libmyadd.so myadd.cpp -galbert@home-pc:/mnt/d/data/cpp/testleak$ g++ test.cpp -ldl -o test -Wl,-rpath=. -g -fsanitize=leakalbert@home-pc:/mnt/d/data/cpp/testleak$ ./test618519 + 1 = 520, input:618===================================================================817==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4 byte(s) in 1 object(s) allocated from: #0 0x7fc89076d815 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd815) #1 0x7fc88f0f06b7 (&lt;unknown module&gt;) #2 0x400bc2 in main (/mnt/d/data/cpp/testleak/test+0x400bc2) #3 0x7fc88fe1083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 4 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ unknown module当使用 dlopen 的方式加载的动态库时，产生的内存泄漏常显示为 (&lt;unknown module&gt;)，那是因为内存检测工具在程序退出时分析泄漏情况，而这时自定义加载的动态库往往已经手动调用 dlclose 关闭了，这时就会显示成 0x7fc88f0f06b7 (&lt;unknown module&gt;) 的显示。 maps针对于出现 (&lt;unknown module&gt;) 的这种情况，可以通过查询 /proc/pid/maps 来辅助查询，maps 文件显示进程映射后的内存区域和访问权限，是程序正在运行时的信息，数据格式如下： 12345678910111213141516171819207f8c7adb6000-7f8c7adba000 rw-p 00000000 00:00 07f8c7adc0000-7f8c7af32000 r-xp 00000000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7af32000-7f8c7af3f000 ---p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7af3f000-7f8c7b132000 ---p 0017f000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7b132000-7f8c7b13c000 r--p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7b13c000-7f8c7b13e000 rw-p 0017c000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7b13e000-7f8c7b142000 rw-p 00000000 00:00 07f8c7b150000-7f8c7b153000 r-xp 00000000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b153000-7f8c7b154000 ---p 00003000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b154000-7f8c7b352000 ---p 00004000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b352000-7f8c7b353000 r--p 00002000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b353000-7f8c7b354000 rw-p 00003000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b360000-7f8c7b39f000 r-xp 00000000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b39f000-7f8c7b3a2000 ---p 0003f000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b3a2000-7f8c7b59e000 ---p 00042000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b59e000-7f8c7b5a0000 r--p 0003e000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b5a0000-7f8c7b5a1000 rw-p 00040000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b5a1000-7f8c7c1f4000 rw-p 00000000 00:00 07f8c7c200000-7f8c7c225000 r-xp 00000000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7f8c7c225000-7f8c7c226000 r-xp 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so 第一列：7f8c7b360000-7f8c7b39f000，表示本段内存映射的虚拟地址空间范围。 第二列：r-xp，表示此段虚拟地址空间的属性。r表示可读，w表示可写，x表示可执行，p和s共用一个字段，互斥关系，p表示私有段，s表示共享段，-表示没有权限。 第三列：00000000，表示映射偏移。对有名映射，表示此段虚拟内存起始地址在文件中以页为单位的偏移。对匿名映射，它等于0或者vm_start/PAGE_SIZE。 第四列：00:00，表示映射文件所属设备号。对有名映射来说，是映射的文件所在设备的设备号，对匿名映射来说，因为没有文件在磁盘上，所以没有设备号，始终为00:00。 第五列：247365，表示映射文件所属节点号。对有名映射来说，是映射的文件的节点号。对匿名映射来说，因为没有文件在磁盘上，所以没有节点号，始终为0。第六列：/usr/lib/x86_64-linux-gnu/liblsan.so.0.0.0，表示映射文件名或堆、栈。对匿名映射来说，是此段虚拟内存在进程中的角色。[stack]表示在进程中作为栈使用，[heap]表示堆。对有名来说，是映射的文件名。其余情况则无显示。 7f8c7b360000-7f8c7b39f000 r-xp 00000000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.0 这一行就展示了 liblsan.so 这个动态库映射的内存中位置和权限情况，liblsan.so 也就是 ASAN 工具用来检测内存泄漏的工具所依赖的动态库。 具体操作 启动一个终端，然后运行 test 程序，因为程序中要求从控制台读取一个变量，所以运行后程序会一直停留在控制台等待输入 1albert@home-pc:/mnt/d/data/cpp/testleak$ ./test 重新打开一个终端，查询 test 程序的进程id，然后拷贝对应的 maps 文件 1234albert@home-pc:/mnt/d/data/cpp/testleak$ ps -ef | grep testalbert 986 889 0 21:42 pts/0 00:00:00 ./testalbert 988 953 0 21:42 pts/1 00:00:00 grep --color=auto testalbert@home-pc:/mnt/d/data/cpp/testleak$ cp /proc/986/maps testmaps 在第一个终端中输入数字，程序运行结束，显示出内存泄漏信息 123456789101112131415161718albert@home-pc:/mnt/d/data/cpp/testleak$ ./test# 以下为新的信息，输入了515515519 + 1 = 520, input:515===================================================================986==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4 byte(s) in 1 object(s) allocated from: #0 0x7f8c7b36d815 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd815) #1 0x7f8c79cf06b7 (&lt;unknown module&gt;) #2 0x400bc2 in main (/mnt/d/data/cpp/testleak/test+0x400bc2) #3 0x7f8c7aa1083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 4 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 从检测报告中看到 0x7f8c79cf06b7 (&lt;unknown module&gt;)，在备份的 testmaps 文件中查找范围，发现处于下面一段之中 1234567640000000000-640000003000 rw-p 00000000 00:00 07f8c79cf0000-7f8c79cf1000 r-xp 00000000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79cf1000-7f8c79cf2000 ---p 00001000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79cf2000-7f8c79ef0000 ---p 00002000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79ef0000-7f8c79ef1000 r--p 00000000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79ef1000-7f8c79ef2000 rw-p 00001000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79f00000-7f8c7a000000 rw-p 00000000 00:00 0 至此发现问题出现在 libmyadd.so 这个动态库中，再用 0x7f8c79cf06b7 减去动态链接库基地址 7f8c79cf0000，得到偏移量为 0x6b7，此时使用 addr2line 工具进行转化。 123albert@home-pc:/mnt/d/data/cpp/testleak$ addr2line -C -f -e /mnt/d/data/cpp/testleak/libmyadd.so 0x6b7add/mnt/d/data/cpp/testleak/myadd.cpp:4 至此就找到了内存泄漏的确切位置，在/mnt/d/data/cpp/testleak/myadd.cpp文件第4行的 add 函数之中。 总结 在 C++11 之后尽可能使用智能指针来管理在堆上申请的内存，shared_ptr、weak_ptr、unique_ptr 能帮我们减少许多麻烦 想要检测程序内存用用问题， AddressSanitizer 是一个不错的选择，其中有关内存泄漏的检测已经被整合到 LeakSanitizer 工具中 当程序中的内存泄漏发生在 dlopen 加载的动态库中时，常常出现 (&lt;unknown module&gt;) 的情况，这时需要借助 proc/pid/maps 文件和 addr2line 工具来完成精确定位。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 祝融落地。一百年了，还没有什么事情是做不到的，我们需要的是时间，我等着看你们在真正的力量面前瑟瑟发抖~ 2021-5-16 00:08:24]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>AddressSanitizer</tag>
        <tag>addr2line</tag>
        <tag>内存泄漏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux文件权限简单备忘知识点]]></title>
    <url>%2Fblog%2F2021%2F04%2F30%2Flinux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E7%AE%80%E5%8D%95%E5%A4%87%E5%BF%98%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[前言已经拖更好几天了，有些坚持的事情一旦中断便很难再捡起来了，所以还是尽可能的坚持下去，今天就简单记录一下文件权限相关的操作。 这里先扔出一张图，这么懒的我居然画了一张图！不过前几天感冒状态不太好，这张图画了好几晚上才完成，就凑活看吧。 基本上看注释就能明白各列的含义，关于“文件硬链接数”这一列还有点疑问，在CentOS上测试，确实如注释所言，普通文件显示的硬链接数，目录文件显示的是第一级子目录的个数，但是在WSL上却不符合这个说法，暂时还没有找到原因，接下来还是列举下文件权限相关的操作吧。 基础概念在linux系统中的每个用户必须属于一个组，不能独立于组外。而每个文件的权限区分所有者、所在组、其它人，就像图片中展示的那样 所有者：指创建文件的人，拥有第一组权限 所在组：指的是和所有者在同组的人，拥有第二组权限 其他人：指排除掉前两类的其他人，拥有第三组权限 每组权限都分为读、写、执行三种具体权限，对应数字分别是4、2、1，字母表示为r、w、x，没有权限可以用-表示，也就是0，因为有这些表示方法，修改文件的命令形式也有很多种。 一个文件的访问权限是可以进行修改的，用户可以使用 chmod 命令来重新设定不同的访问权限，可以使用 chown 命令来更改某个文件或目录的所有者，也可以利用 chgrp 命令来更改某个文件或目录的用户组。 chmod 命令上面说到文件的权限分为所有者、同组人、其他人三组，每组权限又有r、w、x三种权限，分别用数字4、2、1表示，举几个对照的例子如下 123rwx r-x r–x 755rw- r–- –-- 640rw- rw- r–- 664 因为表示的多样性，该命令有通常有字母表达式和数字表达式两种用法，格式如下: 1chmod ［who］ ［+ | – | =］ ［mode］ filename 命令中各选项的含义为： 命令中的 who 可是下述字母中的任一个或者它们的组合： u 表示所有者user g 表示同组用户group o 表示其他人other a 表示所有用户all，是系统默认值 权限改变的符号可以是： + 添加某个权限 – 取消某个权限 = 赋予给定权限并取消其他所有权限（如果有的话） 命令中的 mode 所表示的权限可用下述字母的任意组合： r 可读权限 w 可写权限 x 可执行权限 X 只有目标文件对某些用户是可执行的或该目标文件是目录时才追加 x 属性 u 与文件属主拥有一样的权限 g 与和文件属主同组的用户拥有一样的权限 o 与其他用户拥有一样的权限 常见写法如下： 123$ chmod a+x happy.txt$ chmod ug+w file.xml$ chmod 755 a.out chgrp 命令该命令能改变文件或目录所属的组，语法如下： 1chgrp ［option］ group filename option参数可选： -f,–quiet,–silent: 不显示错误信息 -R,–recursive: 递归处理，将指定目录下的所有文件及子目录一并处理 -v,–verbose: 显示指令执行过程 命令中group可以是用户组ID，也可以是/etc/group文件中用户组的组名。文件名是以空格分开的要改变属组的文件列表，支持通配符。如果用户不是该文件的属主或超级用户，则不能改变该文件的组的。 1$chgrp -R mysql /opt/local/my.ini chown 命令该命令可以更改某个文件或目录的属主，也就是所有者。语法如下： 1chown ［option］ groupname|username filename chown可以将指定文件的拥有者改为指定的用户或组。用户可以是用户名或用户ID。组可以是组名或组ID。与 chgrp命令一样，参数文件也是以空格分开的要改变权限的文件列表，支持通配符。 option参数可选： -f: 若该文件拥有者无法被更改也不要显示错误讯息 -h: 只对于连结(link)进行变更，而非该 link 真正指向的文件 -v: 显示拥有者变更的详细资料 -R: 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更) 修改文件所有者的例子如下： 1$ chown redis /ect/redis/redis.config 总结 ls -l 命令执行后的第5列，表示文件的硬连接数 chmod a+x happy.txt 可以修改文件的权限信息 chgrp a+x fruit.txt 可以修改文件的所有组信息 chown redis /ect/redis/redis.config 可以修改文件的所有者信息 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 鲁迅先生说“我向来是不惮以最坏的恶意，来推测中国人的，然而我还不料，也不信竟会下劣凶残到这地步。”其实黑暗中往往也透露着光明~ 2021-5-1 01:26:36]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>chmod</tag>
        <tag>文件</tag>
        <tag>权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[protobuf中SerializeToString和SerializePartialToString的区别]]></title>
    <url>%2Fblog%2F2021%2F04%2F18%2Fprotobuf%E4%B8%ADSerializeToString%E5%92%8CSerializePartialToString%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言protobuf是Google提出的序列化方案，此方案独立于语言和平台，目前提供了如c++、go、python等多种语言的实现，使用比较广泛，具有性能开销小，压缩率高等优点，是值得学习的优秀开源库。 protobuf有 v2 和 v3 两个主要的并且差异很大的版本，有一些关于protobuf的文章中并没有说明版本，有些描述的内容给人造成了疑惑，所以在使用protobuf前要明确自己使用的版本，查找对应的特性。 proto2这个版本在编写 .proto 文件时的字段有三种限定符，分别是required、optional 和 repeated。 required：必须设置该字段，如果是在debug模式下编译 libprotobuf，则序列化一个未初始化（未对required字段赋值）的 message 将导致断言失败。在release模式的构建中，将跳过检查并始终写入消息，但解析未初始化的消息将返回false表示失败。 optional：可以设置也可以不设置该字段。如果未设置可选字段值，则使用默认值，也可以用[default = value]进行设置。 repeated：该字段可以重复任意次数（包括零次），可以将 repeated 字段视为动态大小的数组。 message定义定义一个简单的 message 结构如下： 123456message Person &#123; required string name = 1; optional string email = 2; optional int age = 3 [default = 18]; repeated bytes phones = 4;&#125; 观察 message 定义可以看到每个字段后面都有 = 1、= 2 的标记，这些被称为 Tags，在 protobuf 中同一个 message 中的每个字段都需要有独一无二的tag，tag 为 1-15 的是单字节编码，16-2047 使用2字节编码，所以1-15应该给频繁使用的字段。 关于tag的取值，还有一种范围是[1,536870911]的说法，同时 19000 到 19999 之间的数字也不能使用，因为它们是 protobuf 的实现中保留的，也就是 FieldDescriptor::kFirstReservedNumber 到 FieldDescriptor::kLastReservedNumber 指定的范围，如果使用其中的数字，导出 .proto 文件时会报错，此处存疑，需要验证一下。 message扩展在使用的了 protobuf 的项目发布以后，绝对会遇到扩展原有 message 结构的需求，这一点不可避免，除非发布后的项目不再升级维护了，要想扩展就需要兼容之前的代码逻辑，这里有一些必须遵守的规则，否则就达不到兼容的目的。 不能更改任何现有字段的 tag 不能添加或删除任何 required 字段 可以删除 optional 或 repeated 的字段 可以添加新的 optional 或 repeated 字段，但必须使用新的tag，曾经使用过又删除的 tag 也不能再使用了 注意事项proto2 中对 required 的使用永远都应该非常小心。如果想在某个时刻停止写入或发送 required 字段，直接将字段更改为可选字段将会有问题。一些工程师得出的经验是，使用 required 弊大于利，他们更喜欢只使用 optional 和 repeated。 proto3proto3 比 proto2 支持更多语言但更简洁，去掉了一些复杂的语法和特性。 在第一行非空白非注释行，必须写：syntax = &quot;proto3&quot;; 直接从语法层面上移除了 required 规则，取消了 required 限定词 增加了对 Go、Ruby、JavaNano 等语言的支持 移除了 default 选项，字段的默认值只能根据字段类型由系统决定 序列化将 message 结构对象序列化的函数有很多，即使是序列化成字符串也有多个函数可以使用，比如 SerializeToString、SerializePartialToString、SerializeAsString、SerializePartialAsString 等等。 SerializeToString和SerializeAsString区别这两个还是很好区分的，从源码角度一眼就能够分辨出来： 1234567891011121314std::string MessageLite::SerializeAsString() const &#123; // If the compiler implements the (Named) Return Value Optimization, // the local variable 'output' will not actually reside on the stack // of this function, but will be overlaid with the object that the // caller supplied for the return value to be constructed in. std::string output; if (!AppendToString(&amp;output)) output.clear(); return output;&#125;bool MessageLite::SerializeToString(std::string* output) const &#123; output-&gt;clear(); return AppendToString(output);&#125; 从源代码可以很容易看出，两者仅仅是参数和返回值的类型不同，其内部调用的函数都是一样的，SerializePartialToString 和 SerializePartialAsString 两个函数也是这种区别，可以根据外部逻辑所需来调用合适的函数。 12345678910bool MessageLite::SerializePartialToString(std::string* output) const &#123; output-&gt;clear(); return AppendPartialToString(output);&#125;std::string MessageLite::SerializePartialAsString() const &#123; std::string output; if (!AppendPartialToString(&amp;output)) output.clear(); return output;&#125; SerializeToString和SerializePartialToString区别这两个函数的区别在于内部调用的函数不同，一个调用 AppendToString，另一个调用 AppendPartialToString，两个被调用函数的源代码如下： 12345678910111213bool MessageLite::AppendToString(std::string* output) const &#123; GOOGLE_DCHECK(IsInitialized()) &lt;&lt; InitializationErrorMessage("serialize", *this); return AppendPartialToString(output);&#125;bool MessageLite::AppendPartialToString(std::string* output) const &#123; size_t old_size = output-&gt;size(); size_t byte_size = ByteSizeLong(); if (byte_size &gt; INT_MAX) &#123; GOOGLE_LOG(ERROR) &lt;&lt; GetTypeName() &lt;&lt; " exceeded maximum protobuf size of 2GB: " &lt;&lt; byte_size; return false;&#125; 原来 AppendToString 函数调用了 AppendPartialToString, 只是在调用之前先执行了一句 GOOGLE_DCHECK(IsInitialized()) &lt;&lt; InitializationErrorMessage(&quot;serialize&quot;, *this); 这句话什么意思呢？ 其实就是一个调试状态下的检查，类似于 assert 这个断言函数吧，检查的内容是判断这个 message 是否初始化，之前提到 required 修饰的字段必须要设置一个值，否者就是未初始化的状态，那么现在两个函数的区别就知道了，带有 “Partial” 函数其实是忽略 required 字段检查的，另外还有没有别的不同需要再进一步研究下源码了。 总结 protobuf有 v2 和 v3 两个主要的并且差异较大的版本，使用前请注意版本号 proto3 直接从语法层面上移除了 required 规则，移除了 default 选项，字段的默认值只能根据字段类型由系统决定 SerializeToString和SerializeAsString区别在于参数和返回值的不同，内部调用的函数是相同的 SerializeToString和SerializePartialToString区别在于SerializePartialToString会忽略 required 字段必须赋值的要求 在应用过程中尽可能重用 message 结构，这样protobuf内部实现中内存的重用 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 没有什么捷径，继续努力就好了短期内不会看到什么成果，甚至说一辈子都可能看不到，但只有努力了才有可能看得到 2021-4-18 21:31:34]]></content>
      <categories>
        <category>protobuf</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>protobuf</tag>
        <tag>序列化</tag>
        <tag>required</tag>
        <tag>message</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中一些可以在偷懒时直接使用的函数]]></title>
    <url>%2Fblog%2F2021%2F04%2F11%2FC-%E4%B8%AD%E4%B8%80%E4%BA%9B%E5%8F%AF%E4%BB%A5%E5%9C%A8%E5%81%B7%E6%87%92%E6%97%B6%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言在解决一些算法题时，会遇到一些“嵌套”问题，也就是一个题目中包含多个小的算法知识点，比如计算一个整数的二进制表示中1的个数，或者计算两个数的最大公约数，如果这些小问题本身就是题目，那么就只能“手撕”了。 但是如果这些内容只是解决题目中的一小部分，我们其实是可以偷个懒的，有很多函数已经被纳入函数库，可以直接拿过来使用，接下来我们可以简单看几个。 求解最大公约数自定义实现求最大公约数的一种常用方法叫做辗转相除法，又名欧几里德算法(Euclidean algorithm)，算法本身并不复杂，可以写成如下逻辑实现： 12345678int my_gcd(int x, int y) &#123; while (y != 0) &#123; int z = x % y; x = y; y = z; &#125; return x;&#125; 或者简单写成递归的实现： 123int my_gcd(int x, int y) &#123; return y ? my_gcd(y, x%y) : x;&#125; 因为计算机处理加减法的性能要远高于计算乘除法，所以辗转相除法有很多变形实现，比如辗转相减、用移位运算代替除法计算等。 库函数其实在C++17中，最大公约数计算已经被加到了函数库中，头文件为 &lt;numeric&gt;，直接调用 std::gcd() 就可以了，本身是一个模板函数，定义如下： 12template&lt; class M, class N&gt;constexpr std::common_type_t&lt;M, N&gt; gcd(M m, N n); 计算一个整数的二进制表示中有多少个1自定义实现这也是一道经典的算法题了，常见的实现如下： 12345678int count1(int n) &#123; int cnt = 0; while (n &gt; 0) &#123; cnt++; n &amp;= (n-1); &#125; return cnt;&#125; 这种实现方法不能说最优解法，但是也算的上是一个优秀的实现思路了。 内建函数关于二进制的形式的各种操作，GCC提供了一系列的builtin函数，可以实现一些简单快捷的功能来方便程序编写，并且可用来优化编译结果。 __builtin_popcount12// 返回n的二进制表示形式中1的个数int __builtin_popcount(unsigned int n) __builtin_ffs12// 返回n的二进制表示形式中最后一位1的是从后向前第几位int __builtin_ffs(unsigned int n) __builtin_clz12// 返回n的二进制表示形式中前导0的个数int __builtin_clz(unsigned int n) __builtin_ctz12// 返回n的二进制表示形式中结尾0个个数int __builtin_ctz(unsigned int n) __builtin_parity12// 返回n的奇偶校验位，即n的二进制表示形式中的1的个数模2的结果int __builtin_parity(unsigned int n) 上述列举的这些函数参数都是 unsigned int 类型，如果参数为 usigned long 或者 usigned long long，只需要在函数名后面加上 l 或 ll 就可以了，比如 __builtin_popcountl。 遗憾的是，这些builtin函数一般没有可移植性，使用时要注意。 库函数但值得庆幸的是，这些优秀的函数在C++20中得以转正，成为了C++的标准函数，比如 std::popcount，定义在头文件 &lt;bit&gt; 中，函数定义如下： 12template&lt;class T&gt;constexpr int popcount(T x) noexcept; 更快速的源码计算一个整数的二进制表示中包含1的个数，除了前面提到的 n &amp;= (n-1) 外，还有下面这种变形的二分法实现： 123456789unsigned popcount (unsigned int u)&#123; u = (u &amp; 0x55555555) + ((u &gt;&gt; 1) &amp; 0x55555555); u = (u &amp; 0x33333333) + ((u &gt;&gt; 2) &amp; 0x33333333); u = (u &amp; 0x0F0F0F0F) + ((u &gt;&gt; 4) &amp; 0x0F0F0F0F); u = (u &amp; 0x00FF00FF) + ((u &gt;&gt; 8) &amp; 0x00FF00FF); u = (u &amp; 0x0000FFFF) + ((u &gt;&gt; 16) &amp; 0x0000FFFF); return u;&#125; 采用这种二分法的实现，基本上可以媲美单字节打表的速度了，上述二分法是利用变量u来分组统计1的个数，两两合并到一起进而得到最后结果的。 总结 计算两个数的最大公约数可以在C++17环境下使用 std::gcd() 函数 计算一个整数二进制表示中1的个数可以在C++20环境下使用 std::popcount() 函数 __builtin 开头的函数是GCC提供的方便程序编写的函数，并且可用来优化编译结，但是使用时要注意不可移植性 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 在繁华中自律在落魄中自愈谋生的路上不抛弃良知谋爱的路上不抛弃尊严 2021-4-11 21:27:25]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>函数</tag>
        <tag>语法糖</tag>
        <tag>函数库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写给自己的KMP——C++版本]]></title>
    <url>%2Fblog%2F2021%2F04%2F04%2F%E5%86%99%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84KMP%E2%80%94%E2%80%94C-%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[一、前言又翻到了这个算法，一个常用的子串（子数组）匹配算法，看一遍学一遍，学一遍忘一遍，反反复复，不过每次回忆起来所用的时间越来少了，其本质上就是在暴力搜索的基础上加上 next 数组加速匹配，算法的关键在于 next 数组的理解和求解方法。 不想画图，缺少图解的算法很难给初学者讲清楚，所以本文也仅仅是个人的笔记而已，用于记录算法中关键点、帮助回忆或者理解其中的一些关键因素，如果想从头学习 KMP，还是去搜索其他资料吧，相关的内容有很多，有些文章写的很详细的。 今天的示例代码用C++来写，上一版的自己写的KMP我查了一下是C语言版本的，初看起来已经有点费劲了，随着时间的推移，我决定根据理解再写一次，写完才发现，和之前的风格判若两人。 二、暴力搜索在原字符串中搜索模式串，最容易想到的就是暴力搜索，匹配则向后移动，不匹配则原串回溯，模式串归0，代码很容易实现，列举如下： 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;int violence_find(string s, string p) &#123; int m = s.size(), n = p.size(), i = 0, j = 0; while (i &lt; m &amp;&amp; j &lt; n) &#123; if (s[i] == p[j]) &#123; // match character ++i; ++j; &#125; else &#123; // mismatch i -= j - 1; j = 0; &#125; &#125; return j == n ? i - j : -1;&#125;int main() &#123; string s("abdfdjfdkekfdaa5gsdsf"); string p("fdkekfd"); cout &lt;&lt; violence_find(s, p) &lt;&lt; endl; return 0;&#125; 典型的 O(MN) 解法，这种解法慢就慢在原字符串的回溯上，也就是语句 i -= j - 1; 的效果，当出现失配时，原字符串之前的匹配几乎“白费”，每次最多移动一个字符，而 KMP 算法决定利用之前的“努力成果”。 三、KMP算法在 KMP 算法中先利用模式串构建一个 next 数组，当出现失配情况时根据模式串前缀和后缀情况，最大程序利用已经匹配的部分来达到加速查找的目的，只需要求一个 next 数组，其他部分和暴力匹配的代码很像： 1234567891011121314151617181920int kmp_tmp(string s, string p) &#123; int m = s.size(), n = p.size(), i = 0, j = 0; vector&lt;int&gt; next = std::move(gen_next(p)); while (i &lt; m &amp;&amp; j &lt; n) &#123; if (s[i] == p[j]) &#123; ++i; ++j; &#125; else &#123; // mismatch j = next[j]; if (j == -1) &#123; ++i; j = 0; &#125; &#125; &#125; return j == n ? i - j : -1;&#125; 和暴力搜索的代码对照下，只有 else 中语句块不太一样，这个 i 只前进不后退了，其实这个里的 j == -1 语句可以合并到判定相等的 if 语句块中，完成 KMP 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;using namespace std;vector&lt;int&gt; gen_next(string p) &#123; int n = p.size(), i = 0, j = -1; vector&lt;int&gt; next(n, -1); while (i &lt; n - 1) &#123; if (j == -1 || p[i] == p[j]) &#123; ++i; ++j; next[i] = j; &#125; else j = next[j]; // mismatch, move j &#125; return next;&#125;int kmp(string s, string p) &#123; int m = s.size(), n = p.size(), i = 0, j = 0; vector&lt;int&gt; next = std::move(gen_next(p)); while (i &lt; m &amp;&amp; j &lt; n) &#123; if (j == -1 || s[i] == p[j]) &#123; ++i; ++j; &#125; else j = next[j]; // mismatch, move j &#125; return j == n ? i - j : -1;&#125;int main() &#123; string s("abdfdjfdkekfdaa5gsdsf"); string p("fdkekfd"); cout &lt;&lt; kmp(s, p) &lt;&lt; endl; return 0;&#125; 四、关键点记录 next[i] 中记录的实际上是 p[0,i-1] 这个字符串中所有前缀和所有后缀交集中最长字符串的长度，比如&#39;fdkekfd&#39; 这个字符串所有前缀和所有后缀交集中最长字符串是 &#39;fd&#39;，其长度是2。 字符串的前缀和后缀不包括字符串本身。 next[0] 初始化成-1仅仅是一个编程技巧，你可以初始化成任意值，只要你分辨出是失配的情况即可，这里初始成 -1 正好可以和 s[i] == p[j] 这种情况合并，所以初始化成 -1 会常用一点。 在 KMP 算法中原串索引 i 比较傲娇，它只前进不会回溯，这也是 KMP 速度快的一个主要原因。 当出现失配时，模式子串的前缀和后缀有重合，可以直接移动模式串的前缀到刚刚匹配的后缀部分，但要记住如果没有重合的前缀和后缀，失配时移动模式串的速度会更快，这里容易弄反。 五、总结 KMP 算法的关键是求解 next 数组，是一个被称为部分匹配表(Partial Match Table)的数组 KMP 算法相比暴力匹配时间复杂度提升到了O(N+M)，但是并不是最优秀的字符串匹配算法 想要更快或者选择更合适的算法可以了解下从模式串的尾部开始匹配的 BM算法，以及从模式串的头部开始匹配的 Sunday算法 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 想要看到更高的风景，除了让自己跳的更高以外，还可以选一个更高的平台站上去。找到一个2米高的平台并努力爬上去，远比你原地起跳2米要容易的多~ 2021-4-5 00:22:35]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>find</tag>
        <tag>算法</tag>
        <tag>KMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中常见的字符判断与处理方法]]></title>
    <url>%2Fblog%2F2021%2F03%2F28%2FC-%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E5%AD%97%E7%AC%A6%E5%88%A4%E6%96%AD%E4%B8%8E%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言字符串处理是编程世界中一项基础技能，特别是对于C/C++的程序员们，远没有那么多华丽的工具可以使用，大多数时候都需要一个个字符来判断和处理，甚至对于C语言来说都没有字符串类型，字符数组是其常见的等价结构，所以稳扎稳打的基本功尤其重要。 对于C++而言，确实有string这个字符串类型，在使用的时候有一些技巧和函数可以使用，比C语言要方便许多了，只是有些时候我们并不知道可以这样用，有时一些很朴素的写法会让程序更加简洁，而一些技巧的表达当明白之后也会感叹自己曾经的无知。 ASCII 码作为字符编码的基础，ASCII码是需要先弄明白的，即使不能把所有的ASCII码对应的字符都记住，也要把常见的字母、数字、特殊字符记住，这样在处理字符问题时可以得心应手，常见的ASCII码对照表如下： 其中需要注意的知识点： 前32个为非打印控制字符，后面的字符为打印字符 数字字符 &#39;0&#39;-&#39;9&#39; 对应的ASCII码范围是48-57 大写字母 &#39;A&#39;-&#39;Z&#39; 对应的ASCII码范围是65-90 小写字母 &#39;a&#39;-&#39;z&#39; 对应的ASCII码范围是97-122 NULL 对应ASCII码0，回车的ASCII码是13，换行的ASCII码是10 仔细观察这个ASCII表你会发现很多“秘密”，比如 windows 中的文件放到 linux 上打开时常常显示许多的 ^M，其实这就是\r 的表现，因为在 windows 上用 \r\n 表示换行，而 linux 上使用 \n 换行，那么多余的 \r 在 linux 上就会显示成 ^M。 再比如小写字母 a 和大写字母 A 中间差了32，为什么不是26呢？为什么要在中间插入几个别的字符，搞成连续的不好吗？之前没想过这个问题，但是前两天看了一个高手的代码后，我大概明白了，这个32的差距应该是一种“炫技”的表现，它可以使得许多代码逻辑变得简单。 判断字符范围的函数C 语言C语言中判断的字符范围的函数都在头文件 &lt;ctype.h&gt; 中，常见的有下面这些 int isalnum(int c)：检查所传的字符是否是字母和数字 int isalpha(int c)：检查所传的字符是否是字母 int isdigit(int c)：检查所传的字符是否是十进制数字 int islower(int c)：检查所传的字符是否是小写字母 int isupper(int c)：检查所传的字符是否是大写字母 int ispunct(int c)：检查所传的字符是否是标点符号字符 C++C++ 中其实大部分还是引用C语言里的这些函数，但是头文件的名字为 &lt;cctype&gt;，在C++11中加了一个 int isblank(int c) 函数。 字符判断技巧判断两个字符互为大小写看到这个问题第一直觉是什么？很简单的问题有木有？因为知道一个字母的大小写差了32，所以会写出下面的代码： 1234int isOk(char x, char y) &#123; return x - y == 32 || y - x == 32;&#125; 不过我前两天看到一段代码，它是这样写的： 1(x ^ 32) == y; 看到这里你还以为 A 和 a 之间差32而不是26感到迷惑吗？简单的字符编排透露着巨大的智慧。 哨兵的使用比如取出一个字符串 string s 中所有的数字，问题很简单，但是结尾字符的处理往往体现了编程的功底，加上一个哨兵字符可以使得编程逻辑简单许多，无须再对结尾字符特殊判断。 1234567891011void find(string s) &#123; s = s + 'A'; string ans; for (auto c : s) &#123; if (isdigit(c)) ans += c; else &#123; cout &lt;&lt; ans &lt;&lt; endl; ans = ""; &#125; &#125;&#125; 总结 &#39;0&#39; 的ASCII码是48，&#39;A&#39; 的ASCII码是65，&#39;a&#39; 的ASCII码是97 isdigit 可以判断字符是否是数字，isalpha 可以判断字符是否为字母 一个字母的大小写对应的ASCII码正好差32，判断互为大小写时可以使用异或符号 (x ^ 32) == y 字符串结尾加哨兵字符可以使得处理逻辑更加简单统一，这种编程技巧在其他结构中也常常出现 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 今天看到一个一直作为榜样的知识输出者宣布财富自由，满心羡慕，是真的羡慕！关键人家比我年轻，比我工作时间还短，已经依靠短短4、5年的努力达到了自由状态，不过了解他的经历会发现他确实付出了很多，而我们大多数作为普通人太安于现状了，有时候选择比努力重要，如果选择对了又付出了加倍的努力，那…… 2021-3-28 23:27:32]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>string</tag>
        <tag>char</tag>
        <tag>判断</tag>
        <tag>查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中反向遍历map时怎样删除元素]]></title>
    <url>%2Fblog%2F2021%2F03%2F21%2FC-%E4%B8%AD%E5%8F%8D%E5%90%91%E9%81%8D%E5%8E%86map%E6%97%B6%E6%80%8E%E6%A0%B7%E5%88%A0%E9%99%A4%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[前言今天在解决一个问题 《5710. 积压订单中的订单总数》 时用到了map的反向遍历，看到问题时首先想到了优先队列，但是需要维护一个大根堆和一个小根堆，感觉操作起来比较麻烦，突发奇想使用map就能够解决。map本身就是有序的，正向遍历可以得到从小到大的序列，而反向遍历就可以得到从大到小的序列，这个思路本身没有错，但是解题时卡在了反向遍历时如何删除元素的知识点上，特此记录一下。 map的正向遍历map的正向遍历是一个基础知识点了，先简单复习一下，不管是用 for 还是 while，只要控制迭代器持续前进就可以了。 12345678910111213map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (map&lt;int, string&gt;::iterator it = mp.begin(); it != mp.end(); ++it) &#123; cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;&#125;/* 输出内容1 A2 E3 I4 O6 U*/ 引入 auto 关键字以后，定义表示式的时候会更加方便一点 12345map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it = mp.begin(); it != mp.end(); ++it) &#123; cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;&#125; 引入冒号以后表达式更加简短，要注意的是这里的 it 已经不是指针了，而是 value_type 类型，所以需要是用 . 来访问 12345map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it : mp) &#123; cout &lt;&lt; it.first &lt;&lt; " " &lt;&lt; it.second &lt;&lt; endl;&#125; 引入了结构化绑定声明之后，遍历方式还可以写成下面这样 12345map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto&amp; [a, b] : mp) &#123; cout &lt;&lt; a &lt;&lt; " " &lt;&lt; b &lt;&lt; endl;&#125; map 遍历时删除元素map 遍历时删除需要注意迭代器失效问题，常用的有下面两种写法 1it = mp.erase(it); 或者1mp.erase(it++); 遍历删除时的例子： 1234567891011121314151617map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it = mp.begin(); it != mp.end();) &#123; if (it-&gt;second == "I") mp.erase(it++); else it++;&#125;for (auto it : mp) cout &lt;&lt; it.first &lt;&lt; " " &lt;&lt; it.second &lt;&lt; endl;/* 输出内容1 A2 E4 O6 U*/ map 的反向遍历map 反向遍历时可以使用 reverse_iterator 迭代器，配合 rbegin() 和 rend() 方法就可以完成反向遍历 123456789101112map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it = mp.rbegin(); it != mp.rend(); it++) &#123; cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;&#125;/* 输出内容6 U4 O3 I2 E1 A*/ map 反向遍历时删除元素一开始也是用 erase 函数来删除元素，但是会报下面的编译错误 123error: no matching function for call to ‘std::map&lt;int, std::__cxx11::basic_string&lt;char&gt; &gt;::erase( std::reverse_iterator&lt;std::_Rb_tree_iterator&lt;std::pair&lt;const int, std::__cxx11::basic_string&lt;char&gt; &gt; &gt; &gt;)’ mp.erase(it++); 查询文档发现，erase 函数重载只有下面几种实现: 12345678void erase( iterator pos ); (until C++11)iterator erase( const_iterator pos ); (since C++11)iterator erase( iterator pos ); (since C++17)void erase( iterator first, iterator last ); (until C++11)iterator erase( const_iterator first, const_iterator last ); (since C++11)size_type erase( const key_type&amp; key ); 参数是迭代器的函数并不支持 reverse_iterator，需要将 reverse_iterator 转化成 iterator 才可以，这时就需要用到 base 函数，对 reverse_iterator 类型的迭代器使用 base 函数得到的是上一个元素“原始指针”，这一点比较有意思，具体的解释可以参考 《std::reverse_iterator::base》，这种操作决定了我们遍历删除的写法，应该是先自增再调用 base 函数，代码如下； 12345678910111213141516map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it = mp.rbegin(); it != mp.rend();) &#123; if (it-&gt;second == "I") mp.erase((++it).base()); else it++;&#125;for (auto it = mp.rbegin(); it != mp.rend(); it++) cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;/* 输出内容6 U4 O2 E1 A*/ 总结 map 默认会按照 key 排序，是一个常用的有序容器 配合使用 rbegin() 和 rend() 函数可以完成 map 的反向遍历 对 reverse_iterator 类型迭代器使用 base() 函数，可以转化成 iterator 相关类型，然后进行删除操作 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 搬起砖，我抱不了你，放下砖 … 我尽力！ 2021-3-21 19:44:27]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>iterator</tag>
        <tag>reverse_iterator</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决git命令会将结果输出到单独窗口必须按q才能退出的问题]]></title>
    <url>%2Fblog%2F2021%2F03%2F14%2F%E8%A7%A3%E5%86%B3git%E5%91%BD%E4%BB%A4%E4%BC%9A%E5%B0%86%E7%BB%93%E6%9E%9C%E8%BE%93%E5%87%BA%E5%88%B0%E5%8D%95%E7%8B%AC%E7%AA%97%E5%8F%A3%E5%BF%85%E9%A1%BB%E6%8C%89q%E6%89%8D%E8%83%BD%E9%80%80%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言平时会在 windows+linux 两种环境下进行开发，版本控制软件用 git 比较多，但是一直有个小问题，在 windows 下使用 git Bash 比较顺手，但是在 linux 使用 git 部分命令的时候，常常会打开一个新的窗口，按q键才能退出，比如 git branch、git log、git show 等等。 如果是使用 git log 查询日志后想进行合并或者回退等操作时，因为日志信息已经退出没有显示在当前窗口，这种情况会比较麻烦，有时候还需要额外再查询一次，还有就是 git branch 命显示内容常常较少，单独打开一个窗口也没有必要，所以想单独设置这种情况，后来查询资料发现，这与 git 的 pager 设置有关，特此记录一下。 什么是 pagerpager 其实就是分页器，也就是对一大段内容进行分页显示的工具，git 在一些版本中默认使用的是 less 工具，不同的版本默认设置会有差异，这也就是造成我在 windows 下没有自动分页，而在 linux 下会打开新窗口进行分页的原因。 git 的分页器可以通过 core.pager 来进行设置，他会被 git 命令行解释，影响分页器的变量有多个，他们起作用的顺序依次是 $GIT_PAGER 环境变量，core.pager git配置，$PAGER 环境变量，如果这些都没有设置，默认会选择编译时的选项（通常为less），具体细节可以参考官方文档 git core.pager。 设置 core.pager了解了上面的原理，我们就知道只要单独修改 git 配置就可以了，默认的分页器是 less，我们只要设置了 core.pager 就可以影响结果，所以在 git Bash 中执行下面的语句即可。 1git config --global core.pager '' 其实就是将分页器清空就行了，这样再执行 git branch 的时候就不会出现分页的情况了。 更精细的设置设置 core.pager 这项配置后对 less 分页器进行了全局屏蔽，虽然 git branch 这种显示内容少的命令比较方便了，但是执行 git show 的时候不分页反而会显得混乱，有没有单独设置每个 git 命令的配置呢？ 答案当然是肯定的，比如上面提到的这种情况，我们只想屏蔽 git branch 命令的分页，而想保留git show 和 git log 的分页显示，就可以单独执行下面的命令。 1git config --global pager.branch false 这样就可以达到只屏蔽 git branch 命令的分页结果了。 less and more临时插播个知识点，less 和 more 都是 linux 的文本显示工具，那么它们谁更厉害一点呢？从名字上来看应该是 more 更厉害，但实际上是 less 更厉害，less 在 more 的基础上加上了后退功能（据说最初版本more不能后退，现在的常见版本已经支持后退功能了），支持上下键翻页，并且速度更快一点，所以在 linux 的世界一直流传着 “less is more” 这句话。另外 more 退出后会在 shell 上留下刚显示的内容，而 less 不会。 总结 使用 git config --global core.pager &#39;&#39; 命令可以屏蔽 git 默认的分页器 less 使用 git config --global pager.branch false 命令可以只关闭 git branch 命令的分页显示 less 命令比 more 命令更加强大，支持上下键翻页，退出后不会在 shell 显示刚才的内容 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 学而不思则罔，思而不学则殆。动而不思则徒，思而不动则颓。 2021-3-14 17:14:55]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>log</tag>
        <tag>branch</tag>
        <tag>config</tag>
        <tag>pager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中int、str、bytes相互转化，还有2进制、16进制表示，你想要的都在这里了]]></title>
    <url>%2Fblog%2F2021%2F03%2F05%2FPython%E4%B8%ADint%E3%80%81str%E3%80%81byte%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96%EF%BC%8C%E8%BF%98%E6%9C%892%E8%BF%9B%E5%88%B6%E3%80%8116%E8%BF%9B%E5%88%B6%E8%A1%A8%E7%A4%BA%EF%BC%8C%E4%BD%A0%E6%83%B3%E8%A6%81%E7%9A%84%E9%83%BD%E5%9C%A8%E8%BF%99%E9%87%8C%E4%BA%86%2F</url>
    <content type="text"><![CDATA[前言数据类型转换是个很基础的操作，很多语言中都要做这些转换，例如前一段时间刚刚总结了《C/C++中string和int相互转换的常用方法》，python 自从分离出 python3 版本之后，str 和 bytes 两个类型弄蒙了一大票人，在这两种类型的转换上我可是花了不少时间，记住一点，别随随便便使用 str() 函数，很多数据使用 str() 变成字符串之后再想恢复可就难了。 本文所有示例均在 Python 3.7.5 上测试，Python2 已经被我抛弃了，我来试着把常见的转换都放到一起，把踩过的坑拿来开心一下，如果有些常用的类型转换这里没有的话，也欢迎小伙伴们提出来，我将持续补充，好了，可以开始了。 数据类型转化数字中除了整数，还有浮点数、复数等，但是 int 是最常见的类型，所有转换中的数字只涉及 int 数字类型。 int -&gt; str使用 str() 函数12345num = 10val = str(10)print(type(val), val)#&lt;class 'str'&gt; 10 使用 format() 函数12345num = 10val = '&#123;0&#125;'.format(num)print(type(val), val)#&lt;class 'str'&gt; 10 使用 hex() 转换成16进制形式12345num = 10val = hex(num)print(type(val), val)#&lt;class 'str'&gt; 0xa 使用 bin() 转换成2进制形式12345num = 10val = bin(num).replace('0b','')print(type(val), val)#&lt;class 'str'&gt; 1010 str -&gt; int这个转换比较专一，只使用 int() 函数就可以了，这个函数实际上有两个参数，第二个参数表示进制，默认是10进制，你可以改成2进制或者16进制，甚至是3进制、5进制等等 使用 int() 进行各进制数字转换1234567891011121314151617181920val = int('10')print(type(val), val)val = int('0xa', 16)print(type(val), val)val = int('a', 16)print(type(val), val)val = int('0b1010', 2)print(type(val), val)val = int('1010', 2)print(type(val), val)val = int('101', 3)print(type(val), val)val = int('60', 5)print(type(val), val)# 结果均为 &lt;class 'int'&gt; 10 使用 int() 函数的时候要主要注意一点，如果提供的字符串不能转换成指定进制的数字，那么会报异常，就像下面这样，所以在使用这个函数的时候最好放到 try 语句中。 123456789val = int('128', 2)'''Traceback (most recent call last): File "D:\python\convert\convert.py", line 41, in &lt;module&gt; val = int('128', 2)ValueError: invalid literal for int() with base 2: '128'[Finished in 0.1s with exit code 1]''' 什么是bytes在列举 bytes 相关的转化前，我们来先认识一下这个类型，在 Python3 中 int、str、bytes 类型的变量实际上都是一个 “类” 的对象，而 bytes 相比 str 而言更接近底层数据，也更接近存储的形式，它其实是一个字节的数组，类似于 C 语言中的 char []，每个字节是一个范围在 0-255 的数字。 bytes 其实就是这样一连串的数字，计算机世界所有的信息都可以用这样一串数字表示，一幅画，一首歌，一部电影等等，如果对编码感兴趣可以看看这篇《简单聊聊01世界中编码和解码这对磨人的小妖儿》，现在清楚bytes是什么了，我们可以看看和它相关的转化了。 int -&gt; bytes使用 to_bytes() 转换成定长bytes12345num = 4665val = num.to_bytes(length=4, byteorder='little', signed=False)print(type(val), val)#&lt;class 'bytes'&gt; b'9\x12\x00\x00' 这段代码就是把数字 4665 转化成定长的4个字节，字节序为小端，我们来简单看一下是怎么转换的： 上面我们提到 bytes 类型一串 0-255 范围的数字，4665 肯定超出了这个范围，可以先转化成256进制，就变成了 ，也就是 4665 = 18 * 256 + 57，我们发现两个字节就能存储这个数字，一个18，一个57，要想组成4个字节的数组需要补充两个空位，也就是补充两个0，这时就涉及到一个排列顺序，是 [0,0,18,57] 还是 [57, 18, 0, 0] 呢，这就是函数参数中的字节序 byteorder，little 表示小端，big 表示大端，这里选择的小端 [57, 18, 0, 0] 的排列。 看到这里可能会迷糊，好像和结果不一样啊，其实这只是一个表示问题，57 的 ASCII 码对应这个字符 ‘9’，18 表示成16进制就是 ‘0x12’，这里写成 b’9\x12\x00\x00’ 只是便于识别而已，实际上内存存储的就是 [57, 18, 0, 0] 这一串数字对应的二进制编码 ‘00111001 00010010 00000000 00000000’。 使用 bytes() 函数把int数组转成bytes参考上面的生成的数组，可以通过数组生成相同的结果 12345num_array = [57, 18, 0, 0]val = bytes(num_array)print(type(val), val)#&lt;class 'bytes'&gt; b'9\x12\x00\x00' 使用 struct.pack() 函数把数字转化成bytes12345num = 4665val = struct.pack("&lt;I", num)print(type(val), val)#&lt;class 'bytes'&gt; b'9\x12\x00\x00' 这里的 &quot;&lt;I&quot; 表示将一个整数转化成小端字节序的4字节数组，其他的类型还有： 参数 含义 &gt; 大端序 &lt; 小端序 B uint8类型 b int8类型 H uint16类型 h int16类型 I uint32类型 i int32类型 L uint64类型 l int64类型 s ascii码，s前带数字表示个数 bytes -&gt; int明白了上面的转化过程，从 bytes 转化到 int 只需要反着来就行了 使用 from_bytes() 把 bytes 转化成int12345bys = b'9\x12\x00\x00'val = int.from_bytes(bys, byteorder='little', signed=False)print(type(val), val)#&lt;class 'int'&gt; 4665 使用 struct.unpack() 把 bytes 转化成int12345bys = b'9\x12\x00\x00'val = struct.unpack("&lt;I", bys)print(type(val), val)#&lt;class 'tuple'&gt; (4665,) str 和 bytes前面的这些转化还算清晰，到了字符串str 和字节串 bytes，就开始进入了混沌的状态，这里会出现各种编码，各种乱码，各种报错，牢记一点 str 到 bytes 是编码过程，需要使用 encode() 函数， bytes 到 str 是解码过程，需要使用 decode() 函数，请勿使用 str 函数，否则后果自负。 使用 encode() 函数完成 str -&gt; bytes12345s = '大漠孤烟直qaq'val = s.encode('utf-8')print(type(val), val)# &lt;class 'bytes'&gt; b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq' 使用 decode() 函数完成 bytes -&gt; str12345bys = b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq'val = bys.decode('utf-8')print(type(val), val)# &lt;class 'str'&gt; 大漠孤烟直qaq 假如使用了 str() 函数从上面来看字符串和字节串的转化蛮简单的，甚至比整数的转化都要简单，但是你如果把一个 bytes 变量用 str() 转化成字符串，你就得手动来处理了，这个函数写过n次了，暂时还没找到好的处理办法。 123456789101112131415161718192021222324bys = b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq's = str(bys)print(type(s), s)#&lt;class 'str'&gt; b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq'def str2bytes(str_content): result_list = []; pos = 0 str_content = str_content.replace("\\n", "\n").replace("\\t", "\t").replace("\\r", "\r") content_len = len(str_content) while pos &lt; content_len: if str_content[pos] == '\\' and pos + 3 &lt; content_len and str_content[pos + 1] == 'x': sub_str = str_content[pos + 2: pos + 4] result_list.append(int(sub_str, 16)) pos = pos + 4 else: result_list.append(ord(str_content[pos])) pos = pos + 1 return bytes(result_list)val = str2bytes(s[2:-1])print(type(val), val)# &lt;class 'bytes'&gt; b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq' 什么时候会遇到这种情况，就是有些数据是以 bytes 的形式给的，但是经过中间人复制转发变成了字节流形式的字符串，格式还不统一，有些已经翻译成了字符，有些还保留了0x或者\x形式，这时就要手工处理了。 转化表格上面的转化方式和解释穿插在一起有些乱，这里总结一个表格，便于今后拿来就用 源类型 目标类型 方式 结果 int str str(10)、&#39;{0}&#39;.format(10) 10 =&gt; &#39;10&#39; int str（16进制） hex(10) 10 =&gt; &#39;0xa&#39; int str（2进制） bin(10).replace(&#39;0b&#39;,&#39;&#39;) 10 =&gt; &#39;1010&#39; str int int(&#39;10&#39;) &#39;10&#39; =&gt; 10 str（16进制） int int(&#39;0xa&#39;, 16) &#39;0xa&#39; =&gt; 10 str（2进制） int int(&#39;1010&#39;, 2) &#39;1010&#39; =&gt; 10 int bytes num.to_bytes(length=4, byteorder=&#39;little&#39;, signed=False) 4665 =&gt; b&#39;9\x12\x00\x00&#39; int bytes struct.pack(&quot;&lt;I&quot;, 4665) 4665 =&gt; b&#39;9\x12\x00\x00&#39; bytes int int.from_bytes(b&#39;9\x12\x00\x00&#39;, byteorder=&#39;little&#39;, signed=False) b&#39;9\x12\x00\x00&#39; =&gt; 4665 bytes int struct.unpack(&quot;&lt;I&quot;, b&#39;9\x12\x00\x00&#39;) b&#39;9\x12\x00\x00&#39; =&gt; 4665 int[] bytes bytes([57, 18, 0, 0]) [57, 18, 0, 0] =&gt; b&#39;9\x12\x00\x00&#39; bytes int[] [x for x in b&#39;9\x12\x00\x00&#39;] b&#39;9\x12\x00\x00&#39; =&gt; [57, 18, 0, 0] str bytes &#39;美好&#39;.encode(&#39;utf-8&#39;) &#39;美好&#39; =&gt; b&#39;\xe7\xbe\x8e\xe5\xa5\xbd&#39; str bytes bytes(&#39;美好&#39;, &#39;utf-8&#39;) &#39;美好&#39; =&gt; b&#39;\xe7\xbe\x8e\xe5\xa5\xbd&#39; bytes str b&#39;\xe7\xbe\x8e\xe5\xa5\xbd&#39;.decode(&#39;utf-8&#39;) b&#39;\xe7\xbe\x8e\xe5\xa5\xbd&#39; =&gt; &#39;美好&#39; bytes bytes（无\x） binascii.b2a_hex(b&#39;\xe7\xbe\x8eqaq&#39;) b&#39;\xe7\xbe\x8eqaq&#39; =&gt; b&#39;e7be8e716171&#39; bytes bytes（有\x） binascii.a2b_hex(b&#39;e7be8e716171&#39;) b&#39;e7be8e716171&#39; =&gt; b&#39;\xe7\xbe\x8eqaq&#39; bytes str（hex） b&#39;\xe7\xbe\x8eqaq&#39;.hex() b&#39;\xe7\xbe\x8eqaq&#39; =&gt; &#39;e7be8e716171&#39; str（hex） bytes bytes.fromhex(&#39;e7be8e716171&#39;) &#39;e7be8e716171&#39; =&gt; b&#39;\xe7\xbe\x8eqaq&#39; 总结 Python3 对字符串和二进制数据流做了明确的区分，不会以任意隐式的方式混用 str 和 bytes bytes 类型是一种比特流，它的存在形式是 01010001110 的形式，需要解码成字符才容易被人理解 struct 模块中的 pack() 和 unpack() 可以实现任意类型和 bytes 之间的转换 binascii.b2a_hex 和 binascii.a2b_hex 可以实现16进制 bytes 的不同形式转换，不过转换前后长度发生了变化 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 初识不知曲中意，再闻已是曲中人 2021-3-6 20:36:10]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>int</tag>
        <tag>hex</tag>
        <tag>Python</tag>
        <tag>str</tag>
        <tag>bytes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下的mount命令到底有什么玄机]]></title>
    <url>%2Fblog%2F2021%2F02%2F22%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84mount%E5%91%BD%E4%BB%A4%E5%88%B0%E5%BA%95%E6%9C%89%E4%BB%80%E4%B9%88%E7%8E%84%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[前言linux 环境下的 mount 命令可以挂载一个文件系统，这个命令目前是我所知命令中最陌生的一个，至今没有使用过，真的一次都没有挂载过，平时用的最多是的 Windwos 系统，完全不需要挂载，安装新硬盘自动就会出现盘符，稍微复杂点就是通过磁盘管理器来分区，或者使用魔术分区助手搞一点复杂的分区或格式化操作。 而 linux 系统下，无论是申请的开发机，还是购买的服务器，硬盘都是挂载好的，至今没有遇到需要手动挂载的情况，所以对这个命令并不是很熟悉，今天决定抽时间学一下，总结一下设备相关的知识，特别是磁盘相关的操作。 文件类型“Everything is a file”（一切皆文件）是 linux 中的特点，所以在学习磁盘、设备之前先来看看文件类型，在 linux 并不通过后缀名来判断文件类型的，可以利用 ll （或 ls -l）命令可以列举指定目录下的内容，通过每一行的首个字母就可以判断出文件的大类型： -rw-r-r-：-开头的都普通文件 brw-r-r-：b开头的是块设备文件 crw-r-r-：c开头的是字符设备文件 drw-r-r-：d开头的是目录文件 lrw-r-r-：l开头的是软链接文件，也叫符号链接 prw-r-r-：p开头的是管道文件 srw-r-r-：s开头的是socket文件 我们可以查一下CentOS系统设备目录 dev，这里的文件类型非常丰富，由于内容非常多，这里只展示一部分内容： 123456789101112131415161718192021222324252627[root@VM-0-3-centos ~]# ll /devtotal 0crw------- 1 root root 10, 235 Feb 9 11:13 autofsdrwxr-xr-x 2 root root 120 Feb 9 11:13 blockdrwxr-xr-x 2 root root 60 Feb 9 11:13 bsgcrw------- 1 root root 10, 234 Feb 9 11:13 btrfs-controldrwxr-xr-x 3 root root 60 Feb 9 11:13 buslrwxrwxrwx 1 root root 3 Feb 9 11:13 cdrom -&gt; sr0drwxr-xr-x 2 root root 2560 Feb 9 11:13 charcrw------- 1 root root 5, 1 Feb 9 11:13 consolelrwxrwxrwx 1 root root 11 Feb 9 11:13 core -&gt; /proc/kcoredrwxr-xr-x 3 root root 60 Feb 9 11:13 cpucrw------- 1 root root 10, 61 Feb 9 11:13 cpu_dma_latencycrw------- 1 root root 10, 62 Feb 9 11:13 crashdrwxr-xr-x 6 root root 120 Feb 9 11:13 diskdrwxr-xr-x 2 root root 60 Feb 9 11:13 dricrw-rw---- 1 root video 29, 0 Feb 9 11:13 fb0lrwxrwxrwx 1 root root 13 Feb 9 11:13 fd -&gt; /proc/self/fdcrw-rw-rw- 1 root root 1, 7 Feb 9 11:13 fullcrw-rw-rw- 1 root root 10, 229 Feb 9 11:13 fusecrw------- 1 root root 10, 228 Feb 9 11:13 hpetdrwxr-xr-x 2 root root 0 Feb 9 11:13 hugepagescrw------- 1 root root 10, 183 Feb 9 11:13 hwrnglrwxrwxrwx 1 root root 25 Feb 9 11:13 initctl -&gt; /run/systemd/initctl/fifodrwxr-xr-x 3 root root 240 Feb 9 11:13 inputcrw-r--r-- 1 root root 1, 11 Feb 9 11:13 kmsgsrw-rw-rw- 1 root root 0 Feb 9 11:13 log 其实还有一个更精准的，专门用来查询文件类型的命令，那就是 file 命令，下面可以测试一下，执行后会输出关于文件类型的描述： 123456789[root@VM-0-3-centos ~]# lltotal 12-rwxr-xr-x 1 root root 67 Feb 9 13:44 connecttendis.shdrwxr-xr-x 2 root root 4096 Feb 9 13:44 tarlistdrwxr-xr-x 4 root root 4096 Feb 9 13:40 tendis[root@VM-0-3-centos ~]# file connecttendis.shconnecttendis.sh: Bourne-Again shell script, ASCII text executable[root@VM-0-3-centos ~]# file tendis/tendis/: directory 描述展示说明 connecttendis.sh 是一个 shell 脚本，而 tendis/ 是一个目录。 文件系统说完文件类型还得说说文件系统，什么是文件系统，其实从名字就可以看出来，文件系统就是管文件的呗，不同的文件系统所支持的最大容量、对单个文件的大小限制、存取性能、是否可压缩、是否提供校验等都是不太一样的，当我们做系统或者烧制U盘、格式化硬盘时常常提到文件系统这个词。 在 Windows 中常用的文件系统有 FAT16、FAT32、NTFS、exFAT 等，而 Linux 中常用的文件系统有 ext2、ext3、ext4、tmpfs、proc 等，这些文件系统无需全部记住，在 Linux 上可以通过查看 /proc/filesystems 文件得知当前系统都支持哪些文件系统。 123456789101112131415161718192021222324252627[root@VM-0-3-centos ~]# cat /proc/filesystemsnodev sysfsnodev rootfsnodev ramfsnodev bdevnodev procnodev cgroupnodev cpusetnodev tmpfsnodev devtmpfsnodev debugfsnodev securityfsnodev sockfsnodev daxnodev bpfnodev pipefsnodev configfsnodev devptsnodev hugetlbfsnodev autofsnodev pstorenodev mqueue ext3 ext2 ext4 iso9660nodev binfmt_misc 观察上述结果，第一列为空的文件系统需要挂载到块设备上才能访问其中的内容，之后才可以正常使用。 挂载硬盘Windows 中的目录是分盘符的，每个分区可以指定一个盘符，每个盘符就是这个分区的根，可以通过各个盘符依次向下访问其中的内容。在 Linux 中是没有盘符概念的，只有一个根目录 /，只存在一棵完整的目录树，硬盘设备需要挂载到这棵目录树上才能被正常使用。 我们知道 /dev 目录下存放着几乎所有的设备文件，从中就可以找到硬盘设备： 123456[root@VM-0-3-centos ~]# ll /dev/ | grep diskdrwxr-xr-x 6 root root 120 Feb 9 11:13 diskbrw-rw---- 1 root disk 7, 0 Feb 9 11:13 loop0crw-rw---- 1 root disk 10, 237 Feb 9 11:13 loop-controlbrw-rw---- 1 root disk 253, 0 Feb 9 11:13 vdabrw-rw---- 1 root disk 253, 1 Feb 9 11:13 vda1 从命令结果可以看出，这台计算机只有一块普通硬盘 vda，硬盘只有一个分区 vda1，这是目前一种通用的命名方式，hd 是指IDE接口的硬盘，sd 是指SATA接口的硬盘、vd 指 virtio 磁盘，现在的内核一般都会把硬盘，移动硬盘，U盘等识别为sdX的形式，第一块硬盘使用 a 作为后缀，例如 sda，后面的硬盘依次命名为 sdb、sdc 等，硬盘的第一个分区后缀为1，后面分区号依次递增。 查硬盘分区硬盘作为一种设备可以在 /dev 目录下查询，但是可以通过 fdisk 做更细致的查询 1234567891011[root@VM-0-3-centos dev]# fdisk -lDisk /dev/vda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000d64b4 Device Boot Start End Blocks Id System/dev/vda1 * 2048 104857566 52427759+ 83 Linux 通过 fdisk -l 命令查询出的结果与 /dev 目录下查到的信息一致 查看当前挂载的文件系统查看当前挂载的所有文件系统只需要一个 mount 命令就够了，也可以加 -t tmpfs 参数查看指定类型 123456789101112131415161718[root@VM-0-3-centos ~]# mountsysfs on /sys type sysfs (rw,relatime)proc on /proc type proc (rw,relatime)devtmpfs on /dev type devtmpfs (rw,nosuid,size=930496k,nr_inodes=232624,mode=755)securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)devpts on /dev/pts type devpts (rw,relatime,gid=5,mode=620,ptmxmode=000)tmpfs on /run type tmpfs (rw,nosuid,nodev,mode=755)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)...cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)configfs on /sys/kernel/config type configfs (rw,relatime)/dev/vda1 on / type ext4 (rw,noatime,data=ordered)debugfs on /sys/kernel/debug type debugfs (rw,relatime)hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)mqueue on /dev/mqueue type mqueue (rw,relatime)tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=188204k,mode=700)binfmt_misc on /proc/sys/fs/binfmt_misc type binfmt_misc (rw,relatime) 输出每一行格式一致，均包含6列内容，通过空格来分割，具体形式为： &lt;块设备或者其他文件系统&gt; on &lt;挂载目录&gt; type &lt;文件系统类型&gt; &lt;(与文件系统内容相关的选项)&gt; 以 ‘/dev/vda1 on / type ext4 (rw,noatime,data=ordered)’ 这一行为例，就表示 /dev/vda 这个硬盘的第一个分区挂载到了一个 类型为 ext4 的文件系统上，挂载点是根目录 /，括号中的 rw 表示可读写，data=ordered 表示有序，relatime 是一种更新文件访问时间属性的一种方式，此外还有 noatime 和 lazytime 类型，这一块内容也比较多，此处就不展开了，有兴趣可以转到 《文件系统中 atime,lazytime,relatime 详聊》 这篇文章中了解一下。 格式化磁盘分区使用 mkfs 可以格式化一个磁盘分区，格式化的同时可以修改文件系统的类型 1[root@VM-0-3-centos ~]# mkfs -t ext4 /dev/sda2 挂载新的磁盘分区使用 mount 命令可以将新的磁盘分区挂载到目录树上 普通挂载12[root@VM-0-3-centos ~]# mkdir -p /mnt/data[root@VM-0-3-centos ~]# mount /dev/sda2 /mnt/data 以只读方式挂载1[root@VM-0-3-centos ~]# mount -o ro /dev/sda2 /mnt/data 只读挂载改为读写模式1[root@VM-0-3-centos ~]# mount /mnt/data -o rw,remount 挂载光驱挂载光驱的方式与挂载硬盘是一样的，只不过光驱是单独的设备，对应着不同的文件 1[root@VM-0-3-centos ~]# mount /dev/cdrom /media/cdrom 挂载 windows 共享文件挂载 windows 共享文件时需要用到 windows 计算机的IP、用户名和密码 12[root@VM-0-3-centos ~]# mkdir -p /mnt/share[root@VM-0-3-centos ~]# mount -t cifs -o username=admin,password=94741 //10.2.49.172/share /mnt/share 挂载 ios 文件直接挂载 iso 文件就不需要光驱了，使用起来更加方便，先模拟创建一个 iso 文件 12345678910111213141516[root@VM-0-3-centos ~]# genisoimage -o test.iso tendis/I: -input-charset not specified, using utf-8 (detected in locale settings)Using TENDI000.;1 for tendis/bin/tendisplus (tendisplus_static) 2.77% done, estimate finish Sat Feb 27 23:03:08 2021 5.53% done, estimate finish Sat Feb 27 23:03:08 2021 8.30% done, estimate finish Sat Feb 27 23:03:08 2021... 49.78% done, estimate finish Sat Feb 27 23:03:10 2021... 99.55% done, estimate finish Sat Feb 27 23:03:11 2021Total translation table size: 0Total rockridge attributes bytes: 0Total directory bytes: 6144Path table size(bytes): 50Max brk space used 1a000180825 extents written (353 MB) 把 iso 直接挂载到目录就可以了，之后就可以通过 /mnt/iso 目录访问 test.iso 文件中的内容了 123[root@VM-0-3-centos ~]# mkdir -p /mnt/iso[root@VM-0-3-centos ~]# mount test.iso /mnt/isomount: /dev/loop0 is write-protected, mounting read-only 计算机设备中的概念计算机科学中的概念有很多，常常记不清或者记混它们的内容，比如扇区、块、簇、页等，每次看完过后就忘记了，在此简单总结一下。 扇区硬盘的读写以扇区为基本单位，属于物理层面的概念，操作系统是不直接与扇区交互的，可以通过 fdisk -l 查看扇区大小 1234567891011[root@VM-0-3-centos dev]# fdisk -lDisk /dev/vda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000d64b4 Device Boot Start End Blocks Id System/dev/vda1 * 2048 104857566 52427759+ 83 Linux 其中 Sector size，就表示扇区大小，从结果来看还分逻辑扇区和物理扇区，不过本例中为均为 512 bytes。 块、簇块和簇其实是一种东西，通常 linux 系统叫做块，而 windows 系统叫做簇，它是文件系统读写数据的最小单位，每个磁盘块可以包括相邻的 2、4、8、16、32 或 64 个扇区，是操作系统所使用的逻辑概念，可以通过命令 stat /boot 来查看 123456789[root@VM-0-3-centos mnt]# stat /boot File: ‘/boot’ Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: fd01h/64769d Inode: 18 Links: 5Access: (0555/dr-xr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2018-08-08 11:20:35.109000000 +0800Modify: 2021-02-09 11:14:21.799238058 +0800Change: 2021-02-09 11:14:21.799238058 +0800 Birth: - 结果中 IO Block 就是磁盘块大小 4096 Bytes，也就是 4K，这样将相邻扇区合并为块来存取数据是为了更高效地、更好地管理磁盘空间。操作系统规定一个磁盘块中只能放置一个文件，这就会造成空间的浪费，那就意味着大多数情况会出现文件所占用的磁盘空间大于文件大小的情况。 页这是一个内存相关的概念，是内存的最小存储单位。它大小通常为磁盘块大小的 $2^n$ 倍，可以通过命令 getconf PAGE_SIZE 来获取页的大小 12[root@VM-0-3-centos mnt]# getconf PAGE_SIZE4096 总结 mount 是 linux 系统中非常重要的一个命令，但是我感觉用的比较少 系统新挂载的硬盘关机后会被卸载，可以将挂载信息添加到 /etc/fstab 文件实现开机后自动挂载 可以把多个设备挂载到同一个目录，默认后面挂载的内容会隐藏前面挂载的内容，卸载后面的挂载，前面挂载的内容还会出现 可以把同一个设备挂载到不同的目录，并在挂载时可以指定不同的权限，这样在不同的目录中看到的是同样的内容，但是权限不同 扇区是硬盘的读写的基本单位，属于物理概念 块/簇是操作系统读写数据的基本单位，属于逻辑概念 页是内存的最小存储单位，通常为磁盘块大小的 $2^n$ 倍 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 万里山河都踏过 天下又入谁手 分分合合 不过几十载春秋~ “你在教我做事啊” 2021-2-27 23:41:34]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mount</tag>
        <tag>unmount</tag>
        <tag>挂载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go环境配置时遇到的GOPATH路径以及包管理问题]]></title>
    <url>%2Fblog%2F2021%2F02%2F19%2FGo%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84GOPATH%E8%B7%AF%E5%BE%84%E4%BB%A5%E5%8F%8A%E5%8C%85%E7%AE%A1%E7%90%86%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言过了个年回到工作岗位，发现之前好好的 Go 环境无法进行调试了，于是又重新配置了一次，弄好之后的新参数与原来有一些不一样的地方，甚至还有一些矛盾的地方，真搞不清楚原来的配置参数怎么能成功调试的，也是奇了怪了。配置过程中还遇到了一些问题，特别记录一下，防止今后遇到类似问题还要苦苦寻找。 Go 的环境变量说起环境变量，写 Java 的时候倒是配置了不少，一般都需要配置 JAVA_HOME 和 CLASSPATH，后来 C/C++ 写的比较多，完全不需要这个东西，直接使用 include 把绝对路径或者相对路径引用进来就可以，或者使用VS的项目属性界面配置包含路径，也可以将包含路径写在 CMakeLists.txt 中。但是为了能找到和使用各种工具和软件，它们所在的路径一般会加到环境变量Path中。 Go 中也有两个环境变量非常重要，它们分别是 GOROOT 和 GOPATH，其中 GOROOT 比较好理解，就是 Go 软件安装的目录，可以类比一下 JAVA_HOME/bin，而 GOPATH 就是一个神奇的存在，在go1.12版本之前，Go 语言编写的项目代码和下载的包都必须在 GOPATH 目录下，想像一下，在一个 GOPATH 目录下无数个项目go文件，那感觉真是酸爽。 GOPATH 的设计GOPATH设计的出发点是好的，将代码包统一存储到一个目录下，直接引用包名就可以了，可是这样设计也缺少了自由，下载的第三方包和自己的项目文件混在一起虽然可以方便查看代码，但是结构看起来确实很乱。 什么？你说 GOPATH 可以指定多个目录，确实很多资料说 GOPATH 支持多个目录，下载的包会默认放在 GOPATH 指定的第一个目录下，需要注意的一个点是在windows下指定多个目录需要用分号分隔，而Linux下指定多个目录需要用冒号分隔，并且指定的目录需要是用绝对路径，如果指定的目录中包含相对路径，会报一个 go: GOPATH entry is relative; must be absolute path: &quot;... 错误，记得 GOPATH 变量末尾不要加 : 或者 ;。 春节前我就是配置的多个目录，本来调试用的好好的，结果过完年现在不让用了，一调试就会报错 unexpected directory layout:，具体的报错内容结构如下： 1234567unexpected directory layout: import path: _/go/src/firstgo root: /go/src dir: /go/src/firstgo expand root: /go expand dir: /go/src/firstgo separator: / 后来参考了下面两篇文章，把 GOPATH 改成单一目录就好了。 cmd/go: unexpected directory layout while building project go get 报错 unexpected directory layout 也就是在linux下的 ~/.profile 文件中把 export GOPATH=/home/albert/go:/home/albert/WorkSpace/go 改成 export GOPATH=/home/albert/go 就可以了。 GOPATH 的发展早期版本的 GOPATH 设计所有包下载到指定的目录，并且没有版本号，如果多个项目引用的同一个包的不同版本那就歇菜了，所以说这时的 Go 管理仅仅处于能用的状态，也就相当于一个下载器，达不到软件包管理器的及格水平，后来出现了一些例如 dep、Godep 的包管理工具，均属于官方推荐的第三方管理工具，都非 Go 语言自带。 Go 的包管理工具并不像 Python 的 pip，或者 JS 的 npm 那样统一，本质上还是设计不同导致的，Go 想做的包管理是一种分布式的，没有Python 或者 JS 那种中心仓库，这样又带来了一个弊端，如果包的提供者频繁提交新版本怎么办，所以在包管理的工作中，版本号是必须要存在的。 关于之前GO项目为什么非要放在 GOPATH 下，以及 GO的包管理发展历程可以参考下面文章： GO问答之为什么项目要在 GOPATH/src 目录下 Go 包管理的前世今生 直到 go mod 出现以后，在Go 中引入第三方模块算是方便了不少，参考《拜拜了，GOPATH君！新版本Golang的包管理入门教程》 go mod使用 go mod 怎么导入本地其它项目的包？ 谈谈go.sum Go 的常用命令Go 作为一种语言，同时也代表了一系列工具和生态环境，它的命令有不少，下面列举一些常见的： go env: 打印go的环境信息 go fmt: 运行gofmt对go代码进行格式化 go build: 编译包和依赖 go run: 编译并运行go程序 go version: 显示go程序的版本 go help: 打印命令的帮助信息 go get: 下载并安装包和依赖（-v 显示操作流程的日志及信息；-u 下载丢失的包，但不会更新已经存在的包） 其中 go help 不仅仅打印了这些命令的基本信息，还可以打印出一些概念的帮助信息， 例如 go help gopath，内容比较多，下面只列举一小部分: 1234567891011121314151617181920...Here's an example directory layout: GOPATH=/home/user/go /home/user/go/ src/ foo/ bar/ (go code in package bar) x.go quux/ (go code in package main) y.go bin/ quux (installed command) pkg/ linux_amd64/ foo/ bar.a (installed package object)...... 运行 go env 展示一下当前使用的环境，方便以后做个对照： 123456789101112131415161718192021222324252627282930313233343536$ go envGO111MODULE=""GOARCH="amd64"GOBIN=""GOCACHE="/home/albert/.cache/go-build"GOENV="/home/albert/.config/go/env"GOEXE=""GOFLAGS=""GOHOSTARCH="amd64"GOHOSTOS="linux"GOINSECURE=""GOMODCACHE="/home/albert/WorkSpace/go1/pkg/mod"GONOPROXY=""GONOSUMDB=""GOOS="linux"GOPATH="/home/albert/WorkSpace/go1"GOPRIVATE=""GOPROXY="https://proxy.golang.org,direct"GOROOT="/usr/local/go"GOSUMDB="sum.golang.org"GOTMPDIR=""GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"GCCGO="gccgo"AR="ar"CC="gcc"CXX="g++"CGO_ENABLED="1"GOMOD=""CGO_CFLAGS="-g -O2"CGO_CPPFLAGS=""CGO_CXXFLAGS="-g -O2"CGO_FFLAGS="-g -O2"CGO_LDFLAGS="-g -O2"PKG_CONFIG="pkg-config"GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build602227063=/tmp/go-build -gno-record-gcc-switches" VSCode Remote 时修改的环境变量不生效这个问题也是在这次配置 Go 调试环境时遇到的，我是在 ~/.profile 文件中修改的 GOPATH的内容，但是在VSCode中调试时就是报错，其表现就跟修改的变量未生效一致，后来查资料发现，原来出现这种情况和配置文件的加载顺序有关，具体参考下列文章： VSCode Remote环境变量加载——续 remote-ssh: .profile not sourced for bash shells, only .bashrc? #83 What is the difference between interactive shells, login shells, non-login shell and their use cases? 其实在linux中的shell有 interactive shell、 non-interactive shell、 login shell、 non-login shell 的区分，每种情况下调用的初始化脚本不同，涉及到 /etc/profile、~/.bash_profile ~/.bash_login / ~/.profile等等，而在VSCode远程连接Linux时还会继承之前的环境，多次尝试之后还是不起效果，此时不得不高呼“重启大法好”，重启能解决80%的问题，剩下的20%只能靠重做系统来解决了。 在我这修改脚本内容不生效，脚本间调用还搞出了死循环的问题，不知道是不是因为我使用 zsh 这个 shell作为默认环境出的问题，但重启大法依然奏效，电脑重启后环境变量成功修改了。 C++ 的包管理器捣鼓这么久 Go，突然想到一个问题，C++ 有没有包管理器呢？答案是有的，并且有很多，但是都存在着这样或那样的问题，可以重点看一下 conan，具体使用可以搜索官网，或者看看下面这些总结。 从零开始的C++包管理器CONAN上手指南 conan使用(一)–安装和应用 conan 是一款使用 python 开发的包管理工具，所以需要依赖 Python 环境，CentOS 平台上安装 Python 环境可以参考 Centos安装python3.6和pip步骤记录，虽然CentOS即将被放弃，但是目前在各种云服务器上依旧是主流系统，安装 Python 环境时需要注意一步步跟着做，最好不要投机取巧，我就是因为少安装了一个依赖，导致我使用 pip install conan命令安装 conan 是报错 ModuleNotFoundError: No module named &#39;_ctypes&#39;，此时可以安装依赖，重新编译安装 ·conan· 就可以了，也就是运行 yum install libffi-devel -y，重新 make clean &amp;&amp; make &amp;&amp; make install 就可以了。 总结 Go 的项目在 1.12 版本之后不必放到 GOPATH 路径中了，灵活度大大提高 C++ 也是有包管理器的，其中 conan 排名比较靠前，它是由 Python 语言编写的 go get 是安装依赖包常常要用到的 Go 命令， go help 会提供 Go 相关的许多知识 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有时正确的选择比刻苦努力更加重要，用战术上的勤奋来掩盖战略上的懒惰，其结果只是感动了自己，而不会带我们达到目标。将者，智信仁勇严也~ 2021-2-21 01:19:01]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>GOPATH</tag>
        <tag>mod</tag>
        <tag>包管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP协议簇中的子网掩码有什么作用]]></title>
    <url>%2Fblog%2F2021%2F02%2F14%2FTCP-IP%E5%8D%8F%E8%AE%AE%E7%B0%87%E4%B8%AD%E7%9A%84%E5%AD%90%E7%BD%91%E6%8E%A9%E7%A0%81%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言前几天在配置数据库主从结构时查询了一些IP配置，后来编写远程执行脚本时又配置了一些IP数据，在linux上使用 ifconfig 命令查询IP配置时发现子网掩码不是常用的3个255，而是 255.255.254.0，只是简单记得这样配置扩大了子网的范围，但是具体子网掩码怎么使用还是记不清楚了，所以查询了这些还给老师的知识，在此总结一下。 IP地址分类关于IPv4的地址分类之前有专门的总结，可以参考这一篇《IP地址常见分类：A类、B类、C类、D类、E类》，其中常用到的也就是A类、B类、C类这三种，默认的子网掩码分别为 255.0.0.0、255.255.0.0、255.255.255.0，通过IP地址和子网掩码进行与运算可以得出IP地址所对应的网络地址。 子网掩码子网掩码通过它的名字来判断肯定和子网有关系，掩码又有遮遮掩掩之意，合在一起其实就是通过其作用来进行的命名，也就是对IP地址进行遮掩，然后得到子网地址的作用。 掩码的作用刚刚说通过子网掩码和IP地址可以获得网络地址，那么没有子网掩码可不可以呢？实际上如果进行某些约定就可以不使用子网掩码来获得网络地址，比如IPv4是通过4个字节来表示一个IP地址的，分为网络号和主机号两部分，A类IP地址可以使用第一字节表示网络号，B类地址使用前两个字节表示网络号，C类地址使用前3个字节表示网络号，其实这也是默认的表示方法。 加入子网掩码只是为了更灵活的配置子网，如果都按照默认的“规矩”进行划分就太死板了，难以适应复杂的需求环境，比如一个A类地址 10.0.48.36 默认的子网 10.0.0.0，可以容纳16777214台主机，这样如果一个子网的所需IP数量远远小于这个数字就会产生巨大的浪费。 再比如一个C类地址 192.168.0.1 的默认子网是 192.168.0.0，可以容纳254台主机，如果一个公司或者组织有500人，那么一个C类的子网就无法满足要求，需要分配B类地址才能解决问题，所以才有了子网掩码的出现，可以使子网的划分更加合理。 掩码表示掩码的表示比较简单，它由四个字节构成，表示成二进制形式时为前1后0，可以像IP地址一样的格式来书写，也可以写出掩码中有多少个1，举个例子，下面表示一个IP和子网： 192.168.1.1 和 255.255.254.0 也可以将掩码中1的个数写在IP地址后面表示成： 192.168.1.1/23 子网拆分增加子网掩码中1的个数可以将子网进行拆分，比如地址 192.168.1.1 和 192.168.1.130 在子网掩码是 255.255.255.0时都属于 192.168.1.0 这个子网，但是将子网掩码换成 255.255.255.128， 那么这两个IP地址就分别属于 192.168.1.0 和 192.168.1.128 这两个子网。 子网合并减少子网掩码中1的个数可以将子网进行合并，比如地址 192.168.1.1 和 192.168.0.1 在子网掩码是 255.255.255.0时分别属于 192.168.1.0 和 192.168.0.0 这两个子网，但是将子网掩码换成 255.255.254.0， 那么这两个IP地址就都属于 192.168.0.0 这个子网了。 网络包的传播网络包在子网内是以广播的形式传播，靠的是MAC地址，通过ARP协议可以获得，但是子网间的传播依赖IP地址，需通过路由或者三层交换机才能实现。 总结 通过IP地址和子网掩码进行按位与运算可以计算出IP地址所在的网络地址 通过子网掩码可以更合理的划分子网，对默认子网进行拆分与合并，一定程度上减少浪费和拥堵 子网内通信需要MAC地址，广播的形式来传递消息，这就是为什么网卡设置为混合模式可以截获到发给他人的消息 子网间通信需要借助IP地址才能实现，不过MAC地址也是需要的，通常被设置为网管的MAC地址 平时使用的 ping 命令，其实是 ICMP 协议的一部分，它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 勤劳的人总能挤出时间来坚持自己的习惯，懒惰的人总能找到借口来逃避应该做的事情，认定的目标就要大胆的开始，无论什么想法只要迈出了第一步就不算晚~ 2021-2-14 23:57:52]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>ip</tag>
        <tag>ping</tag>
        <tag>子网掩码</tag>
        <tag>MAC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++中string和int相互转换的常用方法]]></title>
    <url>%2Fblog%2F2021%2F02%2F08%2FC-C-%E4%B8%ADstring%E5%92%8Cint%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言字符串操作是各种算法题中的常客，很多数据常常以字符串形式给出，其中有的需要自己转化成整数，而一些整型数据有时转换成字符串处理起来更加方便，比如判断一个整数是否是回文数，所以字符串和整数的转换是一些问题处理的基础步骤，C++ 在处理这类问题时并不像 Python 那样方便，但是也有许多方法能够实现，为了今后查找方便，整理如下。 int 转 string通过 std::to_string() 函数转换12345678#include &lt;iostream&gt;int main()&#123; int num = 123; std::cout &lt;&lt; std::to_string(num); return 0;&#125; 这种方式在 C++11 中才能使用，编译时记得加上 --std=c++11 的选项 通过 ostringstream 转换1234567891011#include &lt;iostream&gt;#include &lt;sstream&gt;int main()&#123; int num = 123; std::ostringstream ss; ss &lt;&lt; num; std::cout &lt;&lt; ss.str(); return 0;&#125; 这是一种通过字符流的方式将整数转换成字符串，这种方式在C++11之前也可以使用 通过 sprintf 转换1234567891011#include &lt;stdio.h&gt;int main()&#123; int num = 123; char buffer[256]; sprintf(buffer, "%d", num); printf("%s", buffer); return 0;&#125; 这是一种C语言中的转换方式，sprintf 也可以换成更安全的 snprintf 函数 string 转 int通过 istringstream 转换1234567891011121314#include &lt;iostream&gt;#include &lt;sstream&gt;int main()&#123; std::string str = "668"; int num = 0; std::istringstream ss(str); ss &gt;&gt; num; std::cout &lt;&lt; num; return 0;&#125; 使用 istringstream 可以从字符流中读取整数，与 ostringstream 是一种相反的操作 使用 sscanf 来转化123456789101112#include &lt;iostream&gt;#include &lt;stdio.h&gt;int main()&#123; std::string str = "668"; int num = 0; sscanf(str.c_str(), "%d", &amp;num); std::cout &lt;&lt; num; return 0;&#125; 注意 sscanf 函数的第一个参数类型是 const char *，string类型的参数需要转换一下 使用 atoi 转换123456789#include &lt;iostream&gt;#include &lt;stdlib.h&gt;int main()&#123; std::string str = "668"; std::cout &lt;&lt; atoi(str.c_str()); return 0;&#125; atoi 函数的头文件是 stdlib.h，同样是一个C语言中的函数 总结 itoa 不是c语言标准函数，在跨平台的整数转字符串的代码中不要使用这个函数 atoi 是一个标准函数，需要将它和 itoa 区别开来，这一点很容易记混的 如果是在C++环境中进行转换，推荐使用 stringstream 字符流的形式和 to_string 函数 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 这个世界真的很有趣，只要活着，就会有无限可能~ 2021-2-9 00:44:34]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>string</tag>
        <tag>int</tag>
        <tag>相互转化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中一些方便的算法函数和吃不够的语法糖]]></title>
    <url>%2Fblog%2F2021%2F01%2F30%2FC-%E4%B8%AD%E4%B8%80%E4%BA%9B%E6%96%B9%E4%BE%BF%E7%9A%84%E7%AE%97%E6%B3%95%E5%87%BD%E6%95%B0%E5%92%8C%E5%90%83%E4%B8%8D%E5%A4%9F%E7%9A%84%E8%AF%AD%E6%B3%95%E7%B3%96%2F</url>
    <content type="text"><![CDATA[前言C++由于其复杂性，学习成本很高。但是作为接近底层的语言，学会之后能做的事情相当多，C++给了开发者极大的自由，只要符合规范就可以尽情的折腾，不过对于日常使用来说确实不太“方便”，特别是相比于Python、JS这类脚本语言，处理一些小问题时前奏太长，很多常用操作都需要自己造轮子，这一点在刷题时感觉很明显，C++一碰到字符串分析就“头大”，Python用一行搞定，用C++则需要N行。 变化其实很多人对C++的认识还停留在 C++98 或者 C++03 版本，然而从 C++11 版本开始C++就发生了翻天覆地的变化，近期在使用的过程中发现C++也有很多方便的函数，越高的版本越方便，语法糖也越多，今天先总结几个，后续再补充吧。 优秀函数和语法糖使用find系列实现trim函数trim函数在很多语言中都是内置函数，可以去除收尾两端的空格，在C++中是没有trim函数的，需要自己实现一下，需要用到的工具函数有下面两个： find_first_not_of：在字符串s中找到第一个不等于指定字符序列ACDE..的位置 find_last_not_of：在字符串s中找到最后一个不等于指定字符序列ACDE..的位置 find_first_not_of(&quot;hello world&quot;, &quot;he&quot;) 指的就是找到第一个不等于 h 且不等于 e 字母的字符位置，要想去除字符串首尾空格就需要找到第一个不等于空格的位置，和最后一个不等于空格的位置，保留这两个位置中的部分即可，实现如下： 123456std::string&amp; trim(std::string &amp;s) &#123; if (s.empty()) return s; s.erase(0, s.find_first_not_of(" ")); return s.erase(s.find_last_not_of(" ") + 1);&#125; 测试代码如下： 12345678910111213141516171819#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;std::string&amp; trim(std::string &amp;s) &#123; if (s.empty()) return s; s.erase(0, s.find_first_not_of(" ")); return s.erase(s.find_last_not_of(" ") + 1);&#125;int main() &#123; string s(" Hello world "); cout &lt;&lt; "before trim ==&gt;" &lt;&lt; s &lt;&lt; "&lt;&lt;==" &lt;&lt; endl; cout &lt;&lt; "after trim ==&gt;" &lt;&lt; trim(s) &lt;&lt; "&lt;&lt;==" &lt;&lt; endl; return 0;&#125; 测试结果如下： 1234albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ g++ stringtrim.cpp -o stringtrimalbert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ ./stringtrimbefore trim ==&gt; Hello world &lt;&lt;==after trim ==&gt;Hello world&lt;&lt;== 使用regex实现split函数split 也是一个常用但C++不提供的函数，在C语言和早期的C++中一般通过 strtok 函数来实现，但是从 C++11 开始可以通过 regex 来实现，可以看下面这个例子： 12345678910111213141516#include &lt;regex&gt;#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; string s("c++11 test split"); regex reg(" "); // "\\s+" for blank vector&lt;string&gt; v(sregex_token_iterator(s.begin(), s.end(), reg, -1), sregex_token_iterator()); for (auto str : v) cout &lt;&lt; "==&gt;" &lt;&lt; str &lt;&lt; "&lt;&lt;==" &lt;&lt; endl; return 0;&#125; 测试结构如下： 12345albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ g++ stringsplit.cpp -o stringsplit --std=c++17albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ ./stringsplit==&gt;c++11&lt;&lt;====&gt;test&lt;&lt;====&gt;split&lt;&lt;== 使用auto遍历map结构很早以前C++中遍历map、set等复杂结构的时候需要写很长的代码来定义迭代器，自从出现了auto之后这种遍历简单了许多，最近发现针对map的遍历还有更简单的方法，使用方法如下： 123456789101112131415#include &lt;map&gt;#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; map&lt;string, int&gt; m&#123;&#123;"tom", 20&#125;, &#123;"albert", 18&#125;, &#123;"bella", 19&#125;, &#123;"bily", 30&#125;&#125;; for (auto&amp; [name, age] : m) &#123; if (age &gt; 18) cout &lt;&lt; name &lt;&lt; endl; &#125; return 0;&#125; 测试结果如下： 12345albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ g++ autonew.cpp -o autonew --std=c++17albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ ./stringsplitbellabilytom 总结 find_first_not_of、find_last_not_of、find_first_of 这系列函数功能虽简单，但是使用它们可以简化代码逻辑 正则表达式是处理字符串查找的强有力的工具，合理的使用正则表达式可以达到事半功倍的效果，C++中请使用 std::regex C++17中引入了结构化绑定声明，可以使用auto来声明多个变量，所有变量都必须用中括号括起来 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 做自己认为对的事情，剩下的交给时间~ 2021-1-31 23:04:17]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>语法糖</tag>
        <tag>auto</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启用make多任务参数让构建过程加速完成]]></title>
    <url>%2Fblog%2F2021%2F01%2F23%2F%E5%90%AF%E7%94%A8make%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%8F%82%E6%95%B0%E8%AE%A9%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B%E5%8A%A0%E9%80%9F%E5%AE%8C%E6%88%90%2F</url>
    <content type="text"><![CDATA[前言是不是有时候觉得使用make命令编译时太慢了，特别是紧急修改BUG的时候，恨不得钻进电脑里帮着编译器一起干活，其实make命令是可以加速的，使用 -j 选项即可指定make启动的任务数，它是 --jobs[=jobs] 的缩写形式，比如 make -j4 就表示同时启动4个任务并行构建，虽然达不到4倍的关系，但是要比原来快多了。 寻找寻找这样一个参数的原因还是觉得有时编译太慢了，特别是修改一个公共的头文件时，几乎要从头编译到尾，启用 -j 参数的过程异常顺利，没有任何报错，迅速的就构建完成，这让我想起了之前一个项目中遇到过的问题。 同样是编译过程比较慢，但是老大哥告诉我启用多线程编译会报错，作为新手小白的我就默默记住了，也没有过多的探究，毕竟编译的次数不是那么多，偶尔长时间编译一次也没有什么关系，但是现在突然想知道当时为什么使用多线程编译会报错呢？ 可能的原因当知识渐渐丰富以后，面对这样的问题还是有些头绪的，启用 make -jn时被称为多任务并行构建，也有的文章会写多线程编译或者多进程编译，从表现来看至少是多进程的，因为在任务列表可以看到不同的进程id，不过这里的名字不是重点，重点是有多个任务在同时干活。 当启用多任务构建时原来的串行构建逻辑就变成了并行，那么此时构建失败多是由依赖关系指定不正确导致，这种依赖关系通常有两种： B模块编译需要用到A模块的函数 B模块构建过程中需要的临时数据由A模块构建时产生，两种共用临时数据，但是有序 总结来说就是逻辑上需要A构建完了，才能开始构建B，如果此时先构建B任务就会导致出错，这就能解释为什么使用 make 可以成功，但是使用 make -j4 就构建失败，也能解释为什么失败之后，多次执行这个命令可能还会成功，因为多次执行以后可能会把A模块先构建完，这样后面再构建B就不会出错了。 加速从上面的分析可以得知，模块间的依赖关系决定了多任务构建时应有的顺序，那么是不是所有的构建任务都可以通过 -j 来加速呢？答案当然是否定的，如果要构建项目的所有模块的依赖关系完全是线性的，那么就没有办法并行完成，比如下面这样的： 123456graph TB A--&gt;B; B--&gt;C; C--&gt;D; D--&gt;E; E--&gt;F; 但是整个任务如果可以进行拆分，整个依赖图中出现分叉，那么就可以通过这种方式来加速，比如像这样的依赖关系： 123456graph TB A--&gt;B; A--&gt;C; B--&gt;D; B--&gt;E; C--&gt;F; 上面所表示的关系中，虽然 B 和 C 都需要依赖A完成，但是当 A 完成后，B 和 C 的构建就可以并行开始，这样就可以达到加速构建的目的。 视网膜效应之前也没有注意到 make 命令这个 -j 的选项，自从在项目中使用了一次，我发现在很多项目说明中都看到了这个参数，比如安装 global 的时候，编译 tendis 的时候等等，之前也有这样的情况，就是当你刚接触一个事物，或者进入一个新领域的时候，发现其实周围很多人都在讨论这些事物，自己以前都没注意到，上网查了一下，原来这叫做视网膜效应。 视网膜效应这个学术名词用白话文来讲就是“心眼”，每一个人的眼睛，都是跟着心走！简单地说，这种效应的意思就是一个人的身心状况会影响他的视线，当他自己拥有一件东西或一项特征时，他就会比平常人更会注意到别人是否跟他一样具备这种特征，即越关注什么就越出现什么。 视网膜效应是一种狭隘视野与思维的反映，它会导致看问题不全面，甚至会出现牛走羊肠道、鼠钻牛角尖的极端现象。其实每个人的特质中，都有很多优点和缺点。当一个人只知道自己的缺点是什么，而不知发掘优点时，视网膜效应就会促使这个人发现他身边也有许多人拥有类似的缺点，进而使他的人际关系无法改善，生活也不会快乐。 随便聊聊make从刚接触这个命令的时候就认为它是编译的意思，其实这是一种先入为主的思想，因为之前在linux安装软件时常需要下面这三步： 123./configure./make./make install 从源码安装软件就需要进行编译，所以一直认为这三步是配置、编译、安装的意思，其实 make 本身并不会编译，它只是编译命令的搬运工。 它的真实含义应该是构建，这个构建可不一定是编译，可以是任何逻辑化的事物，只不过常常用 make 来完成编译任务，所以把它和编译绑定到了一起，构建时需要图纸的，这个图纸就是 Makefile 文件，只要我们画好了Makefile图纸，那么 make 命令就可以根据它来完成任务。 所以当你运行 make 命令时，仿佛在说: “Here’s your drawing, go go go！”，又仿佛在说：“图纸搁这儿呢，可劲儿造吧”，以这个观点来看 make 的一系列命令就有意思了： make：图纸搁这儿呢，可劲儿造吧 make clean：把你弄得这堆破烂儿，拾掇拾掇 make install：把你鼓捣出的那玩意，搬到旮旯去 make dist：赶紧把那玩意打包拉走 cmake既然 make 是编译命令的搬运工，那么 cmake 又是什么意思呢？大胆猜测他就是 config make 的意思，它的作用是生成 Makefile 文件，换句话来说就是给 make 造图纸的。 总结 使用 make -j4 命令可以开启4个任务并行构建，大大加快构建速度 make 本身并不能进行编译，它只是各种编译命令的搬运工，需要Makefile图纸才能进行工作 cmake 的作用是生成 make 所需的图纸，有了它可以更快更方便的生成一些规范的Makefile文件 视网膜效应指的是越关注什么就越出现什么的效应，是一种狭隘视野与思维的反映，会导致看问题不全面 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 未经他人苦，莫劝他人善，世上总有一些你无法理解，但却真实存在的生活~ 2021-1-23 17:24:15]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>linux</tag>
        <tag>编译</tag>
        <tag>make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次解决Intel 9462无线网卡的笔记本安装Ubuntu16.04后无法连接WIFI问题的艰难历程]]></title>
    <url>%2Fblog%2F2021%2F01%2F16%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E8%A7%A3%E5%86%B3Intel-9462%E6%97%A0%E7%BA%BF%E7%BD%91%E5%8D%A1%E7%9A%84%E7%AC%94%E8%AE%B0%E6%9C%AC%E5%AE%89%E8%A3%85Ubuntu16-04%E5%90%8E%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5WIFI%E9%97%AE%E9%A2%98%E7%9A%84%E8%89%B0%E9%9A%BE%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言一台新申请的ThinkPad-X390笔记本需要安装Ubuntu系统，为了统一工作开发环境，选择了大家一直都在使用Ubuntu16.04版本，虽然这个系统版本旧一点，不过同一个版本出现开发环境问题也好排查，但没想到的是这里埋下了一个坑，导致成功安装系统后却不能上网，足足折腾了一整天才搞定，记录一下，没事回来看看还能乐呵乐呵。 安装环境 ThinkPad-X390 笔记本 Intel® Wireless-AC 9462 无线网卡 Ubuntu 16.04.7 系统 解决方案根本原因：Ubuntu 16.04内核版本太低，无法自动识别网卡，需要手动升级内核，安装无线网卡驱动。如果你遇到的情况也是新版本无线网卡，安装老版本系统时无法连接WIFI，请参考这个无私博主的解决方法，有这一篇足够了 联想 Yoga C740：：关于Ubuntu16.04下无法识别Intel WIFI6 AX201无线网卡的解决方案，https://blog.csdn.net/dieju8330/article/details/101422743 安装过程下面开始碎碎念内容，虽然现在看来只要是上面那一个链接就可以解决问题，但是这却是浏览了几百个网页后，不断尝试才证明它是有效的，所以我也简单记录一下这个过程，以下的内容可能对你解决问题可能没什么帮助了，不过你要是喜欢听故事倒是可以继续瞧瞧。 初始环境一台预装了Win10系统的笔记本，连接WIFI可以正常上网，一开始是打算装双系统的，但是台式机是Win10，笔记本再保留一个Win10有些浪费空间，准备只安装一个Ubuntu算了。 又要装Ubuntu了，我在CSDN的第一篇文章的就是有关Ubuntu的，《ubuntu 12.04系统黑屏，登录界面黑屏，命令行界面可用》，当时靠着几个命令把问题解决了，这次解决问题同样需要几个命令，但是这个过程就复杂多了。 安装Ubuntu16.04.2本地找到了之前在虚拟机中安装使用过的 ubuntu-16.04.2-desktop-amd64.iso 镜像，使用UltraISO刻录U盘发现一直占用，遂改用rufus来刻录，发现这个软件操作也很方便，顺利完成启动盘制作，安装过程也比较顺利，网上很多大佬的教程都可以拿来参考，比如下面这几篇： 《Win10 + Ubuntu 16.04双系统完美安装教程【详细】》 《Windows10安装ubuntu16.04双系统教程》 《卸载win10装Ubuntu笔记》 注意，前两篇是装双系统的，和我们的只安装一个Ubuntu16.04的目标不太一致，但是绝大部分步骤都是一致的，网络上的知识就是这样，要想完全一样太难了，需要学会变通。 比较懒我就不截图了，去大佬文章里看吧，说一说其中的几个关键点： 安装Ubuntu需要关闭BOIS中的Secure Boot 选项 注意一下安装双系统和单个系统时进行分区操作的差异，选对主分区和逻辑分区 烧制U盘和安装系统时涉及到分区表GPT和MBR的选择，烧制U盘与硬件分区模式一致就好，在windows通过命令或者“管理”都很容易查到 启动方式有Legacy和UEFI区别，记住UEFI是新的，能用它就用它，不行就换成Legacy，大不了重新装一次，当然你根据硬件情况能准确判断最好 安装过程中有一步是勾选“为图形或无线硬件…安装第三方软件”，看到很多教程都没勾选，我也没选（伏笔1），感觉我已经入坑了 非常自信的干掉了Win10系统（伏笔2），干干净净的只安装了Ubuntu，安装很快完成，顺利进入系统，期间跳出来几行错误，我都没看清就闪过去了，应该是不重要，进去后准备部署开发环境，发现没有WIFI列表，这就奇怪了，网卡肯定没问题，之前Win10是可以上网的，肯定是设置的不对，上网查查为什么吧。 硬件禁止问题这是我看到的第一个解决方案，需要改 /etc/Network/NetworkManage.conf 文件中的 manage=false 改成 manage=true, 参考下面这帖子 解决Ubuntu下的WiFi列表不显示问题（硬件禁止问题） 现在回过头来想想我根本就不是这个问题，很多特点都对不上，完全是病急乱投医的处理方式，使用 rfkill list 命令根本找不到我的网卡，这一点也是后来才发现的 激活无线网卡很多文章开局就是下面这两个命令 12sudo apt-get updatesudo apt-get install --reinstall bcmwl-kernel-source 要知道使用 sudo apt-get update 是需要联网的，现在我连不上无线，不存在有线，所以上不去网，然后翻了N个页面之后找到了大神离线安装的方法，其实这个安装文件在镜像里就有，所以打开刚刚烧制的U盘就行了，但是直接安装会有依赖问题，解决这个问题就是一个个在镜像中找到，然后安装上。 《Ubuntu离线安装网卡驱动》 《【菜鸟向】Ubuntu无法连接wifi问题的解决》 一步步操作之后完全没有作用，现在来看如果能起作用还怪了，这是bcm博通网卡的处理方法，而我的网卡是Intel的，又是做了无用功 更新软件列表附加驱动这种解决方案网上流传的最多，其实就是依靠Ubuntu的软件仓库来解决 Ubuntu16.04无法连接Wifi解决方案 （绝对有效） ubuntu16.04中不能连接无线网络 解决这个问题的关键就是先联网，仿佛陷入了死循环，我现在就是要解决无线网卡连不上网的问题，你却让我先联网更新，其实对于可以连接有线网络的电脑来说没什么问题，但是我这里连不上有线网啊！ 然后我就学到了一个逆天的操作，原来USB可以供网，可以通过USB线让笔记本使用手机的4G网络，瞬间打开了一扇大门，说干就干，先更新一下 Ubuntu 的软件源，可以参考下面这个文章，其实有很多源的，比如常用的清华源、阿里源等等，但是要注意版本，网上流传着一些错误版本的地址，需要自己甄别一下。 Ubuntu16.04更换国内源 这个软件源其实类似于应用商店的地址，换了软件源就相当于换了一个应用商店，还完源更新之后就可以从软件仓库中有哪些软件，下次再安装软件时就可以找到了，在这列举一个常用的Ubuntu16.04的软件源，替换 /etc/apt/source.list 文件内容就好了。 123456789101112131415deb http://mirrors.aliyun.com/ubuntu/ xenial maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial maindeb http://mirrors.aliyun.com/ubuntu/ xenial-updates maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates maindeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security maindeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe 准备就绪，USB连接手机，然后共享网络，执行 sudo apt-get update 命令，照着教程操作一通，发现并没有在附加驱动选项卡中发现可用的驱动，失败了，同时耗费了1G流量导致手机网络报警。 还是回过头来看看，很多人说这种方式解决了问题，但我没有成功，看看成功的那些截图还是博通的网卡，而我这个Intel的网卡有点难搞啊 更新内核1从现在开始渐渐步入正确轨道了，因为我发现的网上各种设置网卡参数的命令在我这没有任何反应，比如执行 rfkill list 结果显示根本没有网卡，只有两个蓝牙，执行 lshw -c network 之后发现 *-network UNCLAIMED 内容，开始搜索相关内容，然后发现了下面的帖子： Network unclaimed on Ubuntu 提问者根据回答人提供的步骤一步步解决了自己的问题，而我在安装这个内核版本时提示已经最新无法继续安装，并且在回答中提到安装 16.04.4 版本的镜像不需要做任何设置就可以使用无线网了，想到自己安装的是 16.04.2，就怀疑是这里的问题，准备安装高版本镜像 安装Ubuntu16.04.7由于Ubuntu官网下载比较慢，所以找了国内的镜像网站下载了 16.04.7版本，刻录U盘启动盘，准备安装 中科大ubuntu镜像站 几个ubuntu16.04镜像下载地址 这次安装的熟练了一些，完成后进入系统并未有什么改变，依旧连不上网，尝试更新软件试试，于是再一次连接手机USB网络，执行下面命令 12sudo apt-get updatesudo apt-get upgrade 又是几百兆流量进去了，依然没有效果 查找网卡版本至此虽然已经连不上网，但是基本确定了问题的原因，不是网卡设置的问题，而是驱动版本不对，系统无法识别网卡型号，没有办法正常工作。 可是怎么才能知道网卡的型号呢？如果正常驱动的无线网卡可以使用 lspci | grep Net 这个命令，但是我这里不行，只显示Intel设备，无法显示设备类型和具体型号，倘若我的win10还在，直接在设备管理器中查就可以了，但是他已经被我自信的格式化了，此处捶胸顿足。 那怎么办呢？问问旁边的小伙伴？关键电脑型号不一样啊！撬开电脑后盖？估计无线网卡在哪我都找不到。对了，去联想官网看看配置，应该能找到的吧！ 说干就干，打开联想官网居然找不到这个具体的型号，只能找到 ThinkPad-X390 系列，而我这个子系列是20SD，官网上有20SC、20SX、20Q0等等，就是没有20SD，没办法了，看看这个相似的这几个无线网卡都是什么型号吧，发现大多数都是 Intel® 9560AC，部分提到了 Intel® Wi-Fi 6 AX200，这两个网卡差的可是有点多，需求的Ubuntu内核版本差了一个大版本，他们的对应关系可以参考Intel提供的这个文档 Linux* Support for Intel® Wireless Adapters 联想的官网只提供Windows上的驱动下载，Linux版本只能自己去Intel上找了，现在需要确定网卡版本来下载相应的驱动，但是目前查不到这个网卡的具体型号，还能怎么办呢？去找联想客服吧！ 在联想的官网上找到了联想的售前，说明问题后给出的答案是大概率是 Intel® 9560AC 型号，然后转接技术处理，等待N久之后查了SN码说应该是 Intel® 9560AC，但是我认为应该是网卡型号应该是 Intel® Wi-Fi 6 AX200，如果是 Intel® 9560AC 型号的网卡，需要 Ubuntu的内核是 4.14+，而我安装的内核版本是 4.15.7，系统没理由不认识这个网卡。 想想还有什么办法查网卡型号呢？既然windows上可以看，那我省点事安装个WinPE进系统看一下。下载常用的微PE工具箱，烧制U盘后进了系统，点开设备管理器，一大串设备都带着问号，原来在WinPE里也没有这些设备的驱动，我又跑去联想官网下载了Windows上的网卡驱动，放到WinPE上安装，结果因缺少DLL而失败，这条路也行不通了。 只能重新做个win10的系统看看了，再次对删除win10前没看网卡型号而捶胸顿足，下载了5个多G的镜像文件，刻录Win10启动盘，然后安装发现引导不进去，也是醉了，应该是文件系统格式不正确，导致系统无法引导，算了，死马当活马医吧，我直接把无线网卡当成最高的版本，按照Intel® Wi-Fi 6 AX200进行处理，把内核升级了应该就可以了 更新内核2WinPE方式失败后决定升级内核，于是按照文章开头提供的那篇解决方案来处理，但是内核文件安装失败了，无法升级，内心崩溃…… Ubuntu应用—安装 Intel Wireless-AC 9462 无线网卡驱动（无法连接wifi，完美解决） 这一篇也是类似的，同样无法成功更新。 安装Ubuntu20.04.1既然升级内核失败了，我干脆安装个自带高内核版本的系统算了，下载了 Ubuntu20.04.1 的镜像 清华大学开源软件镜像站 经过漫长的等待下载完成，刻录Ubuntu启动盘，安装设置一气呵成，搞定了，无线网的小雨伞出现了，简直泪流满面… 然后通过 lspci | grep Net 来查看网卡型号，结果出现了 Intel Corporation Wireless-AC 9462，看到这里我都惊呆了，原来这个网卡比 Intel® 9560AC 版本还要低，联想客服的回答是错误的，Intel官方驱动的对应关系也不准确，真是没什么可信的了。 安装Ubuntu18.04.5既然无线网卡都可以识别了，为什么还要折腾呢？因为Ubuntu20.04.1这个版本跟周围小伙伴使用的系统版本差的有点多，并且之前出现过项目编译问题，所以换个低一点的版本试试。 下载镜像，刻录U盘，安装系统轻车熟路，真是越来越顺利了，安装完成后WIFI正常，果然是内核版本的事情，高版本的系统直接就可以使用，根本不需要任何设置。 再次安装Ubuntu16.04.7因为不服输，我又折腾回来了，既然是内核版本的问题，我就再试试在这个低版本系统上升级内核，这次安装时勾选了“为图形或无线硬件…安装第三方软件”，因为我突然意识到其他教程不让勾选都是台式机，而我是笔记本，我的个乖乖啊，感觉被坑了。 这次一步步操作，安装完系统就不再做任何设置，直接升级内核，访问内核软件网站，下载下面4个包： linux-headers-5.2.16-050216_5.2.16-050216.201909190832_all.deb linux-headers-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.deb linux-image-unsigned-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.deb linux-modules-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.deb 依次按照下面的命令安装，这次居然成功了 1234sudo sudo dpkg -i linux-headers-5.2.16-050216_5.2.16-050216.201909190832_all.debsudo sudo dpkg -i linux-headers-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.debsudo sudo dpkg -i linux-modules-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.debsudo sudo dpkg -i linux-image-unsigned-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.deb 然后从网站下载linux固件版本 linux-firmware_1.190_all.deb 使用如下命令安装成功 1sudo sudo dpkg -i linux-firmware_1.190_all.deb reboot重启系统，WIFI的小雨伞终于出现了，功夫不负有心人，彻底搞定了，至于WIFI可用却连不上公司的无线网，那就是另一个悲伤的故事了，和技术无关，就不在此展开了。 踩过的其他坑其实整个过程远不止上面提到的这些，安装了10几次系统，至少翻了几百个页面，有一些处理方式尝试过但没有什么效果，也列举在这里，留个纪念 Ubuntu16.04无附加驱动无法连接WiFi总结 Ubuntu-server 10.04.4查看无线网卡型号 Ubuntu安装后未发现wifi适配器解决 ubuntu16.04+intel无线网卡无法连接WiFi解决方法，下载驱动，升级内核（亲测有效） ubuntu18.04 无法连接有线 总结 现在安装Ubuntu特别溜，刻录U盘启动盘+安装完系统也就10分钟 手机通过USB连接笔记本可以直接给笔记本提供网络，这波操作有点优秀 认识了一堆之前没用过的命令，特别是 lspci 显示设备信息和 iwconfig 管理无线网络 出现问题时还是要找到根本原因，不能病急乱投医，一些博通网卡的处理方式用到Intel上显然不合适 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当个人实力撑不起自己的野心时，需埋下头来刻苦修炼，当目前的能力无法满足自己的欲望时，需抑制一些不切实际的想法，但愿多年之后回想这些隐忍的瞬间能内心充满感恩而不是悔恨~ 2021-1-16 22:48:02]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ubuntu</tag>
        <tag>系统</tag>
        <tag>WIFI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搬迁声明]]></title>
    <url>%2Fblog%2F2021%2F01%2F12%2F%E6%90%AC%E8%BF%81%E5%A3%B0%E6%98%8E%2F</url>
    <content type="text"><![CDATA[我的博客即将同步至 OSCHINA 社区，这是我的 OSCHINA ID：osc_57262839，邀请大家一同入驻：https://www.oschina.net/sharing-plan/apply]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在比较Linux和Windows命令差异时意外发现了Windows Terminal]]></title>
    <url>%2Fblog%2F2021%2F01%2F05%2F%E5%9C%A8%E6%AF%94%E8%BE%83Linux%E5%92%8CWindows%E5%91%BD%E4%BB%A4%E5%B7%AE%E5%BC%82%E6%97%B6%E6%84%8F%E5%A4%96%E5%8F%91%E7%8E%B0%E4%BA%86Windows-Terminal%2F</url>
    <content type="text"><![CDATA[前言目前工作时的开发环境是 Windows + Linux，常常需要切换测试环境，比如写一个脚本既要在 Windows 使用，同时也要在 Linux 下运行，命令的使用是 Linux 的强项，它自带了很多方便的命令程序，比如过滤字符串的 grep，统计文件行数的 wc，查看目录文件的 ls 等等，如果要在 Windows 中运行相同逻辑的脚本，那我们需要先找到这些命令在 Windows 上的等价写法。 常用命令下面列举几个常见的命令，功能上基本等价，如果有更好的写法也欢迎小伙伴们留言补充呀，有些命令在命令框里执行感觉没有作用，但是放到脚本中就很方便了，比如下面第一个： 显示当前目录 linux 中的 pwd 12albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ pwd/mnt/d/app/ScreenToGif/Logs windows 中 cd 12D:\app\ScreenToGif\Logs&gt;cdD:\app\ScreenToGif\Logs 这个显示当前目录的命令在这来看确实没什么用，因为在命令提示框里已经显示的当前所在的目录，但是用到脚本中时就可以发挥作用了，windows 中的 cd 命令不仅可以切换目录，当命令后面不接任何参数时就可以显示当前所在的目录位置。 显示目录下内容 linux 中的 ls 12albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ ls20_04_06 11_09_57_942.txt 20_04_06 11_09_58_049.txt 20_04_06.txt windows 中 dir 12345678910111213D:\app\ScreenToGif\Logs&gt;dir 驱动器 D 中的卷没有标签。 卷的序列号是 0E68-747E D:\app\ScreenToGif\Logs 的目录2020/04/06 23:09 &lt;DIR&gt; .2020/04/06 23:09 &lt;DIR&gt; ..2020/04/06 23:09 507 20_04_06 11_09_57_942.txt2020/04/06 23:09 518 20_04_06 11_09_58_049.txt2020/04/06 23:10 2,754 20_04_06.txt 3 个文件 3,779 字节 2 个目录 56,929,013,760 可用字节 这两个命令都可以显示当前目录下的内容，但是windows下的 dir 更详细一些，当然，linux 下的 ls 命令也可以附加一些参数来达到显示详细信息的目的。 过滤文本 linux 中的 grep 123albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ ls -l | grep 09_-rwxrwxrwx 1 albert albert 507 Apr 6 2020 20_04_06 11_09_57_942.txt-rwxrwxrwx 1 albert albert 518 Apr 6 2020 20_04_06 11_09_58_049.txt windows 中 findstr 123D:\app\ScreenToGif\Logs&gt;dir | findstr 09_2020/04/06 23:09 507 20_04_06 11_09_57_942.txt2020/04/06 23:09 518 20_04_06 11_09_58_049.txt 这两个命令基本上是一样的，都是按行过滤文本，当不加参数时效果基本一致 查看文件内容 linux 中的 cat 12345678910111213141516171819albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ cat "20_04_06 11_09_57_942.txt"► Title - Automatic feedback▬ Message - Value cannot be null.Parameter name: path1○ Type - System.ArgumentNullException♦ [Version] Date/Hour - [2.14.1] 04/06/2020 23:09:57▲ Source - mscorlib▼ TargetSite - System.String Combine(System.String, System.String, System.String)♠ StackTrace - at System.IO.Path.Combine(String path1, String path2, String path3) at ScreenToGif.Model.ApplicationViewModel.SendFeedback()---------------------------------- windows 中 type 12345678910111213141516171819D:\app\ScreenToGif\Logs&gt;type "20_04_06 11_09_57_942.txt"► Title - Automatic feedback▬ Message - Value cannot be null.Parameter name: path1○ Type - System.ArgumentNullException♦ [Version] Date/Hour - [2.14.1] 04/06/2020 23:09:57▲ Source - mscorlib▼ TargetSite - System.String Combine(System.String, System.String, System.String)♠ StackTrace - at System.IO.Path.Combine(String path1, String path2, String path3) at ScreenToGif.Model.ApplicationViewModel.SendFeedback()---------------------------------- 统计文件行数 linux 中的 wc -l 12albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ cat "20_04_06 11_09_57_942.txt" | wc -l19 windows 中 find /c /v &quot;&quot; 12D:\app\ScreenToGif\Logs&gt;type "20_04_06 11_09_57_942.txt" | find /c /v ""19 wc 这个命令没什么好说的，专门为统计字符数、字节数、行数而生的，而 windows 下的 find 命令通过 /c 显示匹配函数，/v &quot;&quot; 查找非空行这样的参数组合也能达到统计行数的目的。 计算文件摘要 linux 中的 [md5sum|sha1sum|sha256] 文件名 1234567albert@home-pc:/mnt/d/data/cpp$ md5sum simple.cppa00eba0276e396de58fabc92b325672a simple.cppalbert@home-pc:/mnt/d/data/cpp$ sha1sum simple.cpp7acf1e59ca2608b7591ec526d48ce041cddf49d2 simple.cppalbert@home-pc:/mnt/d/data/cpp$ sha256sum simple.cppf07bcd585fa7e49897676105797dd984c12f63411f0ba8db62a57f6ef03bbaec simple.cppalbert@home-pc:/mnt/d/data/cpp$ windows 中 certutil -hashfile 文件名 [MD5|SHA1|SHA256] 1234567891011121314d:\data\cpp&gt;certutil -hashfile simple.cpp MD5MD5 的 simple.cpp 哈希:a00eba0276e396de58fabc92b325672aCertUtil: -hashfile 命令成功完成。d:\data\cpp&gt;certutil -hashfile simple.cpp SHA1SHA1 的 simple.cpp 哈希:7acf1e59ca2608b7591ec526d48ce041cddf49d2CertUtil: -hashfile 命令成功完成。d:\data\cpp&gt;certutil -hashfile simple.cpp SHA256SHA256 的 simple.cpp 哈希:f07bcd585fa7e49897676105797dd984c12f63411f0ba8db62a57f6ef03bbaecCertUtil: -hashfile 命令成功完成。 在 linux 上不同的摘要算法对应着不同的程序，但是在 windows 上是同一个程序有着不同的参数。 其他命令 功能 linux命令 windows命令 拷贝 cp copy/xcopy 移动 mv move/rename 删除 rm del/rd 创建目录 mkdir md/mkdir 先整理这么多，后续用到新的等价命令再补充吧！ 半路杀出个PowerShell一开始接触 PowerShell 把它认为是 cmd 的升级版，感觉就是原来的 cmd 太寒酸了，然后又搞出个 PowerShell 稍微好看了一点，功能又增加了一些而已，但是随着使用次数的增多，我发现之前的理解不太对，PowerShell 和 cmd 它俩完全没关系啊，唯一的相同点就是都被叫做命令行而已。 PowerShell 通常被描述成是面向对象的脚本语言，并且是在 .Net公共语言运行时(CLR-Common Language Runtime)和.Net Framework的基础上构建的，可以接受和返回.Net对象，引入了cmdlet的概念，是cmd的超集。 看到一些说法提到“cmd命令都可以在Powershell中执行”，但事实并非如此，比如我最近发现在cmd运行的 type a.txt | find /c /v &quot;&quot; 统计行数的命令在 PowerShell 中就不管用了，所以我认为把 PowerShell 作为 cmd 的升级不太准确。 PowerShell完全是一个新发明的东西，它里面有一些原来cmd中常用的命令，比如 cd、dir，同时它还兼容了一些 Linux 上的常用命令，比如 ls、man 等，关于这些可以简单看几个例子。 显示目录下内容123456789101112131415161718192021222324albert@home-pc D:\app\ScreenToGif\Logs&gt; ls 目录: D:\app\ScreenToGif\LogsMode LastWriteTime Length Name---- ------------- ------ -----a---- 2020/4/6 23:09 507 20_04_06 11_09_57_942.txt-a---- 2020/4/6 23:09 518 20_04_06 11_09_58_049.txt-a---- 2020/4/6 23:10 2754 20_04_06.txtalbert@home-pc D:\app\ScreenToGif\Logs&gt; dir 目录: D:\app\ScreenToGif\LogsMode LastWriteTime Length Name---- ------------- ------ -----a---- 2020/4/6 23:09 507 20_04_06 11_09_57_942.txt-a---- 2020/4/6 23:09 518 20_04_06 11_09_58_049.txt-a---- 2020/4/6 23:10 2754 20_04_06.txt 我们可以看到这两个命令在 PowerShell 中执行之后结构完全一样，然后我们使用 man 命令来查看一下两个命令的帮助文档 查看帮助文档123456789101112131415161718192021222324252627282930313233343536373839404142434445464748albert@home-pc D:\app\ScreenToGif\Logs&gt; man dir名称 Get-ChildItem语法 Get-ChildItem [[-Path] &lt;string[]&gt;] [[-Filter] &lt;string&gt;] [&lt;CommonParameters&gt;] Get-ChildItem [[-Filter] &lt;string&gt;] [&lt;CommonParameters&gt;]别名 gci ls dir备注 Get-Help 在此计算机上找不到该 cmdlet 的帮助文件。它仅显示部分帮助。 -- 若要下载并安装包含此 cmdlet 的模块的帮助文件，请使用 Update-Help。 -- 若要联机查看此 cmdlet 的帮助主题，请键入: "Get-Help Get-ChildItem -Online" 或 转到 https://go.microsoft.com/fwlink/?LinkID=113308。albert@home-pc D:\app\ScreenToGif\Logs&gt; man ls名称 Get-ChildItem语法 Get-ChildItem [[-Path] &lt;string[]&gt;] [[-Filter] &lt;string&gt;] [&lt;CommonParameters&gt;] Get-ChildItem [[-Filter] &lt;string&gt;] [&lt;CommonParameters&gt;]别名 gci ls dir备注 Get-Help 在此计算机上找不到该 cmdlet 的帮助文件。它仅显示部分帮助。 -- 若要下载并安装包含此 cmdlet 的模块的帮助文件，请使用 Update-Help。 -- 若要联机查看此 cmdlet 的帮助主题，请键入: "Get-Help Get-ChildItem -Online" 或 转到 https://go.microsoft.com/fwlink/?LinkID=113308。 结果还是完全一样，原来在PowerShell中，ls 和 dir 都是命令 Get-ChildItem 的别名，这时我才发现，原来PowerShell自己的命令其实都是这种动宾短语的形式，原来 cmd 中的命令和一些扩展的 Linux 命令大多是以别名的形式存在的。 所以从这里来看，PowerShell是一款新产品，并不是cmd的简单升级，只是这个新的产物将原来大部分的cmd命令以别名的方式进行了兼容处理而已 统一命令行的渴望上面提到了 cmd、PowerShell、linux Shell 等等，每天使用这些工具时都要开很多窗口，比较麻烦，有没什么途径把它们统一一下呢？这里的统一只是想使用的更方便而已，语法上暂时没办法统一了，偶然间发现了 Windows Terminal 这个工具，一下子打开了新的世界。 Windows TerminalWindows Terminal 是一个全新的、功能强大的命令行终端工具。包含多 Tab 支持、富文本、多语言支持、可配置、主题和样式等诸多特性，并且属于微软的亲儿子，支持的力度你懂得。 安装直接在 Windows 上打开应用商店安装即可，这时我第二次打开应用商店了，上一次是安装 WSL 的时候。 官方提供的截图也很炫酷： 我自己也尝试着美化了一下，效果还不错 命令行窗口的统一确实不想开很多个命令窗口，比如执行windows脚本命令的cmd、PowerShell，版本管理工具git bash，windows子系统WSL命令行，远程服务器命令行工具XShell等，要是把它们都放在一起就好了，使用Windows Terminal可以轻松实现这个愿望，其中cmd、PowerShell、WSL命令行都是Windows Terminal自带的，git bash 要想显示在 Windows Terminal 中需要手动配置下，运行效果如下： 配置和美化方法比较简单，网上搜索会有很多教程可以学习 遇到的问题目前使用 Windows Terminal 替代各种命令行工具只遇到了一个问题，就是它不知道 Zmodem 协议，所以无法使用 sz、rz，对于我这种需要经常上传脚本文件到服务器的人来说不太方便。 好消息是在2019年的时候已经有人在 Windows Terminal 的 github 项目中提了issue，据说会考虑这个功能，但是还得评估下，但是目前已经1年多过去了还没有反馈，焦急等待中，有了它就可以不使用XShell了。 总结 cmd 和 PowerShell 是两个不相同的命令行程序，有些cmd命令无法直接在PowerShell中运行 cmd 能做的 PowerShell 几乎都能做，但是不能直接拿过来就运行，有时需要简单的改写才可以 PowerShell 的命令采用动宾的格式，并且使用别名的形式兼容 cmd 命令和一些 Linux 命令 Windows Terminal 目前来看很优秀，可以将cmd、PowerShell、WSL Shell等运行在同一个窗口中 由于 Windows Terminal 不支持 Zmodem 协议，所以暂时还不能使用 sz、rz 命令来传送文件 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 不过是大梦一场空，不过是孤影照惊鸿，不过是白驹之过一场梦，梦里有一些相逢……歌词写的真好！ 2021-1-10 00:33:11]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>windows</tag>
        <tag>ternimal</tag>
        <tag>powershell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020年终总结！新的起航，新的征程]]></title>
    <url>%2Fblog%2F2020%2F12%2F27%2F2020%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%EF%BC%81%E6%96%B0%E7%9A%84%E8%B5%B7%E8%88%AA%EF%BC%8C%E6%96%B0%E7%9A%84%E5%BE%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[奇变偶不变，符号看象限，真正的知识就是这么朴实无华~ 前言从2019年开始意识到年终总结的意义，通过反思自己的得失进而确定今后的方向，既然去年开了头，今年也不能中断，依旧写写每天的流水账，细数最近一年发生的故事。 今年最直观的、印象最深的感受就是时间不够用，一首《时间都去哪了》旋律时常出现在脑海中，对比之前长久不变的工作环境，今年确实紧张了许多，习惯了原来的安逸，对各种可能发生的问题轻车熟路的就能解决掉，但来到新环境，一切变得不是那么轻松了。 回顾2020毕竟到了年终时刻，之前的flag可不能忘，依旧是从工作、学习、生活三个方面来回忆一下去年flag的完成情况。 工作 2020 flag：新的一年不能再碌碌无为，真的需要去闯一闯了 完成度：95% 年初如愿的换了一份新工作，之前的公司呆的时间太长了，工作内容单一，熟悉了安逸的环境，虽然每天也在学习，但是进步缓慢，在2019年末开始寻找新的发展环境，在2020初就成功找到了新的工作机会，提前完成flag内容。 与老东家分手过程可以说不太愉快，给不了梦想，撑不起现实，一味的拖着可不是解决问题的办法，不过事情已经过去，不想再提，毕竟也为之奋斗过，近2000个日日夜夜，梦开始的地方，也是认清现实的地方。 离开原公司来到新公司算是无缝衔接吧，中间在家只待了3天，如果能早点意识到离职流程这么狗血，我真应该在家躺一个月，不过新公司的入职流程还真是暖心，提前先网上入职，帮我把社保问题解决了，感谢流程中出现的每一个人。 新的工作内容刚开始肯定要吃力一些，首先是熟悉开发环境，大体和原来相同，不过复杂的申请制度较原来规范了许多，同时也麻烦了许多，版本控制是一个重要的改变，之前主要使用SVN，如今换成了 Git + SVN，以前经常使用的 git 三部曲——add、commit、push 已经不够用了，随着不断的使用学习，渐渐熟悉了 checkout、merge、rebase、cherry-pick reset 等等子命令，还是那句话多用才能学会，如果只停留在纸面上，只是眼睛学会了，闭上眼睛就忘记了。 选择这份新的工作也是想拓宽自己的知识面，之前一直在做分区分服的游戏，想学习一下分布式游戏的开发和一些需要需要注意的问题，所以说这份工作是用来补充我这方面知识短板的，从学习了一年的结果来看，确实达到了这个目的，虽然现在的规模还没有那么大，有些逻辑来不太完善，但是这已经给我做出了示范，在今后的一段时间内，将沿着这条路不断的前行，去探索自己未知的领域。 新的工作认识新的伙伴，大家相处起来还是很友好的，一年来的工作算是中规中矩，不过年底这俩月有点滑铁卢的味道，接连在同一个位置摔倒几次，还是老大给背了锅，内心愧疚不已，无以为报，愿以更饱满的热情投入工作中，实践当初并肩战斗的诺言，得一伙伴不易，愿共同进步。 今年的工作强度明显高于以往，倒不是工作时间长了，而是在新环境中，长时间聚精会神的盯在一件事上本来就很累，头发掉的明显比之前多了，眼睛的视力也下降的厉害，这两点要注意一下了，可以调整一下工作的节奏，注意劳逸结合，我可不想“聪明绝顶”。 学习 2020 flag：看两本有关分布式知识的图书，多看一些开源项目的代码 完成度：85% 有关分布式架构的书今年只看了一本，另外基本都是和育儿知识相关，他们分别是： 从零开始学架构 ——照着做，你也能称为架构师 你就是孩子最好的玩具 正面管教 小狗钱钱2 今年的1本技术书籍对比去年的7本少了很多，一方面换了新工作后需要花很多时间来熟悉新的业务逻辑，用来读书的时间被侵占了一部分，另一方面宝宝一天天长大，需要花些时间去陪伴她，虽然技术书读的少了，可是故事书我可没少读，一年读了好几十本： 这些故事书中描述的故事小时候没觉得有什么不妥，不过以现在成年人的身份来看，有些故事太离奇了，比如大灰狼和七只小羊的故事，山羊妈妈居然剪开狼的肚皮救出了被吃掉的六只小山羊，然后在狼的肚子里装满石头缝上了，整个过程大灰狼都没有醒，我感觉山羊妈妈可能是个麻醉科的护士。 虽然故事内容离奇，但是并不妨碍小娃娃听到津津有味，小孩子的世界还真是单纯，有故事听就老老实实的等着，而我化身为一个播报员，一遍一遍的重复着书中的故事。 博客总结今年也没有丢，一共写了45篇，比去年还要多几篇，总体来看类别很多，总结的内容并不难懂，写到博客中主要是为了方便日后的查找，这一年的总结绝大部分都是晚上和周末花时间写的，只有尝试过才明白，想写好一篇总结需要花费很多很多时间，今年的成长可以对比两年数据来看一下。 C++11的使用在这一年里变得更加熟练，之前的项目中无法使用C++11的特性，所以很多知识都是自己额外花时间来测试学习，来到新项目可以参考已有的代码，进一步巩固C++11的使用，知识还是越用掌握的越好。 语言方面还有Python今年用的比较多，得益于工作中的多次锻炼，一些常用的函数，类型可以很熟练的写出来了，虽然没有用来写过什么大的软件，但是也在不断尝试着使代码更规范，比如使用class、装饰器、继承、记录运行日志等等。 Go语言今年算是简单入门吧，之前只能算是听说过，今年快速的浏览了一遍Go语言的语法，编写了一些测试程序，但是对于Go语言的条条框框还是不太习惯，需要慢慢适应下。 刷题今年一直在坚持着，之前使用的国际账号一时找不到了，今年新注册了国内版LeetCode账号，本着刷简单冲中等的态度，一直在默默的洗刷刷，不过今年参加了几次竞赛，一直是两题选手，只有一次题比较简单全答出来了，当时还是很开心的，继续加油吧。 阅读开源项目源码方面，今年也有了很大的进步，在新项目中接触了好几个之前没有使用过的开源库，虽然没有完全整明白，但最起码开阔了眼界，比如 easyloging++、nolhmann json库等等，另外今年还意外获得了 Github 的 Arctic Code Vault Contributor 勋章，看来我写的BUG要被雪藏千年了，哈哈。 生活 2020 flag：尽最大可能陪陪家人、投资达到2019的水平 完成度：55% 多陪陪家人这一项今年应该算完成了，因为疫情今年也没有出去疯，就是出门买买菜，其他的闲暇时间都和家人在一起，天气好的时候去附近的公园逛逛，对比2019年，每天晚上回家的时间晚了一点，但是多了每周多了一整天可以和家人一起度过。 现在每天还是很充实的，早上起来妈妈已经做好早饭，赶紧洗漱和家人一起吃个早饭，然后和宝宝告别去上班。晚上回来大多数情况宝宝已经睡了，不过有时候也能挺到我回家，和我玩一会再睡。周末陪宝宝出去玩，透透新鲜空气，等宝宝睡觉的时候对近期所学的知识做一个总结，基本上周末的时间陪娃、总结55开吧。 这个flag完成度不高主要是投资理财这块今天基本上是停滞了，未达到2019年水平，每天除了工作就是学习，账户情况没怎么看，整个一个过山车行情，所以基本上处于不赔不赚的情况。 节前的最后一天是大涨行情，算是一个好的结尾，全年放养的状态收益率3个多点，勉强跑赢余额宝的收益，估计跑不过今年的通货膨胀了！ 目前的生活状况就是每天都相似，但确实很满足，早上起床一家人吃早饭，和宝宝告别后送媳妇上班，然后自己骑个自行车来公司上班，努力完成一天的工作再骑个小车回家，如果宝宝没睡还可以陪她玩一会儿，等宝宝睡着开始一天的总结，温暖而又充实。 2020年初养了一年的栀子花开花了，正好在我过年回老家的时候开的花，之前我好好浇水通风的时候它却连个花骨朵都没有，好像再告诉我只要我不管它就能好好开花，可是在10月份的时候枯死了。还有一盆文竹在夏天涨势良好，修剪了几次，但没有熬过寒冷的冬天，在11月份左右干枯了。 在上两盆花相继离开之后，我赶紧又补充了新的生机，一盆栀子花和一盆茉莉花，目前长势良好，茉莉花已经开花了，希望它俩在新的一年里花香不断。 展望2021工作 脚踏实地做好本职工作 额外挤出时间去尝试技术提升（优化、解决痛点） 在熟悉业务的同时更多参与设计的工作，拓宽自己的认知范围 学习 博客总结继续，基本保持在1周一篇，可以适当偷懒，一年懒10次可以产出40篇 开源代码还是要继续学习，libevent需要详细看一下，今年的出镜率太高 读2本技术类书籍，可以是开阔眼界的，也可以是现有技能提升的 读2本经济学、金融理财相关的书籍 生活 陪娃娃，陪家人，工作内容适应后可以多拿出一点时间和家人在一起（需要比2020多一些） 投资理财还是要多花一点时间研究下，目标7%（靠工资是不可能财富自由的，必须开源才行） 注重身体的保养，身体是本钱，可不能把身体搞垮了，愿丢掉体检时的小毛病~ 总结 2020年在工作上是一个新的开始，同时也面临着新的挑战 2020年的flag完成度大概70%，大部分愿望已经实现，未实现部分还需努力 2021年已经悄然开始，新的flag已经在路上，为了新的目标加油努力吧 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生旅途中运气与实力都很重要，但是强大的实力可以帮助你提升运气，减少不确定性（记一次事故后提心吊胆的等待），比如买一注彩票中一千万很难，但是如果你的实力可以强大到买下大部分甚至是所有的组合，那么要中一千万只需要等到开奖就可以了~ 凡是过去，皆为序幕 2020-12-27 00:32:07]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[system_clock::now()和time()时间函数混用带来的踩坑经历]]></title>
    <url>%2Fblog%2F2020%2F12%2F13%2F%E6%97%B6system-clock-now-%E5%92%8Ctime-%E6%97%B6%E9%97%B4%E5%87%BD%E6%95%B0%E6%B7%B7%E7%94%A8%E5%B8%A6%E6%9D%A5%E7%9A%84%E8%B8%A9%E5%9D%91%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[前言时间是一个可怕的东西，听说能用来杀猪。在编程世界中，时间也控制着一个维度，常常伴随着程序运行而流逝，有时也会影响着程序的运行的逻辑，所以在程序中处理时间时还是要仔细一些，最近连续踩坑，总结一下给自己提个醒，有些逻辑还是需要抱着怀疑的态度去看待。 时间函数混用我们在写一个小程序时基本不会去混用时间函数，比如只用 time(NULL) 去控制时间，或者只使用 chrono::system_clock::now() 来记录时间消耗，关于 chrono 的用法，之前简单总结过，可传送至 C++11中的时间库std::chrono。 但是当程序变得复杂起来，这个时间函数混用的高压线还是有可能触碰到的，当程序逻辑对时间要求越发精确时，混用所带来的后果将越发严重。在此记录一个结果：连续调用 time(NULL) 和 chrono::system_clock::now() 两个函数得到的时间戳可能是不同的。 可能你会说，函数是先后调用的，肯定是不同的，后面的函数调用时的时间戳要比前面的大，但事实却是两个函数所取得的时间戳大小不确定，可能是第一个函数的时间戳比较大，也可能是第二个时间戳更大一些。 测试的例子下面展示一段代码，先后调用两个时间函数，打印所获得的时间戳，可以看看有什么特点： 12345678910111213141516171819#include &lt;stdint.h&gt;#include &lt;sys/time.h&gt;#include &lt;time.h&gt;#include &lt;iostream&gt;#include &lt;chrono&gt;using namespace std;int main()&#123; int64_t t1, t2; while (true) &#123; t1 = chrono::duration_cast&lt;chrono::milliseconds&gt;(chrono::system_clock::now().time_since_epoch()).count(); t2 = time(0); if (t1/1000 != t2) cout &lt;&lt; t1 &lt;&lt; " " &lt;&lt; t2 &lt;&lt; endl; &#125; return 0;&#125; 编译运行结果如下： 1234567891011albert@home-pc:testtime$ g++ testtime.cpp -std=c++11albert@home-pc:testtime$ ./a.out1607779917993 16077799181607779957999 16077799581607780080001 16077800791607780103001 16077801021607780150001 16077801491607780202001 16077802011607780327999 16077803281607780440001 1607780439... 运行之后很快就出现了一些不一致，对比可以发现，两个时间戳一个是毫秒，一个是秒，同时把单位转化成秒来比较时，两者大小不定，从仅有的这几行结果来看，最大的误差是7毫秒。 再加一个时间函数除了上面提到的两个函数，还有一个 gettimeofday() 函数也是在获取时间时常常使用的，把它也放到测试函数中对比一下： 1234567891011121314151617181920212223#include &lt;stdint.h&gt;#include &lt;sys/time.h&gt;#include &lt;time.h&gt;#include &lt;iostream&gt;#include &lt;chrono&gt;using namespace std;int main()&#123; int64_t t1, t2, t = 0; struct timeval tv; while (true) &#123; t1 = chrono::duration_cast&lt;chrono::milliseconds&gt;(chrono::system_clock::now().time_since_epoch()).count(); t2 = time(0); gettimeofday(&amp;tv, NULL); if (t1/1000 != t2 || t2 != tv.tv_sec) if (t != t1) cout &lt;&lt; t1 &lt;&lt; " " &lt;&lt; t2 &lt;&lt; " " &lt;&lt; tv.tv_sec &lt;&lt; "," &lt;&lt; tv.tv_usec &lt;&lt; endl; t = t1; &#125; return 0;&#125; 运行后查看结果： 1234567891011121314151617181920albert@DESKTOP-6746UC3:/mnt/d/data/cpp/testtime$ g++ testtime.cpp --std=c++11albert@DESKTOP-6746UC3:/mnt/d/data/cpp/testtime$ ./a.out1607876993000 1607876992 1607876993,21607876994000 1607876993 1607876994,31607876995000 1607876994 1607876995,31607876996000 1607876995 1607876996,21607876997000 1607876996 1607876997,11607876998000 1607876997 1607876998,21607876999000 1607876998 1607876999,21607877000000 1607876999 1607877000,31607877001000 1607877000 1607877001,11607877002000 1607877001 1607877002,31607877003000 1607877002 1607877003,21607877004000 1607877003 1607877004,21607877005000 1607877004 1607877005,11607877006000 1607877005 1607877006,31607877007000 1607877006 1607877007,21607877008000 1607877007 1607877008,111607877009000 1607877008 1607877009,3... 真是各不相同，这要是在发射火箭时混用两个时间函数，那估计探月卫星就凉凉了…… 总结 常用来获取时间戳的函数有 time()、chrono::system_clock::now() 和 gettimeofday() 时间函数不要混用，否则会给精密计算带来巨大的麻烦，造成计算结果的不可控 测试发现 chrono::system_clock::now() 和 gettimeofday() 时间非常接近，有微秒级别的误差，但也不建议混用 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有的人走了只留下一撮灰烬，有的人离开却千古留名，但在时间长河中就是那么一瞬，意义何在，有差吗？ 2020-12-14 00:12:01]]></content>
      <tags>
        <tag>C++</tag>
        <tag>时间</tag>
        <tag>time</tag>
        <tag>system_clock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++中有符号数隐式类型转换成无符号数需注意的问题]]></title>
    <url>%2Fblog%2F2020%2F12%2F07%2FC-C-%E4%B8%AD%E6%9C%89%E7%AC%A6%E5%8F%B7%E6%95%B0%E9%9A%90%E5%BC%8F%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E6%88%90%E6%97%A0%E7%AC%A6%E5%8F%B7%E6%95%B0%E9%9C%80%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言隐式类型转换转换是一个挺基础的概念，即使对于初学者来说都不会陌生，一般情况下是指数据类型的转换是由编译系统自动进行的，不需要人工干预的类型转换方式。与之相对的是强制类型转换，在进行转换时必须使用强制类型转换运算符进行转换，这种也被称为显式转换。 举例隐式转换12short sn = 999;int n = sn; 显示转换12float f = 110.741f;int n = (int)f; 这两种转换方式平时经常用到，不管是函数传参时进行转换，还是数学计算时进行强转，一直也没有发现有什么问题，直到昨天遇到了一个有符号数隐式转换成无符号数时，才发现这里也是一个知识盲点，当时脑瓜儿嗡嗡的，怎么连隐式类型转换也这么陌生了呢？ 其实隐式类型转换一般发生在小类型转换成大类型时，有个常用的关系链 char -&gt; short -&gt; int -&gt; long -&gt; float -&gt; double，当关系链条中出现无符号数字时，情况有些难以理解了（实际上是有符号数字的锅）。 问题看一下这几行代码，如果你能准确说出程序的输出值，那么你已经掌握了这个知识点，后面的内容可以不用看了： 1234567891011#include &lt;iostream&gt;using namespace std;int main()&#123; char c = 128; unsigned int n = c; cout &lt;&lt; n &lt;&lt; endl; return 0;&#125; 这段代码的输出值是 4294967168，发生了啥？也就是说老板给你发工资时，本来想发128块，但是发工资的函数参数是 unsigned int 类型的，结果就给你发了 4294967168，一下就实现了40多个小目标。 查找原因针对上面的代码我们改写一下，把变量 c 换成无符号类型： 1234567891011#include &lt;iostream&gt;using namespace std;int main()&#123; unsigned char c = 128; unsigned int n = c; cout &lt;&lt; n &lt;&lt; endl; return 0;&#125; 这次的输出值变成了 128, 符合我们的预期，回过头来再看看刚才出错的代码，区别就是变量c是否有符号，结果差了好几十亿。 这里导致结果差异的原因实际上是符号位引起的，如果是无符号数字，从小类型到大类型隐式类型转换的结果数字都不会变，但是如果是有符号的数字，在转换成大类型数字的时候就要考虑符号位了，就以第一段代码为例来解释这个现象。 char c = 128; 这一句实际上已经超出了变量 c 的范围，因为变量c是有符号数字，所以它的范围是-128~127，这里赋值成128，实际在内存中的bit排列是 10000000，而有符号数的第一位bit表示正负号，这里是1表示这是一个负数，计算机存储负数是以补码的形式存储的，那么把这个数据按位取反再加1，得到 1000000 还是原来的数字，好神奇哦！ 不过这里就可以计算出 c 实际上代表-128，那么它在隐式类型转换成更大的有符号数字时，需要保证值不变，一个int的-128怎么表示呢？根据补码的定义应该是11111111 11111111 11111111 10000000，这个数字再转换成 unsigned int 就是前面提到的 4294967168 啦。 总结 有符号数字在转换成范围更大的无符号数字时需要注意转换所得数值是否正确，失之毫厘差之千里。 总结一个规律，有符号的整形数字在进行隐式类型转换时实际上是在数字的二进制表示前面补充符号位。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 一个人不能做完所有的事情，但是所有人都可以做一些事情，怕什么真理无穷，进一寸有进有一寸的欢喜~ 2020-12-8 00:04:05]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>符号</tag>
        <tag>隐式类型</tag>
        <tag>转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单聊聊01世界中编码和解码这对磨人的小妖儿]]></title>
    <url>%2Fblog%2F2020%2F11%2F28%2F%E7%AE%80%E5%8D%95%E8%81%8A%E8%81%8A01%E4%B8%96%E7%95%8C%E4%B8%AD%E7%BC%96%E7%A0%81%E5%92%8C%E8%A7%A3%E7%A0%81%E8%BF%99%E5%AF%B9%E7%A3%A8%E4%BA%BA%E7%9A%84%E5%B0%8F%E5%A6%96%E5%84%BF%2F</url>
    <content type="text"><![CDATA[前言在程序员生活的01世界中有两大Boss级难题，分别是缓存失效和命名问题，对比这两大难题来说，编码和解码只能算是小妖儿了，只不过这两个小妖儿出镜率很高，有时确实很磨人的，得多花些时间捋顺一下。 编码问题不仅仅出现在计算机中，广义的说，编码问题涉及到人类社会的方方面面，比如古人规定指定长度是一寸，然后规定十寸为一尺，其实就是当时人们对长度的一种编码，但是由于每个地方的编码不统一，导致人们在交流的时候出现了很多问题，直到秦始皇统一了文字、度量衡，相当于统一了描述当时社会的编码，使得知识和文明得以快速传播。 计算机中的编码今天想说的编码和解码特指计算中使用的编码和解码，通俗点说：编码是给计算机看的，解码是为了让人能看懂的。可能大家对这句话还不太理解，不过没关系，这个说法本身不太严谨，也可以举出一些反例，但是大部分情况下确实是这样的。 为什么要编码，想想谍战片里近代社会中的发电报过程，滴答、滴滴答、滴滴滴答答就这个样子，怎么来表达“敌人发动进攻了”，这时候就用到了编码，提前约定好“滴答”代表“敌”，“滴滴答”代表“人”，这样在收到“滴答、滴滴答”你就知道了“敌人”这个信息，那个密码本记录的内容和规则其实就是对所有电传信息的一种编码。 计算机中的编码也是一样的，从我们开始接触到计算机的时候就听说过计算机只认识0和1，虽然现代计算机技术发展迅速，但是计算机只认识0和1这一点一直未变，所以你想让他看懂你的信息，保存你的数据，就要把这些信息和数据编码成0和1，计算机才能进行处理和存储。 所以计算机中为什么要对数据进行编码，这里可以给一个狭义的理解：计算机编码是为了让数据便于传输、存储和处理。 那有为什么要进行解码呢？其实就是为了人能看懂，给你一串二进制 01010111100011111111...，相信你即使有最强大脑也不能迅速把所有数据解开，这可能是一篇优美的散文、一幅美丽的图画，或者是一部励志的电影，这一切都需要解码后才能知道。 本来想画一幅“编码”和“解码”这两个小妖的画像，但是作为灵魂画手的我还没构思好，此处留空，后面补充。。。 补上了&gt;&gt; 初识编码问题自从接触计算机就开始接触编码问题，比如你抄同学发来的作业文档，打开后却发现是一堆乱码，那时仅仅知道是编码错了，但是不知道怎么解决，或者直接让同学再发一份算了，后来在工作中需要做游戏多语言版本时才真正开始处理编码问题。 解决第一个编码问题大概是14年，当时做上线游戏的多语言版本、配置文件中的中文保存为 ANSI 编码，相同的配置文件放到日韩的系统上居然变成了其他的含义，查询解决方案决定使用 UTF-8 编码来保存配置文件，所以当时利用工具将所有的配置文件转换成了UTF-8编码，也是那个时候第一次接触到了Python，转换之后将其中的中文翻译成日韩的语言，从此知道了 UTF-8 这个编码方式，也清楚了在中日韩、越南、缅甸这个圈做产品，千万要远离 ANSI 编码。 其实 ANSI 并不是某一种特定的字符编码，而是一个编码集合，在不同的系统中，可以表示不同的编码，比如英文系统中的 ASCII编码，简体中文系统中的 GBK编码，韩文系统中 EUC-KR 编码等等 编码变迁小八卦计算机是美国人发明用于科学计算的，所以他们也是第一批考虑编码的，而英文只有26个字母，所以他们发明了ASCII码，只使用了0-127这128个空间就表示了所有可能用的字符，但后来计算机技术飞速发展，已经不仅仅用于科学计算，已经融入到社会的方方面面，并且迅速在全球流行。 随着计算机火遍全球，其它国家发现自己国家经常使用的字符，在 ASCII 码中找不到啊，于是就有人想啊，ASCII 码中的一个字节中不是才用了一半吗，我们使用这个最高位来扩展把，于是很多国家就开始用最高位来扩展这个 ASCII 编码以便能够表示自己国家的一些字符，但是对于我博大精深的中国文化来说，这一个字节远远不够啊，我们的汉字那可就有好几万个，你就给我一个字节，我肯定不干。 既然一个字节搞不定，那我们就用两个字节好了，我们规定一个小于等于127的字符的意义与原来相同，此处为了兼容ASCII码，但两个大于127的字符连在一起时，就表示一个汉字，前一个字节从0xA1用到0xF7，后面一个字节从0xA1到0xFE，我们将常用的6000多汉字在这个范围内定义代码点，并将这种编码方式称为 GB2312。 在 GB2312 这种编码中我们考虑了数学符号、希腊字母、全角标点等等，但是只有简体字没有繁体字啊，这下对面海岸的同胞们不乐意了，自己搞了一套 Big5 编码，用来处理繁体字。 后来随着电脑深入各个领域，常用汉字已经不能满足使用需求了，所以又把 GB2312 编码中没有使用的位置拿出来又进行代码点定义，处理了20000多个汉字，发明了 GBK 编码，但没过多久（2000年）发现还是不够用，又提出了变长的 GB18030 编码，每个字符占用1、2、4个字节。 大统一的Unicode刚刚简单提到了在中日韩这个圈里，每个国家都对 ASCII编码进行了扩充，也就是对 ANSI 编码进行了自己的定义，通常是用两个字节来表示一个文字和符号，这样就出现了一种情况，相同的两个字节在不同的系统上显示了不同的文字，如果每个国家的人只使用自己的语言也是没问题的，但是当中日韩文字混排的时候就出现了问题，这两个字节到底应该转换成中日韩哪个国家的符号呢？ 为了解决这种混乱的局面，大佬们设计了一种名为 Unicode 的字符集，又称万国码或者统一码。Unicode 的诞生是为整合全世界的所有语言文字。理论上任何字符在Unicode中都对应一个值，这个值被称为代码点，通常写成 \uABCD 的格式。 UCS-4 和 UCS-2起初使用两个字节来表示代码点，其取值范围为 \u0000～\uFFFF，这种文字和代码点之间的对应关系被描述为UCS-2，也就是 Universal Character Set Coded in 2 octets 的缩写，最多可以记录65536个字符的代码点。 后来为了能表示更多的文字，人们又提出了UCS-4，即用四个字节表示代码点。它的范围为 \u00000000～\u7FFFFFFF，其中 \u00000000～\u0000FFFF和UCS-2是一样的。 从这里可以看出 UCS-4 与 UCS-2 只是一种扩展的关系，UCS-4 是兼容 UCS-2 的，在 UCS-2 的每个代码点加入两个值为0的字节就变成了 UCS-4。 UCS-2 LE 和 UCS-2 BE这里的 LE 和 BE 指的是计算机中常提到的小端字节序和大端字节序，因为 UCS-4 是 UCS-2 的扩展，所以 UCS-4 也存在大端和小端的问题。 小端字节序，是指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中，而大端字节序，是指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中，这和我们平时的阅读习惯一致。 如果没接触过大端和小端可能会有点懵，举个例子就明白了，C++中一个int类型的数字通常占4个字节，假如一个int类型的变量值是256，那么他再内存中是怎样表示的呢？我们知道计算机中除了1就是0，这在计算机中对应一个bit，而计算机中表示数据的单位是字节，每个字节有8个bit大小，那么int变量值 256 翻译成二进制是 00000000 00000000 00000001 00000000 一共占用4个字节。 对照前面大端和小端的定义，这4个字节在内存中如果从高到低排列，就是小端字节序，如果这4个字节在内存中如果从低到高排列，就是大端字节序。因为UCS-2是两个字节表示一个代码点，所以在表示的时候存在字节排列顺序问题，对于值为 256 的这个代码点，可以是0x0100，也可以是0x0001。 Unicode 和 UCS-2Unicode 是一个字符集，这一点应该很好理解，它表示的是字符和代码点的对应关系，比如简体字“汉”对应的Unicode代码点是 \u6C49，而 UCS-2 究竟是一种字符集还是一种编码方式呢？ 我个人偏向于它是一种编码方式，因为它存在大端、小端这种说法，如果是一种字符集只会考虑对应关系，不会考虑字节序，这只是我个人观点，有些软件上确实是这样标注的，但有些文章也会把UCS-2当成一种字符集，这样也能说的通，不用太纠结这里的区别。 其实 UCS-2 编码对应的字符集是UCS，这些是历史原因导致的，一方面国际标准化组织（ISO）于1984年创建ISO/IEC JTC1/SC2/WG2工作组，试图制定一份通用字符集（Universal Character Set，简称UCS），并最终制定了ISO 10646标准。而另一方面统一码联盟，也很想做这个统一编码的武林盟主，由Xerox、Apple等软件制造商于1988年组成，并且开发了Unicode标准。 然后1991年左右，两个项目的参与者都认识到，世界不需要两个不兼容的字符集。于是，它们开始合并双方的工作成果，并为创立一个单一编码表而协同工作。从Unicode 2.0开始，Unicode采用了与ISO 10646-1相同的字库和字码。ISO也承诺，ISO 10646将不会替超出\u10FFFF的UCS-4编码赋值，以使得两者保持一致。两个项目仍都独立存在，并独立地公布各自的标准。不过由于Unicode这一名字名字起的好，比较好记，因而它使用更为广泛。 从这段历史我们可以看到，虽然 UCS-4 将 UCS-2 从2个字节扩展成了4个字节，但是范围并没到使用到 \u00000000～\uFFFFFFFF，而是将范围集中到 \u000000～\u10FFFF 内，保证了 UCS 和 Unicode 各个字符代码点的统一，也奠定了UTF-8实现标准Unicode时最多需要4个字节的基础。 UTF-8 的诞生按理说 Unicode 已经给世界范围内的所有字符定义了代码点，无论是什么字符，使用4个字节都能表示出来，为什么要搞出一个UTF-8呢？是因为使用者发现，对于ASCII码范围内的字符，本来1个字节就能正确表示，现在居然要4个字节表示，即使使用 UCS-2编码，占用的空间也扩大了1倍，有些太浪费了。 为了解决这种空间浪费问题，就出现了一类变长的通用转换格式，即UTF（Universal Transformation Format），常见的UTF格式有：UTF-7，UTF-7.5，UTF-8，UTF-16 以及 UTF-32。 这类格式中最常见的就是 UTF-8 编码了，UTF-8 是针对于 Unicode 字符集中各个代码点的编码方式，是一种 Unicode 字符的实现方式，采用变长字节来表示Unicode编码，最长使用4个字节来表示标准的Unicode代码点，在有些资料中可能会看到5、6个字节的编码方式，这些都是非标准的Unicode代码点，根据规范，这些字节值将无法出现在合法 UTF-8序列中。 Unicode 和 UTF-8UTF-8在对标准Unicode字符编码时最多使用4个字节，其代码点范围与UTF-8编码后的形式对应如下： Unicode/UCS-4（十六进制） 字节数 UTF-8编码格式（二进制） 000000-00007F 1 0xxxxxxx 000080-0007FF 2 110xxxxx 10xxxxxx 000800-00FFFF 3 1110xxxx 10xxxxxx 10xxxxxx 010000-10FFFF 4 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx UTF-8编码示例只看上面这种对应关系，可能还不太清楚是怎样表示，接下来可以举一个例子试一下，比如一个常用的简体中文字——“好”，查询它的Unicode代码点是 \u597D，对照上面的表格发现在 000800-00FFFF 这个范围，应该采用3个字节的表现形式。 先把这个数值翻译成二进制为 0101100101111101，然后按照3个字节的形式分成3组，0101、100101 和 111101，把这些内容天填充到xxx这样的空位中就得到了“好”这个字的UTF-8编码—— 11100101 10100101 10111101，表示成十六进制就是 0xE5A5BD。 这个过程还是比较简单的，其他编码要转换成UTF-8编码都要经过Unicode这一步中转，先通过转换表查到其他编码对应字符的Unicode编码，然后再转换成UTF-8的表示格式。 优点和缺点根据 UTF-8 的编码规则，任何一个 byte 漏传，多传，传错只影响当前字符，前后字符都不受影响，而 Unicode 如果从一个字的中间截断会导致接下来所有的字符解析都是错的，这使得UTF-8编码的数据在不够可靠的网络传输中是有利的。 兼容ASCII，并且是字节顺序无关的。它的字节顺序在所有系统中都是一样的，因此它实际上并不需要BOM，不过在文件开头常常保存 0xEFBBBF 三个字节来表明文件编码是UTF-8。 缺点是因为UTF-8是一种变长编码，无法从直接从Unicode字符直接判断出UTF-8文本的字节数。除了ASCII字符集内的字符，其他情况实际上都增加了固定的头数据，占用了无效空间。 编码和解码在编程中的应用编码和解码在网站页面和数据库存储时用的非常多，一不小心就搞出一堆乱码，这种编码和解码操作在Python3中很直观，Python2中 string 和 bytes 混合在一起，编码和解码操作不太明显，而在python3中 string 和 bytes 是完全不同的两个类型，string编码成bytes，而bytes解码成string。 相比于python3中的编码、解码对应两个类型，C++中的编码和解码操作的前后都是字符串，这在一定程度上会给人造成误解，接下来我们使用Python3来简单测试一下编码和解码操作。 编码操作编码通常是把人类可以理解的字符转换成计算机可以认识二进制数据，这个过程在python3中对应的是把string转化成bytes，测试如下： 12345word = '好好'print(type(word), word)result = word.encode('utf-8')print(type(result), result) 运行结果如下： 12&lt;class &apos;str&apos;&gt; 好好&lt;class &apos;bytes&apos;&gt; b&apos;\xe5\xa5\xbd\xe5\xa5\xbd&apos; 解码操作解码操作通常是把计算机中存储和传输的数据转换成人类能看懂的字符，这个过程在python3中对应的是把bytes转化成string，测试如下： 12345data = b'\xe5\xa5\xbd\xe5\xa5\xbd'print(type(data), data)result = data.decode('utf-8')print(type(result), result) 运行结果如下： 12&lt;class &apos;bytes&apos;&gt; b&apos;\xe5\xa5\xbd\xe5\xa5\xbd&apos;&lt;class &apos;str&apos;&gt; 好好 乱码产生从上面的两个例子来看编码和解码非常简单，那怎么还能出现乱码呢？计算机说到底还是一种工具，你在把可见字符编码后交给计算机存储和传输时，你要记住这些二进制的编码方式，在你想看这些数据时还要用相反的方式进行解码，否则就会出现乱码，比如下面这种使用 utf-8 编码，却使用 gbk 这种方式来解码，就得不到你想要的数据。 12345678word = '好好'print(type(word), word)result = word.encode('utf-8')print(type(result), result)new_word = result.decode('gbk')print(type(new_word), new_word) 运行结果如下： 123&lt;class &apos;str&apos;&gt; 好好&lt;class &apos;bytes&apos;&gt; b&apos;\xe5\xa5\xbd\xe5\xa5\xbd&apos;&lt;class &apos;str&apos;&gt; 濂藉ソ 虽然结果是可以看得见的字符，但是这不是我们想要的数据，所以 濂藉ソ 对于我们来说也是一种乱码，在处理字符编码时我们必须清楚知道要用什么方式来进行编码和解码，如果编码和解码的方式不一致，那么就会产生乱码现象。 总结 Unicode 是一种字符集，描述了人类范围内用于交流的所有字符的代码点，给与唯一的数字进行对应 Unicode 规定的代码点范围是 \u000000-\u10FFFF，这与 UCS-4 规定的范围达成了统一，共定义了17个Plan UTF-8 是Unicode字符集的一种实现，采用变长的方式，标准规范最多使用4个字节表示一个Unicode字符 编码是为了把人类用来交流的字符转换成二进制数据便于存储和传输 解码是为了把存储在计算机中的二进制数据转换成人们能看得懂的字符 编码和解码不一致时就会造成乱码，比如使用UTF-8编码，使用GBK来解码就会造成乱码现象 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 对未知的事物充满恐惧，过于保守的看待当下的一切，有时候太稳反而会失去很多~ 2020-11-29 19:23:13]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>编码</tag>
        <tag>解码</tag>
        <tag>encode</tag>
        <tag>decode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下使用netstat命令查看网络信息]]></title>
    <url>%2Fblog%2F2020%2F11%2F22%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8netstat%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8B%E7%BD%91%E7%BB%9C%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言netstat 这个命令一直以为是 net status 的缩写，今天一查发现并没有找到官方的这种说法，然后参考了 man 手册，发现这个词更像是 net statistics 的缩写，命令的作用是显示网络连接、路由表、接口连接、无效连接和多播成员关系的，man 手册中描述这个命令如下： netstat - Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships 这个命令很强大，但是我经常使用的参数很简单，通常用来查询端口占用问题，命令为 netstat -anp | grep xxxPORT，因为在我测试自己项目程序的时候，总有一些进程企图占用我使用的端口，比如那个 被我 kill 了 n 次的 TIM 客户端，使用 netstat 可以方便的找到是哪个进程占用了你的端口。 虽然这个命令经常使用，但是其中的这些参数含义却不是很清楚，所以特地总结一下，综合其他常见的用法，记录下来以备后续查找使用。 参数选项 -a：显示所有连接，包括 LISTEN 状态的连接 -l：仅显示 LISTEN 状态的连接 -t：仅显示tcp相关选项 -u：仅显示udp相关选项 -n：拒绝显示别名，能显示数字的全部转化成数字 -o：显示信息中包括与网络计时器相关的信息 -e：显示扩展信息，例如uid等 -p：显示建立相关链接的程序名 -r：显示路由信息，路由表 -s：按各个协议进行统计 -c：每隔一个固定时间，执行该netstat命令。 无参数执行该命令无参数执行时显示数据会少一些，便于我们看清命令执行的结果，内容如下： 123456789101112131415161718[root@node1 ~]# netstatActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 192.168.0.201:ssh 178.129.126.124.b:19450 ESTABLISHEDtcp 0 52 192.168.0.201:ssh 178.129.126.124.b:17626 ESTABLISHEDtcp 0 0 192.168.0.201:57784 101.200.35.175:https TIME_WAITtcp 0 0 192.168.0.201:42298 100.125.2.72:https ESTABLISHEDActive UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 11550 /run/systemd/shutdowndunix 2 [ ] DGRAM 13355412 /var/run/chrony/chronyd.sockunix 3 [ ] DGRAM 1228 /run/systemd/notifyunix 2 [ ] DGRAM 1230 /run/systemd/cgroups-agentunix 5 [ ] DGRAM 1241 /run/systemd/journal/socketunix 16 [ ] DGRAM 1243 /dev/logunix 3 [ ] STREAM CONNECTED 15663unix 3 [ ] STREAM CONNECTED 15662... 输出结果可以分为 Active Internet connections 和 Active UNIX domain sockets 两个部分： Active Internet connections 指有效的网络连接，默认显示6列内容： Proto：协议名字，包括tcp, udp, udpl, raw等 Recv-Q：表示网络接收队列，表示收到的数据已经在本地接收缓冲，还有多少没有被应用程序取走 Send-Q：表示网络发送队列，表示存在本地缓冲区，但对方没有收到的数据或者没有 ACK 的 Local Address：本地IP地址和端口 Foreign Address：外部IP地址和端口 State：网络连接状态，包括 ESTABLISHED、SYN_SENT、SYN_RECV、FIN_WAIT1、FIN_WAIT2、TIME_WAIT、CLOSE、CLOSE_WAIT、LAST_ACK、LISTEN、CLOSING、UNKNOWN 等状态 Active UNIX domain sockets 是指本地套接口，我们知道 socket 也可用于同一台主机的进程间（IPC）通讯，但是 socket 用于IPC更有效率：不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号和应答等，只是将应用层数据从一个进程拷贝到另一个进程并且它是是全双工的，API接口语义丰富，相比其它进程间通信机制有明显的优越性。 常用命令组合查询端口占用12[root@node1 /]# netstat -anp | grep 8889tcp 0 0 0.0.0.0:8889 0.0.0.0:* LISTEN 27584/tinyproxy 这是我目前最常用的命令，在windows可以改为 netstat -ano | findstr 8889 显示tcp连接123456789101112[root@node1 /]# netstat -atActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:ddi-tcp-2 0.0.0.0:* LISTENtcp 0 0 localhost:smtp 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:ssh 0.0.0.0:* LISTENtcp 0 52 192.168.0.201:ssh 178.129.126.124.b:19450 ESTABLISHEDtcp 0 0 192.168.0.201:ssh 178.129.126.124.b:17626 ESTABLISHEDtcp 0 0 192.168.0.201:42298 100.125.2.72:https ESTABLISHEDtcp6 0 0 [::]:squid [::]:* LISTENtcp6 0 0 localhost:smtp [::]:* LISTENtcp6 0 0 [::]:ssh [::]:* LISTEN 显示处于 LISTEN 状态的端口123456789101112131415161718192021[root@node1 /]# netstat -lActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:ddi-tcp-2 0.0.0.0:* LISTENtcp 0 0 localhost:smtp 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:ssh 0.0.0.0:* LISTENtcp6 0 0 [::]:squid [::]:* LISTENtcp6 0 0 localhost:smtp [::]:* LISTENtcp6 0 0 [::]:ssh [::]:* LISTENudp 0 0 0.0.0.0:bootpc 0.0.0.0:*udp 0 0 0.0.0.0:ntp 0.0.0.0:*udp 0 0 localhost:323 0.0.0.0:*udp 0 0 0.0.0.0:56034 0.0.0.0:*udp6 0 0 [::]:42035 [::]:*udp6 0 0 localhost:323 [::]:*Active UNIX domain sockets (only servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ACC ] STREAM LISTENING 11533 /run/lvm/lvmpolld.socketunix 2 [ ACC ] STREAM LISTENING 6848304 /var/run/rpcbind.sockunix 2 [ ACC ] STREAM LISTENING 11584 /run/lvm/lvmetad.socket... 分类统计每种协议的信息1234567891011121314151617181920212223242526272829303132333435363738394041[root@node1 /]# netstat -sIp: 7902622 total packets received 60675 forwarded 127 with unknown protocol 0 incoming packets discarded 7841813 incoming packets delivered 7270606 requests sent out 8 dropped because of missing routeIcmp: 928210 ICMP messages received 25426 input ICMP message failed. InCsumErrors: 8 ICMP input histogram: destination unreachable: 71154 timeout in transit: 484 echo requests: 856165 echo replies: 337 timestamp request: 54 896502 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 40039 echo request: 244 echo replies: 856165 timestamp replies: 54Tcp: 274517 active connections openings 66347 passive connection openings 187800 failed connection attempts 90950 connection resets received 3 connections established 6359177 segments received 5808198 segments send out 494062 segments retransmited 4 bad segments received. 452720 resets sentUdp: 539313 packets received 14902 packets to unknown port received.... 每秒显示一次信息12345678910[root@node1 /]# netstat -cActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 52 192.168.0.201:ssh 178.129.126.124.b:19450 ESTABLISHEDtcp 0 0 192.168.0.201:ssh 178.129.126.124.b:17626 ESTABLISHEDtcp 0 0 192.168.0.201:42298 100.125.2.72:https ESTABLISHEDActive UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 11550 /run/systemd/shutdownd... 显示核心路由信息1234567[root@node1 /]# netstat -rKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Ifacedefault 192.168.0.1 0.0.0.0 UG 0 0 0 eth0169.254.169.254 192.168.0.254 255.255.255.255 UGH 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 显示网络接口列表123456[root@node1 /]# netstat -iKernel Interface tableIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgdocker0 1500 36248 0 0 0 33647 0 0 0 BMUeth0 1500 9119246 0 0 0 8277212 0 0 0 BMRUlo 65536 27700 0 0 0 27700 0 0 0 LRU 总结 netstat -anp | grep 8889 命令可用于查询8889端口被哪个进程占用了，在Windows上翻译为 netstat -ano | findstr 8889 netstat 命令查询出的网络连接信息中，Recv-Q 和 Send-Q 通常应该为0，如果长时间不为0可能是有问题的，需要尽快排查 如果 Recv-Q 数值一直处于不为0的状态，可能是遭受了拒绝服务 DOS 攻击，导致本地消息处理过慢 如果 Send-Q 数值一直处于不为0的状态，可能是有应用向外发送数据包过快，或者是对方接收处理数据包不够快 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 以史为鉴可以知兴替，以铜为鉴可以正衣冠，以人为鉴可以明得失。人的成长需要对比，总有人比你更加优秀~ 2020-11-23 01:17:59]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>sort</tag>
        <tag>排序</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习cmake从成功编译一个小程序开始]]></title>
    <url>%2Fblog%2F2020%2F11%2F14%2F%E5%AD%A6%E4%B9%A0cmake%E4%BB%8E%E6%88%90%E5%8A%9F%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[前言在 Windows上开发我使用最多的IDE还是 Visual Studio，编写、编译一条龙服务，导致了不少编译流程知识的缺失，这种大型的IDE确实好用，诸多配置通过在界面上勾选一下就可以了，但是在编译细节的掌握上还是漏掉了一些知识。 在 linux 开发环境下通常会使用 gcc 或者 g++ 进行编译，可是编译选项有点多，当工程非常大的时候需要写的编译参数太多了，这时可以使用make命令来帮助我们编译 C++ 程序，编译时依赖一些规则，这些规则就写在一个叫 Makefile 的文件中。 后来发现写 Makefile 还是太麻烦了，这个文件也相当大。于是“懒惰”的程序员们又开发出了各种各样的工具用来生成 Makefile 文件，我使用过的目前就只有 automake 和 cmake。 生成Makefile之前使用的生成 Makefile 文件的工具是 automake，被称为是“八股文”一样的操作，每次操作都是固定的几个步骤，比如每次都要运行 autoscan、aclocal、autoconf、automake、./confiugre等命令，需要个人发挥的地方并不多，之前使用的时候也不是完全从0开始一点点写的，往往是写一个项目模板之后，对照着在Makefile.am文件中修改几个参数就好了。 现在新的工作内容中使用 cmake 来生成 Makefile，这个 cmake 之前还确实接触过一些，大概是2012年的时候，那时在编译 OpenCV 库还有增强现实插件的时候用过几次，当时感觉安装起来太麻烦了，对那个红绿蓝的图标记忆犹新，感觉和当时的新闻联播的图标有些亲戚关系。 其实当时根本分不清什么是编译器，什么是 Makefile，对于各种库文件的编译完全是按照文档来操作，现在回过头来看看 cmake 生成 Makefile 还是比较简单的，最起码要比 automake 省了很多步骤，只要编写一个 CMakeLists.txt 文件就好了。 编写CMakeLists.txt生成Makefile为了练习使用编写CMakeLists.txt生成Makefile，进而编译C++项目，我们可以从头来实现一个小例子，目标是编写一个计算加法的静态库和一个计算减法静态库，然后实现一个测试工程来使用这两个函数库，整个工程使用 cmake 来生成 Makefile，然后使用 make 命令完成编译。 实现简单的代码文件加法和减法都是常用的简单计算，用来举例子很容易理解，接下来展示要用到的几个文件内容，每个文件只有几行，只为了说明问题，文件内容如下： 123//myadd.hint add(int a, int b); 1234567//myadd.cpp#include "myadd.h"int add(int a, int b) &#123; return a + b;&#125; 123//mysub.hint sub(int a, int b); 1234567//mysub.cpp#include "mysub.h"int sub(int a, int b) &#123; return a - b;&#125; 123456789101112//test.cpp#include "myadd.h"#include "mysub.h"#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; "happy birthday!" &lt;&lt; std::endl; std::cout &lt;&lt; "519 + 1 = " &lt;&lt; add(519, 1) &lt;&lt; std::endl; std::cout &lt;&lt; "1320 - 6 = " &lt;&lt; sub(1320, 6) &lt;&lt; std::endl; return 0;&#125; 使用常规方法编译首先使用最简单 g++ 命令来编译这个样例程序： 查看目录下文件 12albert@home-pc:testcmake$ lsmyadd.cpp myadd.h mysub.cpp mysub.h test.cpp 将 myadd.h 和 myadd.cpp 编译成静态库 libmyadd.a 12345albert@home-pc:testcmake$ g++ -c myadd.cppalbert@home-pc:testcmake$ ar crv libmyadd.a myadd.oa - myadd.oalbert@home-pc:testcmake$ lslibmyadd.a myadd.cpp myadd.h myadd.o mysub.cpp mysub.h test.cpp 将 mysub.h 和 mysub.cpp 编译成静态库 libmysub.so 1234albert@home-pc:testcmake$ g++ -c mysub.cppalbert@home-pc:testcmake$ g++ -shared -fPIC -o libmysub.so mysub.oalbert@home-pc:testcmake$ lslibmyadd.a libmysub.so myadd.cpp myadd.h myadd.o mysub.cpp mysub.h mysub.o test.cpp 编译链接静态库 libmyadd.a、动态库 libmysub.so 和测试文件生成可执行程序 test 123albert@home-pc:testcmake$ g++ test.cpp libmyadd.a -L. -lmysub -o test -Wl,-rpath=.albert@home-pc:testcmake$ lslibmyadd.a libmysub.so myadd.cpp myadd.h myadd.o mysub.cpp mysub.h mysub.o test test.cpp 运行查看结果，成功计算表达式的值 1234albert@home-pc:testcmake$ ./testhappy birthday!519 + 1 = 5201320 - 6 = 1314 使用cmake方式上面展示了最原始的编译方法，每次都要敲这些命令，接下来编写一个 CMakeLists 文件，使用 cmake 生成Makefile，以后只要运行 make 命令就可以完成编译了。 调整一下目录结构如下： 123456789albert@home-pc:testcmake$ tree.|-- myadd| |-- myadd.cpp| `-- myadd.h|-- mysub| |-- mysub.cpp| `-- mysub.h`-- test.cpp 进入 myadd 目录新建 CMakeLists.txt 编写内容如下： 1234aux_source_directory(. SRC_LIST) #将此目录的源文件集合设置为变量SRC_LISTadd_library(myadd STATIC $&#123;SRC_LIST&#125;) #库的名称，库的类型，静态库的源文件列表set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/lib) #库的输出路径为根目录下的lib文件夹 进入 mysub 目录新建 CMakeLists.txt 编写内容如下： 1234aux_source_directory(. SRC_LIST) #将此目录的源文件集合设置为变量SRC_LISTadd_library(mysub SHARED $&#123;SRC_LIST&#125;) #库的名称，库的类型，动态库的源文件列表set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/lib) #库的输出路径为根目录下的lib文件夹 在工程主目录下新建 CMakeLists.txt 编写内容如下： 12345678910111213141516171819202122232425262728293031323334# 指定cmake版本cmake_minimum_required(VERSION 3.5)# 指定项目的名称，一般和项目的文件夹名称对应project(testcmake)# 指定子目录add_subdirectory(myadd)add_subdirectory(mysub)# 添加c++ 11标准支持set(CMAKE_CXX_FLAGS "-std=c++11" )# 特殊宏，之前编译mysqlcppconn8用到过add_definitions(-DGLIBCXX_USE_CXX11_ABI)# 头文件目录include_directories(myadd mysub)# 源文件目录aux_source_directory(. DIR_SRCS)# 设置环境变量，编译用到的源文件全部都要放到这set(TEST_MATH $&#123;DIR_SRCS&#125;)# 库文件目录link_directories(lib)# 添加要编译的可执行文件add_executable($&#123;PROJECT_NAME&#125; $&#123;TEST_MATH&#125;)# 添加可执行文件所需要的库target_link_libraries($&#123;PROJECT_NAME&#125; myadd)target_link_libraries($&#123;PROJECT_NAME&#125; mysub) 新建build目录和lib目录，整个工程目录关系如下： 12345678910111213141516albert@home-pc:testcmake$ tree.|-- CMakeLists.txt|-- build|-- lib|-- myadd| |-- CMakeLists.txt| |-- myadd.cpp| `-- myadd.h|-- mysub| |-- CMakeLists.txt| |-- mysub.cpp| `-- mysub.h`-- test.cpp4 directories, 8 files 进入 build 目录下依次运行 cmake .. 和 make 命令 123456789101112131415161718192021222324252627282930313233343536albert@home-pc:testcmake/build$ cmake ..-- The C compiler identification is GNU 5.4.0-- The CXX compiler identification is GNU 5.4.0-- Check for working C compiler: /usr/bin/cc-- Check for working C compiler: /usr/bin/cc -- works-- Detecting C compiler ABI info-- Detecting C compiler ABI info - done-- Detecting C compile features-- Detecting C compile features - done-- Check for working CXX compiler: /usr/bin/c++-- Check for working CXX compiler: /usr/bin/c++ -- works-- Detecting CXX compiler ABI info-- Detecting CXX compiler ABI info - done-- Detecting CXX compile features-- Detecting CXX compile features - done-- Configuring done-- Generating done-- Build files have been written to: testcmake/buildalbert@home-pc:testcmake/build$ makeScanning dependencies of target mysub[ 16%] Building CXX object mysub/CMakeFiles/mysub.dir/mysub.cpp.o[ 33%] Linking CXX shared library ../../lib/libmysub.so[ 33%] Built target mysubScanning dependencies of target myadd[ 50%] Building CXX object myadd/CMakeFiles/myadd.dir/myadd.cpp.o[ 66%] Linking CXX static library ../../lib/libmyadd.a[ 66%] Built target myaddScanning dependencies of target testcmake[ 83%] Building CXX object CMakeFiles/testcmake.dir/test.cpp.o[100%] Linking CXX executable testcmake[100%] Built target testcmakealbert@home-pc:testcmake/build$ ./testcmakehappy birthday!519 + 1 = 5201320 - 6 = 1314albert@home-pc:testcmake/build$ 至此，使用cmake方式编译工程的例子就写完了。 总结 cmake 和 automake 本身不提供编译功能，只是可以按照编写的 CMakeLists.txt 文件生成 Makefile make 可以根据 Makefile 文件调用 gcc/g++ 命令对源代码进行编译工作 -Wl,-rpath=. 这个选项可以指定可执行文件查找动态库的路径，感觉比 export LD_LIBRARY_PATH 要方便一点 -DGLIBCXX_USE_CXX11_ABI 这个宏可坑了我不少时间，编译使用libmysqlcppconn8的时候，如果不禁用会报编译错误 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有你，真好~ 2020-11-15 23:55:35]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>linux</tag>
        <tag>编译</tag>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下sed命令的基础用法]]></title>
    <url>%2Fblog%2F2020%2F11%2F07%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8Bsed%E5%91%BD%E4%BB%A4%E7%9A%84%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言每次看到sed这个词就想起“种子”，心里明知道是把它和seed弄混了，但是先入为主的思想怕是改不过来了，不过现在还好，虽然把第一印象的意思弄错了，但还是很快能反应过来这是一个很“厉害”的linux命令，也有一些网友评论到，每次看到这个命令就双腿发抖，我虽然没抖，但是谈到这个命令还是有些挠头，心里有些发怵。 一味地逃避困难是不可取的，虽然心里感觉这是个很难的命令，但是今天还是要硬着头皮学一下，边学边记录，易于下次复习，那些打败不了我的困难终将使我更加强大。 sed功能其实sed并不是一个单词，而是 stream editor 的缩写，本意为面向字符流的编辑器，说白了sed就是用来编辑文件的命令，编辑文件是我们每天经常做的工作，但是如果每天的编辑工作都类似，我们就要考虑使用sed工具来提高工作效率了，比如说把今天新增的100个文件的第一行都加上版本信息，虽然手动编辑也能做，但是你想体验一下敲个命令瞬间搞定这件事情的快感吗，我们来学习sed命令吧？ 命令格式1sed [OPTION]... &#123;script-only-if-no-other-script&#125; [input-file] sed 的选项不是太多，最常用的有下面两种形式： sed -e &#39;sed命令脚本&#39; input-file sed -f &#39;sed命令脚本文件&#39; input-file 命令选项 -e ：命令行模式，选项后直接跟sed编辑脚本，在只有一组脚本的情况下可以省略 -f ：脚本文件模式，选项后跟写有sed编辑脚本的文件名，运行后会执行脚本文件内的编辑动作 -i ：直接修改文件内容，如果不加这个选项是不修改源文件的，只将修改后的文件输出 -n ：只打印模式匹配的行，便于查看所作修改 以上列举只是一些常见选项，还有些比如 -l 指定每行长度，-s 指定换行的分隔符等等，用到了再来分析学习。 寻找匹配既然是编辑文件，首先要找到需要编辑的位置，在sed命令中可以使用行号，或者字符查找等方式找到需要修改的位置，然后再执行编辑动作，常见的范围： x：指定的行号，表示第x行 x,y：指定的行号范围，表示第x行到y行 /pattern：查询到包含指定模式的行 x,y!：指定的行号范围，表示不包括第x行到y行 sed操作sed几乎可以实现文件的所有编辑工作，接下来尝试一些常见的用法： 打印内容使用编辑命令 p，可以向匹配行后面插入内容。 打印文件第2行和第3行的内容，命令为sed -n &#39;2,3p&#39; data.txt 123456789albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed -n '2,3p' data.txtabcxyz 追加内容使用编辑命令 a，可以向匹配行后面插入内容。 在第2行后面添加文本newline，命令为sed &#39;2anewline&#39; data.txt 12345678910111213albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '2anewline' data.txt1234abcnewlinexyz1==123 在最后一行后面添加文本endline，命令为sed &#39;$aendline&#39; data.txt 1234567891011121314albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '$aendline' data.txt1234abcxyz1==123endlinealbert@DESKTOP-6746UC3:/mnt/d/data/shell$ 插入内容使用编辑命令 i，可以在匹配的那一行插入内容。 在第1行插入文本firstline，命令为sed &#39;1ifirstline&#39; data.txt 12345678910111213albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '1ifirstline' data.txtfirstline1234abcxyz1==123 在包含文本 “123” 的行插入文本insertline，命令为sed &#39;/123/iinsertline&#39; data.txt 1234567891011121314albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '/123/iinsertline' data.txtinsertline1234abcxyz1==insertline123 更改行内容使用编辑命令 c，可以修改匹配行的内容。 将包含文本 “123” 的行替换为 “456”，命令为sed &#39;/123/c456&#39; data.txt 123456789101112albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '/123/c456' data.txt456abcxyz1==456 将3、4、5行内容更改为newworld，命令为sed &#39;3,5cneworld&#39; data.txt 12345678910albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '3,5cneworld' data.txt1234abcneworld 替换行内容使用编辑命令 s，可以替换匹配行的内容，需要注意和 c 的区别，c 是整行的内容都改变，而 s 是只替换命令中指定的部分。 将文件中的文本 “123” 替换为 “456”，命令为sed &#39;s/123/456/g&#39; data.txt 123456789101112albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed 's/123/456/g' data.txt4564abcxyz1==456 综合运用 删除空行并给所有内容是 “123” 的文本加上小括号 12345678910111213albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed -e 's/123/(&amp;)/g' -e '/^$/d' data.txt(123)4abcxyz1==(123) 删除内容使用编辑命令 c，可以删除匹配行。 删除空行，命令为sed &#39;/^$/d&#39; data.txt 12345678910111213albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '/^$/d' data.txt1234abcxyz1==123 从第一行开始，每两行删除掉一行，命令为sed &#39;1~2d&#39; data.txt 123456789albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '1~2d' data.txtabc1== 删除2行和3行以外的行，命令为sed &#39;2,3!d&#39; data.txt 123456789albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '2,3!d' data.txtabcxyz 删除指定行数范围内的匹配行，命令为sed &#39;1,3{/123/d}&#39; data.txt 1234567891011albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '1,3&#123;/123/d&#125;' data.txtabcxyz1==123 总结 sed 是 stream editor 的缩写，表示为面向字符流的编辑器 sed 命令常用的几个选项，-e、-f、-i、-n sed 命令常用的几个编辑动作，也就是选项后的常用命令有 p（打印）、a（追加）、i（插入）、c（改变）、s（替换）、d（删除） ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 不能打败我的困难终将使我更加强大，绊不倒我的石头最后只会被拿来踩踏，拥抱一个个困难，生活本来就是一条打怪升级之路，那有什么一帆风顺~ 2020-11-7 22:47:01]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>文件</tag>
        <tag>sed</tag>
        <tag>编辑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于数据一致性的思考]]></title>
    <url>%2Fblog%2F2020%2F10%2F24%2F%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[前言也不知道是谁这么有才，居然发明了1024这个程序员的节日，其他的节日都是买买买，唯独这个节日促销活动少的可怜，连早上买菜都是占了第二天重阳节的便宜，除了程序员们自嗨，也没人来给码农们庆祝了。 今天也嗨了一把，程序员的节日必须在工位上，飞速的敲着代码才是对1024最大的尊重，在这一天的结尾之际还是聊聊最近开发中的一些问题，其中数据一致性的问题确实需要梳理一下。 什么是一致性关于一致性常常在两个地方听到，一个是数据库，另一个是分布式，两者都叫一致性，但是含义却不同。 数据库一致性数据库中的一致性其实代表不破坏完整性，所有的数据从一个状态转化到另一个状态时不发生逻辑问题，比如说A通过手机银行给B转了100万，这件事情发生后A账户少了100万，B账户多了100万，这样就保证了数据的一致，如果转账结束A账户的钱少了100万，B账户却只多了100块，那完蛋了，A和B肯定一起去找银行打架去了。 分布式一致性很多资料对于分布式一致性理解的都是数据冗余副本，当所有副本的数据一样时，那么此时的状态就是一致的。按照我自己的理解，这里的冗余副本不一定指的是数据形式完全一样，比如玩家在游戏服拥有金币资产200万，然后全服排行榜上的展示面板上显示资产也是200万，可能具体数据的形式不同，但这应该也是一种数据一致性的表现。 两个概念容易混淆，因为经常在分布式的架构下更新数据库，两种一致性也常常在同一个操作中有所体现。其实我也经常混着用，反正知道这个意思就好了，最近遇到的问题也是两个概念的集合，不过还是先来理解一下分布式的一致性吧 分布式一致性分类 强一致性： 要求无论更新操作是在哪一个节点副本上执行，之后获取的数据都是最新的。 弱一致性： 能容忍部分或全部节点都看不到最新数据，数据改变时尽量通知可能多的节点。 最终一致性： 是弱一致性的一种特例表现，需要保证用户最终能够读取到最新的数据。 我们当然希望能实现强一致性，但这样需要付出相当大的代价，往往要通过牺牲可用性才能达到。 一致性的保证如果要想达到强一致性，那么就得保证任何数据在改变之后必须通知所有节点，等待所有节点更新完毕后才能给用户提供服，这就要在开始更新时加一把大锁，先锁住数据，等待所有节点完成更新时释放锁，这样才能提供数据的强一致性保证。 如果节点太多的话，这个锁的机制将会消耗大量的时间来等待，可能导致应用长时间不能提供正常服务，在一些应用上显然是不合适的，所以是否要保证强一致性需要根据具体的业务逻辑来选择。 还有一个经常听到的观点就是在分布式系统中一致性和可用性我们只能选择一个，这一般是从CAP理论中得到的结论，但是这样说是不准确的，关于CAP理论最初版的大意为：“对于一个分布式计算系统，不可能同时满足一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance ）三个设计约束”。 通过CAP理论告诉我们分布式系统只能选择CP或者AP，但其实这里的前提是分布式系统发生了“分区”现象。如果当前系统没有发生分区现象，我们没有必要放弃C或者A，应该C和A都可以保证。 还有一点个人的理解，由于数据传输是需要时间的，那么当一个节点修改了数据同步到另一个节点时不可能瞬间完成，所以数据不一致总是时刻存在，而我们前面提到的数据一致总是指对用户而言的，虽然数据在传输过程中是不一致的，但是我们可以规定在数据完成同步前，用户看到的都是旧数据，这样就对用户而言数据就是一致的。 而数据同步过程中的不一致，如果在不一致期间还发生了中断、崩溃等问题，就必须通过日志来恢复了，个人觉得，总是有那么一种极限情况，连日志都救不了你，毕竟记录日志的也是一种程序，但是这类事情发生的概率也比较小了。 总结 程序世界的一致性常常指数据库中的一致性和分布式中的一致性 CAP理论告诉我们分布式系统在发生了分区现象时，才需要选择CP或者AP，否则应该可以保证CA ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 欲穷千里目，更上一层楼。最近越来越发现古诗的精妙之处了，随着阅历的增加，之前背诵的古诗有些突然就明白了，不知道应该开心还是难过~ 2020-10-26 00:27:02]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>游戏</tag>
        <tag>架构设计</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下使用sort命令完成常见排序操作]]></title>
    <url>%2Fblog%2F2020%2F10%2F14%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8sort%E5%91%BD%E4%BB%A4%E5%AE%8C%E6%88%90%E5%B8%B8%E8%A7%81%E6%8E%92%E5%BA%8F%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言linux 系统下的命令常常给人一种短小精悍的感觉，使用起来就像一把把锋利的小刀，在自己专注的领域做到极致，今天要聊的就是 linux 环境下的排序命令 sort，处理文本按列排序非常方便，最近使用 sort命令来排序日志查找问题，为了防止一段时间不用又会忘记，所以记录下来便于下次查找。 命令作用sort 命令默认会将待排序内容以空格划分为多个列，然后对内容进行按列排序，命令本身不会修改待排序内容，而是将排序结果重新输出，如果想修改待排序源文件的内容，可以通过重定向命令来实现。 命令格式为： 1sort [选项] 文件名 常见选项sort 作为一个强大的命令，参数选项还挺多的，不过我只列举一些常见的参数，方便日常使用即可。 -k： 指定排序依据的列数，可以分多次指定 -o： 将排序后的结果存入指定文件 -c： 检查指定文件是否已经排好序 -u： 删除所有重复行 -b： 忽略每行或字段前面开始出的空格字符 -f： 排序比较时忽略大小写 -n： 转化为数字，按照数值的大小排序 -r： 反向排序，从大到小 -t： 指定排序时划分列数的分隔字符 数据文件为了展示 sort 命令的作用，专门利用 ls 命令产生了一段数据，并保存在了 data.txt 文件中，之后会利用这个文件来展示 sort 的用法，文件内容展示如下： 123456789101112131415albert@home-pc:~$ cat data.txt-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrcdrwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwx------ 1 albert albert 4096 Jul 16 00:52 .config/-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw------- 1 root root 2257 Jul 16 01:10 .viminfodrwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority 这个文件中的内容在使用 sort 命令排序时默认以空格分割，所以共有9列，在指定列数时从1开始，接下来我们用这些数据来测试一下排序命令的用法。 核心参数对于我来说 sort 命令的核心参数是 -k，其完整的参数列表为 -k START_F[.START_C][OPTIONS][,END_F[.END_C][OPTIONS]]，参数列表很长，但是不要恐惧，逐步分析就可以了。 -k 后面这已打算都是用来指定排序依据的范围的，其中 START_F 和 END_F 表示开始和结束的字段，也就是列数，.START_C 和 .END_C 表示指定字段开始和结束的字符数，OPTIONS 是由一个或多个单个字母排序的选项[bdfgiMhnRrV]，这些选项中常用的已经列举在前面了，写在此处的选项会覆盖全局排序选项。 这样文字叙述有些枯燥，可以看下这个参数 -k 6.2b,6.3b，这个排序选项的含义是把内容按照第6列的第2个字符到第6列的第3个字符排序，查找字符位置的时候要去掉前面的空白。 用法展示看了以上的参数可能还是不太清楚具体怎样用，所以举了下面这些例子，可以方便的处理常用的排序工作。 按照指定列排序这是最普通的排序要求了，也是我用的最多的情况，需要使用-k参数 按照第3列排序 123456789101112131415albert@home-pc:~$ sort -k3,3 data.txtdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrc-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw------- 1 root root 2257 Jul 16 01:10 .viminfo root 已经被排到了所有albert的后面 将排序结果存入指定文件12345678910111213141516albert@home-pc:~$ sort -k3,3 data.txt -o dst.txtalbert@home-pc:~$ cat dst.txtdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrc-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw------- 1 root root 2257 Jul 16 01:10 .viminfo 排序结果已经被存储到了文件 dst.txt 中，其实这个命令还可以改写成 sort data.txt &gt; dst.txt 查看文件是否已经排序好 测试没排好序的文件 12albert@home-pc:~$ sort -k3,3 data.txt -csort: data.txt:2: disorder: -rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history 测试已经排序的文件 12albert@home-pc:~$ sort -k3,3 dst.txt -calbert@home-pc:~$ 对于已经拍好序的文件使用 -c 参数没有任何输出，如果是未排序的文件则会给出提示 去掉排序结果中的重复行123albert@home-pc:~$ sort -k3,3 data.txt -u-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw------- 1 root root 2257 Jul 16 01:10 .viminfo 这里的重复行参考是你指定排序依据的列数，也就是第3列如果重复就会认为是重复行，结果中只能出现一次 按照数值结果进行排序123456789101112131415albert@home-pc:~$ sort -k5n,5 data.txt-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrcdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro* 这里使用了 -k5n,5 作为排序选项，其中的 n 表示以数值方式排序，如果不加 n 的排序结果如下： 123456789101112131415albert@home-pc:~$ sort -k5,5 data.txt-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrcdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out* 数据看起来很乱，其实也是按照第5列排好序的，仔细分析你会发现是把这些数字当成字符串排的序 反向排序123456789101112131415albert@home-pc:~$ sort -k3,3 data.txt -r-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrc-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw------- 1 albert albert 35 Jul 19 14:14 .lesshstdrwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwx------ 1 albert albert 4096 Jul 16 00:52 .config/ 按照第3列反向排序，root就排到了所有albert的前面 自定义分割字符sort 命令默认是以空格作为列的分割符号的，可以使用 -t 选项自定义分割符，比如我们使用 : 作为分隔符，然后以第二列进行排序 123456789101112131415albert@home-pc:~$ sort -t ":" -k2,2 data.txtdrwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw------- 1 albert albert 35 Jul 19 14:14 .lesshstdrwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrcdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful 结果是以分钟数进行的排序 综合排序学习了上面这么多参数，可以做一个综合的例子，以第6列的月份从小到大排序，以第5列文件大小逆序排列，通过组合上面的参数，可以使用下面的命令： 123456789101112131415albert@home-pc:~$ sort -k6,6 -k5rn,5 data.txt-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*drwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrc-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority 总结 sort 命令中的 -k 选项是最重要的参数，可以指定排序依据的列数 sort 命令中的 -n 选项也是常用的参数，可以进行数值比较 在实际问题中常常需要综合运用这些参数，参考综合例子中的方式逐步确定参数选项就可以了。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 业精于勤，荒于嬉；行成于思，毁于随。没有人能随随便便成功~ 2020-10-18 15:43:51]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>sort</tag>
        <tag>排序</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言在解决实际问题时的优点与不便]]></title>
    <url>%2Fblog%2F2020%2F10%2F14%2FGo%E8%AF%AD%E8%A8%80%E5%9C%A8%E8%A7%A3%E5%86%B3%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%E6%97%B6%E7%9A%84%E4%BC%98%E7%82%B9%E4%B8%8E%E4%B8%8D%E4%BE%BF%2F</url>
    <content type="text"><![CDATA[前言Go语言，全称golang，是Google开发的一种静态强类型、编译型、并发型并具有垃圾回收功能的编程语言。 从2007年末由 Robert Griesemer、Rob Pike、Ken Thompson 主持开发，其中的 Ken Thompson 可是和 Dennis Ritchie 一起发明了C语言的大佬。Go 语言于2009年11月正式宣布成为开放源代码项目， 并在2012年初，发布了Go 1.0稳定版本，此后便开启了稳步发展的道路。 Go 语言作为一个从2007年开始诞生的语言，在庞大的语言家族中算是一个晚辈，和C++、Python这种老牌语言相比查了将近20年，和 C 语言相比资历就更低了，但是这个新晋的语言在 Google 光环的强大加持下也在飞速发展着，由于前辈们在发展的途中趟了很多坑，所以 Go 在发明之初就避免了其他语言的很多不便，可以说是站在巨人的肩膀上发展起来的。 但是即便这样，Go 语言的特点也不能被所有人喜欢，和许多人一样，我在学习这门语言的过程中也发现一些很方便特性和一些不太方便的特点，下面简单说几个点，有不对的地方希望小伙伴能及时指出，防止我在错误的思想上越走越远（怎么有种新闻发言稿的感觉~）。 不便之处这里的不便之处只是我在使用过程中感觉不太方便，可能很多人并没有这个感觉，或许还有很多其他的解决方法和替代方案，烦请小伙伴能指点一下。 三目运算符这个三目运算符是个很常用的逻辑处理部件，也是我在逻辑中经常使用到的，在Python、Lua等语言中也不存在，但是我都找到了简单的替代方式，但是在Go 中不得不写成中规中矩的 if 条件判断，这让很多算法的解题代码看起来并不那么优雅，比如一个简单的约瑟夫环问题： N个人围成一圈，从第一个人从开始报数，报到m的人出圈，剩下的人继续从1开始报数，报到m的人出圈；如此往复，直到所有人出圈，输出最后一个出圈人最初始的编号。 这个问题的解法最简单的是模拟法，使用数组模拟一个环来按照规则运行，最后一个出圈的人的编号就可以输出到结果，还有一种思路就是找规律，可以找到出圈前后的序号对应关系，进而写出一行代码的解决方案。 1234// 索引从0开始，只要对结果加1就好了int joseph_ring(int n, int m) &#123; return n == 1 ? 0 : (joseph_ring(n - 1, m) + m) % n&#125; 但是在没有三目运算符的 Go 中要实现这个算法，就不得不多写几行了，和 C 语言相比就没有那么简洁了。 1234567func joseph_ring(n int, m int) int &#123; if n == 1 &#123; return 0 &#125; return (joseph_ring(n - 1, m) + m) % n&#125; if 单行语句也要加大括号Go 语言本身带有自己的格式化命令，可以保证编写时不同的缩进样式格式化之后得到相同的代码，if 后面的条件语句可以不加小括号，但是后面的语句块必须加大括号，这样的规定对于我经常写的代码有点不太友好，比如下面这些C++代码： 12345678if (a &gt; 0) break;if (b &lt; 0) continue;if (c == 0) return; 有时候为了看起来紧凑，可能会写成这样： 123if (a &gt; 0) break;if (b &lt; 0) continue;if (c == 0) return; 但是放到 Go 语言中，就不得不写成好几行了，并且还要加大括号，看起来代码有些松散。 1234567891011if a &gt; 0 &#123; break;&#125;if b &lt; 0 &#123; continue;&#125;if c == 0 &#123; return;&#125; 优秀特性上面提到了 Go 语言中不方便的地方，现在可以来说说 Go 语言相对于 C、C++ 更优越的特性： 多个变量同时赋值在 C++ 中交换两个变量的通常使用中间变量来完成，比如交换 a、b 两个变量的值： 123456int a = 1;int b = 6;int tmp = a;a = b;b = tmp; 针对于这种整形的变量，一些大牛们发明了特殊的算法来处理，避免使用中间变量： 123456int a = 1,;int b = 6;a = a ^ bb = b ^ aa = a ^ b 但是在 Go语言中这种情况非常好处理，直接从左到右依次赋值就好了 1234var a int = 1var b int = 6b, a = a, b defer 声明defer 可以用于在当前函数返回前执行一些清理代码，而不管此函数如何退出。defer 在函数中可以随时出现，这使得清理代码可以尽可能在需要清理的地方运行，比如我们常常要释放申请的资源，常见的需要释放的资源有文件描述符： 12345file, err := os.Open(fileName)if err != nil &#123; return&#125;defer file.Close() 有了 defer 终于不再在担心，资源没回收的问题，也不用在各个提前返回的条件分支中添加释放资源的重复代码了。 goroutine 并发goroutine 是Go并行设计的核心，说到底其实就是协程，但是它比线程更小并且在Go语言内部帮你实现了这些 goroutine 之间的内存共享。执行 goroutine 只需要极少的栈内存，可同时运行成千上万个并发任务。goroutine 一定程度上比 thread 更易用、更高效、更轻便。 使用起来也非常方便，创建 goroutine 只需在函数调用语句前添加 go 关键字，就可以创建并发执行单元。开发人员无需了解任何执行细节，调度器会自动将其安排到合适的系统线程上执行，这是解放生产力的又一创举，简单示例代码如下： 12345678910111213141516171819202122package mainimport ( "fmt" "time")func new_task() &#123; for i := 0; i &lt; 10; i++ &#123; fmt.Println("this is a newTask") time.Sleep(time.Second) //延时1s &#125;&#125;func main() &#123; for i := 0; i &lt; 3; i++ &#123; go new_task() //新建一个协程， 新建一个任务 &#125; time.Sleep(time.Second * 15) //延时15s fmt.Println("this is a main goroutine")&#125; 总结 Go 语言作为编程语言中的新晋小弟，吸收了前人的经验，现阶段发展迅猛 虽然 Go 出于一些目的规定了语言的标准，但是类似于没有三目运算符这种特点还是有些不方便 Go 这门语言还很年轻，相信随着不断发展它会越来越优秀，但没有任何语言是完美无缺的 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 陪伴是最长情的告白，而守护是最沉默的陪伴。国庆中秋双节合并，放假了，陪家人待在一起真的很开心，什么都不用做，就静静的待在一起很满足，聊聊天、抬抬杠，假期嗖嗖嗖地溜掉了~]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Go</tag>
        <tag>优点</tag>
        <tag>缺点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Spreadsheet Compare工具对比Excel文件差异]]></title>
    <url>%2Fblog%2F2020%2F10%2F04%2F%E4%BD%BF%E7%94%A8Spreadsheet-Compare%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94Excel%E6%96%87%E4%BB%B6%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[前言与 Spreadsheet Compare 这个工具的相遇是通过 TortoiseSVN 来牵线的，在使用 SVN 管理 Excel 表格时发现，TortoiseSVN自带的比较工具不能对比Excel文件的版本差异，这对于通过日志查找问题来说非常不方便，通过搜索发现了 Spreadsheet Compare 这款优秀的工具，特此记录一下，方便日后查找并快速配置。 Spreadsheet CompareSpreadsheet Compare 是 Microsoft Office 自带的一款工具软件（2013版本以后），可以用来比较不同 Excel 表格的差异，既能显示数据的不同，也可以显示出表结构的不同。这是一款带界面的工具软件，布局分为左右两部分，与 Beyond Compare 这个工具界面类似，但是功能更加强大。 Beyond Compare 也可以用来对比表格差异，但是只能比较两个Excel的当前工作表，如果每个 Excel 文件中包含多个工作表时就会对比错误的情况，而 Spreadsheet Compare 在这一点上更加优秀，可以对比多个表格数据。 Spreadsheet Compare 使用起来也非常简单，可以操作的按钮很少，界面简洁， 启动软件后单击左上角的 Compare Files 按钮，选择要对比的文件即可，非常方便，数据差异、结构差异等都会用不同的颜色标记出来，还可以导出对比结果。 命令模式这种模式对于是提供给 TortoiseSVN 使用的前提，因为 TortoiseSVN 无法像人一样一步步操作选择待比较的 Excel 表格，而是需要一个命令脚本，将要比较的参数传给 Spreadsheet Compare 工具进而完成比较工作。 找到工具想要编写命令脚本，首先要找到这个比较的工具，我找到的路径是在 &quot;C:\Program Files (x86)\Microsoft Office\Root\Office16\DCF\SPREADSHEETCOMPARE.EXE&quot;，相信大家的路径都差不多，在 Office 工具目录下应该就能找到了，可以在开始菜单中找到工具，然后通过属性找到可执行程序所在目录，工具的可执行文件名字叫做 SPREADSHEETCOMPARE.EXE。 编写脚本编写脚本之前有一点需要强调一下，SPREADSHEETCOMPARE.EXE 有点奇怪，大多数软件在比较差异的时候会将两个文件作为参数使用，但是 SPREADSHEETCOMPARE.EXE 在比较之前，需要将两个待比较的文件名分成两行写入一个文件，再将这个文件作为参数传给工具使用，比如要比较 ExcelA.xlsx 和 ExcelB.xlsx 两个文件，需要将两个文件写入一个临时文件 ExcelCompare.txt 中: ExcelA.xlsxExcelB.xlsx 然后再把这个文件作为参数传给工具： 1SPREADSHEETCOMPARE.EXE ExcelCompare.txt 脚本内容有了上面的说明，我们就可以写出一个较为通用的版本，比如我的脚本名字是 SC.bat，内容如下： 1234567@echo offchcp 65001set batpath=%~dp0echo %~1&gt; "%batpath%ExcelCompare.txt"echo %~2&gt;&gt; "%batpath%ExcelCompare.txt""C:\\Program Files (x86)\\Microsoft Office\\Root\\Office16\\DCF\\SPREADSHEETCOMPARE.EXE" "%batpath%ExcelCompare.txt" 脚本执行直接在 cmd 命令行中输入以下命令就可以对比 ExcelA.xlsx 和 ExcelB.xlsx 两个文件了： 12D:\data\bat&gt;D:\data\bat&gt;SC.bat ExcelA.xlsx ExcelB.xlsx 供给SVN调用个人比较懒，不喜欢截图，在 TortoiseSVN 工具的设置中找到“差异查看器”选项，选择该选项然后点击界面上的高级设置，点击增加按钮，增加根据扩展名指定差异比较程序，填写 .xlsx 和所需命令 D:\data\bat&gt;SC.bat %base %mine 命令中的 %base 和 %mine 参数是 TortoiseSVN 提供的，代表原始文件和自己修改的文件，这次再通过 SVN 查看表格差异就可以启动 Spreadsheet Compare 程序方便地查看两个表格的差异啦。 总结 Spreadsheet Compare 是一款强大的表格比较工具，在表格比较时比 Beyond Compare 还要优秀 脚本调用 SPREADSHEETCOMPARE.EXE 程序时参数是一个包含了待比较文件名的临时文件，这一点和其他的比较工具有些不同 脚本中我们生成的临时文件无需手动处理，再打开待比较文件后会自动删除 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 急需找到一个看得见摸得着的目标为之努力，不然真的有点止步不前了，至今还未找到可以废寝忘食之事，长此以往终将碌碌无为，继续找寻，此事可以不伟大，但应该有趣~ 2020-10-12 00:16:52]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Excel</tag>
        <tag>SVN</tag>
        <tag>Spreadsheet-Compare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[东拉西扯01世界的沧海桑田]]></title>
    <url>%2Fblog%2F2020%2F09%2F26%2F%E4%B8%9C%E6%8B%89%E8%A5%BF%E6%89%AF01%E4%B8%96%E7%95%8C%E7%9A%84%E6%B2%A7%E6%B5%B7%E6%A1%91%E7%94%B0%2F</url>
    <content type="text"><![CDATA[前言此篇非技术总结，但却与技术相关，写这篇总结的起因是前两天看了一节关于虚幻四的公开课，这节课也不是讲开发技术，更多的是讲创作艺术，课程开始前看到虚幻编辑器的画面，还是有一种很熟悉的感觉，毕竟使用了3年多的时间，外观几乎没有变化，使用方式依旧是原来的步骤，但随着课程的进行，我发现它变了。 其中有一段内容提到，虚幻四已经不再将自己作为一款游戏开发引擎，而是定位成一个艺术创作和开发的平台，也就是说它不仅仅可以做游戏，同时可以用来出影视剧、国漫、特定素材等等，它已经将自己的势力范围扩张，变得丰富而强大，后来又提到虚幻五带来的种种提升。 什么？虚幻五已经出了，这是我之前不知道的，我知道今年 Redis 出了最新的 6.0 版本，MySQL 一跃发行了 8.0 版本，IOS 也更新到的 14.0.1 版本，似乎各种技术都在飞速的发展着，但是人的精力毕竟有限，很难把它们成长历程尽收眼底，一不留神就发现某种技术已经悄悄从你身边跨了过去。 技术的发展记得我第一篇博客记录的是处理 Ubuntu 黑屏的解决方案，我去翻了翻当时记录的版本是 12.04，时间已经过去了7年，Ubuntu 20.04 已经发行，当年解决黑屏的经验或许已经毫无用处。 10年接触的第一种计算机编程语言是 C 语言，当时开发环境是 Turbe C 2.0，后来使用 VC++6.0，接着就是VS系列，期间用过 Dev-C++、CodeBlocks等编辑器，但是 VS 还是用的最多的，直到目前使用的 VS2017，可是刚刚一个好学的小朋友问我 VS2019 相关的问题，我发现这款用了这么久的工具，之前一直无变化的菜单布局在 VS2019 版本上发生了改变。 之前一直号称单线程内存数据库 Redis 在今年5月份发布的 6.0 版本中，加入了网络多线程，使得整体性能提升近一倍，这被认为是 Redis 最大的一次改版。 MySQL 直接从 5.7 版本跳到了 8.0 版本，因为之前一直是 5.6 、5.7 这样的小版本提升，一跃跳到 8.0 一时让人好奇到底改了什么？其实 6.0 是一个过渡版本，而 7.0 是作为集群的保留版本，所以这次直接到了 8.0。其中一个亮点增加了 MySQL 文档存储，可以存储 json 格式，开始支持向 NoSQL 格式转化。 差点忘了C++，这个庞然大物目前已经从最开始的 C++98，发展到现在的 C++20，我在想它如果真的存活到 2098 年，应该怎么命名它呢？新标准的内容很长，需要慢慢来消化，可以发现一些很好玩的东西，比如三向比较运算符 &lt;=&gt;，也叫飞船运算符，感兴趣的可以去了解下。 经历的和未经历的变化还有很多，想好了再来补充。。。 技术公司的发展当年找工作的听说过的巨头就是 BAT，而今天晚上问一个即将毕业进入工作岗位的同学，哪些是他心目中的大厂，他给出了四个名字，“阿里、腾讯、字节、美团”，很明显百度已经掉队，但是瘦死的骆驼比马大，短时间内百度的技术底蕴不会消失殆尽，这些手握资源和技术的大厂很早就给自己挖好了护城河，一般企业很难追赶的上。 相比早期的 BAT，我感觉后来的字节、美团能够赶上他们实属不易，记得当年开玩笑说 TX 除了发明了一套钻石收费系统，其他的都是抄的，任何公司有了好点子，不是被他合并了就是被他抄走了，所以说能在某个它无法掌控的赛道上超越它也是非常厉害了。 有些东西是其他公司无法做到的，比如疫情期间的健康宝，每天上班、去商场、去公园都要看，都要打开微信和支付宝的APP，这个日活的数据放到其他任意一款软件上都是庞大的数字，可是他们就在这两大巨头这自然的发生着。 总结 技术每时每刻都在发展，有时你发现它陌生了，其实只是你关注的少了 好的赛道大多数已被别人占领，从夹缝中寻找到机会还需好好把握，才能做出一定的成绩 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 年年岁岁花相似，岁岁年年人不同 2020-9-27 00:20:42]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++求解组合数的具体实现]]></title>
    <url>%2Fblog%2F2020%2F09%2F19%2FC-%E6%B1%82%E8%A7%A3%E7%BB%84%E5%90%88%E6%95%B0%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言很少写关于具体算法的总结笔记，因为很难把一个算法从头到尾的叙述清晰并且完整，容易造成误解。这次想总结一下组合数的具体实现，原因是最近总是碰见组合数，所以决定来写写，免得每次从头推导公式耽误时间。排列组合经常会作为一个问题解决方案中一部分，通常是求某个问题有多少个解，达到某种状态有多少种操作方式等等。 问题起因今天下午解一道简单题，难度简直刷新了我的认知，其中需要用到组合数，但这仅仅是解题的一小部分，没办法，从头推导的，简单优化下，写出了如下代码： 1234567int C(int a, int b)&#123; int ans = 1; for (int i = a; i &gt; a - b; i--) ans *= i; for (int i = b; i &gt; 1; i--) ans /= i; return ans;&#125; 因为时间紧迫，范围也比较小，同时可以控制 a 和 b 的大小，所以临时写下的这段代码可以运行，不然这段代码会出现各种错误的。 组合公式既然是想做总结，还是从头来看看组合公式，根据原始公式实现算法，并尝试优化它，当熟悉这个套路之后，就可以直接拿来用了，可以节省不少时间，组合公式的常见表示方式如下： $$C^m_n = \frac{n!}{m!(n-m)!} = C^{n-m}_n,(n \geq m \geq 0)$$ 这个公式写出来清晰多了，n!表示n的阶乘，计算方式为 n*(n-1)*(n-2)*(n-3)*…*3*2*1， 相信很多人都清楚，我们只要把这个数据公式翻译成代码就可以了： 12345678int C2(int n, int m)&#123; int a = 1, b = 1, c = 1; for (int i = n; i &gt;= 1; --i) a *= i; for (int i = m; i &gt;= 1; --i) b *= i; for (int i = n-m; i &gt;= 1; --i) c *= i; return a/(b*c);&#125; 代码比较简单，依次计算公式中三个数的阶乘，然后再做乘除法就可以了，但是你有没有思考过一个问题，int 类型的整数最大能表示的阶乘是多少？是12!，它的值是 479,001,600，它是 int 表示范围内最大的阶乘数，看来这种实现方式局限性很大，如果 n 大于12就没有办法计算了。 公式变形实际上根据阶乘的定义，n! 和 (n-m)! 是可以约分的，将这两个式子约分后，公式可以化简为： $$C^m_n = \frac{n!}{m!(n-m)!} = \frac{n(n-1)(n-2)…(n-m+1))}{m!},(n \geq m \geq 0)$$ 公式写成这样之后可以少计算一个阶乘，并且计算的范围也会缩小，代码实现和一开始展示的代码思想是一样的： 1234567int C3(int n, int m)&#123; int a = 1, b = 1; for (int i = n; i &gt; n - m; --i) a *= i; for (int i = m; i &gt;= 1; i--) b *= i; return a/b;&#125; 这段代码虽然经过了化简，但是当 n 和 m 非常接近的时候，分子还是接近于 n!，所以表示的范围还是比较小。 递推公式直接给出的公式经过化简后还是受制于计算阶乘的范围，得想个办法看看能不能绕过阶乘计算，方法总是有的，并且前辈们已经给我们整理好了，我们总是站在巨人的肩膀上，下面就是递推公式： $$\begin{cases} {C^m_n} = 1,\qquad\qquad\qquad (m=0 或 m=n) \ {C^mn} = {C^m{n-1}} + {C^{m-1}_{n-1}},\qquad(n &gt; m &gt; 0) \end{cases}$$ 递归实现有了上面的分段函数表示，就满足了递归的条件，既有递归调用缩小规模，也有递归出口，这样实现起来很简单，代码如下： 12345int C4(int n, int m)&#123; if (n == m || m == 0) return 1; return C4(n-1, m) + C4(n-1, m-1);&#125; 这两行代码是不是很秀？不过使用递归常常会出现一问题，那就是相同子问题多次计算，导致效率低下，这个计算组合数的方式同样存在重复计算子问题的缺点，我们以调用C4(5, 3)为例，看看下面的调用关系图： 1234567891011121314151617181920graph TB A(5,3)--&gt;B(4,3) B(4,3)--&gt;C(3,3); B(4,3)--&gt;D(3,2); D(3,2)--&gt;E(2,2); D(3,2)--&gt;F(2,1); F(2,1)--&gt;G(1,1); F(2,1)--&gt;H(1,0); A(5,3)--&gt;O(4,2) O(4,2)--&gt;P(3,2) O(4,2)--&gt;Q(3,1) P(3,2)--&gt;R(2,2) P(3,2)--&gt;S(2,1) Q(3,1)--&gt;T(2,1) Q(3,1)--&gt;U(2,0) S(2,1)--&gt;V(1,1) S(2,1)--&gt;W(1,0) T(2,1)--&gt;X(1,1) T(2,1)--&gt;Y(1,0) 从这个图可以清晰看出C4(3, 2) 和 C4(2, 1) 都被计算了多次，当 m 和 n 的数字比较大的时候，会进行更多次的重复计算，严重影响计算的效率，有没有什么办法解决重复计算的问题呢？ 备忘递归解决重复计算的常用方法是利用一个备忘录，将已经计算式子结果存储起来，下次再遇到重复的计算时直接取上次的结果就可以了，我们可以将中间结果简单存储到map中。 假设 n 不超过10000，这比12已经大太多了，我们可以使用 n * 10000 + m 作为map的键，然后将结果存储到map中，每次计算一个式子前先看查询备忘录，看之前有没有计算过，如果计算过直接取结果就可以了，代码简单实现如下： 1234567891011121314int C5(int n, int m, map&lt;int, int&gt;&amp; memo)&#123; if (n == m || m == 0) return 1; auto itora = memo.find((n-1)*10000+m); int a = itora != memo.end() ? itora-&gt;second : C4(n-1, m); if (itora == memo.end()) memo[(n-1)*10000+m] = a; auto itorb = memo.find((n-1)*10000+m-1); int b = itorb != memo.end() ? itorb-&gt;second : C4(n-1, m-1); if (itorb == memo.end()) memo[(n-1)*10000+m-1] = b; return a + b;&#125; 使用 map 作为备忘录可以避免重复计算，这是解决递归效率低下的常用方法，那么有了递推公式不使用递归实现可不可以呢？当然可以了，针对于这个问题，有了递推公式我们还可以使用动态规划（dp）的方式来实现。 动态规划动态规划常常适用于有重叠子问题和最优子结构性质的问题，试图只解决每个子问题一次，具有天然剪枝的功能。基本思想非常简单，若要解一个给定问题，我们需要解其不同子问题，再根据子问题的解以得出原问题的解。 再回顾一下递推公式： $$\begin{cases} {C^m_n} = 1,\qquad\qquad\qquad (m=0 或 m=n) \ {C^mn} = {C^m{n-1}} + {C^{m-1}_{n-1}},\qquad(n &gt; m &gt; 0) \end{cases}$$ 翻译成人话就是，当m等于0或者等于n的时候，组合数结果为1，否则组合数结果等于另外两个组合数的和，我们可以采用正向推导的方式，将 n 和 m 逐步扩大，最终得到我们想要的结果，定义dp表格如下： n\m (0) (1) (2) (3) (4) (5) (0) 1 (1) 1 1 (2) 1 2 1 (3) 1 3 3 1 (4) 1 4 6 4 (5) 1 5 10 ==&gt;10 从表格可以清晰的看出求解 C(5,3) 只需要计算5行3列（从0开始）的数据，其余的值可以不用计算，这样我们就可以对照着表格写代码啦，定义一个dp数组，然后双重for循环就搞定了： 123456789101112int C6(int n, int m)&#123; if (n == m || m == 0) return 1; vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(m+1)); for (int i = 0; i &lt;= n; i++) for (int j = 0; j &lt;= i &amp;&amp; j &lt;= m; j++) if (i == j || j == 0) dp[i][j] = 1; else dp[i][j] = dp[i-1][j] + dp[i-1][j-1]; return dp[n][m];&#125; 至此，我们就采用了非递归的方式求解出了组合数的结果，但是这里的空间有点浪费，每次都要花费O(mn)的空间复杂度，有没有办法降低一点呢？我们可以找找规律进行压缩。 压缩DP观察之前的动态规划实现的代码，我们发现求解第 i行的数据时只与第 i-1 行有关，所以我们可以考虑将二维数据压缩成一维，还是逐行求解，只不过可以用一维数组来记录求解的结果，优化代码如下： 123456789101112int C7(int n, int m)&#123; if (n == m || m == 0) return 1; vector&lt;int&gt; dp(m+1); for (int i = 0; i &lt;= n; i++) for (int j = min(i, m); j &gt;= 0; j--) if (i == j || j == 0) dp[j] = 1; else dp[j] = dp[j] + dp[j-1]; return dp[m];&#125; 这样我们就将空间复杂度降低到了O(m)，需要注意的是在计算dp时，因为采用了压缩结构，为防止前面的修改影响后续结果，所以采用里倒序遍历，这是一个易错的点。 其他优化代码实现到这里，我们的时间复杂度是O(nm)，空间复杂是O(m)，其实还有进一步的优化空间： 减小m： 因为题目是求解C(n, m)，但是我们知道组合公式中，C(n, m) 和 C(n, n-m) 相等，所以当 n-m 小于 m 的时候求解C(n, n-m)可以降低时间复杂度和空间复杂度。 部分剪枝： 观察函数int C7(int n, int m)，实际上当i为n时，j没必要遍历到0，只需要计算j等于m的情况就可以了，可以提前计算出结果。 缩小计算范围： 从上面的剪枝操作得到启示，其实每一行没必要全部计算出来，以 C(5,3) 为例，我们只需要计算出表格中有数字的位置的结果就可以了： n\m (0) (1) (2) (3) (4) (5) (0) 1 (1) 1 1 (2) 1 2 1 (3) 3 3 1 (4) 6 4 (5) ==&gt;10 这样来看每行最多需要计算3个值，那么时间复杂度可以降低到 O(3n)，去掉常数，时间复杂度降为 O(n)。 总结 计算组合数可以采用逆向递归和正向递推两种方式，递归时注意写好递归出口 采用正向递推方法时利用动态规划思想，使用子问题的解拼凑出最终问题的解 计算组合数若使用了计算阶乘应注意范围，避免在计算时产生溢出，int最多能表示 12! 使用动态规划方法时可以逐步优化空间和时间，这其实就是优化算法的过程，也是提升的过程 关于组合数的求解方式，我们可以找到时间复杂度O(n)、空间复杂度O(m)的非递归解法 补充感谢 @小胡同的诗 同学的补充和提醒，让我再次感受到数学力量的深不可测，原来求解组合数还有这样一个递推公式： $$\begin{cases} {C^m_n} = 1,\qquad\qquad\qquad (m=0 或 m=n) \ C_n^m=\frac{n-m+1}{m}C_n^{m-1},\qquad(n &gt; m &gt; 0) \end{cases}$$ 这个公式厉害就厉害在它是一个线性的，不存在分叉的情况，也就是说即使递归也不会出现重复的计算，我们简单实现一下。 反向递归12345int C8(int n, int m)&#123; if (n == m || m == 0) return 1; return C8(n, m-1) * (n-m+1) / m;&#125; 代码非常紧凑，也不存在重复计算的情况，当然我们也可以使用正向计算的方式来实现。 正向递推12345678910int C9(int n, int m)&#123; if (n == m || m == 0) return 1; int ans = 1; m = min(m, n-m); for (int i = 1; i &lt;= m; i++) ans = ans * (n-i+1) / i; return ans;&#125; 这段代码将时间复杂度降到了O(m)，空间复杂度降到了O(1)，不过特定的场景还是要选择特定的实现，虽然C9函数在时间复杂度和空间复杂度上都优于 C5 函数，但是如果一个实际问题中需要用到多个组合数的时候，C5 这种采用缓存的方式可能会是更好的选择。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 想讲故事？没人倾听？那是因为你还未到达一个指定的高度，当你在某个领域站稳了脚跟，做出了成绩，自然有的是时间去讲故事或者“编”故事，到时候随便一句话都会被很多人奉为圭臬，甚至会出现一些鸡汤莫名其妙的从你嘴里“说”出来。在你拥有了讲故事权利的同时，批判的声音也将随之而来~ 2020-9-20 12:32:37]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>recursion</tag>
        <tag>combination</tag>
        <tag>dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中优先队列priority_queue的基础用法]]></title>
    <url>%2Fblog%2F2020%2F09%2F11%2FC-%E4%B8%AD%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97priority-queue%E7%9A%84%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言学习优先队列之前先看个单词队列 queue， 这个单词的读法很多人都能读对吧，音标是 /kjuː/ ，再看一个双端队列 deque，它的音标是 /dek/，应该有人读错了吧，反正我是没读对，刚开始看见一次错一次，现在还好了，基本能记住怎么读了，可是这些队列怎么用呢？ 队列就不用多说了，一个先进先出的经典数据结构，那么优先队列是个什么鬼，其实它就是在队列的基础上加上优先两个字，想想怎样才能优先呢？没错——排队！只有排好了队伍才会有落后和优先之分，否则一团乱糟糟的，怎么才能分出优先的，所以优先队列一定应用了排序。 可是排序要怎样实现呢？其实排序这个底层逻辑你是不用管的，你只要把想要的数据放到优先队列里，然后取出的必定是当前状态下最优的，当然，究竟什么是最优的条件是需要你来设定的，也就是说我们需要定义排序的规则。 头文件优先队列 priority_queue 是队列 queue 的一个变种，头文件是#include &lt;queue&gt;，使用优先队列必须要包含这个头文件。 结构定义优先队列的结构定义是一个模板类，需要提供三个类型参数： 12345template&lt; class T, class Container = std::vector&lt;T&gt;, class Compare = std::less&lt;typename Container::value_type&gt;&gt; class priority_queue; 从定义可以看出，虽然要结构是三个参数，但是后两个参数带了默认值，所以针对于普通的数据类型，一般情况下指提供第1个参数就可以了，比如 priority_queue&lt;int&gt; 实际上等价于 priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt;。 这三个参数的含义分别为：数据类型，容器类型和比较函数，实际上优先队列就是维护了一个装有 T 类型元素的容器 Container，并在入队和出队时对容器内元素使用 Compare 比较函数进行了排序。 这3个参数还要满足一定的要求，并且在使用过程中有些注意事项： 如果类型 T 和 Container 容器中元素类型不一致，那么行为未定义，所以要避免这种情况。 Container 必须是序列容器，其实C++中序列容器很多的，比如std::array、std::vector、std::deque、std::list等 Container 还必须要支持随机访问，并且有 front()、push_back()、pop_back() 等函数 这样来看只有 std::vector、std::deque 满足容器条件了，而优先队列中使用的默认参数也是 std::vector。 队列排序一直在说优先队列里使用了排序，而常用的容器是 std::verctor，那么究竟用的是什么排序，又是在什么时候进行的排序呢？实际上这里的排序并不是我们通常拿到数据后使用的冒泡排序、快速排序等，优先队列中的排序本质上是堆排序，但是它不是每次都进行完整的堆排序，而是通过 Container 维护了一个堆结构，每次入队和出队时都进行一次堆调整，所花时间为 log(n)，所以用在数据量大的地方，速度比较快。 优先队列使用当我们大概了解了优先队列的原理后，可以通过使用来进一步熟悉这个结构，下面来看几个例子。 实现排序1234567891011121314151617#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;void common_sort()&#123; int source_data[10] = &#123;3, 5, 8, 1, 10, 2, 9, 15, 13, 16&#125;; // 默认大根堆，实现由大到小排序 priority_queue&lt;int&gt; q; for (auto n : source_data) q.push(n); while (!q.empty()) &#123; cout &lt;&lt; q.top() &lt;&lt; endl; q.pop(); &#125;&#125; priority_queue&lt;int&gt; 默认构建的是一个大根堆，所以每次从头取数据得到的是一个从大到小的队列排序 123456789101112albert@home-pc:/mnt/c++/datastruct$ g++ priorityqueue.cpp -o commonsort -std=c++11albert@home-pc:/mnt/c++/datastruct$ ./commonsort16151310985321 如果是完整排序使用优先队列就有些麻烦了，还不如直接调用 std::sort 函数，但是如果只取部分数据的话，优先队列还是非常方便快速的，比如下面这个问题。 取出数组中最大的k个数这是一个经典的算法题，最容易想到的办法就是遍历，先找到最大的，然后排出这个数再找到最大的，这样找k次就好了，所需时间大概表示为 O(kN)。 还有一个方法是排序，使用 std::sort 排序后，然后依次取出前 k 个数就行了，排序使用快速排序的话可以达到所需时间为 O(Nlog(N))，其实这样已经很优秀了，但是还可以通过优先队列来加速，下面来写一下代码： 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;void max_k_num()&#123; int source_data[10] = &#123;3, 5, 8, 1, 10, 2, 9, 15, 13, 16&#125;; int k = 5; // 小根堆 priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; for (auto n : source_data) &#123; if (q.size() == k) &#123; if (n &gt; q.top()) &#123; q.pop(); q.push(n); &#125; &#125; else q.push(n); &#125; while (!q.empty()) &#123; cout &lt;&lt; q.top() &lt;&lt; endl; q.pop(); &#125;&#125; 这里是定义了一个小根堆，堆顶是最小值，当有新元素大于堆顶元素时，并且队列中元素等于k个，需要移除堆顶元素，然后插入新的元素，这样就能保证优先队列中始终拥有最大的k个数，运行结果如下： 1234567albert@home-pc:/mnt/c++/datastruct$ g++ priorityqueue.cpp -o max_k_num -std=c++11albert@home-pc:/mnt/c++/datastruct$ ./max_k_num910131516 因为这里控制堆的规模最大为k，所以这个算法的执行时间大概是O(Nlog(k))，绝大多数情况是由于快速排序的。 自定义结构使用优先队列时常常要用到自定义结构，这时候就需要自己来写比较函数了，比如输出成绩最好的三个人的信息： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;struct student &#123; string name; int score;&#125;;struct cmp_custom &#123; bool operator()(student&amp; x, student&amp; y) &#123; return x.score &gt; y.score; &#125;&#125;;void max_k_score()&#123; vector&lt;student&gt; stu_list = &#123;&#123;"Andy", 89&#125;, &#123;"Bella", 79&#125;, &#123;"Cary", 92&#125;, &#123;"Dick", 60&#125;, &#123;"Ray", 70&#125;&#125;; int k = 3; // 小根堆 priority_queue&lt;student, vector&lt;student&gt;, cmp_custom&gt; q; for (auto stu : stu_list) &#123; if (q.size() == k) &#123; if (stu.score &gt; q.top().score) &#123; q.pop(); q.push(stu); &#125; &#125; else q.push(stu); &#125; while (!q.empty()) &#123; cout &lt;&lt; q.top().name &lt;&lt; ":" &lt;&lt; q.top().score &lt;&lt; endl; q.pop(); &#125;&#125; 输出结果如下，每个人的名字后面跟着分数，结果是分数最大的3个人的信息： 12345albert@home-pc:/mnt/c++/datastruct$ g++ priorityqueue.cpp -o max_k_score -std=c++11albert@home-pc:/mnt/c++/datastruct$ ./max_k_scoreBella:79Andy:89Cary:92 自定义比较函数的另一种写法看到上个例子中自定义比较函数的写法比较怪，一般我们在排序时定义的比较函数使用lambda表达式就可以，而这里是不能直接这样写的，需要多转化一步，写成下面这种形式： 12auto cmp = [](student&amp; x, student&amp; y) &#123; return x.score &gt; y.score; &#125;;priority_queue&lt;student, vector&lt;student&gt;, decltype(cmp)&gt; q(cmp); 虽然看起来还是有点怪，但总比下面这样要好看的多： 123456struct cmp_custom &#123; bool operator()(student&amp; x, student&amp; y) &#123; return x.score &gt; y.score; &#125;&#125;;priority_queue&lt;student, vector&lt;student&gt;, cmp_custom&gt; q; 常用函数优先队列的常用函数与队列类似，常用的有以下这些，如果想了解详细的用法，请戳在线文档 函数名 含义 top 访问队列的头部元素 empty 判断优先队列内是否有元素 size 返回优先队列内元素个数 push 向优先队列中插入元素 emplace 在优先队列中构造元素 pop 从优先队列头部弹出元素 swap 与其他容器交换元素 总结 优先队列在一些需要部分排序的场景可以加快访问速度，降低时间复杂度 优先队列加速所付出的代价就是构建堆结构所需的内存，时间和空间总是一对矛盾共同体 以自定义结构作为元素的优先队列需要单独编写比较函数，可以使用lambda表达式，并用 decltype(cmp) 推导类型 需要注意的是这里的优先队列定义，第三个参数的需要的是比较函数的参数类型，而不是比较函数，区分与 std::sort 的不同 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人在比较中奋进，同在比较中消亡，起初面临差距时会奋起直追，但是当努力过后发现距离反而越来越远时，便会麻木懈怠，曾经的努力没有用吗？我觉得不是，努力过不一定会成功，但是努力的过程已经印在了骨子里，这本身就是生活的一部分。你可以选择这条艰苦的路，同样也可以选择跳过，至于跳过时错失了什么，谁又知道呢？毕竟人生无法再来过，重新读档只发生在游戏世界中~ 2020-9-12 17:06:10]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>queue</tag>
        <tag>priority_queue</tag>
        <tag>heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git log根据特定条件查询日志并统计修改的代码行数]]></title>
    <url>%2Fblog%2F2020%2F09%2F05%2Fgit-log%E6%A0%B9%E6%8D%AE%E7%89%B9%E5%AE%9A%E6%9D%A1%E4%BB%B6%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%E5%B9%B6%E7%BB%9F%E8%AE%A1%E4%BF%AE%E6%94%B9%E7%9A%84%E4%BB%A3%E7%A0%81%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言随着年龄的增长和知识的积累，最近常常有种豁然开朗的感觉，或者对一个已经存在的事物突然有了新的认识，比如统计这个词很早就接触了，从没考虑过它是什么意思，而这篇总结的题目中用了统计一词，第一感觉应该是汇总、记录的意思，后来去查了词条定义，也确实就是类似的解释，从没有刻意去学这个词的含义，但是在每天的生活中已经潜移默化地归纳、总结出来了。 想要统计就得有数据源，而 git log 命令恰恰就能提供这个数据源，git log 本身就是一个显示日志的命令，日志记录的是代码库变化的数据，类似于描述代码库变化的 “史书”，想要描述历史就需要大量的数据支撑，想要统计修改的代码行数，只要我们从历史记录中找到需要计算的部分就可以了。 git log在统计之前我们需要先整理数据，杂乱无章的数据不是不能统计，只是计算起来更加的麻烦，所以在统计前需要先将数据规范化，所以我们需要先学习一下 git log 的相关操作。 我们以 redis 开源库为例，切换到 6.0 分支，提交记录定位到 7bf665f125a4771db095c83a7ad6ed46692cd314，以此为数据源，学习一下git log 的常用的查询方法，其实使用不同的条件查询就是整理、归类数据的过程。 git log 的用法多种多样，我们主要关心两个大类，分别是条件筛选和显示格式。 条件筛选git log 条件筛选的选项非常多，使用条件筛选的选项会影响显示的提交记录的范围，查找到想要显示的提交记录。 查询最近几条log使用 -number 参数可以查询最近几条提交提交记录： 1234567891011121314151617181920$ git log -3commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6.commit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branchcommit e15528bf1da1f1232fd08801ad382c915be94662Author: Itamar Haber &lt;itamar@redislabs.com&gt;Date: Thu Jul 16 21:31:36 2020 +0300 Adds SHA256SUM to redis-stable tarball upload (cherry picked from commit 5df0a64d30e7815c0a4a75a80f165fdee0bd1db6) 查询指定作者提交使用 --author 参数可以查询指定作者的提交记录： 12345678910111213Albert@DESKTOP-6746UC3 MINGW64 /d/data/maingit/redis (6.0)$ git log -2 --author='Oran Agra'commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6.commit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branch 查询指定时间段的日志这个可选参数比较多，比如 --since、--until、--before、--after 等等，从意思很容易分辨怎么使用： 查询2020-01-01到2020-04-01的提交记录 123456789101112$ git log -2 --after=2020-01-01 --before=2020-04-01commit 957e917a84ac9979f18145a4d0b53386f5ce4fd9 (tag: 6.0-rc3)Author: antirez &lt;antirez@gmail.com&gt;Date: Tue Mar 31 17:56:04 2020 +0200 Redis 6.0-RC3.commit ef1b1f01a84e969ea368e7fdbaf0d10615743269Author: antirez &lt;antirez@gmail.com&gt;Date: Tue Mar 31 17:41:23 2020 +0200 cast raxSize() to avoid warning with format spec. 恰好逮到了原作者的提交~ 查询1年前的提交记录 1234567891011121314151617181920$ git log -2 --until=1.year.agocommit 86aade9a024c3582665903d0cc0c5692c6677cfdMerge: 89ad0ca56 3bfcae247Author: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Thu Sep 5 13:30:26 2019 +0200 Merge pull request #6364 from oranagra/fix_module_aux_when Fix to module aux data rdb format for backwards compatibility with old check-rdbcommit 3bfcae247a1c51788940bd4d2f32751ead451e42Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Thu Sep 5 14:11:37 2019 +0300 Fix to module aux data rdb format for backwards compatibility with old check-rdb When implementing the code that saves and loads these aux fields we used rdb format that was added for that in redis 5.0, but then we added the 'when' field which meant that the old redis-check-rdb won't be able to skip these. this fix adds an opcode as if that 'when' is part of the module data. 查询包含指定描述内容的提交记录这里用可以使用 --grep 参数，可以过滤出包含指定内容的提交记录，这里指的是在 commit 描述中筛选符合条件的提交，比如查找提交描述中包含 client 的提交记录： 123456789101112131415161718192021222324$ git log -2 --grep='client'commit 0f75036c07db48dfcf605e090216a4447edc38fcAuthor: Wen Hui &lt;wen.hui.ware@gmail.com&gt;Date: Wed Jul 15 05:38:47 2020 -0400 correct error msg for num connections reaching maxclients in cluster mode (#7444) (cherry picked from commit d85af4d6f5fbe9cb9787b81583627cd74b47f838)commit f89f50dbd06247677b8cb3927cbb88c1b5384061Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Tue Jul 14 20:21:59 2020 +0300 diskless master disconnect replicas when rdb child failed (#7518) in case the rdb child failed, crashed or terminated unexpectedly redis would have marked the replica clients with repl_put_online_on_ack and then kill them only after a minute when no ack was received. it would not stream anything to these connections, so the only effect of this bug is a delay of 1 minute in the replicas attempt to re-connect. (cherry picked from commit a176cb56a3c0235adddde33fcbaee2369a5af73e) 查找指定分支的提交记录使用 git log 默认查找的是当前分支的提交记录，如果想查询其他分支的记录直接在命令后面加上分支名字就行，比如查询 arm 分支上的提交记录： 123456789101112131415161718192021222324$ git log -2 armcommit 7329cc39818a05c168e7d1e791afb03c089f1933 (origin/arm, arm)Author: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Sun Feb 19 15:07:08 2017 +0000 ARM: Avoid fast path for BITOP. GCC will produce certain unaligned multi load-store instructions that will be trapped by the Linux kernel since ARM v6 cannot handle them with unaligned addresses. Better to use the slower but safer implementation instead of generating the exception which should be anyway very slow.commit 4e9cf4cc7ed4b732fc4bb592f19ceb41d132954eAuthor: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Sun Feb 19 15:02:37 2017 +0000 ARM: Use libc malloc by default. I'm not sure how much test Jemalloc gets on ARM, moreover compiling Redis with Jemalloc support in not very powerful devices, like most ARMs people will build Redis on, is extremely slow. It is possible to enable Jemalloc build anyway if needed by using "make MALLOC=jemalloc". 其实在 git 体系中，分支名、commit、标签等拥有几乎相同的含义，所以在很多场景下可以扩展互换，比如 git log 后面加上分支名就可以查询指定分支的提交记录，如果加上 commit 就会查询这个 commit 之前的提交记录，如果加上标签，就可以查询这个标签之前的提交记录，比如我们加一个 commit 试试： 123456789101112131415161718192021222324$ git log -2 7329cc39818a05c168e7d1e791afb03c089f1933commit 7329cc39818a05c168e7d1e791afb03c089f1933 (origin/arm, arm)Author: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Sun Feb 19 15:07:08 2017 +0000 ARM: Avoid fast path for BITOP. GCC will produce certain unaligned multi load-store instructions that will be trapped by the Linux kernel since ARM v6 cannot handle them with unaligned addresses. Better to use the slower but safer implementation instead of generating the exception which should be anyway very slow.commit 4e9cf4cc7ed4b732fc4bb592f19ceb41d132954eAuthor: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Sun Feb 19 15:02:37 2017 +0000 ARM: Use libc malloc by default. I'm not sure how much test Jemalloc gets on ARM, moreover compiling Redis with Jemalloc support in not very powerful devices, like most ARMs people will build Redis on, is extremely slow. It is possible to enable Jemalloc build anyway if needed by using "make MALLOC=jemalloc". 因为 commit id 就是之前的 arm 分支最新的记录，所以这个命令等价于 git log -2 arm 查询指定 commit 之间的提交记录如果想查询两个 commit 之前的提交记录，可以将两个 commit id 依次放在命令后面并用 .. 连接就可以了，格式为 git log commit1..commit2，需要注意的是这样查询出来的提交记录列表中不包含 commit1，其实列举出的就是 commit1 之后又做了哪些修改提交。 123456789101112$ git log e15528bf1da1f1232fd08801ad382c915be94662..7bf665f125a4771db095c83a7ad6ed46692cd314commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6.commit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branch 这个特性有一个应用就是在 merge 分支之前可以查询究竟会 merge 哪些记录，常见的用法比如 git log feature..dev 就是列举出 feature 分支合并到 dev 分支将要合并的提交记录有哪些。 123456$ git log 6.0..unstablecommit 324e22accf457edc996971bc97f5474349cd7c4c (unstable)Author: antirez &lt;antirez@gmail.com&gt;Date: Fri Dec 20 12:29:02 2019 +0100 Fix ip and missing mode in RM_GetClusterNodeInfo(). 查询指定文件的提交记录查询指定文件的提交记录一般直接在 git log 命令后面跟上文件名就可以，但是为了避免和分支名产生分歧，所以通常在文件名前面加上 -- 用来区分，-- 这个标识符就是用来防止混淆的，放在 -- 前面的是分支名，放在后面的是文件名，相同的作用不仅仅在 git log 命令中，在其他命令比如 git checkout 中也有相同的用法。 12345678910111213141516171819$ git log -2 -- redis.confcommit 7a536c2912be1fd9f62b26b7022a00644c88ef8bAuthor: Yossi Gottlieb &lt;yossigo@users.noreply.github.com&gt;Date: Fri Jul 10 11:33:47 2020 +0300 TLS: Session caching configuration support. (#7420) * TLS: Session caching configuration support. * TLS: Remove redundant config initialization. (cherry picked from commit 3e6f2b1a45176ac3d81b95cb6025f30d7aaa1393)commit 8312aa27d47c0befcf69eb74d0a5dc19745ffd32Author: antirez &lt;antirez@gmail.com&gt;Date: Mon Jun 22 11:21:21 2020 +0200 Clarify maxclients and cluster in conf. Remove myself too. (cherry picked from commit 59fd178014c7cca1b0c668b30ab0d991dd3030f3) 显示格式git log 除了可以筛选提交记录，还可以控制显示格式，普通不加参数，会显示作者、邮件、提交描述信息、日期等信息。 123456$ git log -1commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6. 通过添加参数可以控制和改变显示格式，下面来看几条常见的 显示单行信息git log 默认会显示多行信息，使用 --oneline 后每条提交记录只显示一行信息，可以在一屏幕中查看到更多的信息 1234567891011$ git log -10 --oneline7bf665f12 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0) Redis 6.0.6.a5696bdf4 Run daily CI on PRs to release a branche15528bf1 Adds SHA256SUM to redis-stable tarball uploade28aa99af Support passing stack allocated module strings to moduleCreateArgvFromUserFormat (#7528)305143004 Send null for invalidate on flush (#7469)29b20fd52 Notify systemd on sentinel startup (#7168)5b3668121 Add registers dump support for Apple silicon (#7453)0f75036c0 correct error msg for num connections reaching maxclients in cluster mode (#7444)b1a01fda9 Fix command help for unexpected options (#7476)83f55f61a Refactor RM_KeyType() by using macro. (#7486) 显示每条记录中文件修改的具体行数和行体统计使用 --stat 参数就可以显示每条记录的中修改文件的具体行数和行数统计 1234567891011121314151617181920$ git log -2 --statcommit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6. 00-RELEASENOTES | 245 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ src/help.h | 4 +- src/version.h | 2 +- 3 files changed, 248 insertions(+), 3 deletions(-)commit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branch .github/workflows/daily.yml | 6 ++++-- 1 file changed, 4 insertions(+), 2 deletions(-) 显示每条提交记录中文件的增加行数和删除行数使用 --numstat 参数会把 --stat 参数中合并显示的修改行数拆分成增加行数和删除行数 123456789101112131415161718$ git log -2 --numstatcommit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6.245 0 00-RELEASENOTES2 2 src/help.h1 1 src/version.hcommit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branch4 2 .github/workflows/daily.yml 依次罗列各提交记录中每个文件中增加的行数和删除的行数要想达到这个目的需要用到 --prety=tformat: --numstat 参数，这样的显示格式便于统计 12345$ git log -2 --pretty=tformat: --numstat245 0 00-RELEASENOTES2 2 src/help.h1 1 src/version.h4 2 .github/workflows/daily.yml 统计修改的代码行数有了前面的铺垫，想要统一修改的行数就容易了，只要配合 awk 工具就可以完成统计了 12$ $ git log -2 --pretty=tformat: --numstat | awk '&#123;adds += $1; subs += $2; diffs += $1 - $2&#125; END &#123;printf "added lines: %s removed lines: %s, diff lines: %s\n", adds, subs, diffs&#125;'added lines: 252 removed lines: 5, diff lines: 247 还可以统计两个分支相差的代码行数 12$ git log 6.0..unstable --pretty=tformat: --numstat | awk '&#123;adds += $1; subs += $2; diffs += $1 - $2&#125; END &#123;printf "added lines: %s removed lines: %s, diff lines: %s\n", adds, subs, diffs&#125;'added lines: 5 removed lines: 2, diff lines: 3 到这里可以发现前面的知识都可以用上，前面筛选的参数变了，得到的结果就变了，我们可以根据需求来调整不同的参数 总结 git log 就是一部代码库记录的“史书”，对于曾经所做的修改可以做到有史可查 git log 的选项参数可以分为筛选参数和格式参数，筛选参数可以选择记录范围，格式参数可以控制显示样式 统计就是按照一定规律来将数据进行汇总，在进行汇总前需要将数据进行整理，这样汇总的工作才会更加顺利 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 慌慌张张，匆匆忙忙，原来生活就是这样~ 2020-9-7 00:05:18]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>awk</tag>
        <tag>git</tag>
        <tag>log</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中时间戳、时间字符串、时间结构对象之间的相互转化]]></title>
    <url>%2Fblog%2F2020%2F08%2F27%2FPython%E4%B8%AD%E6%97%B6%E9%97%B4%E6%88%B3%E3%80%81%E6%97%B6%E9%97%B4%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E6%97%B6%E9%97%B4%E7%BB%93%E6%9E%84%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言使用 Python 写程序的时候常常要查看中间结果，需要把一些内容记录到日志中，为了不让新产生的日志覆盖之前的日志文件，所以通常使用时间字符串来作为文件名，类似于 2020827_233842.log 这种格式，就是表示程序在 2020-8-27 23:38:42 启动时产生的日志文件。 日志文件名需要一个字符串，但是这个时间字符串不能直接得到，需要将时间戳经过转化才可以，每次用到都要查一次有些浪费时间，总结到一起方便自己今后快速查找。 通过学习总结发现，操作时间和日期常用的模块有 time 和 datetime 这两个，并且 time 模块与 C 语言中的时间处理函数颇为相似，下面来一起看一下吧。 时间的表示形式显示生活中的时间表示形式多种多样，比如15分钟可以说成是1刻钟，半夜12点可以叫做子时，在程序中也有几种常用的表示形式，比如 python 中的时间戳、时间结构对象和时间字符串，分别对应 C 语言中的time_t、struct tm 和 char[]，处理函数的名字也很相近，自己可以扩展学习下，本文只列举 Python 的用法了 时间戳在Python中被实现成一个浮点数，表示从1970年1月1日00:00:00到当前时间所经历的秒数，因为是浮点数所以可以表示不足1秒的时间，而在有些语言中，比如C 语言中使用整数来表示这个值，在 python 中使用 time.time() 函数来获取时间戳： 12345678import timeval = time.time()print(val, type(val))'''输出结果1598769108.8337526 &lt;class 'float'&gt;''' 时间结构对象在 python 中使用 time.struct_time 这个类用来表示时间结构，其实是一个九元组，可以参考C语言中的 struct tm结构，表现形式相同，在 python 中这个九元组中元素依次表达的含义是：4位数年份、1-12月、1-31日、0-23小时、0-59分钟，0-59秒，0-6一周第几日，1-366一年第几日，{-1, 0, 1}夏令时标志。 通过代码我们可以尝试构造如下，不够时间使用时通常是通过函数转化，很少直接构造 time.struct_time 对象。 123456789import timeval = time.struct_time([2020, 8, 30, 14, 45, 30, 6, 243, 0])print(val, type(val))'''输出结果time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=14, tm_min=45, tm_sec=30, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;''' 时间字符串时间字符串是本质上是一种普通的字符串，因为用来表示时间所以感觉上有些不同，实际使用过程时会通过转化函数来生成时间字符串，然后就可以当场普通字符串来使用了，比如记录日志时间，作为文件名等都是常见用法。 12345678import timeval = '2019-08-30 15:04:00'print(val, type(val))'''输出结果2019-08-30 15:04:00 &lt;class 'str'&gt;''' 举例说明仅仅认识了这三种类型还是不够的，还要学习经常使用的转化函数才可以，上面提到的这三种类型一般不会从时间戳到字符串或者从字符戳到时间戳，都是通过时间结构对象来转化的，所以常见的转化是时间戳和时间结构对象的转化、时间结构对象和时间字符串的转化，需要用到的函数展示如下图： 示例代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import time# 生成时间戳t = time.time()print(t, type(t))'''1598775821.840567 &lt;class 'float'&gt;'''# 生成时间结构对象(本地时间)l = time.localtime()print(l, type(l))'''time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=16, tm_min=23, tm_sec=41, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;'''# 时间戳 -&gt; 时间结构对象(本地时间)l = time.localtime(t)print(l, type(l))'''time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=16, tm_min=23, tm_sec=41, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;'''# 生成时间结构对象(格林威治时间)g = time.gmtime()print(g, type(g))'''time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=8, tm_min=23, tm_sec=41, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;'''# 时间戳 -&gt; 时间结构对象(格林威治时间)g = time.gmtime(t)print(g, type(g))'''time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=8, tm_min=23, tm_sec=41, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;'''# 生成时间字符串s = time.strftime("%Y-%m-%d %X")print(s, type(s))'''2020-08-30 16:23:41 &lt;class 'str'&gt;'''# 时间结构对象 -&gt; 时间字符串s = time.strftime("%Y-%m-%d %X",time.localtime())print(s, type(s))'''2020-08-30 16:23:41 &lt;class 'str'&gt;''''================================================================='# 定义时间字符串s = '2022-02-18 09:30:00'# 时间字符串 -&gt; 时间结构对象l = time.strptime(s, '%Y-%m-%d %X')print(l, type(l))'''time.struct_time(tm_year=2022, tm_mon=2, tm_mday=18, tm_hour=9, tm_min=30, tm_sec=0, tm_wday=4, tm_yday=49, tm_isdst=-1) &lt;class 'time.struct_time'&gt;'''# 时间结构对象 -&gt; 时间戳t = time.mktime(l)print(t, type(t))'''1645147800.0 &lt;class 'float'&gt;''''================================================================='# 生成固定格式(%a %b %d %H:%M:%S %Y)时间字符串s = time.asctime(time.localtime())print(s, type(s))'''Sun Aug 30 16:23:41 2020 &lt;class 'str'&gt;'''s = time.ctime(time.time())print(s, type(s))'''Sun Aug 30 16:23:41 2020 &lt;class 'str'&gt;''' 格式化符号将时间转化成字符串表示形式的时候，需要使用格式化符号，为了查找方便整理如下： 格式 含义 %a 本地（locale）简化星期名称 %A 本地完整星期名称 %b 本地简化月份名称 %B 本地完整月份名称 %c 本地相应的日期和时间表示 %d 一个月中的第几天（01 - 31） %H 一天中的第几个小时（24小时制，00 - 23） %I 第几个小时（12小时制，01 - 12） %j 一年中的第几天（001 - 366） %m 月份（01 - 12） %M 分钟数（00 - 59） %p 本地am或者pm的相应符 %S 秒（00 - 59） %U 一年中的星期数。（00 - 53星期天是一个星期的开始。） %w 一个星期中的第几天（0 - 6，0是星期天） %W 和%U基本相同，不同的是%W以星期一为一个星期的开始。 %x 本地相应日期 %X 本地相应时间 %y 去掉世纪的年份（00 - 99） %Y 完整的年份 %Z 时区的名字（如果不存在为空字符） 总结 时间戳和时间字符串的转化，通常要经过时间结构对象作为中间结果。 时间戳也可以通过 time.ctime() 函数直接转化为时间字符串，但格式固定。 常用来表示文件名的时间字符串写法：time.strftime(&quot;%Y%m%d_%H%M%S&quot;, time.localtime()) ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有时选择的意义确实高于努力的结果，认清这一点，学会适当的放下，会让焦躁的生活更美好一点，毕竟全部都坚持真的太累了，有时收益真的不高~ 2020-8-30 21:50:19]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>time</tag>
        <tag>datetime</tag>
        <tag>转化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB调试指北-启动调试或者附加到进程]]></title>
    <url>%2Fblog%2F2020%2F08%2F17%2FGDB%E8%B0%83%E8%AF%95%E6%8C%87%E5%8C%97-%E5%90%AF%E5%8A%A8%E8%B0%83%E8%AF%95%E6%88%96%E8%80%85%E9%99%84%E5%8A%A0%E5%88%B0%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言要想使用 gdb 调试程序，必须让 gdb 程序和被调试程序建立联系，这种联系可以通过程序的可执行文件、core文件或者正在运行的进程来建立，具体调试的时候使用的选项不同，涉及到参数的传递，选项的顺序，多进程启动前的设置等等，接下来可以看一些常见用法。 测试样例首先来写一段简单的但是会自动崩溃的代码，主要是为了展示core文件的调试方法，通过调试崩溃产生的core文件是一种很直接的查找问题的方法，可以帮助我们快速定位到问题的栈帧，进而找到具体的逻辑代码。 代码内容新建文件 examplepro.cpp，编写代码内容如下： 123456789101112131415161718#include &lt;iostream&gt;using namespace std;int main(int argc, char* argv[])&#123; if (argc &gt; 1) cout &lt;&lt; "argv[1] = " &lt;&lt; argv[1] &lt;&lt; endl; int a = 3, b = 4; int c = a + b; cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl; int *p = NULL; *p = c; return 0;&#125; 代码编译1g++ examplepro.cpp -o examplepro -g 运行程序123albert@home-pc:~/WorkSpace/cpp$ ./exampleproc = 7Segmentation fault (core dumped) 我们发现程序在运行之后发生了段错误，这是一种比较常见的BUG，通常由访问无效内存导致，查看程序目录下内容，多了一个叫 core 的文件。 12albert@home-pc:~/WorkSpace/cpp$ lscore examplepro examplepro.cpp 通过这一步你可能看不到这个 core 文件，需要检查两点，第一是编译的时候需要加 -g 选项，第二是使用 ulimit -c unlimited 命令设置core文件占用空间的最小限制，默认大小为0，也就是不产生 core 文件，需要改为 unlimited 才可以，如果你确定产生的 core 文件不会太大，也可以设置一个具体的数值。 使用gdb调试有了上面的程序我们就可以进行调试了，因为已经产生了 core 文件，所以先来调试一下 core 文件，看下程序崩溃的原因。 使用gdb调试core文件启动程序的语法如下，gdb 命令之后跟程序名，然后后面跟着 core 文件的名字： 1gdb examplepro core 具体调试的时候需要换成自己的崩溃的程序名，而core文件大多数是 core.进程id 的形式。 调试过程12345678910111213141516171819202122albert@home-pc:~/WorkSpace/cpp$ gdb examplepro coreGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word"...Reading symbols from examplepro...done.[New LWP 19786]Core was generated by `./examplepro'.Program terminated with signal SIGSEGV, Segmentation fault.#0 0x0000000000400932 in main (argc=1, argv=0x7ffd23cc3a18) at examplepro.cpp:1515 *p = c;(gdb) 从调试信息来看一下就定位到了问题，在代码的第15行发生了段错误，也就是我们刚刚给野指针赋值的代码。 使用gdb直接启动程序这种情况就是调试运行，相当于在 gdb 的监控下启动程序，一旦发生错误，gdb 会给出响应的提示，启动方式很简单，gdb 命令之后直接跟着程序名字就可以了。 1gdb examplepro 调试过程123456789101112131415161718192021222324albert@home-pc:~/WorkSpace/cpp$ gdb exampleproGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word"...Reading symbols from examplepro...done.(gdb) runStarting program: /home/albert/WorkSpace/cpp/exampleproc = 7Program received signal SIGSEGV, Segmentation fault.0x0000000000400932 in main (argc=1, argv=0x7fffffffdd18) at examplepro.cpp:1515 *p = c;(gdb) 这种情况下，启动之后需要输入 run 命令才可以运行程序，这时发现程序又崩溃了。 如果被调试的程序有参数的话，需要将启动的命令进行修改，写成 gdb --args examplepro testparam1，加上 --args 选项，然后将参数罗列在后面就好了，因为看这些声明很麻烦，我们利用之前学过的 -q 选项来屏蔽启动说明，测试如下： 1234567891011albert@home-pc:~/WorkSpace/cpp$ gdb -q --args examplepro NBReading symbols from examplepro...done.(gdb) runStarting program: /home/albert/WorkSpace/cpp/examplepro NBargv[1] = NBc = 7Program received signal SIGSEGV, Segmentation fault.0x0000000000400932 in main (argc=2, argv=0x7fffffffdd08) at examplepro.cpp:1515 *p = c;(gdb) 还有一种写法就是启动 gdb 之后再传参数，具体操作方法如下： 1234567891011albert@home-pc:~/WorkSpace/cpp$ gdb -q exampleproReading symbols from examplepro...done.(gdb) run NBStarting program: /home/albert/WorkSpace/cpp/examplepro NBargv[1] = NBc = 7Program received signal SIGSEGV, Segmentation fault.0x0000000000400932 in main (argc=2, argv=0x7fffffffdd08) at examplepro.cpp:1515 *p = c;(gdb) 这种情况是先启动 gdb，然后在执行 run 命令的时候传递参数。 使用gdb调试正在运行的文件这时需要获得被套是程序的进程id，可以使用 ps、top 或者 pidof 命令来获取进程id，然后通过 attch 的方式附加到进程。 比如查到需要调试的 examplepro 程序进程号是 3598，那么可以直接启动 gdb 附加到这个进程： 1gdb examplepro 3598 也可以先启动 gdb，然后使用 attach 命令附加到进程： 123albert@home-pc:~/WorkSpace/cpp$ gdb -q exampleproReading symbols from examplepro...done.(gdb) attach 3598 如果此时提示进程拒绝被附加通常是权限问题，可以使用所属账号调试，或者可以尝试 sudo 命令。 语法对比常见的调试方式就文中提到的这几种，特整理成表格方便对比和查找： 语法 解释 gdb examlepro 直接 gdb 调试启动 gdb examlepro core.3598 调试崩溃的 core 文件 gdb examlepro 3598gdb -p 3598 附加到正在运行的程序进程上 gdb attach 3598 先启动gdb，后附加到程序上 总结 gdb 不但可以调试 core 文件，还可以调试正在运行的程序，这对于难重现的 bug 来说非常有帮助 在调试正在运行的程序时可以使用 pidof 命令来直接获取被调试程序的进程号 gdb 调试附加的进程的时候要注意权限问题，如果不成功可以尝试 sudo 命令 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 兜兜转转又换了一个住所，匆匆忙忙如蝼蚁般迁徙，路程短了，可选的路却少了。回头看看，一个窝、一段事、一群人而已~ 2020-8-25 00:24:01]]></content>
      <categories>
        <category>GDB</category>
      </categories>
      <tags>
        <tag>GDB</tag>
        <tag>manual</tag>
        <tag>调试</tag>
        <tag>启动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis源码-BFS方式浏览main函数]]></title>
    <url>%2Fblog%2F2020%2F08%2F05%2FRedis%E6%BA%90%E7%A0%81-BFS%E6%96%B9%E5%BC%8F%E6%B5%8F%E8%A7%88main%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言欠下的技术债慢慢还，继续为去年吹过的牛而努力。去年年末的时候意识到自己掌握的知识还不够深入，决定开始看一些开源项目的源码，因为当时 Redis 的兴起，所以瞄准了准备从它下手，之后确实看了一部分内容，比如跳表、网络事件库等等，后来过年就鸽了。今年开始一直熟悉新的业务，比较懒没跟进，最近间歇性踌躇满志又发作了，准备抽时间再捋顺一遍，老规矩，还是从 main() 函数下手。 对于 C/C++ 程序一定是从 main() 函数开头的，这是我们切入的一个点，至于怎么找到 main 函数，每个人有不同的方法，最暴力的方法当然就是全文搜索了，不过较为成熟的项目一般搜索出来都不止一个 main 函数，因为整个项目完整构建下来不止一个程序。 像 redis 这个项目最起码有服务器和客户端两个程序，源码中至少包含了两个 main 函数，再加上一些测试程序，main 函数在源码中会有很多。再比如 Lua 的源代码中包含和解释器和编译器，如果直接搜索至少会找到两个 main 函数。 redis 服务器程序的 main 函数在文件 src/server.c 中，之前好像是在 redis.c 文件中后来改名了，这都不重要，反正你需要从搜索出来的 main 函数中找到一个开始的地方，这个花不了多少时间。 看代码的方式标题中提到了 BFS 方式看代码，而 BFS 指的是广度优先搜索，与之相对应的是 DFS 深度优先搜索，对于不含异步调用的单线程程序来说，执行代码是以深度优先搜索的方式，遇到一个函数就调用进去，在函数中又遇到另一个函数再调用进去，当函数执行完成返回到上一层。 为什么选择 BFS 方式看代码呢？因为这样可以在短时间内更全面的了解代码结构，我们先看第一层，当第一层浏览完成之后再进入到第二层，比如我们先看 main 函数，即使 main 函数调用了很多不认识的函数也不要去管，从名字大概判断一些作用就可以了，不用纠结具体的实现内容，当 main 函数全部看完了再进入到第二层去了解它调用的那些函数。 总之使用 BFS 方式看代码就要有一种“不懂装懂”的态度，不然容易陷入细节，无法整体把握。 Redis 服务器的 main 函数redis 服务器的 main 函数代码量不是很大，总共 200 行左右，我选择了 6.0.6 这个版本 7bf665f125a4771db095c83a7ad6ed46692cd314，因为只是学习源码，没有特殊情况就不更新版本了，保证环境的统一，我先把代码贴一份在这，后面再来慢慢看。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214int main(int argc, char **argv) &#123; struct timeval tv; int j;#ifdef REDIS_TEST if (argc == 3 &amp;&amp; !strcasecmp(argv[1], "test")) &#123; if (!strcasecmp(argv[2], "ziplist")) &#123; return ziplistTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "quicklist")) &#123; quicklistTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "intset")) &#123; return intsetTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "zipmap")) &#123; return zipmapTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "sha1test")) &#123; return sha1Test(argc, argv); &#125; else if (!strcasecmp(argv[2], "util")) &#123; return utilTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "endianconv")) &#123; return endianconvTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "crc64")) &#123; return crc64Test(argc, argv); &#125; else if (!strcasecmp(argv[2], "zmalloc")) &#123; return zmalloc_test(argc, argv); &#125; return -1; /* test not found */ &#125;#endif /* We need to initialize our libraries, and the server configuration. */#ifdef INIT_SETPROCTITLE_REPLACEMENT spt_init(argc, argv);#endif setlocale(LC_COLLATE,""); tzset(); /* Populates 'timezone' global. */ zmalloc_set_oom_handler(redisOutOfMemoryHandler); srand(time(NULL)^getpid()); gettimeofday(&amp;tv,NULL); crc64_init(); uint8_t hashseed[16]; getRandomBytes(hashseed,sizeof(hashseed)); dictSetHashFunctionSeed(hashseed); server.sentinel_mode = checkForSentinelMode(argc,argv); initServerConfig(); ACLInit(); /* The ACL subsystem must be initialized ASAP because the basic networking code and client creation depends on it. */ moduleInitModulesSystem(); tlsInit(); /* Store the executable path and arguments in a safe place in order * to be able to restart the server later. */ server.executable = getAbsolutePath(argv[0]); server.exec_argv = zmalloc(sizeof(char*)*(argc+1)); server.exec_argv[argc] = NULL; for (j = 0; j &lt; argc; j++) server.exec_argv[j] = zstrdup(argv[j]); /* We need to init sentinel right now as parsing the configuration file * in sentinel mode will have the effect of populating the sentinel * data structures with master nodes to monitor. */ if (server.sentinel_mode) &#123; initSentinelConfig(); initSentinel(); &#125; /* Check if we need to start in redis-check-rdb/aof mode. We just execute * the program main. However the program is part of the Redis executable * so that we can easily execute an RDB check on loading errors. */ if (strstr(argv[0],"redis-check-rdb") != NULL) redis_check_rdb_main(argc,argv,NULL); else if (strstr(argv[0],"redis-check-aof") != NULL) redis_check_aof_main(argc,argv); if (argc &gt;= 2) &#123; j = 1; /* First option to parse in argv[] */ sds options = sdsempty(); char *configfile = NULL; /* Handle special options --help and --version */ if (strcmp(argv[1], "-v") == 0 || strcmp(argv[1], "--version") == 0) version(); if (strcmp(argv[1], "--help") == 0 || strcmp(argv[1], "-h") == 0) usage(); if (strcmp(argv[1], "--test-memory") == 0) &#123; if (argc == 3) &#123; memtest(atoi(argv[2]),50); exit(0); &#125; else &#123; fprintf(stderr,"Please specify the amount of memory to test in megabytes.\n"); fprintf(stderr,"Example: ./redis-server --test-memory 4096\n\n"); exit(1); &#125; &#125; /* First argument is the config file name? */ if (argv[j][0] != '-' || argv[j][1] != '-') &#123; configfile = argv[j]; server.configfile = getAbsolutePath(configfile); /* Replace the config file in server.exec_argv with * its absolute path. */ zfree(server.exec_argv[j]); server.exec_argv[j] = zstrdup(server.configfile); j++; &#125; /* All the other options are parsed and conceptually appended to the * configuration file. For instance --port 6380 will generate the * string "port 6380\n" to be parsed after the actual file name * is parsed, if any. */ while(j != argc) &#123; if (argv[j][0] == '-' &amp;&amp; argv[j][1] == '-') &#123; /* Option name */ if (!strcmp(argv[j], "--check-rdb")) &#123; /* Argument has no options, need to skip for parsing. */ j++; continue; &#125; if (sdslen(options)) options = sdscat(options,"\n"); options = sdscat(options,argv[j]+2); options = sdscat(options," "); &#125; else &#123; /* Option argument */ options = sdscatrepr(options,argv[j],strlen(argv[j])); options = sdscat(options," "); &#125; j++; &#125; if (server.sentinel_mode &amp;&amp; configfile &amp;&amp; *configfile == '-') &#123; serverLog(LL_WARNING, "Sentinel config from STDIN not allowed."); serverLog(LL_WARNING, "Sentinel needs config file on disk to save state. Exiting..."); exit(1); &#125; resetServerSaveParams(); loadServerConfig(configfile,options); sdsfree(options); &#125; serverLog(LL_WARNING, "oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo"); serverLog(LL_WARNING, "Redis version=%s, bits=%d, commit=%s, modified=%d, pid=%d, just started", REDIS_VERSION, (sizeof(long) == 8) ? 64 : 32, redisGitSHA1(), strtol(redisGitDirty(),NULL,10) &gt; 0, (int)getpid()); if (argc == 1) &#123; serverLog(LL_WARNING, "Warning: no config file specified, using the default config. In order to specify a config file use %s /path/to/%s.conf", argv[0], server.sentinel_mode ? "sentinel" : "redis"); &#125; else &#123; serverLog(LL_WARNING, "Configuration loaded"); &#125; server.supervised = redisIsSupervised(server.supervised_mode); int background = server.daemonize &amp;&amp; !server.supervised; if (background) daemonize(); initServer(); if (background || server.pidfile) createPidFile(); redisSetProcTitle(argv[0]); redisAsciiArt(); checkTcpBacklogSettings(); if (!server.sentinel_mode) &#123; /* Things not needed when running in Sentinel mode. */ serverLog(LL_WARNING,"Server initialized"); #ifdef __linux__ linuxMemoryWarnings(); #endif moduleLoadFromQueue(); ACLLoadUsersAtStartup(); InitServerLast(); loadDataFromDisk(); if (server.cluster_enabled) &#123; if (verifyClusterConfigWithData() == C_ERR) &#123; serverLog(LL_WARNING, "You can't have keys in a DB different than DB 0 when in " "Cluster mode. Exiting."); exit(1); &#125; &#125; if (server.ipfd_count &gt; 0 || server.tlsfd_count &gt; 0) serverLog(LL_NOTICE,"Ready to accept connections"); if (server.sofd &gt; 0) serverLog(LL_NOTICE,"The server is now ready to accept connections at %s", server.unixsocket); if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123; if (!server.masterhost) &#123; redisCommunicateSystemd("STATUS=Ready to accept connections\n"); redisCommunicateSystemd("READY=1\n"); &#125; else &#123; redisCommunicateSystemd("STATUS=Waiting for MASTER &lt;-&gt; REPLICA sync\n"); &#125; &#125; &#125; else &#123; InitServerLast(); sentinelIsRunning(); if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123; redisCommunicateSystemd("STATUS=Ready to accept connections\n"); redisCommunicateSystemd("READY=1\n"); &#125; &#125; /* Warning the user about suspicious maxmemory setting. */ if (server.maxmemory &gt; 0 &amp;&amp; server.maxmemory &lt; 1024*1024) &#123; serverLog(LL_WARNING,"WARNING: You specified a maxmemory value that is less than 1MB (current value is %llu bytes). Are you sure this is what you really want?", server.maxmemory); &#125; redisSetCpuAffinity(server.server_cpulist); aeMain(server.el); aeDeleteEventLoop(server.el); return 0;&#125; main 函数分段解释函数名及参数12345678int main(int argc, char **argv) &#123; struct timeval tv; int j; //... //... return 0&#125; 这就是一个标准的 main 函数，参数 argc 和 argv 对于一个命令行程序来说可以是重头戏，肯定会拿来做重度解析的，函数开头还定义了 tv 和 j 两个变量，不知道干嘛的，接着往下看吧。 启动测试程序12345678910111213141516171819202122232425#ifdef REDIS_TEST if (argc == 3 &amp;&amp; !strcasecmp(argv[1], "test")) &#123; if (!strcasecmp(argv[2], "ziplist")) &#123; return ziplistTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "quicklist")) &#123; quicklistTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "intset")) &#123; return intsetTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "zipmap")) &#123; return zipmapTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "sha1test")) &#123; return sha1Test(argc, argv); &#125; else if (!strcasecmp(argv[2], "util")) &#123; return utilTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "endianconv")) &#123; return endianconvTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "crc64")) &#123; return crc64Test(argc, argv); &#125; else if (!strcasecmp(argv[2], "zmalloc")) &#123; return zmalloc_test(argc, argv); &#125; return -1; /* test not found */ &#125;#endif 当宏定义 REDIS_TEST 存在，并且参数合适的情况下启动测试程序，argv[0] 肯定是指 redis 服务器喽，那 argv[1] 的值如果是 test，而 argv[2] 的值是 ziplist，那么会调用 ziplist 的测试函数 ziplistTest，如果 argv[2] 的值是 zmalloc，那么会调用测试函数 zmalloc_test，为啥这里函数名命名规范不统一呢？挠头。 程序环境初始化12345678910 /* We need to initialize our libraries, and the server configuration. */#ifdef INIT_SETPROCTITLE_REPLACEMENT spt_init(argc, argv);#endif setlocale(LC_COLLATE,""); tzset(); /* Populates 'timezone' global. */ zmalloc_set_oom_handler(redisOutOfMemoryHandler); srand(time(NULL)^getpid()); gettimeofday(&amp;tv,NULL); crc64_init(); 当 INIT_SETPROCTITLE_REPLACEMENT 这个宏存在的时候，调用 spt_init 函数来为设置程序标题做准备 setlocale() 用来设置地点信息，这一句应该是设置成依赖操作系统的地点信息，比如中国，韩国等等 tzset() 设置时区，这里可能影响到程序运行后，调整时区是否对程序产生影响 srand(time(NULL)^getpid()); 初始化随机种子 gettimeofday(&amp;tv,NULL); 这里用到了函数开头定义的一个变量 tv，用来获取当前时间 crc64_init(); 循环冗余校验初始化，crc 神奇的存在 初始化配置信息123456789uint8_t hashseed[16];getRandomBytes(hashseed,sizeof(hashseed));dictSetHashFunctionSeed(hashseed);server.sentinel_mode = checkForSentinelMode(argc,argv);initServerConfig();ACLInit(); /* The ACL subsystem must be initialized ASAP because the basic networking code and client creation depends on it. */moduleInitModulesSystem();tlsInit(); 定一个16字节的空间用来存放哈希种子 随机获取一段16字节数据作为种子 将刚刚获取的种子数据设置到hash函数中 分析命令行参数，判断是否是哨兵模式 初始化服务器配置 ACL 初始化，不用管它具体是什么，进入下一层时自然会看到 初始化模块系统 tls 初始化，存疑，好奇的话进去看看也可以，好吧，原来是 ssl 那一套，够喝一壶的 存储参数信息123456/* Store the executable path and arguments in a safe place in order * to be able to restart the server later. */server.executable = getAbsolutePath(argv[0]);server.exec_argv = zmalloc(sizeof(char*)*(argc+1));server.exec_argv[argc] = NULL;for (j = 0; j &lt; argc; j++) server.exec_argv[j] = zstrdup(argv[j]); 这一小节比较简单，注释写的也很清楚，就是将命令行参数存储起来，方便重启 redis 服务 根据参数确定启动方式123456789101112131415/* We need to init sentinel right now as parsing the configuration file * in sentinel mode will have the effect of populating the sentinel * data structures with master nodes to monitor. */if (server.sentinel_mode) &#123; initSentinelConfig(); initSentinel();&#125;/* Check if we need to start in redis-check-rdb/aof mode. We just execute * the program main. However the program is part of the Redis executable * so that we can easily execute an RDB check on loading errors. */if (strstr(argv[0],"redis-check-rdb") != NULL) redis_check_rdb_main(argc,argv,NULL);else if (strstr(argv[0],"redis-check-aof") != NULL) redis_check_aof_main(argc,argv); 当启用哨兵模式的时候初始化额外的配置，啥是哨兵，现在还不用知道啊，从字面上来看就好了，反正知道命令行里如果指定了哨兵模式就要额外初始化一点东西。 下面这两个参数有点意思，简单扩展下，rdb 和 aof 是 redis 的两种数据落地的持久化方式，这里有意思的地方是判断了 argv[0] 这个参数，一般 argv[0] 是程序的名字，这个是固定不变的，而 redis 这里将程序名字作为参数来判断，也就是说你把可执行程序换个名字运行，它的行为就会发生变化。 处理并加载命令行参数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465if (argc &gt;= 2) &#123; j = 1; /* First option to parse in argv[] */ sds options = sdsempty(); char *configfile = NULL; /* Handle special options --help and --version */ if (strcmp(argv[1], "-v") == 0 || strcmp(argv[1], "--version") == 0) version(); if (strcmp(argv[1], "--help") == 0 || strcmp(argv[1], "-h") == 0) usage(); if (strcmp(argv[1], "--test-memory") == 0) &#123; if (argc == 3) &#123; memtest(atoi(argv[2]),50); exit(0); &#125; else &#123; fprintf(stderr,"Please specify the amount of memory to test in megabytes.\n"); fprintf(stderr,"Example: ./redis-server --test-memory 4096\n\n"); exit(1); &#125; &#125; /* First argument is the config file name? */ if (argv[j][0] != '-' || argv[j][1] != '-') &#123; configfile = argv[j]; server.configfile = getAbsolutePath(configfile); /* Replace the config file in server.exec_argv with * its absolute path. */ zfree(server.exec_argv[j]); server.exec_argv[j] = zstrdup(server.configfile); j++; &#125; /* All the other options are parsed and conceptually appended to the * configuration file. For instance --port 6380 will generate the * string "port 6380\n" to be parsed after the actual file name * is parsed, if any. */ while(j != argc) &#123; if (argv[j][0] == '-' &amp;&amp; argv[j][1] == '-') &#123; /* Option name */ if (!strcmp(argv[j], "--check-rdb")) &#123; /* Argument has no options, need to skip for parsing. */ j++; continue; &#125; if (sdslen(options)) options = sdscat(options,"\n"); options = sdscat(options,argv[j]+2); options = sdscat(options," "); &#125; else &#123; /* Option argument */ options = sdscatrepr(options,argv[j],strlen(argv[j])); options = sdscat(options," "); &#125; j++; &#125; if (server.sentinel_mode &amp;&amp; configfile &amp;&amp; *configfile == '-') &#123; serverLog(LL_WARNING, "Sentinel config from STDIN not allowed."); serverLog(LL_WARNING, "Sentinel needs config file on disk to save state. Exiting..."); exit(1); &#125; resetServerSaveParams(); loadServerConfig(configfile,options); sdsfree(options);&#125; 这段内容很长，但是核心的内容不多，前一部分是判断特殊参数，用来显示程序使用方法，启动内存测试等等，中间部分是分析命令行参数保存到字符串中，最后几行是读取服务器配置文件，并使用字符串中的参数选项覆盖文件中的部分配置。 打印启动和警告信息1234567891011121314serverLog(LL_WARNING, "oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo");serverLog(LL_WARNING, "Redis version=%s, bits=%d, commit=%s, modified=%d, pid=%d, just started", REDIS_VERSION, (sizeof(long) == 8) ? 64 : 32, redisGitSHA1(), strtol(redisGitDirty(),NULL,10) &gt; 0, (int)getpid());if (argc == 1) &#123; serverLog(LL_WARNING, "Warning: no config file specified, using the default config. In order to specify a config file use %s /path/to/%s.conf", argv[0], server.sentinel_mode ? "sentinel" : "redis");&#125; else &#123; serverLog(LL_WARNING, "Configuration loaded");&#125; 打印 redis 服务器启动信息，比如版本号，pid，警告信息等等，没有实际修改数据。 守护模式和初始化123456789server.supervised = redisIsSupervised(server.supervised_mode);int background = server.daemonize &amp;&amp; !server.supervised;if (background) daemonize();initServer();if (background || server.pidfile) createPidFile();redisSetProcTitle(argv[0]);redisAsciiArt();checkTcpBacklogSettings(); 根据守护进程配置和是否受监督来决定是否作为守护进程，什么是受监督，到现在还不知道，但是本着不懂装懂的方式看代码，可以认为我们懂了，后面自然还会有解释的地方。 接着就调用了 initServer(); 函数，这个初始化函数内容是比较长的，之前版本中很多 mian 函数中的内容都移到了这里面，初始化完成后创建 Pid 文件，设置进程名字，显示 redis 的Logo，检查一些配置，这个 backlog 参数之前面试的时候还被问到过，好奇的话可以提前了解一下。 哨兵模式判断启动并加载持久化数据1234567891011121314151617181920212223242526272829303132333435363738 if (!server.sentinel_mode) &#123; /* Things not needed when running in Sentinel mode. */ serverLog(LL_WARNING,"Server initialized");#ifdef __linux__ linuxMemoryWarnings();#endif moduleLoadFromQueue(); ACLLoadUsersAtStartup(); InitServerLast(); loadDataFromDisk(); if (server.cluster_enabled) &#123; if (verifyClusterConfigWithData() == C_ERR) &#123; serverLog(LL_WARNING, "You can't have keys in a DB different than DB 0 when in " "Cluster mode. Exiting."); exit(1); &#125; &#125; if (server.ipfd_count &gt; 0 || server.tlsfd_count &gt; 0) serverLog(LL_NOTICE,"Ready to accept connections"); if (server.sofd &gt; 0) serverLog(LL_NOTICE,"The server is now ready to accept connections at %s", server.unixsocket); if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123; if (!server.masterhost) &#123; redisCommunicateSystemd("STATUS=Ready to accept connections\n"); redisCommunicateSystemd("READY=1\n"); &#125; else &#123; redisCommunicateSystemd("STATUS=Waiting for MASTER &lt;-&gt; REPLICA sync\n"); &#125; &#125;&#125; else &#123; InitServerLast(); sentinelIsRunning(); if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123; redisCommunicateSystemd("STATUS=Ready to accept connections\n"); redisCommunicateSystemd("READY=1\n"); &#125;&#125; 这段代码看起来像是再做一些通知提醒，其中比较重要的几个函数是moduleLoadFromQueue()、 InitServerLast() 和 loadDataFromDisk() ，第一个函数是加载模块的，第二个函数是在模块加载完成之后才能初始化的部分内容，最后一个是从磁盘加载数据到内存，这也是 redis 支持持久化的必要保证。 打印内存警告并启动事件监听123456789/* Warning the user about suspicious maxmemory setting. */if (server.maxmemory &gt; 0 &amp;&amp; server.maxmemory &lt; 1024*1024) &#123; serverLog(LL_WARNING,"WARNING: You specified a maxmemory value that is less than 1MB (current value is %llu bytes). Are you sure this is what you really want?", server.maxmemory);&#125;redisSetCpuAffinity(server.server_cpulist);aeMain(server.el);aeDeleteEventLoop(server.el);return 0; 看到这段代码我们就来到了 main 函数结尾的部分，redisSetCpuAffinity() 是要做些和 CPU 相关的设置或配置，aeMain() 是主逻辑，对于提供服务的程序来说里面大概率是一个死循环，再满足指定的条件下才会打断退出，而 aeDeleteEventLoop() 就是循环结束时清理事件的操作，到此为止 main 函数就执行完啦。 彩蛋这个 main 函数的代码中有一个神奇的用法不知道大家有没有发现，就是下面这句话： 123serverLog(LL_WARNING, "You can't have keys in a DB different than DB 0 when in " "Cluster mode. Exiting."); 是不是看起来有些奇怪，不用管这个函数的定义是怎样的，可以告诉大家这个函数的定义类似于 printf 函数，只不过在最前面加了一个整型参数，那么调用这个函数时传了几个参数呢？3个？2个？，这个地方很神奇的会把两个字符串拼接到一起，类似于下面的写法： 12serverLog(LL_WARNING, "You can't have keys in a DB different than DB 0 when in Cluster mode. Exiting."); 这样的字符串不仅可以分成两行，实际上可以分成任意行，最后都会拼接在一起，是不是很神奇。 总结 j 这个变量在 redis 的源码中经常出现，应该是作者的行为习惯吧，有些人爱用 i，而这个作者 antirez 爱用 j。 不能一口吃个胖子，看代码也是一样，不能期望一次性把所有的内容都看懂，一段时间后自己的代码都看不懂了，跟别说别人写的了。 redis 代码中频繁使用 server 这个变量，从 main 函数分析中也能看到，这个是个全局变量，代表了整个 redis 服务器程序数据。 不懂装懂或者说不求甚解是熟悉代码整体结构的一项优秀品质，这时候只要看个大概就可以了，真到熟悉细节的时候才是需要钻研的时候。 代码风格完全统一还是比较难实现的，从一个 main 函数中也可以看到，大部分函数是驼峰命名法，还要少量的下划线命名和帕斯卡命名。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 你微笑的模样，提醒着我不要躲藏，坚持原来的方向，哪怕最后遍体鳞伤，困难只会让坚持的人越来越强，共勉~ 2020-8-15 23:48:53]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>源码</tag>
        <tag>C</tag>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB调试指北-启动GDB并查看说明信息]]></title>
    <url>%2Fblog%2F2020%2F08%2F01%2FGDB%E8%B0%83%E8%AF%95%E6%8C%87%E5%8C%97-%E5%90%AF%E5%8A%A8GDB%E5%B9%B6%E6%9F%A5%E7%9C%8B%E8%AF%B4%E6%98%8E%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言学习 gdb 使用是一个漫长的过程，先了解一下 gdb 的启动方式和基础信息的查看方法，能够帮助我们更全面的认知这个工具。gdb 是一个交互式命令行程序，在使用 gdb 调试的时候不断的在命令行内输入命令，然后 gdb 程序就会给出反馈信息，这在很大程序上可以帮助我们调试程序问题。 gdb 版本查看gdb 的安装教程网络上有很多，这里就不提供安装步骤了，可以直接通过命令行，也可以从源码安装，找个教程一步步操作就行了，安装完之后使用 which 命令查看一下程序安装的位置： 12albert@home-pc:~$ which gdb/usr/bin/gdb 确认 gdb 已经安装后我们再看一下程序版本，我用的是 Ubuntu 16.04 版本中匹配的 gdb 程序，版本稍微有些低，据说 9.x 版本中对 Python 支持的非常好，调试的时候查看变量更加方便了，这些神奇的特性我们暂时还用不到，先简单了解下就好： 12345678910111213141516albert@home-pc:~$ gdb --versionGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word".albert@home-pc:~$ gdb 启动直接启动gdb 作为一个程序和其他的程序启动方式是一样的，直接敲入 gdb 命令回车就可以了： 12345678910111213141516albert@home-pc:~$ gdbGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word".(gdb) 看着是不是有些眼熟，这就是刚刚查 gdb 版本时看到的那段文字，只不过这段文字结束时不是返回到命令行，而是显现出了 (gdb) 的字样，我们暂时把它叫做 gdb 命令行，这就是我们与 gdb 程序进行交互的主要途径了。 去掉版本信息启动上面启动 gdb 时出现的这段文字很长，有时候反复调试程序时看到这段文字有点烦，想把它去掉怎么办？非常简单，在启动时加上 -q 参数就可以了。 12albert@home-pc:~$ gdb -q(gdb) 怎么样，这次上面那段文字不见了，直接就进入 gdb 命令行了吧。 gdb 信息查看其实刚刚被我们嫌弃的那段文字，里面记录了不少信息，其中还展示了 show copying、show warranty、show configuration 等多个命令，我们可以简单尝试下这些命令有什么作用。 show copying输入 show copying 命令展示的是一份比较长的版本许可证说明，我省略了中间的部分，如果想看的话可以自己输入命令试一下，GPL v3 的许可证看起来很熟悉吧。 12345678910111213141516albert@home-pc:~$ gdb -q(gdb) show copying GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt; Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed....... Later license versions may give you additional or differentpermissions. However, no additional obligations are imposed on anyauthor or copyright holder as a result of your choosing to follow alater version. show warranty命令 show warranty 输出的内容相比之前的命令就短很多了，是一份免责声明，序号从15开始，接着版本许可证的序号往下写的。 1234567891011121314151617181920212223242526272829303132(gdb) show warranty 15. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BYAPPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHTHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTYOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULARPURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAMIS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OFALL NECESSARY SERVICING, REPAIR OR CORRECTION. 16. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITINGWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYSTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANYGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THEUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OFDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRDPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OFSUCH DAMAGES. 17. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability providedabove cannot be given local legal effect according to their terms,reviewing courts shall apply local law that most closely approximatesan absolute waiver of all civil liability in connection with theProgram, unless a warranty or assumption of liability accompanies acopy of the Program in return for a fee. show configuration最后一个 show configuration 展示的 gdb 的一下配置信息，比如 --with-system-gdbinit=/etc/gdb/gdbinit 在进阶版的 gdb 调试技巧中应该经常用到，先有个印象就行。 12345678910111213141516171819(gdb) show configurationThis GDB was configured as follows: configure --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-auto-load-dir=$debugdir:$datadir/auto-load --with-auto-load-safe-path=$debugdir:$datadir/auto-load --with-expat --with-gdb-datadir=/usr/share/gdb (relocatable) --with-jit-reader-dir=/usr/lib/gdb (relocatable) --without-libunwind-ia64 --with-lzma --with-python=/usr (relocatable) --without-guile --with-separate-debug-dir=/usr/lib/debug (relocatable) --with-system-gdbinit=/etc/gdb/gdbinit --with-babeltrace("Relocatable" means the directory can be moved with the GDB installationtree, and GDB will still find it.)(gdb) apropos其实在 gdb 启动说明中还展示了 apropos 这个命令，可以用这个命令来显示与指定词语相关的命令，比如 apropos print 就是查询所有描述中带有 print 的命令，可以执行测试一下： 1234567891011121314151617(gdb) apropos printagent-printf -- Agent-printf "printf format string"alias -- Define a new command that is an alias of an existing commandbacktrace -- Print backtrace of all stack framesbt -- Print backtrace of all stack framescall -- Call a function in the programcommands -- Set commands to be executed when a breakpoint is hitcompile code -- Compilecompile print -- Evaluate EXPR by using the compiler and print resultdisable pretty-printer -- GDB command to disable the specified pretty-printer...info type-printers -- GDB command to list all registered type-printersinfo vector -- Print the status of the vector unitinspect -- Print value of expression EXPmaintenance agent-printf -- Translate an expression into remote agent bytecode for evaluation and display the bytecodesmaintenance btrace packet-history -- Print the raw branch tracing data---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit--- help查询具体的命令可以使用 help 子命令，比如查看 bt 这个查看调用栈帧的命令就可以使用 help bt，输入后回车可以得到这个命令的描述信息。 123456(gdb) help btPrint backtrace of all stack frames, or innermost COUNT frames.With a negative argument, print outermost -COUNT frames.Use of the 'full' qualifier also prints the values of the local variables.Use of the 'no-filters' qualifier prohibits frame filters from executingon this backtrace. gdb工作作为一款调试利器，可以使用的命令是在是太多了，除了这些还有很多命令等着我们去发现，今天的内容仅仅作为入门必备先简单了解一下。 总结 gdb 是一个交互式的命令行调试工具，通过不断执行命令，展示调试信息帮助我们调试程序 当启动 gdb 这个工具后，命令行会变成 (gdb)的形式，等着我们输入命令开始调试使用 gdb 作为一个强大的 GNU 工具，文档比较全，如果觉的文档枯燥，也可以跟着我的总结来熟悉一下基础用法。 开源环境下软件的版权信息和免责声明写的都比较完整，其中有很多描述值得我们学习。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 而世之奇伟、瑰怪，非常之观，常在于险远，而人之所罕至焉，故非有志者不能至也~ 2020-8-5 23:17:04]]></content>
      <categories>
        <category>GDB</category>
      </categories>
      <tags>
        <tag>GDB</tag>
        <tag>manual</tag>
        <tag>调试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白眼中的docker究竟是个什么东西]]></title>
    <url>%2Fblog%2F2020%2F07%2F28%2F%E5%B0%8F%E7%99%BD%E7%9C%BC%E4%B8%AD%E7%9A%84docker%E7%A9%B6%E7%AB%9F%E6%98%AF%E4%B8%AA%E4%BB%80%E4%B9%88%E4%B8%9C%E8%A5%BF%2F</url>
    <content type="text"><![CDATA[前言Docker，中文名：容器引擎，别名：小鲸鱼，生于2013年3月20日，有一个法裔美籍的母亲 Solumon Hykes，父亲是谁，不得而知。虽然只有7岁，但是在 Hello World 平行宇宙中也算进入了青壮年吧，正在飞速的发展着。 Docker 这个动物选的比较有意思，是一只蓝色的鲸鱼，作为地球上最大的动物，用它来代表容器再合适不过了。不过有谁知道为什么编程技术总是和动物挂钩啊？比如 Linux 的企鹅，Python 的大蛇，Hadoop 的大象等等，有知道的小伙伴还请告知一下。 俗话说的好，“程序不逛动物园，肯定技术有点悬”，经常看到网上有人推荐编程学习方法，先学学基础，然后再看几本儿动物书就可以了，看来这些封面上的动物已经深入人心了。 先把这些动物放到一边，来看看这个 docker 究竟是什么，之前我也不知道它是什么，甚至到了现在我也不能准确的说出它是什么，我只是以一个小白的身份来学习和使用，并且把一些弄懂的知识点总结起来，方便日后查找。 关于docker的疑问如果你之前看到 docker 时会有下面这些疑问，可以跟着文章梳理了解一下，如果你对这些问题的答案早已烂熟于胸，那么可以简单浏览下，帮我挑挑毛病，也是帮助想学习的同学们： docker 最近很火啊，它到底能用来做什么？ docker 和虚拟机好像啊，难道就是轻量虚拟机吗？它们两者还有其他的区别吗？ docker 教程里有 ubuntu 上安装 docker，还有 docker 上安装 linux，什么鬼，到底谁装谁啊？ docker 真的这么牛吗？那开发项目必须得用上它啊，显得高端大气上档次！ docker 宣称构建一次，处处运行，那它应该能跨平台吧？ 作为小白我也是带着这些疑问开始慢慢了解 docker 的，特别是那个 ubuntu 上装 docker，docker 上还能装 ubuntu，都给我整蒙圈了，通过不断学习才渐渐弄清了其中的原因。 疑问探索解答docker 是什么关于 docker 我们来看下常见的介绍： Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器或 Windows 机器上，也可以实现虚拟化，容器是完全使用沙箱机制，相互之间不会有任何接口。 从这一段话中我们可以得到以下几个知识点： 它是一个容器引擎 可以用来打包应用 可以发布到 linux 或者 windows 上 可以实现虚拟化 采用沙箱机制，相互隔离 结合查到的资料来了解一下 docker，首先它是一个容器引擎，引擎这个词经常出现，什么游戏引擎，物理引擎，发动机引擎等等，每种引擎其实就是负责一种核心工作的模块或事物，通过封装来简化工作流程，降低工作难度，而 docker 作为容器引擎的作用当然就是生产容器了。 它的目的就是隔离应用，在隔离空间内部署自己独特的环境，需要了解的是它并不是一项新的技术，而是利用了 Linux 核心中的 cgroups 和 namespace 等资源分脱机制来进行隔离，这种被称为容器的进程独立于宿主和其它的隔离的进程，是很早就存在的技术，只不过经过 docker 封装之后使用起来更加方便了。 再说打包应用，这也是 docker 迅速火起来的一个原因，因为环境部署是在太费时费力了，之前在服务器配置一个应用的运行环境，要安装各种软件，Java/Tomcat/MySQL等等。安装和配置这些东西非常麻烦，并且还存在各种版本，而当我们换另一台同样操作系统的服务器还要再配置一遍，有没有办法这些配置直接拷贝过来呢？其实这就是 docker 要做的事情，将应用与运行环境打包到一起，直接在 docker 中运行一个容器就好了，你所依赖的环境直接就装好了。 前面提到 docker 是利用了 linux 内核的一些特性，那么 windows 可以运行吗？如果你查询早期一点的资料会得到不可以的答案，或者说即使在 windows 上运行 docker，也是在中间加了一层 linux 虚拟机。而如今已经 2020 年了，windows 上可以直接安装 docker for windows 来启动提供 docker 服务，而 docker for mac 也使得 docker 运行在 mac 上不再困难，windows 很早就和 docker 进行了合作，最新的 win10 上启动 docker 甚至可以切换内核为 linux 或者 windows，很神奇吧。 说到这里你应该对 docker 有了一个简单的了解，其中有一点很重要，它和宿主机是共享内核的，这是解答上面很多疑惑的钥匙，至于虚拟化，隔离这些都很容易理解了，而这些概念在虚拟机上常常出现，所以很容易把它俩弄混。 容器与虚拟机自从 docker 出现，容器和虚拟机的对比就没有停过，这些对比常常从启动时间、资源占用、隔离性，操作便利性等方面来进行比较，可以用搜索引擎搜一下，大概就是下面这个样子： 特性 Docker容器 VM虚拟机 启动速速 秒级 分钟级 性能 接近原生 明显弱于原生 硬盘使用 相对较小，可以自由分配 创建时分配，易造成浪费 系统支持量 支持上千个 一般几十个 造成这种差异的原因是什么？还是前面说的 docker 和宿主机是共享内核，而虚拟机是自己创建了一整套系统，虽然隔离性更强，但是也造成了资源的浪费和效率的降低。 一直想找一个例子来形象地对比一下虚拟机和容器，我强行编一个吧，比如你是一个财富自由的人，准备回老家养养牛种种菜，包个鱼塘钓钓鱼，顺便再养一窝小白兔，但是小白兔会吃你种的蔬菜，牛偶尔也会踩到小白兔，这时怎么办，把它们隔开呗。 作为一个钱花不完的人，你准备造几个“小地球”，然后把饲养的动物和种植的植物都放到各自的“地球”中放养，每个小地球都是一个密封的环境，里面有自己的太阳、月亮、空气、河流、山川等等，这种方法当然可以，只是成本有些高，一旦建立了这个小地球，它所占有的资源就定下来了，基本上与大地球隔离，但是它还要依赖大地球，还要建立在地球上，可以类比下虚拟机。 因为建造小地球太费时费力了，所以你改了策略，这次不创建完全密闭的环境了，我直接造个篱笆就可以了，阳光、空气、河流我还是使用大地球的，只是在篱笆里我进行定制，做一些鱼塘、蔬菜大棚等等。由于建造篱笆非常省事，我可以批量生产，有需要了我可以直接拿来几个，放在地上就可以使用了，并且不同动物以及植物之间都有篱笆挡着，不会出现相互影响的问题了，这就有点像容器了。 虽然有些牵强，但是这个例子还是可以帮助我们了解容器和虚拟机的区别，实际上容器与虚拟机并不是对立的关系，有时为了防止容器无限制的占用物理机资源，还会现在物理机上运行虚拟机，然后在虚拟机里运行 docker，他们两者只是不同需求下的不同选择而已。 操作系统和容器到底谁安装谁前面说过 ubuntu 上装 docker，docker 上还能装 ubuntu 这个问题困扰了我好久，实际上 windows 可以装虚拟机，而虚拟机中有可以装 windows 这没什么好奇怪的，这里的 dokcker 指的就是 docker 引擎，或者认为是 docker 服务器。 它们确实可以相互安装，但情况是不同的，首先说 ubuntu 上装 docker，docker 说白了还是一种软件，本质上和你在电脑上装个QQ也没有多大差别，只不过这个软件有点特殊，通过它还能下载、安装别的环境，这么说它看起来有点像应用商店了，不过他虽然提供仓库，但是不仅仅是仓库，本质上它就是一种帮助你搭建环境的软件。 再来看看 docker 上装 ubuntu，还记得之前说过的一个重点吗？ docker 上安装的环境与宿主机共享内核，这就决定了他不能安装完整的系统，不管是 ubuntu、CentOS 还是 RedHat，它所安装的系统仅仅包含运行库和工具链，内核还是用宿主机的，相当于在 docker 中给内核套了一个新的壳子而已。 这下应该清楚了，ubuntu 上装 docker 就是在 ubuntu 上装了一个容器软件， docker 上装 ubuntu 就是在 docker 容器中给宿主内核套上了一个新系统的壳子，使其满足应用软件的环境，配备应用软件可使用的工具链。 这么厉害的容器项目中一定要用吗相信这种问题就是不了解容器也可以回答，肯定不是都要用啊，没有什么技术是只有优点没有缺点吧，凡是技术总有其适合的领域和场景，一味的追求最新的技术不一定符合所要开发的项目。 docker 也没有传说的那么神，它也有着这样那样的问题，比如一直津津乐道的资源伸缩机制，不像虚拟机那样创建时便规定了资源大小，即使不使用也占用着，而 docker 可以直接使用宿主资源，避免了很多浪费。但是反过来想，虚拟机规定了资源的多少，如果不够用了只影响它自己，而 docker 如果一个环境出了问题，它可以把整个物理机的资源耗完，影响机器上的所有服务。 另外，docker 建议只部署无状态的服务，它们不应该承载任何交易数据，所有数据应该保存在数据库服务器中，器随时可以停止、或者删除。当容器被删除掉，容器里的数据将会丢失，即使你要把 docker 数据放在主机来存储，它依然不能保证不丢数据，具体的细节我也在学习，有这方面经验的朋友可以发表一下见解。 docker的跨平台先来看看跨平台的概念： 跨平台概念是软件开发中一个重要的概念，即不依赖于操作系统，也不依赖硬件环境。一个操作系统下开发的应用，放到另一个操作系统下依然可以运行。 首先要弄明白你说的跨平台指的是 docker 跨平台，还是它里边的应用使用 docker 就能跨平台了，从定义来说 docker 这个容器软件应该算是跨平台的，毕竟 Linux、 Windows、 Mac 都有了 docker 的安装包，那么他里面的镜像运行之后的容器能跨平台吗？这还要看具体的应用，docker 没有让一个非跨平台软件变成跨平台软件的能力。 关于这一点你还要牢记前面说的，docker 中的环境与宿主机共享内核，你创建了一个自己编写的exe程序的镜像，拿到安装了 docker 的 ubuntu 机器上显然是无法成功运行的。 基础知识点docker的缺点docker 的卖点是让你摆脱配置环境的困扰，但真实情况是你打包镜像的机器和系统版本，最好和你要运行的目标机器和系统版本一致，另外 docker 环境最好也一样，忽然感觉它没有那么神奇了，这不还是要求版本吗？ 试想一下，你用一个高版本的 docker 服务打包，其中使用了一些新特性，然后放到低版本的 docker 服务下怎么能保证成功运行，机器配置也是一样，之前看到过一个问题 “尝试在具有 4.19 或更高内核的 Linux 系统上运行 docker official centos:6 或 centos:5 容器，当尝试启动它时，你会发现内核和程序不兼容”。 这样看来，docker 只适合在相同环境下批量复制，使得实现自动化测试和持续的集成很方便，但还是有些问题需要注意的： docker 是基于64位系统环境的，32位环境下无法使用 隔离性相比 KVM 之类的虚拟化方案还是有些欠缺 容器随着用户进程的停止而销毁，其中的日志、打点等用户数据不便收集 网络管理相对简单，主要是基于 namespace 隔离 容器的 root 和宿主机 root 等同，这使得容器容易受到攻击 … docker 的组成前面一直在说初学 docker 时的疑问，接下来看看 docker 究竟都包括哪些内容，docker 这个容器引擎实际上是一个客户端/服务器应用程序，客户端负责与守护的服务进程进行对话，而服务进程负责构建、运行和分发 docker 容器。docker 客户端和服务进程可以在同一系统上运行，也可以进行远程访问，通过网络接口使用 RESTful API 进行通信。 使用 docker 时常常要接触三个概念：镜像（Image）、容器（Container）和仓库（Repository）。简单来说镜像就是我们的想要打包的程序机加上程序运行环境，打包出来的一个文件，相当于程序安装包。当镜像运行起来我们就得到了容器，镜像与容器的关系就类似于类和对象的关系。仓库就是存放镜像的地方，与代码的仓库 Github 很像，docker 镜像也有一个常用的仓库叫 Docker Hub，方便人们直接下载镜像来运行。 运行一个镜像Hello world 常常被拿来新知识的入门和开头，今天我们用这个例子来做一下收尾吧，首先你得有 docker 环境，说人话就是你得装了 docker 软件，之前不是一直说 docker 就是一个软件吗，你想用它当然得安装了，这类教程很多，假设你已经安装完了， 我们在一台 CentOS 上操作使用一下 docker，查询下系统版本和 docker 版本： 123456789[root@remote-os ~]# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.7.1908 (Core)Release: 7.7.1908Codename: Core[root@remote-os ~]# docker --versionDocker version 19.03.7, build 7141c199a2[root@remote-os ~]# 首先下载 hello world 镜像，使用 docker image pull hello-world 命令 123456[root@remote-os ~]# docker image pull hello-worldUsing default tag: latestlatest: Pulling from library/hello-worldDigest: sha256:49a1c8800c94df04e9658809b006fd8a686cab8028d33cfba2cc049724254202Status: Image is up to date for hello-world:latestdocker.io/library/hello-world:latest 查看本地镜，使用 docker image ls 命令，发现 hell-world 镜像已经在本地了 12345678[root@remote-os ~]# docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest adafef2e596e 3 weeks ago 73.9MBregistry latest 708bc6af7e5e 6 months ago 25.8MBhello-world latest bf756fb1ae65 7 months ago 13.3kBwurstmeister/zookeeper latest 3f43f72cb283 18 months ago 510MBhyper/docker-registry-web latest 0db5683824d8 3 years ago 599MB[root@remote-os ~]# 下载完成之后直接使用 docker container run hello-world 命令运行就可以了，这个镜像运行打印完直接就退出了 12345678910111213141516171819202122[root@remote-os ~]# docker container run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 如果不想退出，运行的镜像应该是能提供某种服务的，比如前面一直说的 ubuntu，它可以在 docker 中运行起来，直接使用 docker container run -it ubuntu bash 命令就行，这里为什么我们不先下载呢？实际上如果你指定的镜像在本地没有的话会自动下载，不需要手动下载完再运行。 1234567891011[root@remote-os ~]# docker container run -it ubuntu bashroot@0577050677ac:/# cat /etc/issueUbuntu 20.04 LTS \n \lroot@0ecfed0920aa:/# lsbin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin srv sys tmp usr varroot@0ecfed0920aa:/# ll home/total 8drwxr-xr-x 2 root root 4096 Apr 15 11:09 ./drwxr-xr-x 1 root root 4096 Jul 31 16:23 ../root@0ecfed0920aa:/# 可以看到上面的操作，我们又进入了 ubuntu 系统，成功运行了镜像，现在得到了一个容器，可以通过 docker container ls 命令查看，还可以通过 docker container rm [containerID] 命令来删除容器。 总结 docker 软件可以运行在windows、linux 和 mac 上 docker 容器与宿主机共享一个系统内核，如果依赖内核版本的应用最好保证物理机系统版本一致 docker 容器与虚拟机并不是对立的，有时候会放在一起配合使用 docker 有自己的镜像仓库，可以直接下载安装，使用起来相当方便，因为网络原因，如果想快速搭建最好提前准备好镜像文件 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 海纳百川有容乃大，壁立千仞无欲则刚~ 2020-8-1 00:31:35]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB调试指北大全]]></title>
    <url>%2Fblog%2F2020%2F07%2F18%2FGDB%E8%B0%83%E8%AF%95%E6%8C%87%E5%8C%97%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[前言GDB 作为程序调试利器，是一个受通用公共许可证（GPL）保护的自由软件，全称是 GNU Debugger，又常常被称为 GNU symbolic debugger 或者 GNU project debugger，能够帮助开发者调试程序，分析应用程序运行过程。目前支持调试 C、 C++、 D、 Go、 Objective-C、 Fortran、 Java、 OpenCL C、 Pascal、 assembly、 Modula-2、 Ada 等多种编程语言。 GDB能做什么GDB 是调试程序的强大武器，能够帮助开发者找出程序出现BUG的原因，但是不要指望它能自己查问题，它仅仅是一个工具，可以帮助我们查找问题原因，常常被用来做以下事情： 分析程序崩溃的原因 查找程序表现出错误行为的原因 找到一些从源码上难以发现的逻辑错误 GDB调试步骤 使用 g++ 附加 -g 参数编译程序，g++ -g mainpro.cpp -o mainpro 使用 gdb 程序来启动调试我们自己构建的程序，gdb mainpro 使用 run、break、print 等命令调试程序 使用 quit 命令退出程序 GDB调试示例 编写示例程序代码，保存到文件 mainpro.cpp 中 1234567891011#include &lt;iostream&gt;int main()&#123; int a = 110, b = 119, c; c = a + b; std::cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; std::endl; return 0;&#125; 使用 g++ 附加 -g 参数编译程序 123albert@home-pc:~$ g++ -g mainpro.cpp -o mainproalbert@home-pc:~$ lsmainpro mainpro.cpp 使用 gdb 命令来启动调试 1234567891011121314151617albert@home-pc:~$ gdb mainproGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word"...Reading symbols from mainpro...done.(gdb) 使用 break 8 在第8行打断点，使用 run 命令启动程序，使用 print c 打印程序变量 12345678910(gdb) break 8Breakpoint 1 at 0x4008b7: file mainpro.cpp, line 8.(gdb) runStarting program: /home/albert/mainproBreakpoint 1, main () at mainpro.cpp:88 std::cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; std::endl;(gdb) print c$1 = 239(gdb) 使用 quit 命令退出程序 1234567(gdb) quitA debugging session is active. Inferior 1 [process 227] will be killed.Quit anyway? (y or n) yalbert@home-pc:~$ GDB启动参数与命令列表（持续更新）GDB 众多的启动参数和命令提供了强大的调试功能，每一条都可以展开得到很多知识，这些知识的学习是一个持续的过程，短时间无法消化和吸收，所以准备总结一个系列，从最简单的命令开始总结，持续更新学习下去，文章链接不定期更新。 GDB命令 备注 参考文章 gdb、gdb -q GDB启动、查看说明 [GDB调试指北-启动GDB与查看说明] gdb pro、gdb pro 123 启动调试程序、调试正在运行的程序 [GDB调试指北-使用GDB启动调试] directory new-path 查看调试源码 GDB调试指北-查找丢失源码文件 set substitute from-path to-path 查看调试源码 GDB调试指北-查找丢失源码文件 总结 GDB 调试技巧更多的是工具本身的功能，所谓“重剑无锋，大巧不工”，熟练利用这个工具才能发挥最大的威力 知识的学习时一个持续的过程，只有不断的学习和总结才能不断进步，而不要被那些花里胡哨的外表所迷惑 有些知识学着学着就通了，前几天看到 printf 这个函数，很疑惑为什么末尾要加个 f，猜想它是格式化 format 的意思 经过查证果然如此，此时距离第一次在 C 语言中学习 printf 函数已经过去了10年 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 大漠孤烟直，长河落日圆~ 2020-7-18 20:09:58]]></content>
      <categories>
        <category>GDB</category>
      </categories>
      <tags>
        <tag>GDB</tag>
        <tag>manual</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb调试解决找不到源代码的问题]]></title>
    <url>%2Fblog%2F2020%2F07%2F13%2Fgdb%E8%B0%83%E8%AF%95%E8%A7%A3%E5%86%B3%E6%89%BE%E4%B8%8D%E5%88%B0%E6%BA%90%E4%BB%A3%E7%A0%81%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言通过 gdb 启动程序，打好断点运行，开始调试输入 list 命令，结果发现找不到源代码，是不是很糟心，让我们来看看怎么解决这种情况。 先來说明我们要处理的情况，调试程序找不到源代码首先你得有源代码，如果编译完程序你把源代码删了，或者单独把执行程序拷贝到一个没有源代码的机器上，那么拜拜吧您嘞，这种情况不是本文能解决的。 如果你确实有源代码，正常编译源代码并且加入了 -g 选项，编译完之后没有改变源代码位置，那么调试的时候基本都会找到源代码，所以这种情况也不在我们的讨论范围之内。 分析到现在就剩下一种情况，程序编译完成之后我移动了代码的位置。实际工作中可能不会这么无聊，故意改变目录位置让调试程序找不到，但是工作中常常会出现发布机编译完代码要在开发机调试的情况，两台机器上的代码时一样的，但是源代码的位置可能放置的不同，那么在个人开发机上调试这样的程序就会找不到源代码，这也就是我们要解决的问题。 找到源代码的必要性其实在我看来找不到源代码的问题没有那么严重，编译程序里记录了文件名，行号等信息，可以在调试的时候对照着本地的源代码进行“盲调”，这种“盲调”的操作之前可没少干，因为线上环境中没有源代码，我只能一边对照着 gdb 调试输出的行号，一边对照本地的源代码进行程序分析，通过这种方法也解决了不少问题。 虽然看着源代码调试没有那么必要，但是如果可以看见那肯定是更好了，所以本文还是列举出最常见的处理方法，解决一下本来有代码，但因为目录不匹配无法正常调试的问题。 涉及到的命令下面几个命令是 gdb 命令，注意要放到和 gdb 交互命令行输入才可以，别管会不会，先混个脸熟，以后要经常用的： show dir dir 目录 set dir 目录1:目录2:目录3 dir pwd cd 目录 set substitute-path from-path to-path gdb怎样找源代码有时候很奇怪，代码明明就在那里，gdb 你睁开眼睛行不行，为什么你就是找不到呢？其实 gdb 也很苦的好不好，一直帮你查问题还要忍受着你每天的埋怨，到底是什么原因导致 gdb 对眼前的代码视而不见呢？ 其实 gdb 查找代码也要遵循一定的规则，不能每次都全盘扫描吧，那不是得给它累死。举个例子吧，我们在安装一些软件，特别是一些命令行工具的时候，总是有一步要求你把工具或软件所在目录添加到环境变量中，这个变量的名字叫做 Path。 这个 Path 其实就是电脑上众多软件所在目录的集合，当你直接使用软件的程序时，会优先从 Path 这个集合中的目录下去找，成功找到就会直接调用，否则提醒你软件不存在。 源代码目录集合而在 gdb 的调试过程中也有这样一个目录集合，我暂且称它为 SourcePathSet，后面就用这个名字了，因为还要涉及到多种查找目录，请注意区分。 gdb 在查找源码的时候首先在 SourcePathSet 中所包含的目录下找，如果找不到就会提示查找失败了，也就是这篇文章所提到的问题。 源代码文件程序在编译的过程中会记录源文件的名字和路径，这个路径可能是绝对路径，比如 /mnt/d/main.cpp，也可能是相对路径 ../main.cpp ，究竟是哪一种取决于编译时使用的参数。 我们以绝对路径为例，比如文件名为 /mnt/d/main.cpp，我们可以把它拆分成包含路径和不包含路径两种形式：/mnt/d/main.cpp 和 main.cpp，当 SourcePathSet 中包含一个路径叫 /mnt/e时， gdb 搜索的路径包括以下几种： /mnt/d/main.cpp /mnt/e/mnt/d/main.cpp /mnt/e/main.cpp 当源文件是相对路径 ../main.cpp 的时候，那么搜索的路径就变成了下面两个： /mnt/e/../main.cpp /mnt/e/main.cpp 说到这里你可能就明白了，当 gdb 找不到源文件的时候，修改 SourcePathSet 就可以了，把想让它搜索的路径添加到 SourcePathSet，如果符合它的搜索规则，那么就可以找到了。 目录集合的默认值SourcePathSet 在 gdb 启动后开始生效，默认值并不是空，而是 $cdir:$cwd，这又是什么鬼？其中的 $cdir 叫做编译目录，是代码在编译时记录到程序中的，$cwd 表示当前的调试目录，可以通过 cd 命令来修改，要注意这个 cd 修改的是 gdb 会话中的当前目录，不会影响启动 gdb 前文件系统中的目录位置。 假设 $cdir 的值是 /usr，cwd 的值是 /home/albert，我们又添加了 /mnt/e 到 SourcePathSet 中，那么此时 SourcePathSet 的值为 /mnt/e:$cdir:$cwd，如果源文件的是 /mnt/d/main.cpp，查找的目录就会出现以下几种： /mnt/d/main.cpp /mnt/e/mnt/d/main.cpp /usr/mnt/d/main.cpp /home/albert/mnt/d/main.cpp /mnt/e/main.cpp /usr/main.cpp /home/albert/main.cpp 查看各种目录先做一下准备工作，编写一段简单代码，另存文件名为 main.cpp，保存在目录 /mnt/d/cpp 下： 12345678910111213#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 1; int b = 2; int c = a + b; cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl; return 0;&#125; 切换到目录 /mnt/d下， 查看 cpp 目录下文件并使用 g++ 编译，编译完成后将文件 mian.cpp 移动到 /mnt 目录下： 123456789albert@home-pc:/mnt/d$ ls cpp/main.cppalbert@home-pc:/mnt/d$ g++ /mnt/d/cpp/main.cpp -g -o mainalbert@home-pc:/mnt/d$ ls mainmainalbert@home-pc:/mnt/d$ sudo mv cpp/main.cpp ../[sudo] password for albert:albert@home-pc:/mnt/d$ ls ../c d e f main.cpp 启动 gdb 调试程序并打好断点，输入 run 运行发现，断点被触发，但是显示出 No such file or directory.，说明没有找到源代码文件。 123456789albert@home-pc:/mnt/d$ gdb -q mainReading symbols from main...done.(gdb) b 8Breakpoint 1 at 0x4008ac: file /mnt/d/cpp/main.cpp, line 8.(gdb) runStarting program: /mnt/d/mainBreakpoint 1, main () at /mnt/d/cpp/main.cpp:88 /mnt/d/cpp/main.cpp: No such file or directory. 查看源代码文件名和编译目录直接在 gdb 命令行中输入 info source 回车就可以了 12345678(gdb) info sourceCurrent source file is /mnt/d/cpp/main.cppCompilation directory is /mnt/dSource language is c++.Producer is GNU C++ 5.4.0 20160609 -mtune=generic -march=x86-64 -g -fstack-protector-strong.Compiled with DWARF 2 debugging format.Does not include preprocessor macro info.(gdb) 通过这个命令发现，源代码文件是 /mnt/d/cpp/main.cpp，编译目录是 /mnt/d 查看源代码搜索目录在 gdb 环境下输入 show dir 命令就可以显示 SourcePathSet 这个集合中都有哪些目录，由于还没有设置过现在还是默认值 $cdir:$cwd 123(gdb) show dirSource directories searched: $cdir:$cwd(gdb) 查看当前目录查看当前目录就比较简单了，直接 pwd 就搞定了 123(gdb) pwdWorking directory /mnt/d.(gdb) 我们“如愿以偿”的让 gdb 找不到代码了，从现在的环境来看，$cdir 和 $cwd 相同都是 /mnt/d，所以此时搜索的目录只有： /mnt/d/cpp/main.cpp /mnt/d/mnt/d/cpp/main.cpp /mnt/d/main.cpp 而代码被我们移动到了/mnt/main.cpp，gdb 自然就找不到了，后面来看看具体怎么处理这种情况。 具体示例说了这么多原理的东西，如果弄明白了这些很容易找到解决问题的办法，下面写一个完整点的例子，来感受一些具体怎么修复这个问题，新建三个文件 mainpro.cpp、mymath.h、mymath.cpp，目录结构和内容如下： 1234567albert@home-pc:/mnt/d$ tree /mnt/d/mainpro//mnt/d/mainpro/|-- core| `-- mainpro.cpp`-- kit |-- mymath.cpp `-- mymath.h 123456789101112131415//mainpro.cpp#include "../kit/mymath.h"#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 1, b = 2; mymath* m = new mymath(); int c = m-&gt;add(a, b); cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl; return 0;&#125; 123456//mymath.hclass mymath&#123;public: int add(int a, int b);&#125;; 12345678//mymath.cpp#include "mymath.h"int mymath::add(int a, int b)&#123; int c = a + b; return c;&#125; 在 /mnt/d/mainpro 目录下编译代码，然后将代码文件所在目录 core 和 kit 拷贝到 /mnt/e/newpro 目录下，将可执行文件拷贝到 /home/albert 目录下。 123456789101112131415albert@home-pc:/mnt/d/mainpro$ g++ /mnt/d/mainpro/core/mainpro.cpp /mnt/d/mainpro/kit/mymath.cpp -g -o mainproalbert@home-pc:/mnt/d/mainpro$ tree.|-- core| `-- mainpro.cpp|-- kit| |-- mymath.cpp| `-- mymath.h`-- mainpro2 directories, 4 filesalbert@home-pc:/mnt/d/mainpro$ mkdir /mnt/e/newproalbert@home-pc:/mnt/d/mainpro$ sudo mv core/ /mnt/e/newpro/albert@home-pc:/mnt/d/mainpro$ sudo mv kit/ /mnt/e/newpro/albert@home-pc:/mnt/d/mainpro$ mv mainpro /home/albert/ 在 /home/albert 目录下启动 gdb 开始调试，先在 main 函数打断点，查询源文件路径和编译目录等信息； 1234567891011121314151617181920212223albert@home-pc:~$ gdb -q mainproReading symbols from mainpro...done.(gdb) b mainBreakpoint 1 at 0x4008de: file /mnt/d/mainpro/core/mainpro.cpp, line 7.(gdb) runStarting program: /home/albert/mainproBreakpoint 1, main () at /mnt/d/mainpro/core/mainpro.cpp:77 /mnt/d/mainpro/core/mainpro.cpp: No such file or directory.(gdb) info sourceCurrent source file is /mnt/d/mainpro/core/mainpro.cppCompilation directory is /mnt/d/mainproSource language is c++.Producer is GNU C++ 5.4.0 20160609 -mtune=generic -march=x86-64 -g -fstack-protector-strong.Compiled with DWARF 2 debugging format.Does not include preprocessor macro info.(gdb) list2 in /mnt/d/mainpro/core/mainpro.cpp(gdb) pwdWorking directory /home/albert.(gdb) show dirSource directories searched: $cdir:$cwd(gdb) 果然找不到源代码了，从上面的调试信息来看，可以得到以下信息： 源代码文件为 /mnt/d/mainpro/core/mainpro.cpp 程序编译目录为 /mnt/d/mainpro 当前目录为 /home/albert 而源代码查找列表中只有 $cdir:$cwd，说明只包含 /mnt/d/mainpro 和 /home/albert，那么查找的目录有： /mnt/d/mainpro/core/mainpro.cpp /mnt/d/mainpro/mnt/d/mainpro/core/mainpro.cpp /home/albert/mnt/d/mainpro/core/mainpro.cpp /mnt/d/mainpro/mainpro.cpp /home/albert/mainpro.cpp 这些目录显然找不到源代码文件了，因为文件已经被我移动到 /mnt/e/newpro/ 目录下了，也就是 /mnt/e/newpro/core/mainpro.cpp，下面来尝试一些解决方法。 使用 dir 命令解决刚才说了源代码查找集合 SourcePathSet 中只有 $cdir:$cwd，我们可以自己加一个嘛，比如像下面这样： 1234567891011121314(gdb) dir /mnt/e/newpro/core/Source directories searched: /mnt/e/newpro/core:$cdir:$cwd(gdb) list2 #include &lt;iostream&gt;3 using namespace std;45 int main()6 &#123;7 int a = 1, b = 2;8 mymath* m = new mymath();910 int c = m-&gt;add(a, b);11 cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;(gdb) 这样就可以找到了，我们接着在 add 函数上下个断点，继续执行 12345678910111213141516(gdb) b mymath::addBreakpoint 2 at 0x4009a6: file /mnt/d/mainpro/kit/mymath.cpp, line 6.(gdb) cContinuing.Breakpoint 2, mymath::add (this=0x613c20, a=1, b=2) at /mnt/d/mainpro/kit/mymath.cpp:66 /mnt/d/mainpro/kit/mymath.cpp: No such file or directory.(gdb) list1 in /mnt/d/mainpro/kit/mymath.cpp(gdb) info sourceCurrent source file is /mnt/d/mainpro/kit/mymath.cppSource language is c++.Producer is GNU C++ 5.4.0 20160609 -mtune=generic -march=x86-64 -g -fstack-protector-strong.Compiled with DWARF 2 debugging format.Does not include preprocessor macro info.(gdb) 结果发现又找不到文件 /mnt/d/mainpro/kit/mymath.cpp 了，因为和之前不是一个文件，这个文件在其他的目录下，所以还要使用 dir 命令，把新的目录加到源代码查找集合 SourcePathSet 中： 1234567891011121314(gdb) dir /mnt/e/newpro/kit/Source directories searched: /mnt/e/newpro/kit:/mnt/e/newpro/core:$cdir:$cwd(gdb) list1 #include "../kit/mymath.h"2 #include &lt;iostream&gt;3 using namespace std;45 int main()6 &#123;7 int a = 1, b = 2;8 mymath* m = new mymath();910 int c = m-&gt;add(a, b);(gdb) 这次又能成功找到了，可是如果有好多个文件要调试，难道要把所有的目录都加进去吗？其实可以有简便方法的，在启动 gdb的时候可以指定搜索的源代码路径，这些路径都会被加到到源代码查找集合 SourcePathSet 中，具体操作如下，先退出gdb，然后重新加参数启动如下： 12345albert@home-pc:~$ gdb -q mainpro `find /mnt/e/newpro/ -type d -printf '-d %p '`Reading symbols from mainpro...done.(gdb) show dirSource directories searched: /mnt/e/newpro/kit:/mnt/e/newpro/core:/mnt/e/newpro:$cdir:$cwd(gdb) 其实这条命令的本来面目是 gdb -q mainpro -d xxxxx，只不过这组合了 find 命令以后使用起来更加方便了，可以把指定目录下的子目录全都添加到参数中 使用 cd 命令解决如果是临时调试倒是用不到上面设置启动参数那么麻烦，因为变量 $cwd 也在搜索集合中，既然在编译时记录的源文件被改变了位置，那么我们调整我们的当前位置，让代码出现搜索路径中，还是上面的这个例子： 123456789101112131415161718192021222324252627albert@home-pc:~$ pwd/home/albertalbert@home-pc:~$ gdb -q mainproReading symbols from mainpro...done.(gdb) b mainBreakpoint 1 at 0x4008de: file /mnt/d/mainpro/core/mainpro.cpp, line 7.(gdb) rStarting program: /home/albert/mainproBreakpoint 1, main () at /mnt/d/mainpro/core/mainpro.cpp:77 /mnt/d/mainpro/core/mainpro.cpp: No such file or directory.(gdb) list2 in /mnt/d/mainpro/core/mainpro.cpp(gdb) cd /mnt/e/newpro/core/Working directory /mnt/e/newpro/core.(gdb) list2 #include &lt;iostream&gt;3 using namespace std;45 int main()6 &#123;7 int a = 1, b = 2;8 mymath* m = new mymath();910 int c = m-&gt;add(a, b);11 cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;(gdb) 上面的操作通过 cd /mnt/e/newpro/core/ 命令直接进入了源代码目录，当然就找到了，但是这还是会有点问题，当碰到需要调试好几个文件的时候就需要使用 cd 命令跳来跳去，要想一劳永逸，请看下面这个方法。 使用 set substitute-path 命令解决我们移动源代码的时候往往会整个目录移动，或者说开发机和发布机上面的代码文件组织结构是一样，只是所在的磁盘位置是不一样的，所以如果可以设置用一个路径替换原代码文件的路径就好了， set substitute-path from-path to-path 这个命令就可以达到想要的目的，这个命令还可以简写成 set substitute from-path to-path，比如还是前面的例子，源代码从 /mnt/d/mainrpo 目录整体移动到了 /mnt/e/newpro 目录，调试时找不到源代码可以使用 set substitute /mnt/d/mainrpo /mnt/e/newpro 命令来指定替换目录，这样就可以找到源代码啦，下面来测试一下： 1234567891011121314151617181920212223242526272829303132333435363738albert@home-pc:~$ gdb -q mainproReading symbols from mainpro...done.(gdb) set substitute-path /mnt/d/mainrpo /mnt/e/newpro(gdb) b mainBreakpoint 1 at 0x4008de: file /mnt/d/mainpro/core/mainpro.cpp, line 7.(gdb) runStarting program: /home/albert/mainproBreakpoint 1, main () at /mnt/d/mainpro/core/mainpro.cpp:77 /mnt/d/mainpro/core/mainpro.cpp: No such file or directory.(gdb) cd /mnt/e/newpro/Working directory /mnt/e/newpro.(gdb) list2 /mnt/d/mainpro/core/mainpro.cpp: No such file or directory.(gdb) set substitute-path /mnt/d/mainpro /mnt/e/newpro(gdb) list 01 #include "../kit/mymath.h"2 #include &lt;iostream&gt;3 using namespace std;45 int main()6 &#123;7 int a = 1, b = 2;8 mymath* m = new mymath();910 int c = m-&gt;add(a, b);(gdb) info sourceCurrent source file is /mnt/d/mainpro/core/mainpro.cppCompilation directory is /mnt/d/mainproLocated in /mnt/e/newpro/core/mainpro.cppContains 14 lines.Source language is c++.Producer is GNU C++ 5.4.0 20160609 -mtune=generic -march=x86-64 -g -fstack-protector-strong.Compiled with DWARF 2 debugging format.Does not include preprocessor macro info.(gdb) pwdWorking directory /home/albert.(gdb) 通过调试信息 Located in /mnt/e/newpro/core/mainpro.cpp 可以看到，果然在新的位置找到了源代码。 总结 调试的时候找不到源码有多种解决方法，需要根据实际情况选择最合适的解决方案。 编译时使用绝对路径时，推荐使用 set substitute-path from-path to-path 的方式。 编译时使用相对路径时，使用 set substitute from-path to-path 或者 dir new-path 都可以。 对于临时查找一个问题，单独调试某一个文件时使用 cd 命令就可以搞定了。 直接在 gdb 环境输入 dir 命令回车确认，可以重置 dir 目录 或者 set dir 目录 命令修改过的源代码搜索目录集合。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当人的才华不足以撑起个人的欲望时就会感到焦虑，当面对不利的情况和事件却又无力改变时就会感到愤怒，而弱肉强食一直都是生活的本质，惟有强大才是解决这一切负面情绪的良药~ 2020-7-18 15:36:53]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>source</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>dir</tag>
        <tag>path</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell脚本中获取命令运行结果、特殊变量使用、条件判断等常用操作]]></title>
    <url>%2Fblog%2F2020%2F07%2F07%2FShell%E8%84%9A%E6%9C%AC%E4%B8%AD%E8%8E%B7%E5%8F%96%E5%91%BD%E4%BB%A4%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C%E3%80%81%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD%E7%AD%89%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言最近在处理一个 Python 局部变量的作用域问题时发现有些奇怪，想起了之前常写的 Lua 脚本，于是想写个函数测试一下，结果发现短短的十几行代码出现了多个错误，这可是我写了近三年的代码啊，才放下半年就记不清了，所以知识这个东西还是要不断“温故”，今天要总结的 Shell 脚本命令也是，基本属于一看就会，一写不对的状态，所以还是要把常用的操作总结到一起，方便查找和复习。 获取命令执行结果脚本中常常要获取一些命令的执行结果，比如当前目录 pwd、当前时间 date 等等，如果在控制台时直接输入后回车就能看到结果，但是在 Shell 脚本中却不能这样做，常见的有以下两种方式。 使用反引号 `command `来执行命令反引号就是键盘上 Tab 键上方的那个按键对应的符号，常写 Markdown 的小伙伴知道这个符号就是包裹代码块的那个符号，在 Shell 脚本中被用来执行命令得到结果，举个简单的例子 1234567#!/bin/bashresult=dateecho $resultresult=`date`echo $result 将上述命令保存到文件 cmd.sh 中运行 ./cmd.sh 得到结果： 123$ ./cmd.shdateTue Jul 7 23:48:03 CST 2020 从运行结果可以看出，如果不加反引号，我们常用的这些命令会被当成普通的字符串处理。 使用括号组合 $(command) 来执行命令除了上面的反引号，使用美元符和小括号组合也可以在 Shell 脚本中运行命令，使用同样的例子测试 1234567#!/bin/bashresult=`date`echo $resultresult=$(date)echo $result 保存到文件 cmd.sh 中运行 ./cmd.sh 得到结果： 123$ ./cmd.shTue Jul 7 23:53:27 CST 2020Tue Jul 7 23:53:27 CST 2020 对比可以看出两种方式在这个命令下运行结果是一样的。 两种方式的区别虽然上述两种方式都可以在 Shell 脚本中得到命令运行的结果，但是有一点是不一样的，那就是反引号执行命令不支持嵌套，不能实现反引号中再出现反引号，而 $(command)的方式是支持嵌套的，关于这一点可以看下面这个例子。 12$ echo $(ls $(pwd))cmd.sh 分析一下这个命令 echo $(ls $(pwd))，最里面的命令是 $(pwd)先执行得到当前目录，然后执行命令 $(ls 当前目录)得到目录下的文件，再通过 echo 命令把这个结果输出，就得到了 cmd.sh 这个文件名，因为我这个目录下只有这一个文件。 系统的命令使用反引号的方式改写就不生效了。 12$ echo `ls `pwd``cmd.shpwd 我们还是仿照上面嵌套来写，但是 echo 后面的内容其实被分成了3部分，一个ls命令，一个pwd字符串、一个空命令，这样就能解释运行结果 cmd.shpwd了。 对照着结果我们就可以知道了， $(command)的方式更加强大，可以支持命令的嵌套，应用更广泛一点，而反引号的方式跟多出现在之前的脚本中。 特殊变量使用从学习语言的第一天起就记住了变量名中只能有数字、字母、下划线，并且数字不能打头（Shell中只能字母开头），但是在 Shell 脚本中有一些特殊的变量，包含各种奇奇怪怪的符号。 $0 $1 $2 …这些是运行 Shell 脚本时传递给脚本的命令行参数。命令行参数用 $n 表示，$0表示当前脚本的文件名，$1 表示第一个参数，$2 表示第二个参数，依次类推，可以类比 Windows 下的 %0、%1、%2… $$当前 Shell 脚本的进程ID。如果在命令行执行得到的是当前 bash 的进程ID，如果放到脚本中，得到的是脚本的进程ID。 $?可以获取上一个命令执行后的返回结果。 $传递给脚本的命令行参数的个数。 $*传递给脚本的命令行参数的所有参数。 $@传递给脚本的命令行参数的所有参数，与 $* 稍有不同。 测试写个脚本测试一下，新建 cmdargs.sh 文件，编写下面代码： 123456789#!/bin/bashecho \$0 is $0echo \$1 is $1echo \$2 is $2echo \$$ is $$echo \$# is $#echo \$* is $*echo \$@ is $@ 先执行 ./cmdargs.sh 脚本， 然后输出 $? 脚本的退出状态，运行结果如下: 1234567891011$ ./cmdargs.sh I love my daughter$0 is ./cmdargs.sh$1 is I$2 is love$$ is 197$# is 4$* is I love my daughter$@ is I love my daughter$ echo $?0 $* 和 $@ 的区别对照这个源码和输出结果，这些特殊变量应该可以分清楚了，其中 $* 和 $@ 都是把所有内容都列出来了，但它俩还是有点区别的，当这两个变量都被双引号包裹时，通过 for 循环会得到不同结果，写个脚本 cmdargs2.sh 试一下 12345678910111213#!/bin/bashecho "test for \"\$*\""for var in "$*"do echo "$var"doneecho "test for \"\$@\""for var in "$@"do echo "$var"done 运行结果如下, &quot;$*&quot;把所有的参数当成了一个整体，而 &quot;$@&quot; 把各个参数都拆分开了，可以通过循环依次打印出来。 12345678$ ./cmdargs2.sh I love my daughtertest for "$*"I love my daughtertest for "$@"Ilovemydaughter 条件判断说起条件判断第一反应就是 if 了，在 Shell 脚本中也有 if 语句，同样是条件判断的中坚力量，先来看看 if 语句的写法： if 语句格式1234567if [ -d $filename ]; then echo "this is a directory."elif [ -a $filename ]; then echo "the file is exist."else echo "the file is not exist."fi 直接提供一个最复杂的情况，如果不需要 elif 或者 else 分支，直接删掉就可以，但是 if、then、fi 这些都是必须的，并且中括号里面的表达式与中括号之间都要有空格，如果挨着写会报错的。 中括号 [] 的作用一度认为 if 条件语句就是这样写，中括号 [] 应该是语法的一部分，但是查询后发现这居然是一个命令，和 ls，pwd 一样是一个可以执行命令，放在 if 条件判断时基本等同于 test 命令。 1234$ which [/usr/bin/[$ which test/usr/bin/test 看着这个查询结果感觉神奇吧，此外还有一个 [[]] 双中括号的操作，这个就不是命令了，而是 Shell 的一个关键字，比 [] 要强大的多。 具体条件Shell 脚本最常见的条件就是文件判断，数字判断和字符串判断了，接下来列举一下这些判断的常见写法。 文件判断 命令 含义 -a $filename 文件存在时为真 -d $filename 文件名对应的是目录时为真 -s $filename 文件非空时为真 -r $filename 文件可读时为真 -w $filename 文件可写时为真 -x $filename 文件可执行时为真 数字判断 命令 含义 n1 -eq n2 n1等于n2时为真 n1 -ne n2 n1不等n2时为真 n1 -gt n2 n1大于n2时为真 n1 -lt n2 n1小于n2时为真 n1 -ge n2 n1大于等于n2时为真 n1 -le n2 n1小于等于n2时为真 字符串判断 命令 含义 -n str1 str1字符串不为空串时值为真 -z str1 str1字符串为空串时值为真 str1 == str2 str1与str2相等时为真 str1 != str2 str1与str2不等时为真 str1 &gt; str2 按字典序str1排在str2后面时为真 str1 &lt; str2 按字典序str1排在str2前面时为真 数字判断特殊写法 命令 含义 ((&quot;$n1&quot; == &quot;$n2&quot;)) n1等于n2时为真 ((&quot;$n1&quot; != &quot;$n2&quot;)) n1不等n2时为真 ((&quot;$n1&quot; &gt; &quot;$n2&quot;)) n1大于n2时为真 ((&quot;$n1&quot; &lt; &quot;$n2&quot;)) n1小于n2时为真 ((&quot;$n1&quot; &gt;= &quot;$n2&quot;)) n1大于等于n2时为真 ((&quot;$n1&quot; &lt;= &quot;$n2&quot;)) n1小于等于n2时为真 逻辑关系运算符 命令 含义 -a 与操作，用于[] 和 test 操作符 -o 或操作，用于[] 和 test 操作符 ！ 取反操作，用于[] 、 test 操作符 和 [[]] 关键字 &amp;&amp; 与操作，用于[[]] 关键字 \ \ 或操作，用于[[]] 关键字 这些逻辑写法千奇百怪的，写两个例子就慢慢就慢慢理解了，比如判断一个字符串不为空，并且这个字符串指定的目录还存在就可以写成 123if [ -n "$1" -a -d "$1" ]; then echo $1 directory is existfi 使用双小括号来比较数值变量，写在双小括号中的变量前面可以不加 $ 符号，还有诸多特权等着你去发现 12345num1=$1num2=$2if (( num1 &gt; num2)); then echo num1 \&gt; num2fi 总之在学习这些条件比较的时候踩了不少坑，有很多情况都没有注意到，不过慢慢也适应了这种语法，但还是免不了会出现一个小问题，这里提供一个 Shell 语法检查的在线网站 《shellcheck》，将要检查的脚本放到页面上检测，会给出详细的错误信息，当然也有命令版本，可以自己到对应的 github 页面上下载哦~ 总结 Shell 脚本中获取命令的执行结果，可以通过反引号`command`，或者小括号 $(command) 的方式得到 Shell 脚本中有一系列 $ 开头的变量，用好他们是脚本和函数传递参数的关键 Shell 脚本中的条件判断对于初学者来说很头大，有许多注意的点要记住，判断形式也多种多样 脚本中有单引号、双引号、反引号，简单来记就是单引号中原样输出，双引号中变量求值后输出，反引号中只能写需要执行的命令 脚本中还要中括号、双中括号、小括号、双小括号等，上面都提到过，可以自己练习下，具体的细节怕是要单独总结了，放到一起太多了 脚本的中的分号起到语句结束的作用，如果有换行就不需要分号了，比如 if 条件后面的 then 如果换行，那么 then 前面的分号可以省略 再记住一个坑，脚本赋值等号两端不能有空格，脚本判断等号两端必须有空格 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有些局，选择不入便立于不败之地，选择介入，即使曾身经百战，也恐难全身而退，更不要谈什么收益了~ 2020-7-11 00:30:00]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>if</tag>
        <tag>Shell</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[根证书的应用和信任基础]]></title>
    <url>%2Fblog%2F2020%2F07%2F06%2F%E6%A0%B9%E8%AF%81%E4%B9%A6%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E4%BF%A1%E4%BB%BB%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[前言人生在世总要相信点什么，信亲人、信朋友、信你面前的陌生人，即便这些你都不信，也要信自己吧，假如连自己都不信了，那在地球上恐怕很难生存了。 我们把钱存入银行，因为我们相信当我们需要用钱时可以通过银行卡把钱取出来；我们拿着车票准时来到候车大厅，因为我们相信除非特殊情况，我们买的那趟车绝不会提前丢下我们而去；遇到纠纷我们会选择打官司，因为我们相信法官最后会给我们公正的判决结果。 生活中我们信任自己的经验，信任自己的亲人朋友，并依赖这些信任来做许多事情，这些信任是我们正常生活学习的前提，缺了这些我们将寸步难行。而在网络中我们同样需要信任，这些信任是筑造网络社会的基石。 有些信任是有条件的，比如银行贷款时不能通过空口白话就借来白花花的银子，而抵押物是贷款银行为了相信你附加的筹码；有些信任是无条件的，比如前面一篇总结 《认证、HTTPS、证书的基本含义》中提到的根证书，我们必须无条件信任，否则我们将置身于网络猜疑的海洋之中，无法正常利用网络带给我们的便利。 信任链我们常听说 HTTPS 更加安全，它是通过非对称加密技术，让我们可以在不确定的网络环境中可以确认对方的身份，安全传输密钥，但这一切都是有前提的，你得相信你的操作环境是安全的，你没有被人监控，你的电脑没有被人控制，你的数据没有被人篡改，抛开环境谈安全都是耍流氓~ 好了，我们可以回顾一下，要想验证一个网站的身份，我们需要得到网站的公钥，如果可以解开网站拿私钥加密的消息，我们就证明了网站的身份，而网站的公钥不能由网站直接发给我们，需要找权威机构给它证明，相当于找了个担保人。 权威机构会用自己的私钥把网站的信息和公钥合在一起生成证书，当我们访问网站时首先得到这个证书，然后用权威机构的公钥来解开证书内容，得到网站的信息和网站的公钥，然后进行信息比对和公钥解密来认证身份，这时我们需要思考，权威机构的公钥从哪里来？ 权威机构可以找更加权威的机构按照相同的方式给它做证书，这样一环一环的就走下去，形成了信任链，然后就无穷无尽了，一个权威机构给另一个权威机构证明，我可以玩到天荒地老，到底什么时候是个头啊，其实我们可以人为的确定一个，那就是根证书，他不需要找别的人给它证明，如果一个网站证书最终信任链顶端是有效的根证书，那么网站身份被确认。 根证书接下来看看根证书在哪呢？它内置在我们的浏览器（Firefox）和操作系统中，我们需要无条件的信任，从理论上讲没办法判断根证书的真假，它是自证清白的。这里需要注意，根证书不止有一个，它可以有很多个，“根”只是说明信任链到此为止，整条信任链上的节点都是“可信”的。所以说还是不要随意安装根证书，因为有了它就可以在你的电脑为所欲为。 说到这里有些人会想，根证书内置在操作系统和浏览器（Firefox）中，如果我下载一个被恶意修改的浏览器岂不是危险了，这种担心是有必要的，所以请尽量在正规网站下载，可是怎么证明哪些网站是正规网站呢？可以使用系统自带的根证书判断。 如果我的系统是盗版系统，根证书被人改过，那不是更危险了，事实确实如此，算了吧，还是暂时不相信网络了，我去买个系统光盘吧（不知道现在还有没有人用光盘装系统），可是卖你光盘的人能保证光盘的内容不被篡改吗？你说那不能，因为他是微软高级经理的小舅子，应该不能卖盗版碟吧。 即使光盘不是盗版的，但是制作光盘的内容有没有人动过手脚呢？这些我们还是无法确认，我们能做的只是尽可能的在正规渠道购买正版系统，这种情况遇到证书被篡改的情况很小，然后就无条件相信这个系统了，这就是我文章开头说的，我么总要信点什么，试想如果盖茨在 Windows 操作系统的证书中留有后门，你又能做些什么呢，所以还是不要纠结了，既然用就在正常使用的前提下信任它。 应用及分析说是应用，实际上我只是想吐槽而已，在吐槽之前我们应该了解，证书可以跟各大证书机构（也就是各种CA）来买，也可以自己生成，可能有人会想了，自己生成挺好啊，不用花钱谁还买啊？可是刚刚说过了，跟CA买的证书都是操作系统内置证书认证过的，自己生成的证书操作系统和浏览器可不认，那怎么办呢？ 干脆自己安装个根证书，自己给自己认证得了，用户岂是你想让安装就安装的，别说，还真是这样，只要你说的情况很危急，必须安装，那么大多数的小白用户是会自动安装的，这时你想到了谁？ 不知道大家想到了谁，反正我是想到了建行网银证书和令人“可歌可气”的12306，接下来简单扒一扒他们两个的故事… 建行网银证书最先接触的证书就是建行网银证书，我的第一代网银盾用了将近10年，去年才刚刚升级成2代，可以说真的是太稳定了，不知道做网银的产品经理是谁，你简直就是程序员的福音，在2020年的今天打开建行的官网，首页倒是好看了许多，但是有些内容，比如证书安装、U盾介绍的页面还是原来丑丑的样子。 之前办理U盾时还花钱，根本都不懂啊，使用U盾必须装证书啊，不装就不安全啊，现在回想起来，和我说这话的人可能根本就不懂什么是安全，什么是不安全，反正装就是了，每次付款都要启动建行验证程序，这也是我手动安装过的次数最多的证书，是它开启了我网上购物的里程。 已经2020年了，打开建行的官网依旧提示我正在使用不安全的连接，使用网银依旧让我自己安装证书，可能作为一个大银行，官方网站迟迟不启用 HTTPS，使用网银盾坚持要用户自己安装证书，应该不仅仅是证书价格的问题，可能还有什么其他的原因。 神奇的12306毕业后直接在12306买票的次数就少了，现在一般使用 APP 来解决，前一阵发现12306居然不要求自己安装证书了，仔细一查原来从2017年开始，12306官网就购买了 DigiCert Inc 认证的证书，确实是一个进步的boy，终于舍得花点钱买证书了，作为一个巨大型的网站，它方便了人们购票的方式，是值得歌颂和称赞的，但是每次购票前还要安装烦人的证书，确实挺令人生气的。 原来“根证书”3个红字显示在页面正中间，确实起到了提醒的作用，挺扎眼的，不过那已经一去不复返了，我再放个图，大家一起回顾一下。 ESET SSL Filter CA最后放一个例子，让你感受下根证书的威力，ESET是总部位于斯洛伐克布拉迪斯拉发的一家世界知名的电脑安全软件公司，主要做杀毒软件，前不久复习 HTTPS 知识的时候发现，我访问各大网站的证书全都变成了 ESET SSL Filter CA，这是什么鬼，难道 ESET SSL Filter CA 是个特别大的证书机构？ 当时还没有意识到是杀毒软件的证书，以为大家都是买的这家证书，后来发现不太对，百度、谷歌、GitHub、Stack Overflow，怎么都是一样的证书，继续深究才发现被“窃听”了。 我们知道使用 HTTPS 通信因为使用了非对称加密，没有私钥是无法窃听加密内容的，但是这款杀毒软件做到了，它有一个HTTPS 内容过滤的功能，做了我的电脑和各大网站的中间人，按理说 HTTPS 是可以检测出中间人的，但是这款软件在电脑中安装了根证书，所有浏览器认为它是合法的，理论上可以窃听你所有内容，甚至为所欲为。 总结 信任不仅是人类社会的基石，在网络世界同样重要 证书之间的层层信任构成了信任链，而根证书是不需要被其他人证明的 不要随意安装来历不明的根证书，那样可能会使的电脑更容易遭受到攻击 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 若衣食无忧，谁甘愿拼搏！努力鞭策自己无非是为了挣得可以选择生活的权利~ 2020-7-5 23:44:41]]></content>
      <categories>
        <category>加密与认证</category>
      </categories>
      <tags>
        <tag>认证</tag>
        <tag>非对称加密</tag>
        <tag>HTTPS</tag>
        <tag>根证书</tag>
        <tag>中间人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[码龄10年工作6年的搬砖小哥，最常访问的学习网站都在这里了]]></title>
    <url>%2Fblog%2F2020%2F06%2F18%2F%E7%A0%81%E9%BE%8410%E5%B9%B4%E5%B7%A5%E4%BD%9C6%E5%B9%B4%E7%9A%84%E6%90%AC%E7%A0%96%E5%B0%8F%E5%93%A5%EF%BC%8C%E6%9C%80%E5%B8%B8%E8%AE%BF%E9%97%AE%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%E9%83%BD%E5%9C%A8%E8%BF%99%E9%87%8C%E4%BA%86%2F</url>
    <content type="text"><![CDATA[前言这完全是一篇水文，主要看别人分享的文章很有收藏价值，所以也想试着总结一下这种类型的文章，不过之前确实用过一些比较好的网站，有些网站是查找问题时找到的，但是解决完问题就找不到了很可惜，所以我养成了收藏网址的习惯，感觉有用就会分门别类的添加到书签中，再次遇到之前解决过的问题就先在书签里搜一下，有时候会加快解决问题的进度，下面这幅图是我浏览器书签中的一部分。 网络技术飞速发展到今天，越来越多的功能被搬到了“云”上，导致原来需要在本地安装的多种开发环境完全不需要搭建了，如果是临时使用完全可以在浏览器中实现，比如对于程序猿来说不可或缺的编程开发环境，已经出现很多在线编译和运行的网站，再比如原来被称作 PS大神 的设计者们必须要在电脑上安装 Photoshop 这个庞然大物，可是现在你可以发现很多在线 PS 的软件，处理简单功能分分钟搞定，这是我截取的网页上 在线PS软件 的一部分，足以以假乱真。 好了，开始进入正题了，作为一个天天写代码的搬砖小伙，每天都在敲敲敲，不是在敲代码就是在敲代码的路上，还有一种可能就是在学习如何敲代码，那么这样的榆木脑袋每天都会访问哪些学习网站呢？接下来我把最常访问的一些网站列举一下，有可能后续会更新，但我要是犯懒就算了。 网站列表接下来会分成几个大类来列举一个我最常用的一些网站，每个人的喜好不同，但是仔细看看，或许有些你会感兴趣哦！ 一、文档项目如果写一个功能有现成的轮子给我用就好了，其实网络上有很多现成的轮子，我们要善于利用别人的成果转换为自己进步的阶梯。 1、cppreference &gt;&gt; https://en.cppreference.com/w/ 首推这个网站其实是有点偏心的，因为每天都在写C++，所以还是首先就想到了这个网站，这个网站中可以查到已经发布的各个C++标准的库函数，特性、头文件等等，对于不确定的函数返回值、新标准的特性、函数的常见用法都可以在这个网站找到，这个网站还有中文版的，学习C++的小伙伴可以常来逛逛。 2、 GitHub &gt;&gt; https://github.com/ 被广大程序员调侃成“全球最大的同性交友网站”怎么能不上榜，GitHub 这个网站就算你不常用但也会常听到吧，上面充满了全世界精英团队编写的轮子，有趣的是这样一个最支持开源的网站居然被微软的这个最大的闭源厂商受够了，不过现在还是发展的越来越好了。你可以在上面阅读一些开源代码，看看那些明星产品究竟是怎样实现的，真正为我所用。 二、数据仓库程序发展离不开数据存储，数据是支撑程序发展的基石，现在的数据库已经不是当初的关系数据满天下了，各种各样的数据库类型被发明了出来，列数据库、文档数据库、键值数据库等等，真的是太多了。 1、 Redis &gt;&gt; https://redis.io/commands 非关系型数据库中最火的一个了吧，在认真学习之前一度认为它是一个新产品，后来才知道 Redis 其实在 2009 年就已经诞生了，作为一款键值型的内存数据库，现在被广泛引用于各个领域，而 Redis 的官方文档是需要不断去翻阅的，最近发布了 Redis 6.0，引入了网络多线程，以后的面试题可能要留神了。 2、 MySQL &gt;&gt; https://dev.mysql.com/doc/refman/8.0/en/ 虽然 NoSQL 数据库在各个领域兴起，但是现在还是关系数据库占据着主导地位，MySQL 就是关系数据库中的明星产品了，自从被 ORACLE 收购以后也在不断发展，最近版本从5.7一跃直接到8.0，据说MySQL 8 要比 MySQL 5.7 快 2 倍，还带来了大量的改进和更快的性能！感兴趣的可以查阅一下 MySQL 的文档，它的文档格式特别棒，看着就让人赏心悦目。 3、 墨天轮 &gt;&gt; https://www.modb.pro/dbRank 墨天轮上聚集了很多数据库爱好者，是一个新兴的数据库技术交流平台，一直渴望成为一个专业的技术社区，高效便捷、开放互助、乐于分享，能够承载我们数据人的学习和成长，促进整个行业的发展和创新，在这个网站上我们可以看到各大数据库排行，了解数据库相关的最新发展和方向。 三、工具集合文章开头也提到了，如今很多工具都搬到了线上，这样既节省了电脑空间，也免去了安装和配置工具的麻烦，只要不是IDE的重度依赖者，使用在线工具还是很方便的。 1、 在线工具 &gt;&gt; https://tool.lu/ 这个网站提供了众多的在线工具，每次一用到时间戳转换或者URL编码等操作，我肯定会第一时间打开这个页面，因为本地调 API 太麻烦了，有时还需要搭建环境，在网站上找到对应的工具直接操作就可以了，还带有实时刷新的功能，完全没必要自己在本地写代码。 2、AlbertWorld &gt;&gt; http://www.008ct.top 这个网站收录了很多有用的网址，不仅仅是工具，文档、教程、数据、资源包括方方面面，其中包括很多讲解原理的知识和有用的素材，很像一个小小的杂货铺，偶尔上新哦！ 四、疑难解答解决问题是程序员每天都要面临的功课，而程序员要解决的问题往往是没见过的，如果一个程序猿天天值只处理那么几个相同的问题，那么他已经走上了被淘汰的道路，查找问题原因，给出解决方案，祝贺你，你今天又进步了。 1、 CSDN &gt;&gt; https://www.csdn.net/ 用了这么久的 CSDN 一直不知道全称是什么？前几天才查了一下全称是 Chinese Software Developer Network，立意很深远的样子，不过确实是一个不错的网站，从去年开始大面积调整，原来的广告真是惨不忍睹，改版后现在好多了，工作中很多解决方案都出自这个网站，之前在论坛里没少逛，解答问题的同时，自己的知识也得到了巩固。 2、 StackOverflow &gt;&gt; https://stackoverflow.com/ 一个和 GitHub 比肩的网站，一个专门解决程序猿问题的网站，你要坚信，作为一个普通的程序搬砖工，你遇到的问题别人也遇到过，所以遇到问题来这个网站查一查，有时问题瞬间就被解决了，特别是一些专业的工具仅仅报了一个错误代码，通过搜索引擎很难定位具体问题，但是在这个网站上的前辈已经为你趟好路了。 五、进阶刷题程序猿就是一个活到老学到老的职业（如果35岁被淘汰就不用学了），必须时刻保证自己的学习状态，更新自己的知识储备，刷题成为了一项锻炼脑力的活动，因为很多公司特别是大公司都会要求算法达到一定的水平，所以没事多刷刷题，不要让自己的大脑锈住了。 1、力扣 &gt;&gt; https://leetcode-cn.com/problemset/all/ 这个网站貌似有很多名字，现在显示的是力扣，之前是在全球服注册的，后来莫名其妙的有注册了一次，变成了家门口的版本，这上面有很多算法题，一段时间没看居然还加上了面试题，不过它搞的那个竞赛挺有意思的，作为长期的两题选手，看着高手们10分钟做完4题，犹如神仙打架一般。 2、 POJ &gt;&gt; http://poj.org/ 这个 Online Judge 有些历史了，不过一直保持着更新，ACM 竞赛时也尝试在这里刷过题，和 LeetCode 比起来这里的题似乎更难一些，如果想挑战更高难度，不妨来这里试一下。 六、教程案例当我们想学习一门新技术的时候，很渴望得到一份简单明了的教程，实际上很多技术的官方网站文档都非常完整，但是对于初学者来说理解起来会有些难度，这时候可以看一些边学边做的教程，在不断尝试中学习知识。 1、廖雪峰官网 &gt;&gt; https://www.liaoxuefeng.com/wiki/1016959663602400 廖雪峰此乃神人也，看看我截取的这篇教程的访问量你就清楚了，前两年我看到这篇文章的时候访问量才几亿，跟着教程完整的学了一遍，现在访问量已经400多亿了，受欢迎程序难以想象，廖大神写得教程浅显易懂，非常适合初学者，从头来一步步的就学会了，想当初我跟着他学爬虫把他的文章都爬了，哈哈~2、 菜鸟教程 &gt;&gt; https://www.runoob.com/ 同样是一个接地气的教程网站，谁刚开始学的时候不是一只菜鸟呢，这个网站教程很多，只要你想学总能找到你喜欢的那款，并且在讲解时会有例子和函数参数说明，非常适合初学者。 总结 总有小伙伴调侃说：收藏从未停止，学习从未开始，其实收藏是一个好苗头，只有想学才有可能去学 如果仔细看了这些网站，你会发现有些网站的设计让人真的很舒服，临时补充一个 https://git-scm.com/ 真正应了那句话，比你优秀的人比你还努力，你的产品都那么强了，网站居然还那么好看，还让不让人活了~ ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 我们没有生活在和平的年代，只是生活在了和平的国度，想开点，珍惜眼前的一切，灾难都会过去，我们还有一双手去争夺属于自己的未来。 2020-6-20 00:16:49]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>收集</tag>
        <tag>网站</tag>
        <tag>working</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的std::lower_bound()和std::upper_bound()函数]]></title>
    <url>%2Fblog%2F2020%2F06%2F15%2FC-%E4%B8%AD%E7%9A%84std-lower-bound-%E5%92%8Cstd-upper-bound-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言问题是躲不掉的，该来的总会来，这不是代码中又遇到了 std::upper_bound() 函数，再来学习一遍好了，在我的印象中每次看到这 lower_bound 和 upper_bound 两个函数都有些别扭，凡是见到他们必须查一遍，因为我记得它们两个函数的作用不对称，这一点记得很清楚，但是它们两个函数查找的细节却记不住，这次总结一下，强化记忆，下次回忆起来应该会快一点。 函数定义今天看到这两个函数时挠挠头又打开了搜索引擎，看到文章里写到 std::lower_bound() 是返回大于等于 value 值的位置，而 std::upper_bound() 是返回第一个大于 value 值的位置，第一反应真是瞎写，怎么俩都是大于，肯定应该是一个大于一个小于啊，这样才“合理”嘛！ 但是当看到多个文章中采用相同的说法时，刚刚还“坚定”的想法开始动摇，然后开始查C++标准文档，一遍遍读着那有些拗口的文字: std::lower_bound returns an iterator pointing to the first element in the range [first, last) that is not less than (i.e. greater or equal to) value, or last if no such element is found. std::upper_bound returns an iterator pointing to the first element in the range [first, last) that is greater than value, or last if no such element is found. 这些标准文档上的文字印证了刚刚查询到的结果，两个函数返回的结果都是迭代器，std::lower_bound() 是在区间内找到第一个大于等于 value 的值的位置并返回，如果没找到就返回 end() 位置。而 std::upper_bound() 是找到第一个大于 value 值的位置并返回，如果找不到同样返回 end() 位置。 两个函数都提到了大于操作，而没有涉及到小于操作，这就是我前面提到的不对称，也是我感觉不合理的地方，但是当尝试使用了几次这两个函数之后，我发现这两个函数的设计的恰到好处，这样的设计很方便我们来做一些具体的操作。 实际例子首先说明这两个函数内部使用了二分查找，所以必须用在有序的区间上，满足有序的结构中有两个常见的面孔：std::map 和 std::set，他们本身就是有序的，所以提供了 std::map::lower_bound() 和 std::set::lower_bound() 这种类似的成员函数，但是原理都是一样的，我们可以弄明白一个，另外类似的函数就都清楚了。 自己设计如果你看了这两个函数的具体含义也和我一样不太理解为什么这样设计，可以思考一下接下来这个需求，找出数组内所有值为2和3的元素，图例如下： 对于一个有序数组，我们在实现 lower_bound() 函数和 upper_bound() 函数时可以让它返回指定的位置来确定取值区间，第①种情况就是标准函数库的实现方式，而第②种和第③种就是我第一印象中感觉应该对称的样子，这样看起来也没什么问题，下面具体来分析下后两种设计有哪些不好的地方。 具体分析假如我们采用第②种实现方式，那么实现打印元素2和3的代码要写成下面这样： 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; std::vector&lt;int&gt; v&#123;1,1,2,2,3,3,3,5,7,8&#125;; std::vector&lt;int&gt;::const_iterator itorLower = std::lower_bound(v.begin(), v.end(), 2); std::vector&lt;int&gt;::const_iterator itorUpper = std::upper_bound(v.begin(), v.end(), 3); while(true) &#123; std::cout &lt;&lt; *itorLower &lt;&lt; std::endl; if (itorLower == itorUpper) break; ++itorLower; &#125; return 0;&#125; 代码看起来还可以，打印完元素后判断到达了结尾直接跳出循环，但是如果要是数组中不包含元素2和3呢，那么也会打印出一个元素，还有可能导致程序崩溃。 如果我们采用第③种实现方式，那么实现打印元素2和3的代码要写成下面这样： 12345678910111213141516#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; std::vector&lt;int&gt; v&#123;1,1,2,2,3,3,3,5,7,8&#125;; std::vector&lt;int&gt;::const_iterator itorLower = std::lower_bound(v.begin(), v.end(), 2); std::vector&lt;int&gt;::const_iterator itorUpper = std::upper_bound(v.begin(), v.end(), 3); for(++itorLower; itorLower != itorUpper; ++itorLower) &#123; std::cout &lt;&lt; *itorLower &lt;&lt; std::endl; &#125; return 0;&#125; 这代码看起来简洁了很多，但是在循环开始前需要先调用 ++itorLower，因为第一个元素并不是需要找到的元素，所以要先跳过它，这样看来确实多做了一步操作，一开始就让 itorLow 指向第一个2就好了呀。 最终版本当你尝试几种实现方式就会发现，还是标准库提供的这种方式使用起来更加方便，虽然采取的不是对称的方式，但是统一了存在查找元素和不存在查找元素的的情况，写出的代码也比较简洁，没有多余的步骤，代码如下： 123456789101112131415#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; std::vector&lt;int&gt; v&#123;1,1,2,2,3,3,3,5,7,8&#125;; auto itorUpper = std::upper_bound(v.begin(), v.end(), 3); for(auto itorLower = std::lower_bound(v.begin(), v.end(), 2); itorLower != itorUpper; ++itorLower) &#123; std::cout &lt;&lt; *itorLower &lt;&lt; std::endl; &#125; return 0;&#125; 总结 有些函数的实现方式和我们想象的并不一样，但是我们可以通过熟练使用来了解它为什么这样设计 对称结构虽然是很美的，但是非对称的结构在编程中常常出现，同样有其美丽所在 遇到类似的问题可以动笔画一画，列举出各种情况会有利于你做出正确的判断 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有时会很焦虑，看到优秀的人比你还努力时总让人感到急迫，但是一味的忧患是无意义的，脚下迈出的每一步才是真真正正的前进，不要去忧虑可能根本就不会发生的事情，那样你会轻松许多 2020-6-26 23:21:40]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>lower_bound</tag>
        <tag>upper_bound</tag>
        <tag>查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认证、HTTPS、证书的基本含义]]></title>
    <url>%2Fblog%2F2020%2F06%2F14%2F%E8%AE%A4%E8%AF%81%E3%80%81HTTPS%E3%80%81%E8%AF%81%E4%B9%A6%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%90%AB%E4%B9%89%2F</url>
    <content type="text"><![CDATA[前言通过前面的总结 《对称加密、非对称加密、公钥、私钥究竟是个啥？》，我们基本了解了对称加密和非对称加密的概念和特点，考虑到效率和安全性，一般可以使用非对称加密来传递对称加密所需密钥，之后就采取对称加密通信了，这样可以大大提高数据发送的的效率。 其实密码技术除了应用在加密领域外还有很多其他的用途，比如验证数据的完整性、用来做认证、提供一些不可否认的证据等，这些应用也常常出现在我们的日常生活中，比如很多官方网站在提供软件下载链接的同时，还附带一个验证的字符串，实际上很多就是md5码或者hash码，这些就是供下载的人来验证完整性的，防止被其他人篡改。 我们下载完软件之后，使用工具来将软件转化成一串字符，听起来很神奇，实际上就是计算一下软件的md5码或hash码，然后和网站上的标注的信息进行对比，如果一致那么软件就是完整的。我曾经就遇到过一次，当时做的游戏发包，同事给我发了测试包，安装之后无法正常使用，检查包的大小与发送端的一样，后来使用检测工具计算发现md5是不同的，原因可能是发送包的时候电脑卡死过，导致最后发来数据包与原始数据产生了差异。 但是你有没有想过，这个软件虽然是完整的，通过md5计算发现也没有被其他人篡改，那么怎么证明你下载的网站真的是官网呢？万一官网也是伪造的呢？如果有人伪造了官网，又将上面的软件进行修改重新计算md5然后上传到自己伪造的界面上，你要怎么来识别呢？ 可能有的人会想到看网址啊，taobao.com 就一定是淘宝的网站吗？这个域名是可以伪造的，所以要验证网站上东西是真的，那么首先要验证你看到的网站是真的，这就涉及到了认证身份，接下来可以简单了解下什么是认证。 认证其实密钥不仅仅可以用来加密，还可以用来认证的，那么什么是认证？认证是一种信用保证形式，表示对一种事物或一个人的信任和认可，比如常见的毕业证书、结婚证书都是对人一段经历或一种关系的证明和认可。最简单的网站登录是基于密码的验证，这实际上就是利用对称加密的认证。 网站保存了你的用户信息和密码，下次再登录的时候输入密码后，网站会用你输入的密码和之前保存的密码进行对比，如果密码相同则认证成功，成功的证明了“你”就是“你”，而非对称加密同样可以用来做认证。 在非对称加密的实现中，私钥是只有自己保存的，而私钥加密的内容可以使用公钥解开，如果一份加密数据可以用 Jerry的公钥解开，那么我们就可以认为这份数据是 Jerry 发出的，因为只有 Jerry 自己有私钥，所以可以通过这种方式来进行认证。 而在网络上想要认证一个网站的身份，确认它不是钓鱼网站，第一个映入脑海中的就是 https，一般提到 https 都会说它是加密的、安全的，是 http 的升级版，但是 https 的安全不仅仅体现在加密上，还有它的认证功能，可以使你免受钓鱼网站的侵害。 HTTPS简单了解下 HTTPS，一般来说网络模型常说的有OSI七层模型和五层模型，HTTPS 的诞生并没有增加模型的层数，HTTP 是建立在 TCP基础上的应用层协议，而 HTTPS 是在 TCP 和 HTTP 之间的会话层中加了一些特殊操作，使原来明文传输的内容，在会话层这一步进行加密，并且可以对数据来源进行认证。 提到 HTTPS 就不得不说 SSL 和 TSL， SSL 是应用在 HTTP上的一个协议加密层，最初是由网络大佬网景公司（Netscape）研发，后来升级为 TSL，简单的理解就是 HTTP + SSL/TSL = HTTPS。 随着网络安全逐渐得到大家的认识，一些主流网站基本都都将访问方式改成了 https，支持 https 的网站在浏览器的地址栏中通常有一把小锁，点开会提醒你访问的是安全的连接，如果你访问的连接疑似被人篡改或者仿冒，那么这把小锁会被斜线划掉，提醒你网站危险请谨慎访问。 那么 https 是怎么判断出来哪些网站是安全的，哪些网站是仿冒的呢？毕竟有些网址都很像甚至可以做成一模一样的，这就用到了非对称加密的认证功能，当我用 A 的公钥可以解密一段消息，那么就可以证明消息是 A 发来的，https 的认证功能正是利用了这个特点。 当访问一个网站的时候，网站先给我发一个用它的私钥加密的数据，然后我用它的公钥来解密，如果解密成功就说明我访问的网站是正常的，可以继续访问，如果解密失败则很有可能是虚假或者仿冒的网站，应该仔细辨别一下了。 这里会有一个问题，我怎么才能得到网站的公钥呢？之前说过密钥配送问题，直接由网站发给我肯定不行，中间可能被篡改，也有可能一个虚假网站把它自己的公钥发给我了，我用假的公钥验证对应的假的私钥也是成功的，这样就起不到认证的效果了，必须给他找个证明人才行，这就要用到我们下面要说的证书了。 证书证书是用来证明一件事情或东西的，刚才说网站的公钥不能它自己来发，这样不能证明它的身份，我们可以找一个权威的机构给它认证一下，我每次从权威机构获得网站的证书，从这个证书中取得公钥，如果用这个公钥可以解开网站私钥加密的内容，那么就可以认证它的身份了。 这里提到的证书就是网站所有者找权威机构申请的，权威机构把网站信息、有效时期、对应的公钥、序列号消息等数据存储到证书中，当我们需要能某个网站的公钥时，去证书中取就可以了，这里的证书有点像营业执照了，由权威机构发布，用来证明你的身份。 但是权威机构的证书怎么发给我呢？我们有理由认为网络是不安全的，那证书如果直接通过网络发给我同样是不安全的，还有一个问题就是网站虽然找了一个权威机构，但是我认为它不够权威怎么办？这时这个权威机构可以找一个更权威的机构证明自己，让更权威的机构给自己颁发一个证书，这样就形成了证书链。 就像现实生活中我要找个人来干活，因为工期比较紧所以找的人要求踏实，必须能老老实实把活干完，不能半路撂挑子，张三过来应聘，正在我犹豫时，走过来一个叫李四的人说张三没问题，但是我还是不能确定，因为李四我也不熟悉，然后李四居然把我爸叫来了，我爸和我说李四这个人特别诚实，从来不说谎，这时一条证书链就形成了，李四为张三证明，我爸为李四证明，那谁来证明我爸说的是真的呢？不需要的，我无条件相信他。 这在计算机的证书链中就是根证书，根证书不需要别人来证明，你只能无条件相信它，它是整个信息链的源头，通常内置在操作系统或者浏览器中，关于根证书还要一些好玩的故事和一些变态的应用，下次再说吧，准备睡觉了~ 总结 密码技术除了应用在加密领域，还可以用来验证数据的完整性、用来做认证、提供一些不可否认的证据 HTTPS 不仅可以用来加密通信内容，还可以用来验证网站的真实性 正规的支持 HTTPS的网站在访问时会地址栏会有一把安全的小锁头，但是有些不出现小锁头的网站并不一定都是非法的 HTTP 网站是没有小锁头的，因为有些数据不需要加密，毕竟绝大多数的 HTTPS 证书是要钱的，有很多网站由于经费问题还未投身于 HTTPS 的怀抱 根证书通常内置的操作系统或者浏览器中，是证书链的源头，你必须无条件的信任他。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当我们拼劲全力到达自己的终点时，可能会看到同行的人正在你的终点线前伸伸懒腰准备出发，然后一骑绝尘消失在你震惊的目光中，但是这不是我们放弃努力的理由，因为如果你不努力，你甚至连他们的背影也看不到~ 2020-6-14 23:20:21]]></content>
      <categories>
        <category>加密与认证</category>
      </categories>
      <tags>
        <tag>加密</tag>
        <tag>认证</tag>
        <tag>非对称加密</tag>
        <tag>HTTPS</tag>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对称加密、非对称加密、公钥、私钥究竟是个啥？]]></title>
    <url>%2Fblog%2F2020%2F06%2F07%2F%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E3%80%81%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E3%80%81%E5%85%AC%E9%92%A5%E3%80%81%E7%A7%81%E9%92%A5%E7%A9%B6%E7%AB%9F%E6%98%AF%E4%B8%AA%E5%95%A5%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[前言进入正题之前先唠叨几句，不久前听到一个名词叫——费曼学习法，核心思想就是用通俗的话语把复杂道理或技术讲清楚，如果你已经明白了这个方法的含义，那么我好像离成功又进了一步。其实这个方法一直在尝试使用，但是没想到它居然有个“洋气”的名字。 由于之前学习时接触了加密、验证、HTTPS、证书等知识，感觉挺有意思的，最近也用到了一些这方面的内容，所以决定把这些概念重新梳理一下，免得一段时间不复习又还给书本了。本打算写一篇总结把这些概念整理到一起，但是初步想了一下很难实现，涉及到的概念实在太多了，所有还是决定分成几次来写吧。 分开写就比较随便了，写到哪完全看心情，不过我还是力图用最简单的描述来讲清楚问题，抛开具体的实现细节（其实我也不懂），梳理流程和概念性的知识，想了解具体的实现细节还是找专业的书籍去补充吧，我曾经看了一遍《图解密码技术》，过了这么久整本书我就记得两个词——异或、求余，再比如使用公钥和私钥来完成非对称加密，就是利用了两个大质数 (p,q) 乘积 (n) 难以逆向求解，这些太细节的东西很难展开一点点讲清楚。 最理想的状态是把学习知识当成是看故事书，阅读完一个个情节就吸收的差不多了，而不是把这些知识当成武功秘籍，然后一点点参悟，最后觉得枯燥而放弃，所以为了知识解惑，也算是将学习成果做个笔记，我们开始从最基础的知识学起。 对称加密对称加密一般指：加密和解密使用的是同一个密钥的加密方式。就像防盗门的钥匙一样，可以用钥匙把门锁上，也可以用这同一把钥匙再把门打开。 对称加密示例至于密钥怎么使用要看具体的加密算法了，可以举一个简单的例子，比如有下面这样一句话： I like cat 我想把它发给一个好朋友，但是又不想被别人看到，万一有其他人一眼看到，那我的喜好就暴露了（那有怎样呢？），这时我们可以把这句话改的稍微隐晦一点，我可以和好友约定一个密码，假设是 1，然后我把原来这句话的每个非空白字母都替换一下，按字母表顺序使用后一个字母替换前一个字母，比如用字母 b 替换字母 a，那么这句话就变成了： J mjlf dbu 这时就不怕被别人一眼看穿消息内容了，没有意义的字符串是比较难记的，但是当我的好友收到这句话时，使用我们约定的密码 1 就知道字母顺序变换了1位，所以他再将将字母反向替换回来就能够将文字还原。 这个例子很简单，但可以说明对称加密的关键，就是加密解密使用同一个密钥，例子中的 1 就是这个密钥，它可以让解密者知道，还原信息时需要反向移动1位即可，消息发送流程如下： 1234graph LRA[I like cat] -- 用1加密 --&gt; B((J mjlf dbu))B((J mjlf dbu))-- 发送给好友 --&gt; C((J mjlf dbu))C((J mjlf dbu))-- 用1解密 --&gt; D[I like cat] 对称加密的问题刚才的例子已经说了对称加密的流程，但是有一个问题需要解决，这个密码 1 我要怎么告诉我的好友呢？直接发消息被别人看到怎么办，打电话也有可能被别人窃听啊！ 密钥配送这就涉及到了一个密钥配送的问题，如果想让对方解密就需要把密码发过去，但是密钥有可能被其他人窃取，这样秘密就不再是秘密了，可能你会想即使密码被别人窃取了也不要紧，因为他根本不知道怎么用。 请不要做这种假设，简单的情况没有密码都能破解，更何况在密码和数据都被窃取的情况下呢，另外在密码领域我们建议使用完全公开的密码算法，这样的算法经过时间的检验才能被用于加密，千万不要独创一套自认为很安全的加密算法，单靠隐藏算法的细节来达到加密的目的是很危险的。 发送密钥可能被窃取，不发送密钥对方无法解密，这个加密的密钥配送问题是使用对称加密必须要解决的，而下面要说的这种非对称就不同了，可以将一把密钥直接发送给对方，即使被窃取也没有关系。 非对称加密看这个名字就知道它有点“针对”那个叫做对称加密的小伙伴，从定义上来说对称加密指的是加密和解密使用相同的密钥（为啥不叫同钥加密咧），而非对称加密指的是加密和解密过程使用不同的密钥来进行。 乍一听好像有点不可思议啊，怎么滴，难道还能两把不同的钥匙开一把锁？确实可以！这有点像中学物理里面的两个开关控制一个灯泡。在一个漆黑的楼梯两端，分别有一个开关，控制着楼梯上方的一个灯泡，上楼前先打开楼梯下面的开关，然后上楼后关掉楼梯上面的开关，而下楼时进行相反的操作，先打开楼梯上面的开关，然后下楼后默认楼梯下面的开关，找了张电路图，感兴趣可以再分析一下。 不过非对称加密和这种双掷开关不完全相同，使用开关时可以在同一端打开或关闭，但是非对称加密时，只能在一端加密，然后在另一端解密，同一端是不能同时加密和解密的。 公钥与私钥具体地，非对称加密指的是根据特殊规则生成两把密钥 A 和 B，分别叫做公钥和私钥。私钥自己保留，公钥则分发给自己的小伙伴用来用来和自己通信，理论上生成的两把密钥选择哪一把作为私钥都可以，但是出于效率和安全等方面的要求，公钥和私钥再生成时会给出特殊的条件，所以在实际使用过程中，两者通常是不会互换的。 非对称加密的示例使用公钥和私钥怎样完成非对称加密呢？下面来看一个具体的场景，比如有 Tom 、Jerry 、Spike 三个小伙伴，有一天 Jerry 想给 Tom 发点小秘密，又不想让 Spike 发现，首先他想到的是对称加密，先和 Tom 约定一个密码，再给 Tom 发送加密消息，但是想到前几天，自己和 Tom 的消息被 Spike 破解了，因为两个人发送密钥和加密消息的过程都被窃听了，如果这次的消息再被窃听到怎么办？ 后来Jerry想起Tom曾经自己生成了一对公钥和私钥，然后把公钥发给了自己和 Spike，那这样就可以使用非对称加密了，Jerry 使用 Tom 给的公钥把要发送的小秘密进行加密，然后发送给了 Tom。这时 Spike 果然在窃听，但是窃听到的消息使用了 Tom 的公钥进行了加密，只有 Tom 拥有解开这条消息的私钥，而 Spike 虽然拥有 Tom 的公钥也是解不开的。 1234graph LRA[I like cat] -- Jerry用Tom公钥加密 --&gt; B((密文))B((密文))-- 发送给Tom --&gt; C((密文))C((密文))-- Tom用自己的私钥解密 --&gt; D[I like cat] 怎么判断解开上面的描述中出现了“解开”一词，这两个字在我刚开始学习加密这些知识的时候困扰了我好久，查了好多讲解也没弄明白，什么叫能解开，什么叫解不开。它不像现实生活中的事物那么形象，比如把电视打开，那么电视就出现图像了，把锁解开门就能打开了。在数据加密的过程中，数据本质上是一堆二进制数据，加密之后还是一堆二进制数据，解密时使用密钥进行特定的运算就会得到解密后的二进制数据，怎么判定这些“解开”的数据是否是原数据呢？ 后来在不断的学习过程中，接触了一些开源的非对称加密算法实现，比如常用的 RSA，基础的函数包括公钥加密、私钥解密、私钥加密、公钥解密等，当你在解密时将密文和密钥传入解密函数进行特定的运算，计算过程和计算结果必须满足特定的条件，这些条件是算法保证的，如果有条件不满足那么解密失败，这就是上面所提到的解不开。 非对称加密的问题之前提到对称加密时，密钥配送问题是一个难题，因为网络上发送密钥很容易被截获，无法保证密钥不被窃取。很多情况下又不能面对面的传递密钥，而非对称加密的出现解决了这个问题，因为公钥是可以被任何人知道的，所以网络上发送公钥就不怕被窃取，但是如果例子中，Jerry 收到的 Tom 的公钥实际上在途中被 Spike 替换了怎么办？ 这就又引入了一个问题——中间人攻击，形象的来表述就是有第三方 Spike 侵入了原本两个人 Tom 和 Jerry 的通信中，Spike 对 Tom 时把自己伪装成 Jerry，和 Jerry 沟通时又将自己伪装成 Tom，这样原本两个人的沟通信息全都被第三方窃取了，这个问题的根本就是获取公钥不可信，不过证书中心可以解决这个问题，后面我们再继续深入了解，这里就不展开了。 对称加密和非对称加密对比 加密类型 常见算法 加密处理速度 遇到的问题 解决办法 对称加密 DES、AES 快 密钥配送问题 面对面交换或者使用非对称加密传送秘密 非对称加密 RSA、DSA 慢 中间人攻击问题 通过证书中心来解决中间人攻击 总结 虽然想写的尽可能的通俗易懂，但还是不免会引入一些令人犯困的概念，一开始记住就好，后面理解了不会觉得枯燥 还在不断尝试表达方式，总结中融入了一些我当时学习时的想法和疑惑，我遇到的这些问题应该很多人也遇到过 使用对称加密方法，速度快，效率高，但是会面临密钥配送的问题 非对称加密虽然很巧妙的，但是效率较低，所以一般的用法是使用非对称加密来传送简短的对称加密密钥，然后再使用对称加密的方式传送数据 为了更好的加密你的数据，应使用公开的加密算法，他们都是经过时间考验的，单靠隐藏加密细节来加密时很危险的 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 见过了外面的大千世界，便不再甘心留在原地，而这种不甘心恰恰就是动力~]]></content>
      <categories>
        <category>加密与认证</category>
      </categories>
      <tags>
        <tag>加密</tag>
        <tag>认证</tag>
        <tag>非对称加密</tag>
        <tag>公钥</tag>
        <tag>私钥</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git在回退版本时HEAD\~和HEAD^的作用和区别]]></title>
    <url>%2Fblog%2F2020%2F05%2F30%2Fgit%E5%9C%A8%E5%9B%9E%E9%80%80%E7%89%88%E6%9C%AC%E6%97%B6HEAD%E2%80%A6-%E5%92%8CHEAD-%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言今天总结一个小知识点，虽然不难，但是对新手有很强的迷惑性，了解一下也挺好。我们在使用 Git 回退到版本的时候，可能见过这种写法 git reset --hard HEAD~，有时候也会遇到这种写法 git reset --hard HEAD^，这两个语句都是将代码库还原到上一个版本，但是只差了一个符号，他们究竟有什么区别呢？这里先给出结论：HEAD~ 和 HEAD^ 含义不同，功能一样！ HEADHEAD 这个词在 git 使用过程中经常出现，作用很像是数据结构中指向二叉树根节点root的指针。有个 root 指针我们就可以对二叉树进行任意操作，它是二叉树的根基。而 git 中的 HEAD 概念也类似一个指针，它指向是当前分支的“头”，通过这个头节点可以追寻到当前分支之前的所有提交记录。 git 的提交记录之间的关系很像一棵树，或者说是一张图，通过当前的提交记录指向上一次提交记录串联起来，形成一个头结构，而在 git 中我们常常说的切换分支，只不过是 git 客户端帮你把要操作的那条路径的头节点，存储到了 HEAD 文件中。 HEAD 在 git 版本控制中代表头节点，也就是分支的最后一次提交，同时也是一个文件，通常在版本库中 repository/.git/HEAD，其中保存的一般是 ref: refs/heads/master 这种分支的名字，而本质上就是指向一次提交的 hash 值，一般长成这个样子 ce11d9be5cc7007995b607fb12285a43cd03154b。 HEAD~ 和 HEAD^在 HEAD 后面加 ^ 或者 ~ 其实就是以 HEAD 为基准，来表示之前的版本，因为 HEAD 被认为是当前分支的最新版本，那么 HEAD~ 和 HEAD^ 都是指次新版本，也就是倒数第二个版本，HEAD~~ 和 HEAD^^ 都是指次次新版本，也就是倒数第三个版本，以此类推。 这个说法在之前的总结 《git checkout/git reset/git revert/git restore常用回退操作》 中提到过，但是并未展开说，今天就来测试一下。 HEAD 后面 ~ 和 ^ 的区别其实 HEAD~ 和 HEAD^ 的作用是相同的，这两者的区别出现在重复使用或者加数字的情况，下面来分情况说明一下。 HEAD~ 和 HEAD^后面都加1加上参数1之后变成了 HEAD~1 和 HEAD^1，其实这就是他们本来的面貌，在参数为 1 的情况下可以省略，HEAD~1 表示回退一步，参数1表示后退的步数，默认推到第一个父提交上，而HEAD^1表示后退一步，直接后退到第n个父提交上，数字1表示是第一个父提交。 这里引入一个父提交的概念，也就是在最新提交之前的最近的提交我称它为父提交，但是父提交会有两个吗？实际上会的，直接的父提交可能会有很多，分支合并是产生父提交的一种常见原因，两个分支合并到一起时，这两个分支的原 HEAD 都会成为合并后最新提交的父提交。 理解了这个概念，我们发现虽然数字是一样的，但是含义却不相同，HEAD~1 中指的是后退的步数，HEAD^1指的是退到第几个父提交上。 HEAD~ 和 HEAD^后面都加0这是一种比较特殊的情况， 加上参数0之后变成了 HEAD~0 和 HEAD^0，其实他们指向的节点没有改变，还是代表了 HEAD，只要了解这种情况就行了，我还没有见过谁这样写过。 HEAD~ 和 HEAD^后面都加大于1的数字这时就会发现两者的不同了，比如我们把数字都定为2，那么 HEAD~2 代表后退两步，每一步都后退到第一个父提交上，而 HEAD^2 代表后退一步，这一步退到第二个父提交上，如果没有第二个父提交就会报出以下错误： fatal: ambiguous argument ‘HEAD^2’: unknown revision or path not in the working tree.Use ‘–’ to separate paths from revisions, like this:‘git […] – […]’ 具体示例上面说了几种加数字的情况，如果是第一次接触可能还是不太明白，没关系，我可以实际操作一下，看个具体的例子就明白了。 准备工作下面是一个测试代码库的分支结构，一共有 dev1、dev2、dev3、dev4 四个分支，最终合并到 dev1 分支，提交记录如下： 123456789101112131415161718192021albert@home-pc MINGW64 /d/gitstart (dev1)$ git alllog* ce11d9b (HEAD -&gt; dev1) Merge branch 'dev3' into dev1|\| * e330eac (dev3) update at dev3 - 3| * 7ab3c98 Merge branch 'dev4' into dev3| |\| | * c8795e8 (dev4) update at dev4 - 2| | * 155d3db update at dev4 - 1| * | ccdf16a update at dev3 - 2| * | 9f08bb0 update at dev3 - 1| |/* | f82b57b update at dev1 - 3* | dcdcb87 Merge branch 'dev2' into dev1|\ \| * | 32d6213 (dev2) update at dev2 - 2| * | ca4db4a update at dev2 - 1| |/| * d8d80b7 update readme at dev2* | 034ccb6 update readme at dev1 - 2* | d58fedc update readme at dev1 - 1 也许有颜色标记会看得更清楚一些，所以截个图放在这： 刚看这种图的时候要注意一点，记录列表中的先后关系不代表提交时间的先后，如果习惯于看SVN的记录以后，很容易在看日志信息时加上时间因素，但是这个时间因素在 git 查看记录时变得不再明显，比如上面记录中的 e330eac 在图形上要比 f82b57b 更接近 HEAD 提交 ce11d9b，但是因为处在不同的分支上，在合并之前他俩的修改时间还真不一定是哪个更早一些。 树形记录在 git 的提交记录图上，我们可以确定当前提交的父提交（所依赖的提交）是哪一个或者哪几个，但是不能确定任意两个提交的时间先后，为了能更清楚的看清分支提交的依赖关系，还是看下面这个树形图更方便一些。 1234567891011121314151617181920graph TB ce11d9b--&gt;f82b57b; f82b57b--&gt;dcdcb87; dcdcb87--&gt;034ccb6; 034ccb6--&gt;d58fedc; d58fedc--&gt;dev1; dcdcb87--&gt;32d6213; 32d6213--&gt;ca4db4a; ca4db4a--&gt;dev2; ce11d9b--&gt;e330eac; e330eac--&gt;7ab3c98; 7ab3c98--&gt;ccdf16a; ccdf16a--&gt;9f08bb0; 9f08bb0--&gt;dev3; 7ab3c98--&gt;c8795e8; c8795e8--&gt;155d3db; 155d3db--&gt;dev4; 查看命令在验证 HEAD~ 和 HEAD^ 之前我们先学习一个命令 git rev-parse HEAD 这个命令可以显示出 HEAD 对应的提交的 hash 值，加上 --short 参数就可以显示出长度为7位的短 hash，用起来比较方便，测试如下： 1234567albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse HEADce11d9be5cc7007995b607fb12285a43cd03154balbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEADce11d9b 开始测试下面可以用 git rev-parse --short 命令来测试 HEAD 后面跟不同参数时对应的提交是哪一个了，测试如下： HEAD~、HEAD^、HEAD~1、HEAD^1123456789101112131415albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~f82b57balbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^f82b57balbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~1f82b57balbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^1f82b57b 测试后发现，这四种写法结果是一样的，都是指向 HEAD 的第一个父提交，这和我们前面说的观点一致。 HEAD~~、HEAD^^、HEAD~2、HEAD^2123456789101112131415albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~~dcdcb87albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^^dcdcb87albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~2dcdcb87albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^2e330eac 这次我们发现，前三个表示方法是一样的，指向同一个提交记录，但是最后一个与他们不同，这时根据前面提到定义来看就行了，HEAD~~ 实际上是 HEAD~1~1的简写，而~ 后的数字就是指的后退的步数，所以 HEAD~~ 等价于 HEAD~2，属于一种合并计算。 HEAD^^ 是 HEAD^1^1 的简写，而 ^ 后面的数字表示后退一步到第几个父提交上，因为数字是1，所以 HEAD^^ 表示退一步到第一个父提交上，在退一步到第一个父提交上，这时与 HEAD~~ 的作用是相同的。 HEAD^2 就有些不同了，它表示后退一步到第二个父提交上，所以对照树形图是第二排的第二个节点。 ~ 和 ^ 混合使用看了上面的例子对于 ~ 和 ^ 的使用应该有些明白了，它俩其实可以组合使用的，比如想退到第5排、第2个节点上，也就是 ca4db4a 上，简单来看需要第一步到第一个父提交上，在退一步到第一个父提交上，然后退一步到第二个父提交上，最后退一步到第一个父提交上。 那么我们根据需求可以写成 HEAD^1^1^2^1，测试一下看看 hash 是否正确： 123albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^1^1^2^1ca4db4a 测试发现没有问题，其实还可以合并啊，我们知道1是可以省略的，所以可以简写成 HEAD^^^2^，另外多个 ^ 还可以写成 ~n 的形式，所以这个节点还可以表示成 HEAD~2^2^的样子，测试如下，结果是一样的。 1234567albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^^^2^ca4db4aalbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~2^2^ca4db4a 关于 git reset 的一点思考刚学习 git reset 的命令时一直认为是一个回退命令，其实学习一段时间之后发现，这个命令其实很符合它的名字，就是一个重置(reset)命令，通过 git reset 命令可以修改 HEAD 指向不同的提交，这个提交甚至都不必是当前分支上的某次提交，测试后发现，只要是版本库中合法提交都可以使用这个命令进行设置，相应的版本库的内容也会发生对应的变化，从这一点来看，它真的太强大了，它可以使你正在开发的 dev 分支瞬间变成 master 分支。 总结 HEAD~ 后面加数字表示后退的步数，每次后退都默认退到第一个父提交上，HEAD~2 表示连退两步。 HEAD^ 后面加数字表示只退一步，但是这一步后退到数字表示的父提交上，HEAD^2 表示退一步到第二个父提交上。 git 在查看多分支提交记录时，日志的先后顺序不代表提交时间的先后顺序。 git reset 命令是一个重置 HEAD 的命令，可以指挥版本库指向任何一个合法提交。 俗话说：人不犯我，我不犯人；可俗话又说：先下手为强，后下手遭殃！俗话说：宁为玉碎，不为瓦全；可俗话又说：留得青山在，不怕没柴烧！…其实只要你变成了那个成功的“俗话”，你说的就是金科玉律，警世哲理！ 2020-5-31 14:51:49]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
        <tag>Git</tag>
        <tag>HEAD</tag>
        <tag>reset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置Beyond Compare 4作为git mergetool来解决git merge命令导致的文件冲突]]></title>
    <url>%2Fblog%2F2020%2F05%2F21%2F%E9%85%8D%E7%BD%AEBeyond-Compare-4%E4%BD%9C%E4%B8%BAgit-mergetool%E6%9D%A5%E8%A7%A3%E5%86%B3git-merge%E5%91%BD%E4%BB%A4%E5%AF%BC%E8%87%B4%E7%9A%84%E6%96%87%E4%BB%B6%E5%86%B2%E7%AA%81%2F</url>
    <content type="text"><![CDATA[前言使用 git merge 命令合并代码的时候可能会产生文件冲突，产生这种冲突的根本原因是文件的同一处同时被多次修改，这种同时修改常体现的不同分支上，当多个分支修改了同一处代码，再合并代码的时候就会产生冲突，因为 git 程序也不知道我们想要保留哪一份修改，这时就需要我们手动修改产生冲突的文件。 当冲突内容很少的时候我们可以打开文本编辑器，找到 &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;、=========== 和 &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 这三行字符包裹的内容就是需要解决冲突的部分，但是当冲突内容特别多时我们还是习惯于通过可视化的工具来处理，Beyond Compare 就是这样一款工具，可以用来比较不同的文本文件、表格文件，还可以比较文件夹内容，之前用着比较习惯，所以在处理 git 冲突的时候也想使用这个工具来做，通过查找技术文档发现了下面的方法。 解决方案鉴于大家都比较急，查找问题时想要直接找到答案，所以我这里直接说明配置步骤，送给不求甚解的小伙伴，也方便今后我可以直接找到，不过配置之前还是要先看一下前提。 前提 在 Windows 上安装了 git 客户端，可以执行 git 命令（废话！没装 git 怎么产生冲突的） 安装了 Beyond Compare 4 这个软件，下载链接很多，自己找一个吧，实在找不到，那就放弃吧（找我要） 配置首先找到 Beyond Compare 的安装路径，比如我的软件安装路径是 D:\mybc4\BComp.exe，然后在 git 命令行客户端中执行下面命令： 1234git config --global merge.tool bc4git config --global mergetool.bc4.cmd "\"D:\\mybc4\\BComp.exe\" \"\$LOCAL\" \"\$REMOTE\" \"\$BASE\" \"\$MERGED\""git config --global mergetool.bc4.trustExitCode truegit config --global mergetool.keepBackup false 至此，git mergetool 就配置完了，当下次冲突的时候，直接使用 git mergetool 命令就可以调用 Beyond Compare 解决冲突文件了，但是你不好奇，这些设置命令都是什么意思吗？为什么执行完这些命令就能调用 Beyond Compare 4 这个软件了，如果你感兴趣可以接下往下看一看。 Beyond Compare这是一款强大的比较工具，前面提到它可以比较文本、比较表格、比较文件夹，但是它的能力不仅限于此，它甚至可以比较MP3、比较图片、比较注册表，我们的目的是调用它的比较功能，但是前提是这款软件允许你调用，如果它不给你提供接口，你就是想调用也得绕上八百个圈才可以。 这一点我们可以查询文档确定，文档是安装软件时自带的，名字为 BCompare.chm，如果找不到，安利你一个叫做 Everything 的软件，装上它以后，电脑中的一切东西都能搜索找到。 这个文档应该很容易找到的，与软件的可执行文件在同一目录，其实我们使用的比较工具应该是 BCompare.exe，但是为什么在配置 git mergetool 的是后用的是 BComp.exe 呢？这一点文档中有写： BCompare.exe: This is the main application. Only one copy will run at a time, regardless of how many windows you have open. If you launch a second copy it will tell the existing copy to start a comparison and exit immediately.BComp.exe: This is a Win32 GUI program. If launched from a version control system, it should work just fine. If launched from a console window, the console (or batch file) will not wait for it. 文档是英文的，但是比较容易理解，总的来说 BCompare.exe 是主程序，BComp.exe 用在版本控制工具中更加优秀，至于文档中提到的主程序只能启动一个副本的说明，我试了一下并不是这样的，但是这不是重点，根据文档建议，我们应该调用 BComp.exe 程序。 关于调用参数，文档中对于每种形式的比较也给出了说明，我们这里只列举两个文件和四个文件这两种参数，两个文件作为参数时常用来对比，我直接使用主程序对比文件就是这种形式，参数格式为 BCompare.exe &quot;C:\Left File.ext&quot; &quot;C:\Right File.ext&quot;，但是使用时我常把文件直接拖拽到软件上进行比较。四个文件作为参数时常用来处理文件冲突，参数类型为 BCompare.exe C:\Left.ext C:\Right.ext C:\Center.ext C:\Output.ext，参数中文件的名字表明处理时的位置和作用，看下面这个图就明白了。 从红框圈定的位置就可以发现和文件的对应关系了，最下面是最终的输出文件，也是我们可以手动修改的文件。 文件冲突及处理产生冲突先看一下 git 仓库的原始情况 12345678910111213141516albert@home-pc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@home-pc MINGW64 /d/gitstart (dev)$ lsREADME.mdalbert@home-pc MINGW64 /d/gitstart (dev)$ cat README.mdlearn git branch commandm2test checkout 在此基础上新建两个分支 dev1 和 dev2 12345678910111213albert@home-pc MINGW64 /d/gitstart (dev)$ git checkout -b dev1Switched to a new branch 'dev1'albert@home-pc MINGW64 /d/gitstart (dev1)$ git checkout -b dev2Switched to a new branch 'dev2'albert@home-pc MINGW64 /d/gitstart (dev2)$ git branch | grep dev dev dev1* dev2 在 dev2 分支上修改 README.md 文件后提交 12345678910111213141516171819albert@home-pc MINGW64 /d/gitstart (dev2)$ echo "this is dev2 test"&gt;&gt;README.mdalbert@home-pc MINGW64 /d/gitstart (dev2)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@home-pc MINGW64 /d/gitstart (dev2)$ git commit -m"update readme at dev2"[dev2 d8d80b7] update readme at dev2 1 file changed, 1 insertion(+)albert@home-pc MINGW64 /d/gitstart (dev2)$ cat README.mdlearn git branch commandm2test checkoutthis is dev2 test 切换回 dev1 分支修改 README.md 文件后提交 1234567891011121314151617181920212223albert@home-pc MINGW64 /d/gitstart (dev2)$ git checkout dev1Switched to branch 'dev1'albert@home-pc MINGW64 /d/gitstart (dev1)$ echo "this is dev1 test"&gt;&gt;README.mdalbert@home-pc MINGW64 /d/gitstart (dev1)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directorygit com -albert@home-pc MINGW64 /d/gitstart (dev1)$ git commit -m"update readme at dev1"[dev1 3136341] update readme at dev1 1 file changed, 1 insertion(+)albert@home-pc MINGW64 /d/gitstart (dev1)$ cat README.mdlearn git branch commandm2test checkoutthis is dev1 test 这时在 dev1 分支上合并 dev2 分支上的修改就会产生冲突 1234567891011121314151617181920albert@home-pc MINGW64 /d/gitstart (dev1)$ git merge dev2Auto-merging README.mdCONFLICT (content): Merge conflict in README.mdAutomatic merge failed; fix conflicts and then commit the result.albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ cat README.mdlearn git branch commandm2test checkout&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADthis is dev1 test=======this is dev2 test&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev2albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ lsREADME.md 冲突产生了，文档中同一位置被两个分支修改后合并导致的，内容里出现了 &lt;&lt;&lt;、===、&gt;&gt;&gt;，包裹的内容被分成了两部分，上面一部分是当前分支修改的，下面一部分是从 dev2 分支合并过来的，还要注意虽然产生了产生了冲突，但是目录中并没有产生其他多余的文件。 解决冲突这样的冲突比较简单，我们只要使用文本工具删除不想要的内容，保存后 git add README.md，然后再 git commit 就完成了冲突的解决，但是因为配置了 git mergetool，我们可以用它来解决冲突，直接在命令行敲命令 git mergetool 就可以: 12345678albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ git mergetoolMerging:README.mdNormal merge conflict for 'README.md': &#123;local&#125;: modified file &#123;remote&#125;: modified file 这时光标不会退出，一闪一闪并且打开 BComp.exe 工具，截图如下： 这时如果你打开 git 库所在目录会发现除了 README.md 还多了下面4个文件： 12345README.mdREADME_BACKUP_584.mdREADME_BASE_584.mdREADME_LOCAL_584.mdREADME_REMOTE_584.md 按照自己的实际情况修改最下面的文件，然后点击箭头所指的保存按钮，关闭 Beyond Compare，查询一下仓库状态 123456789101112albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ git statusOn branch dev1All conflicts fixed but you are still merging. (use "git commit" to conclude merge)Changes to be committed: modified: README.mdalbert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ lsREADME.md 不但冲突文件没有了，还给我们自动执行 git add README.md 命令，我们只需要执行 git commit 就解决完了冲突。 123456789101112albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ git commit[dev1 b348ae6] Merge branch 'dev2' into dev1albert@home-pc MINGW64 /d/gitstart (dev1)$ git adog* b348ae6 (HEAD -&gt; dev1) Merge branch 'dev2' into dev1|\| * d8d80b7 (dev2) update readme at dev2* | 3136341 update readme at dev1|/* 5f4181e (origin/dev, dev) add comments 工具配置的参数含义回过头来再看看 git mergetool 的4句配置到底有什么用 1234git config --global merge.tool bc4git config --global mergetool.bc4.cmd "\"D:\\mybc4\\BComp.exe\" \"\$LOCAL\" \"\$REMOTE\" \"\$BASE\" \"\$MERGED\""git config --global mergetool.bc4.trustExitCode truegit config --global mergetool.keepBackup false git config首先你需要知道 git config 的作用，就是用来配置 git 的，加上了 --global 表示调整全局 git 配置，不加的话就是调整当前库的 git 配置。windows上的全局配置一般在 C:\Users\用户名\.gitconfig，如果你之前用过 git，一般会执行过 git config --global user.name xxx 对吧，这些命令都是来调整 git 配置的，打开这个 .gitconfig 你会看到 123456789101112131415[user] name = albert email = albert@163.com[core] autocrlf = true[alias] st = status adog = &quot;log --all --decorate --oneline --graph&quot;[merge] tool = bc4[mergetool &quot;bc4&quot;] cmd = \&quot;D:\\mybc4\\BComp.exe\&quot; \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; \&quot;$BASE\&quot; \&quot;$MERGED\&quot; trustExitCode = true[mergetool] keepBackup = false 看看最后几行就是我们添加的4项配置，只不过到文件中变成了键值对的形式，经过测试后发现，这些属性最少两级，比如 user.name 、core.autocrlf，最多三级比如 mergetool.bc4.cmd、 mergetool.bc4.trustExitCode，如果级数再多会怎么办，你可以试试 git config --global a.b.c.d.e test，它最终也会被拆成三级如下 12[a &quot;b.c.d&quot;] e = test git mergetool这个需要查一下官方文档了，git mergetool --help 就能打开git官方文档，文档写得真不错，排版格式看着就很舒服。 文档提到添加 --tool-help 选项可以列举可以的合并工具，展示如下 12345678910111213141516171819202122232425262728293031323334353637albert@home-pc MINGW64 /d/gitstart (dev1)$ git mergetool --tool-help'git mergetool --tool=&lt;tool&gt;' may be set to one of the following: vimdiff vimdiff2 vimdiff3 user-defined: bc4.cmd "D:\Program Files\Beyond Compare 4\BComp.exe" "$LOCAL" "$REMOTE" "$BASE" "$MERGED"The following tools are valid, but not currently available: araxis bc bc3 codecompare deltawalker diffmerge diffuse ecmerge emerge examdiff guiffy gvimdiff gvimdiff2 gvimdiff3 kdiff3 meld opendiff p4merge smerge tkdiff tortoisemerge winmerge xxdiffSome of the tools listed above only work in a windowedenvironment. If run in a terminal-only session, they will fail. 这一查才发现，原来 git mergetool 支持的工具有这么多，不过下面这些我都没安装，用一下上面列举的3个，试试 git mergetool --tool=vimdiff，果然打开了一个界面 幸亏不如 Beyond Compare 好用，不然我不是白配置了，不过这些工具确实方便，都不需要配置，只要安装了参数中指定一下就可以用了，比如这个 bc3，我猜它是 Beyond Compare 3，只不过我安装的是 Beyond Compare 4 这个版本。 这些内置工具使用的前提是已经安装了，并且安装软件的目录放在了环境变量 Path 中，如果没有放在这个变量中需要通过 mergetool.&lt;tool&gt;.path 参数来配置，比如我把 Beyond Compare 3 安装在了 D 盘根目录，就可以设置 git config --global mergetool.bc3.path &quot;D:\\&quot;。 我们在可用工具中没有找到 Beyond Compare 4 为什么我们可以用呢？因为 git mergetool 命令还支持自定义合并解决冲突的工具，只要指定 mergetool.&lt;tool&gt;.cmd 就可以调用了，就像 git mergetool --tool-help 查询结果中提到的 user-defined: bc4.cmd &quot;D:\Program Files\Beyond Compare 4\BComp.exe&quot; &quot;$LOCAL&quot; &quot;$REMOTE&quot; &quot;$BASE&quot; &quot;$MERGED&quot;，git mergetool 把 bc4 作为了一个等同于内置合并工具的软件。 再来看看这4句配置的含义： 1234git config --global merge.tool bc4git config --global mergetool.bc4.cmd "\"D:\\mybc4\\BComp.exe\" \"\$LOCAL\" \"\$REMOTE\" \"\$BASE\" \"\$MERGED\""git config --global mergetool.bc4.trustExitCode truegit config --global mergetool.keepBackup false 第一句 git config --global merge.tool bc4 是说把 git mergetool 的默认工具配置成 bc4，如果不指定默认工具在使用时就需要写成 git mergetool --tool=bc4 或者 git mergetool -t bc4 了，可是 bc4 是我们自己起的名字，根本就没有这个名字啊，接着往下看。 第二句 git config --global mergetool.bc4.cmd &quot;\&quot;D:\\mybc4\\BComp.exe\&quot; \&quot;\$LOCAL\&quot; \&quot;\$REMOTE\&quot; \&quot;\$BASE\&quot; \&quot;\$MERGED\&quot;&quot; 指定了工具 bc4 的调用路径和参数，后面的这4个参数都是 git mergetool 命令提供的，依次代表本地修改，被合并分支修改，两端未修改前版本文件，最终合并导出的文本文件。 第三句 git config --global mergetool.bc4.trustExitCode true， 设置为 true 表示信任软件的返回码，并依据返回码确定合并是否成功，如果设置成 false 就会在合并完成后问你是否解决完冲突，设置成 true 会方便很多。 第四句 git config --global mergetool.keepBackup false， 是指定在合并完成后删除备份文件 *.orig，这个文件会在调用 git mergetool 是产生 *.orig 备份文件，成功合并后自动删除就可以了。 思考至此终于弄明白这个 git mergetool 是怎么工作的了，但是想这样一个问题，这个 &lt;tool&gt;.cmd 一定得调用冲突解决工具吗？如果你从头看到这里应该会明白，这里只是给用户提供了一个调用自定义工具的方式，至于你调用什么它是不关心的，你完全可以在 git mergetool 的时候让电脑关机，这些都是可以的，在你明白了原理以后，一切都变得简单了。 总结 Beyond Compare 是一款强大的比较工具，合理的使用可以有效的提升工作效率 git mergetool 内置了很多可以使用的合并工具，并且支持调用自定义的合并工具 git 的官方文档写得真的挺详细，有时间可以多看一看，你会发现很多有意思的功能 急于解决问题时可以不求甚解，解决问题后最好可以明白其中的缘由，这其实就是一种进步 尽管科技很发达，但有些人一旦分开可能真的就是一生不见了]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>git</tag>
        <tag>mergetool</tag>
        <tag>文件冲突</tag>
        <tag>bc4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用c++filt命令还原C++编译后的函数名]]></title>
    <url>%2Fblog%2F2020%2F05%2F16%2F%E4%BD%BF%E7%94%A8c-filt%E5%91%BD%E4%BB%A4%E8%BF%98%E5%8E%9FC-%E7%BC%96%E8%AF%91%E5%90%8E%E7%9A%84%E5%87%BD%E6%95%B0%E5%90%8D%2F</url>
    <content type="text"><![CDATA[前言这个命令功能单一，但是非常强大，可以用来还原C++编译后的函数名，为什么C++的函数名需要单独的命令来还原，因为他们看起来都是这样 _ZNK4Json5ValueixEPKc、这样 _Z41__static_initialization_and_destruction_0ii 或者这样的 _ZN6apsara5pangu15ScopedChunkInfoINS0_12RafChunkInfoEED1Ev，仅通过这一串字母很难知道原函数的名字是什么，参数类型就更难分析了，实际上C++在编译函数时有一套命名函数的规则，每种参数使用什么字母表示都是有约定的，但是通过学习这些约定来还原函数太麻烦了，还好有人编写了 c++filt 命令可以让我们直接得到编译前的函数名，真好…… C++编译后的函数名C++ 编译后的函数名字非常古怪，相比而言 C 语言编译后的函数看起来就正常许多了，extern &quot;C&quot;、函数重载、name mangling 这些知识点都与 C++ 这个奇怪的函数名有些关系，extern &quot;C&quot; 的作用简而言之就是告诉编译器和链接器被“我”修饰的变量和函数需要按照 C 语言方式进行编译和链接，这样做是由于 C++ 支持函数重载，而 C 语言不支持，结果导致函数被 C++ 编译后在符号库中的名字和被 C语言编译后的名字是不一样的，程序编译和连接就会出现问题，此类问题一般出现在 C++ 代码调用 C 语言写的库函数的时候。 而 name mangling 就是实现 C++ 函数重载的一种技术或者叫做方式，要求同名的 C++ 函数参数个数不同或参数类型不同，如果只有返回值类型不同，那么两个函数被认为是相同的函数，无法成功通过编译。接下来我们就来看几个例子，看看 C++ 编译后的函数名有什么变化。 C++和C语言编译后的函数名对比我们来写一段相同的代码，分别使用 gcc 和 g++ 进行编译，从代码到可执行文件需要经历“预处理、编译、汇编、链接”4个步骤，接下来为了看到编译后函数名的不同，我们只进行前两步，生成汇编代码，再来比较不同。 gcc编译simple.c文件123456789101112131415// simple.cint myadd(int a, int b)&#123; return a + b;&#125;int main()&#123; int a = 110; int b = 119; int c = myadd(a, b); return 0;&#125; gcc simple.c -S 生成汇编代码文件simple.s内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 .file "simple.c" .text .globl myadd .type myadd, @functionmyadd:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl %edi, -4(%rbp) movl %esi, -8(%rbp) movl -4(%rbp), %edx movl -8(%rbp), %eax addl %edx, %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size myadd, .-myadd .globl main .type main, @functionmain:.LFB1: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 subq $16, %rsp movl $110, -12(%rbp) movl $119, -8(%rbp) movl -8(%rbp), %edx movl -12(%rbp), %eax movl %edx, %esi movl %eax, %edi call myadd movl %eax, -4(%rbp) movl $0, %eax leave .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE1: .size main, .-main .ident "GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609" .section .note.GNU-stack,"",@progbits g++编译simple.cpp文件123456789101112131415// simple.cppint myadd(int a, int b)&#123; return a + b;&#125;int main()&#123; int a = 110; int b = 119; int c = myadd(a, b); return 0;&#125; g++ simple.cpp -S 生成汇编代码文件simple.s内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 .file "simple.cpp" .text .globl _Z5myaddii .type _Z5myaddii, @function_Z5myaddii:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl %edi, -20(%rbp) movl %esi, -24(%rbp) movl $0, -4(%rbp) movl -20(%rbp), %edx movl -24(%rbp), %eax addl %edx, %eax movl %eax, -4(%rbp) movl -4(%rbp), %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size _Z5myaddii, .-_Z5myaddii .globl main .type main, @functionmain:.LFB1: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 subq $16, %rsp movl $110, -12(%rbp) movl $119, -8(%rbp) movl $0, -4(%rbp) movl -8(%rbp), %edx movl -12(%rbp), %eax movl %edx, %esi movl %eax, %edi call _Z5myaddii movl %eax, -4(%rbp) movl $0, %eax leave .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE1: .size main, .-main .ident "GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609" .section .note.GNU-stack,"",@progbits 虽然只有几行代码，可是生成汇编文件之后变成了50多行，我们只需要关注 myadd() 这个函数编译之后变成了什么就可以了，汇编代码虽然不好读，但是查找一个函数名应该没问题的，对照着上面的代码我们发现，myadd() 这个函数通过 gcc 编译之后的函数名还是 myadd，而通过 g++ 编译之后的函数名变成了 _Z5myaddii，可以明显感觉到最后的两个字母 i 代表的是参数 int，使用 c++filt 命令还原如下： 12$ c++filt _Z5myaddiimyadd(int, int) C++函数重载编译后的函数名对比我们还是在刚才的代码的基础上增加一个参数类型不同的 myadd 函数，修改后的代码如下： 1234567891011121314151617int myadd(int a, int b)&#123; return a + b;&#125;float myadd(float a, float b)&#123; return a + b;&#125;int main()&#123; int c = myadd(110, 119); float d = myadd(52.0f, 13.14f); return 0;&#125; g++ simple.cpp -S 生成汇编代码文件simple.s内容为： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 .file "simple.cpp" .text .globl _Z5myaddii .type _Z5myaddii, @function_Z5myaddii:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl %edi, -4(%rbp) movl %esi, -8(%rbp) movl -4(%rbp), %edx movl -8(%rbp), %eax addl %edx, %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size _Z5myaddii, .-_Z5myaddii .globl _Z5myaddff .type _Z5myaddff, @function_Z5myaddff:.LFB1: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movss %xmm0, -4(%rbp) movss %xmm1, -8(%rbp) movss -4(%rbp), %xmm0 addss -8(%rbp), %xmm0 popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE1: .size _Z5myaddff, .-_Z5myaddff .globl main .type main, @functionmain:.LFB2: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 subq $16, %rsp movl $119, %esi movl $110, %edi call _Z5myaddii movl %eax, -8(%rbp) movss .LC0(%rip), %xmm1 movss .LC1(%rip), %xmm0 call _Z5myaddff movd %xmm0, %eax movl %eax, -4(%rbp) movl $0, %eax leave .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE2: .size main, .-main .section .rodata .align 4.LC0: .long 1095908721 .align 4.LC1: .long 1112539136 .ident "GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609" .section .note.GNU-stack,"",@progbits 这次一共3个函数，生成的汇编代码更长，但是我们一眼就能看见汇编代码中包含 _Z5myaddii 和 _Z5myaddff 两个函数，这就是函数重载的产物，两个参数类型不同的同名函数编译之后生成了不同的名字，_Z5myaddff 函数末尾的两个 f 应该指的就是参数类型 float。 使用c++filt定位问题示例c++filt的作用就是还原函数名字，它可以帮我们查找动态链接库中缺少的函数，还原崩溃堆栈中一大串的函数名字母等等，下面来看一个崩溃堆栈的例子，代码内容尽量简写，只为了说明问题，现实情况可能要复杂的多。 首先定义一个打印函数堆栈的函数，参考之前的总结《linux环境下C++代码打印函数堆栈调用情况》，代码如下： 123456789101112131415161718192021222324252627282930#include &lt;execinfo.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;signal.h&gt;#include &lt;iostream&gt;void show_stack(int nSignal)&#123; static const int MAX_STACK_FRAMES = 12; void *pStack[MAX_STACK_FRAMES]; static char szStackInfo[1024 * MAX_STACK_FRAMES]; char ** pStackList = NULL; int frames = backtrace(pStack, MAX_STACK_FRAMES); pStackList = backtrace_symbols(pStack, frames); if (NULL == pStackList) return; strcpy(szStackInfo, "stack traceback:\n"); for (int i = 0; i &lt; frames; ++i) &#123; if (NULL == pStackList[i]) break; strncat(szStackInfo, pStackList[i], 1024); strcat(szStackInfo, "\n"); &#125; std::cout &lt;&lt; szStackInfo; // 输出到控制台，也可以打印到日志文件中&#125; 再写一段隐藏着崩溃问题的代码： 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;string&gt;class CTest&#123;public: const std::string&amp; get_string() &#123;return s;&#125; void set_string(const std::string&amp; str) &#123;s = str;&#125;private: std::string s;&#125;;void foo(float z)&#123; int *p = nullptr; *p = 110; std::cout &lt;&lt; z;&#125;void test(std::string str)&#123; CTest* pTest = new CTest(); pTest-&gt;set_string("20200517"); const std::string&amp; s = pTest-&gt;get_string(); delete pTest; std::cout &lt;&lt; str &lt;&lt; std::endl; if (s == "20200517") foo(13.14);&#125;void func(int a, int b)&#123; std::string s = std::to_string(a) + std::to_string(b); test(s);&#125;int main()&#123; signal(SIGSEGV, show_stack); func(250, 520); return 0;&#125; 编译运行，果然崩溃了： 12345678910111213$ g++ simple.cpp --std=c++11$ ./a.outstack traceback:./a.out() [0x401aff]/lib/x86_64-linux-gnu/libc.so.6(+0x354b0) [0x7fd5f98b54b0]/lib/x86_64-linux-gnu/libc.so.6(+0x16eff6) [0x7fd5f99eeff6]/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE7compareEPKc+0x3a) [0x7fd5f9f9145a]./a.out() [0x4022b6]./a.out() [0x401d30]./a.out() [0x401e27]./a.out() [0x401ed8]/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0) [0x7fd5f98a0830]./a.out() [0x4019f9] 这时崩溃的堆栈中发现了一个特别长的函数 _ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE7compareEPKc，使用 c++filt 命令来还原函数： 12$ c++filt _ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE7compareEPKcstd::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;::compare(char const*) const 从函数名来看是一个与字符串相关的 compare 函数，查看代码发现是 s == &quot;20200517&quot; 这一句的问题，所以说能确切的知道函数名对我们查找问题来说还是挺有帮助的。 总结 c++filt 命令可以还原 C++ 为实现函数重载采用 name mangling 搞出来的奇奇怪怪的函数名 注册信号回调函数方式：signal(SIGSEGV, show_stack);，SIGSEGV代表无效的内存引用 注意 C 语言和 C++ 在编译后函数命名方式的不同，C 语言不支持严格意义的重载，C++支持 阳光、空气、水，这些真的是好东西，当你真的快要失去它们才意识的到的话就有些晚了…]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>工具</tag>
        <tag>c++filt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[汇编指令入门级整理]]></title>
    <url>%2Fblog%2F2020%2F05%2F09%2F%E6%B1%87%E7%BC%96%E6%8C%87%E4%BB%A4%E5%85%A5%E9%97%A8%E7%BA%A7%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言我们大都是被高级语言惯坏了的一代，源源不断的新特性正在逐步添加到各类高级语言之中，汇编作为最接近机器指令的低级语言，已经很少被直接拿来写程序了，不过我还真的遇到了一个，那是之前的一个同事，因为在写代码时遇到了成员函数权限及可见性的问题，导致他无法正确调用想执行的函数，结果他就开始在 C++ 代码里嵌入汇编了，绕过了种种限制终于如愿以偿，但是读代码的我们傻眼了… 因为项目是跨平台的，代码推送的 Linux 上编译的时候他才发现，汇编代码的语法在 Linux 和 Windows 上居然是不一样的，结果他又用一个判断平台的宏定义“完美”的解决了，最终这些代码肯定是重写了啊，因为可读性太差了，最近在学习左值、右值、左引用和右引用的时候，总是有人用程序编译生成的中间汇编代码来解释问题，看得我迷迷糊糊，所以决定熟悉一下简单的汇编指令，边学习边记录，方便今后忘记了可以直接拿来复习。 什么是汇编语言汇编语言是最接近机器语言的编程语言，引用百科中的一段话解释为： 汇编语言（assembly language）是一种用于电子计算机、微处理器、微控制器或其他可编程器件的低级语言，亦称为符号语言。在汇编语言中，用助记符代替机器指令的操作码，用地址符号或标号代替指令或操作数的地址。汇编语言又被称为第二代计算机语言。 汇编语言产生的原因对于绝大多数人来说，二进制程序是不可读的，当然有能人可以读，比如第一代程序员，但这类人快灭绝了，直接看二进制不容易看出来究竟做了什么事情，比如最简单的加法指令二进制表示为 00000011，如果它混在一大串01字符串中就很难把它找出来，所以汇编语言主要就是为了解决二进制编码的可读性问题。 汇编与二进制的关系换句话来说，汇编语言就是把给机器看的二进制编码翻译成人话，汇编指令是机器指令的助记符，与机器指令是一一对应的关系，是一种便于阅读和记忆的书写格式。有效地解决了机器指令编写程序难度大的问题，并且使用编译器，可以很方便的把汇编程序转译成机器指令程序，比如之前提到的 00000011 加法指令，对应的汇编指令是 ADD，在调用汇编器时就会把 ADD 翻译成 00000011。 寄存器说到汇编指令不得不提到寄存器，寄存器本身是用来存数据的，因为 CPU 本身只负责逻辑运算，数据需要单独储存在其他的地方，但是对于不熟悉寄存器的人来说会有疑惑，数据不是存在硬盘上吗？或者说数据不是存在内存中吗？这些想法都没错，那么寄存器是用来做什么的呢？ 寄存器作用其实硬盘、内存都是用来存储数据的，但是 CPU 的运算速度远高于内存的读写速度，更不用说从硬盘上取数据了，所以为了避免被拖慢速度影响效率，CPU 都自带一级缓存和二级缓存，一些 CPU 甚至增加了三级缓存，从这些缓存中读写数据要比内存快很多，但是还是无法使用飞速运转的 CPU，所以才会有寄存器的存在。 寄存器不是后来增加的，在最初的计算中就已经设计出来，相比而言，多级缓存出现的更晚一些，通常那些最频繁读写的数据都会被放在寄存器里面，CPU 优先读写寄存器，再通过寄存器、缓存跟内存来交换数据，达到缓冲的目的，因为可以通过名称访问寄存器，这样访问速度是最快的，因此也被称为零级缓存。 存取速度比较通过上面的叙述我们可以知道存取速度从高到低分别是: 寄存器 &gt; 1级缓存 &gt; 2级缓存 &gt; 3级缓存 &gt; 内存 &gt; 硬盘，关于它们的存取速度，举个例子很容易就能明白了，比如我们做菜（CPU工作）时，取手中（寄存器）正拿着的肉和蔬菜肯定是最快的，如果没有就需要把案板上（1级缓存）处理好的菜拿过来，如果案板上没有就在更远一点的洗菜池（2级缓存）中找一找，还没找到的话就要到冰箱（3级缓存）中看一看了，这时发现家里真没有，那去楼下的菜店（内存）去买点吧，转了一圈发现没有想要的，最后还是开车去农贸市场（硬盘）买吧。 通过上面这个例子应该能明白它们的速度关系了，既然缓存这么快，为什么不用缓存代替内存，或者将2、3级缓存都换成1级缓存呢？这里边有一个成本问题，速度越快对应着价格越高，如果你买过机械硬盘和固态硬盘应该很容易就理解了。 寄存器分类常用的 x86 CPU 寄存器有8个：EAX 、EBX、ECX、EDX、EDI、ESI、EBP、ESP，据说现在寄存器总数已经超过100个了，等我找到相关资料再来补充，上面这几个寄存器是最常用的，这些名字也常常出现在汇编的代码中。 我们常说的32位、64位 CPU 是指数据总线的宽度或根数，而寄存器是暂存数据和中间结果的单元，因此寄存器的位数也就是处理数据的长度与数据总线的根数是相同的，所以32位 CPU 对应的寄存器也应该是32位的。 常用寄存器用途上面提到大8个寄存器都有其特定的用途，我们以32位 CPU 为例简单说明下这些寄存器的作用，整理如下表： 寄存器 含义 用途 包含寄存器 EAX 累加(Accumulator)寄存器 常用于乘、除法和函数返回值 AX(AH、AL) EBX 基址(Base)寄存器 常做内存数据的指针, 或者说常以它为基址来访问内存. BX(BH、BL) ECX 计数器(Counter)寄存器 常做字符串和循环操作中的计数器 CX(CH、CL) EDX 数据(Data)寄存器 常用于乘、除法和 I/O 指针 DX(DH、DL) ESI 来源索引(Source Index)寄存器 常做内存数据指针和源字符串指针 SI EDI 目的索引(Destination Index)寄存器 常做内存数据指针和目的字符串指针 DI ESP 堆栈指针(Stack Point)寄存器 只做堆栈的栈顶指针; 不能用于算术运算与数据传送 SP EBP 基址指针(Base Point)寄存器 只做堆栈指针, 可以访问堆栈内任意地址, 经常用于中转 ESP 中的数据, 也常以它为基址来访问堆栈; 不能用于算术运算与数据传送 BP 寄存器EAX、AX、AH、AL的关系在上面的图标中每个常用寄存器后面还有其他的名字，它们是同一个寄存器不同用法下的不同名字，比如在32位 CPU 上，EAX是32位的寄存器，而AX是EAX的低16位，AH是AX的高8位，而AL是AX的低8位，它们的对照关系如下: 1234500000000 00000000 00000000 00000000|===============EAX===============|---4个字节 |======AX=======|---2个字节 |==AH===|-----------1个字节 |===AL==|---1个字节 汇编语言指令终于说到汇编常用指令了，因为 linux 和 windows 下的汇编语法是有些不同的，所以下面我们先通过 windows 下的汇编指令来简单学习一下，后续再来比较两者的不同。 数据传送指令 指令 名称 示例 备注 MOV 传送指令 MOV dest, src 将数据从src移动到dest PUSH 进栈指令 PUSH src 把源操作数src压入堆栈 POP 出栈指令 POP desc 从栈顶弹出字数据到dest 算术运算指令 指令 名称 示例 备注 ADD 加法指令 ADD dest, src 在dest基础上加src SUB 减法指令 SUB dest, src 在dest基础上减src INC 加1指令 INC dest 在dest基础上加1 DEC 减1指令 DEC dest 在dest基础上减1 逻辑运算指令 指令 名称 示例 备注 NOT 取反运算指令 NOT dest 把操作数dest按位取反 AND 与运算指令 AND dest, src 把dest和src进行与运算之后送回dest OR 或运算指令 OR dest, src 把dest和src进行或运算之后送回dest XOR 异或运算 XOR dest, src 把dest和src进行异或运算之后送回dest 循环控制指令 指令 名称 示例 备注 LOOP 计数循环指令 LOOP label 使ECX的值减1，当ECX的值不为0的时候跳转至label，否则执行LOOP之后的语句 转移指令 指令 名称 示例 备注 JMP 无条件转移指令 JMP lable 无条件地转移到标号为label的位置 CALL 过程调用指令 CALL labal 直接调用label JE 条件转移指令 JE lable zf =1 时跳转到标号为label的位置 JNE 条件转移指令 JNE lable zf=0 时跳转到标号为label的位置 linux 和 windows 下汇编的区别前面说到 linux 和 windows 下的汇编语法是不同的，其实两种语法的不同和系统不同没有绝对的关系，一般在 linux 上会使用 gcc/g++ 编译器，而在 windows 上会使用微软的 cl 也就是 MSBUILD，所以产生不同的代码是因为编译器不同，gcc 下采用的是AT&amp;T的汇编语法格式，MSBUILD 采用的是Intel汇编语法格式。 差异 Intel AT&amp;T 引用寄存器名字 eax %eax 赋值操作数顺序 mov dest, src movl src, dest 寄存器、立即数指令前缀 mov ebx, 0xd00d movl $0xd00d, %ebx 寄存器间接寻址 [eax] (%eax) 数据类型大小 操作码后加后缀字母，“l” 32位，“w” 16位，“b” 8位（mov dx, word ptr [eax]） 操作数前面加dword ptr， word ptr，byte ptr的格式 （movb %bl %al） 总结 汇编指令是机器指令的助记符，与机器指令是一一对应的 AT&amp;T的汇编语法格式和Intel汇编语法格式的是不同的 常用寄存器：EAX 、EBX、ECX、EDX、EDI、ESI、EBP、ESP 存取速度从高到低分别是: 寄存器 &gt; 1级缓存 &gt; 2级缓存 &gt; 3级缓存 &gt; 内存 &gt; 硬盘 常用的汇编指令：mov、je、jmp、call、add、sub、inc、dec、and、or 如今的每分每秒都是人生，不要总想着将自然发生的事情拖到预定的时刻才进行~]]></content>
      <categories>
        <category>ASM</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>汇编</tag>
        <tag>windows</tag>
        <tag>asm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11在左值引用的基础上增加右值引用]]></title>
    <url>%2Fblog%2F2020%2F05%2F05%2FC-11%E5%9C%A8%E5%B7%A6%E5%80%BC%E5%BC%95%E7%94%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E5%A2%9E%E5%8A%A0%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言右值引用这个词是最开始是学习 easylogging++ 这个日志开源项目的时候遇到的，当时遇到 &amp;&amp; 这样的写法先是一愣，还有这种写法？难道是引用的地址？结果查询资料才明白这叫做右值引用。 右值引用的出现其实右值引用是在 C++11 时增加的新内容，在此之前，引用是没有左值和右值之分的，只存在一种引用，也就是后来 C++11 标准中的左值引用，而右值引用的提出主要是为了解决之前左值引用出现的一些尴尬的问题。 左值和右值说到右值引用需要先了解下左值和右值，这也是我自己学习的过程，之前在 《简单聊聊C/C++中的左值和右值》 这篇笔记中总结过，可以简单理解左值就是放在 = 左边，可以取到地址，可以被赋值的表达式，而右值通常是放在 = 右侧，不能取地址，只能被当成一个“值”的表达式。 右值引用的作用右值引用的出现并不是为了取代左值引用，也不是和左值引用形成对立，而是充分利用右值内容来减少对象构造和析构操作，以达到提高程序代码效率的目的。 也就是说增加右值引用这个特性是为了提高效率，之前的总结中也提到过，在 C++11 中还引入了 std::move() 函数，并用这个函数改写了 std::remove_if() 函数，这就是提高效率的例子。 使用 std::move() 函数意味着放弃所有权，对于一个左值，如果我们明确放弃对其资源的所有权，则可以通过 std::move() 来将其转为右值引用，放弃所有权的这个操作不一定都是方便的，比如 std::auto_ptr 这个第一代的智能指针，就是因为转移了所有权，使用起来不太方便，才在最新标准中被废弃的。但如果你明确要转移所有权，并且合理使用，有时可以有效的提高程序效率。 引用类型的对比在学习使用右值引用之前先复习一下左值引用，对比学习更有利于我们的记忆。 左值引用1234int i = 22;int&amp; j = i;j = 11; 上面这几行代码就是最常见左值引用的例子，变量 j 引用了变量 i 的存储位置，修改变量 j 就修改了变量 i 的值，但是如果引用一个值会怎么样呢？比如下面这行代码： 1int&amp; j = 22; 编译这行代码会得到一个编译错误： error: invalid initialization of non-const reference of type ‘int&amp;’ from an rvalue of type ‘int’ int&amp; j = 22; 像上面这种问题，可以使用常量引用来解决。 常量引用针对上面的编译错误，改成常量引用就可以通过编译了，就像这样： 1const int&amp; j = 22; 使用常量引用来引用数字常量22，可以编译通过是因为内存上产生了临时变量保存了22这个数据，这个临时变量是可以进行取地址操作的，因此变量 j 引用的其实是这个临时变量，相当于下面的这两句： 12const int temp = 22;const int &amp;j = temp; 看到这里我们发现常量引用可以解决引用常量的问题，那么为什么非得新增一个右值引用呢？那是因为使用常引用后，我们只能通过引用来读取数据，无法去修改数据，这在很多情况下是很不方便的。 右值引用常量引用可以使用右值引用来改写，改写之后可以正常编译，并且还可以进行修改： 1int&amp;&amp; j = 22; 这句代码有两个需要注意的点，第一是右值引用是 C++11 中才增加的，所以需要增加 --std=c++11 这个编译选项才能正常编译，第二是右值引用的两个地址符需要连着写成 &amp;&amp;, 如果中间有空格写成 &amp; &amp; 会被认为是引用的引用而导致编译错误，这是不符合语法的。 右值引用的示例前面对引用类型进行了对比，但是还没有发现右值引用的好处，接下来用一个例子来展示一下增加右值引用之前的写法，和使用右值引用的写法，通过对比来了解一下右值引用究竟有什么好处。 我们来实现一个自定义缓冲区，先使用最常见的方法来实现拷贝构造函数和拷贝赋值函数，简单实现如下，功能不太完整，但是可以说明右值引用的作用： 常量引用实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;class CBuffer&#123;public: // 构造函数 CBuffer(int size = 1024): m_size(size) &#123; cout &lt;&lt; "CBuffer(int)" &lt;&lt; endl; m_buffer = new char[size]; &#125; // 析构函数 ~CBuffer() &#123; cout &lt;&lt; "~CBuffer()" &lt;&lt; endl; delete[] m_buffer; m_buffer = nullptr; m_size = 0; &#125; // 拷贝构造 CBuffer(const CBuffer &amp;origin): m_size(origin.m_size) &#123; cout &lt;&lt; "CBuffer(const CBuffer&amp;)" &lt;&lt; endl; m_buffer = new char[origin.m_size]; memcpy(m_buffer, origin.m_buffer, m_size); &#125; // 赋值重载 CBuffer&amp; operator=(const CBuffer &amp;origin) &#123; cout &lt;&lt; "operator=(const CBuffer&amp;)" &lt;&lt; endl; if (this == &amp;origin) return *this; delete[] m_buffer; m_size = origin.m_size; m_buffer = new char[origin.m_size]; memcpy(m_buffer, origin.m_buffer, m_size); return *this; &#125; int get_size() &#123; return m_size; &#125; static CBuffer gen_buffer(const int size) &#123; CBuffer temp_buffer(size); return temp_buffer; &#125;private: char *m_buffer; int m_size;&#125;;int main()&#123; CBuffer b1; CBuffer b2(b1); cout &lt;&lt; "b1.size = " &lt;&lt; b1.get_size() &lt;&lt; endl; cout &lt;&lt; "b2.size = " &lt;&lt; b2.get_size() &lt;&lt; endl; b2 = CBuffer::gen_buffer(100); return 0;&#125; 运行结果是： CBuffer(int)CBuffer(const CBuffer&amp;)b1.size = 1024b2.size = 1024CBuffer(int)operator=(const CBuffer&amp;)~CBuffer()~CBuffer()~CBuffer() 这个例子不具有实用性，只为了说明问题，CBuffer 这个类定义为了拷贝构造函数并且重载了 = 运算符，两个函数参数均使用常量引用的类型，这就是一般的写法。 但是这样实现有一个问题，因为参数是常量引用，所以没办法修改原对象的值，我们看到拷贝构造和赋值重载两个函数中都有申请空间和拷贝的操作，这种操作在操作内存较大的对象是比较耗时，所以应该尽量避免，我们想到可以使用新对象的指针指向旧对象指针来解决，这样就不用拷贝了，可是这样修改会导致两个对象指向同一块内存，这个问题需要解决。 改为左值引用实现报错如果两个对象指向同一块内存，那么对象在析构的时候就会将一块内存释放两次导致奔溃，这时考虑在拷贝构造或者赋值重载时，将原来对象的指针设置成空就可以了，但是参数是常量没有办法修改啊，那我们将 const 关键字去掉试试，将两个函数改成这样： 1234567891011121314151617181920212223// 拷贝构造CBuffer(CBuffer &amp;origin): m_size(origin.m_size)&#123; cout &lt;&lt; "CBuffer(CBuffer&amp;)" &lt;&lt; endl; m_buffer = origin.m_buffer; origin.m_buffer = nullptr; origin.m_size = 0;&#125;// 赋值重载CBuffer&amp; operator=(CBuffer &amp;origin)&#123; cout &lt;&lt; "operator=(CBuffer&amp;)" &lt;&lt; endl; if (this == &amp;origin) return *this; m_buffer = origin.m_buffer; origin.m_buffer = nullptr; origin.m_size = 0; return *this;&#125; 看起来没有什么问题，但是编译的时候会报错： error: invalid initialization of non-const reference of type ‘CBuffer&amp;’ from an rvalue of type ‘CBuffer’ b2 = CBuffer::gen_buffer(100); ^note: initializing argument 1 of ‘CBuffer&amp; CBuffer::operator=(CBuffer&amp;)’ CBuffer&amp; operator=(CBuffer &amp;origin) 这个错误是什么意思呢？其实说的就是在调用 CBuffer::gen_buffer(100); 函数时，会产生一个临时对象，这个临时对象在赋值给 b2 是会调用CBuffer&amp; operator=(CBuffer &amp;origin) 函数，但是这个函数的参数是一个左值引用类型，而临时对象是一个右值，无法绑定到左值引用上，所以报错了。 还有拷贝构造函数也是有相同的问题，当写出类似 b2 = CBuffer(CBuffer(1000)) 类型会产生临时对象的语句时，同样会因为左值引用不能绑定到右值上而报错，这时候就要请出右值引用了。 改为右值引用实现对于赋值重载函数，我们使用右值引用将其改写为: 12345678910111213// 赋值重载CBuffer&amp; operator=(CBuffer &amp;&amp;origin)&#123; cout &lt;&lt; "operator=(CBuffer&amp;&amp;)" &lt;&lt; endl; if (this == &amp;origin) return *this; m_buffer = origin.m_buffer; origin.m_buffer = nullptr; origin.m_size = 0; return *this;&#125; 这时可以正常通过编译，并且只是修改了指针的指向，并没有申请和拷贝另外一份内存。 std::move() 函数如果我们将拷贝构造函数的参数也改成右值引用的形式： 123456789// 拷贝构造CBuffer(CBuffer &amp;&amp;origin): m_size(origin.m_size)&#123; cout &lt;&lt; "CBuffer(CBuffer&amp;)" &lt;&lt; endl; m_buffer = origin.m_buffer; origin.m_buffer = nullptr; origin.m_size = 0;&#125; 编译时就会发现编译错误： error: use of deleted function ‘constexpr CBuffer::CBuffer(const CBuffer&amp;)’ CBuffer b2(b1); ^note: ‘constexpr CBuffer::CBuffer(const CBuffer&amp;)’ is implicitly declared as deleted because ‘CBuffer’ declares a move constructor or move assignment operator class CBuffer 其本质问题就是主函数中 CBuffer b2(b1); 这一句引起的，因为变量 b1 是一个左值，但是拷贝构造函数接受的是右值引用，所以类型不匹配导致了编译错误，这时可以使用 std::move() 函数改成这条语句为 CBuffer b2(std::move(b1)); 就可以正常编译运行了，运行结果为： CBuffer(int)CBuffer(CBuffer&amp;)b1.size = 0b2.size = 1024CBuffer(int)operator=(CBuffer&amp;&amp;)~CBuffer()~CBuffer()~CBuffer() 查看运行结果会发现 b1.size = 0，因为 b1 调用了 std::move() 函数，转移了资源的所有权，内部已经被“掏空”了，所以在明确所有权转移之后，不要再直接使用变量 b1 了。 万能引用听到这个名字就感觉很厉害，什么是万能引用，其实就是可以同时接受左值和右值的引用类型，但是这种完能引用只能发生在推导的情况下，下面给出了一个例子： 12345678910111213141516#include &lt;iostream&gt;using namespace std;template&lt;typename T&gt;void func(T&amp;&amp; val)&#123; cout &lt;&lt; val &lt;&lt; endl;&#125;int main()&#123; int year = 2020; func(year); func(2020); return 0;&#125; 这段代码中 T&amp;&amp; val 就是万能引用，因为是在模板中，类型需要推导，如果是在普通函数中 T&amp;&amp; val 这个形式就是右值引用。 左值引用和右值引用判定的函数文中多次提到左值和右值，可能刚学习这块内容的小伙伴会有些懵，其实 C++ 中提供了判定左值引用和右值引用的函数，头文件为 &lt;type_traits&gt;，函数名为 is_reference、 is_rvalue_reference、 is_lvalue_reference，看名字就可以知道他们的用途，看下面的例子就更清楚了。 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;type_traits&gt;using namespace std;int main()&#123; int i = 22; int&amp; j = i; int&amp;&amp; k = 11; cout &lt;&lt; "i is_reference: " &lt;&lt; is_reference&lt;decltype(i)&gt;::value &lt;&lt; endl; cout &lt;&lt; "i is_lvalue_reference: " &lt;&lt; is_lvalue_reference&lt;decltype(i)&gt;::value &lt;&lt; endl; cout &lt;&lt; "i is_rvalue_reference: " &lt;&lt; is_rvalue_reference&lt;decltype(i)&gt;::value &lt;&lt; endl; cout &lt;&lt; "j is_reference: " &lt;&lt; is_reference&lt;decltype(j)&gt;::value &lt;&lt; endl; cout &lt;&lt; "j is_lvalue_reference: " &lt;&lt; is_lvalue_reference&lt;decltype(j)&gt;::value &lt;&lt; endl; cout &lt;&lt; "j is_rvalue_reference: " &lt;&lt; is_rvalue_reference&lt;decltype(j)&gt;::value &lt;&lt; endl; cout &lt;&lt; "k is_reference: " &lt;&lt; is_reference&lt;decltype(k)&gt;::value &lt;&lt; endl; cout &lt;&lt; "k is_lvalue_reference: " &lt;&lt; is_lvalue_reference&lt;decltype(k)&gt;::value &lt;&lt; endl; cout &lt;&lt; "k is_rvalue_reference: " &lt;&lt; is_rvalue_reference&lt;decltype(k)&gt;::value &lt;&lt; endl; return 0;&#125; 运行结果如下，满足返回1，否则返回0： i is_reference: 0i is_lvalue_reference: 0i is_rvalue_reference: 0j is_reference: 1j is_lvalue_reference: 1j is_rvalue_reference: 0k is_reference: 1k is_lvalue_reference: 0k is_rvalue_reference: 1 总结 右值引用的写法为 T&amp;&amp; val，两个地址符要挨在一起，在模板中被称为万能引用 注意左值引用和右值引用的使用区别，其实本质都是为了减少无效的拷贝 std::move() 函数会转移对象的所有权，转移操作之后将左值转为右值引用，原对象不可再直接使用 可以使用 is_reference、 is_rvalue_reference、 is_lvalue_reference 来判断引用类型 陪伴是最长情的告白，等待是最极致的思念五一离家返工了，心里有些不是滋味，为了家出来奋斗却将“家”抛在了身后，珍惜眼前人吧~]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>左值</tag>
        <tag>右值</tag>
        <tag>左值引用</tag>
        <tag>右值引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单聊聊C/C++中的左值和右值]]></title>
    <url>%2Fblog%2F2020%2F04%2F24%2F%E7%AE%80%E5%8D%95%E8%81%8A%E8%81%8AC-C-%E4%B8%AD%E7%9A%84%E5%B7%A6%E5%80%BC%E5%92%8C%E5%8F%B3%E5%80%BC%2F</url>
    <content type="text"><![CDATA[前言为什么标题要写成简单聊聊，而不是写成什么“C++中左值与右值详解”或者现在很流行的“惊了！看了这一篇左值与右值讲解，他吊打了面试官”，其实带有详解这个词是需要勇气的，最起码要融会贯通之后才敢这么说吧，本来是学习右值引用的，结果涉及到了左值和右值，然后去了解他们历史发现也是有些混乱，操作中又经常涉及到运算符优先级，真是越学越乱了。 问题索性也把右值引用放一边，从头来看看这个左值和右值，其实我跟这两个词一点都不熟，最多就是在编译报错的提示框中看到他们，当然有时候也会看到他们的英文名字 lvalue 和 rvalue，这时候一般就是编译器开始抱怨了，说我写了什么它不能理解的东西，其实嘛，我自己都没完全理解，从现在开始边学边总结了，先展示一个常见报错： 1error: lvalue required as increment operand 这是什么意思，这么绕嘴，左值需要作为增长操作数，请说人话：自增操作需要一个可以赋值的变量作为操作数，需要变量就直说嘛，为什么要左值、右值的把人都绕蒙了。 历史渊源这个世界一直是在变化的，可能之前你一直引以为豪的经验大楼，转眼之间就会倾塌。关于左值和右值的历史，普遍的观点是最初来源于 C 语言，后来被引入到了 C++，但是关于左值和右值的含义和实现却在一直改变和完善，对于它的历史讲解发现一篇总结的比较好的文章 《C/C++ 左值和右值, L-value和R-value》。 这是2012年的一篇文章，文中给出了历史说明依据，最后还举了一些例子来说明 C 和 C++ 关于左值实现的不同，但是实际操作后你会发现，时间的车轮早已向前行进了一大截，文中提到的那些不同，在最新的 gcc 和 g++ 编译器上早已变得相同，文中提到的反例现在看来几乎没有意义了。 简单梳理下，左值的定义最早出现在 《The C Programming Language 》一书中，指的是引用一个对象，放在赋值表达式 = 左边的值。 后来在新的 C 语言标准中提到左值是赋值表达式 = 左边的值或者需要被改变的值，而等号的右边的值被称为右值。左值更好的表达为可以定位的值，而右值是一种表达数据的值，基于这个表述 L-value 可以理解为 locator value，代表可寻址，而 R-value 可以理解为 read value，代表可读取。 不过以上的新解，完全是人们为了理解左值、右值赋予的新含义，从历史发展来看，一开始左值和右值完全就是通过等号的左边和右边来命名的，只不过随着标准的完善和语言的发展、更替，虽然两个名字保留了下来，但是它们的含义却在逐步发生改变，与最初诞生时的 = 左右两边的值这个含义相比，已经相差很多了。 认识左值和右值关于左值右值有几条规则和特点，先列举在这里，后面可以跟随例子慢慢体会： 左值和右值都是指的表达式，比如 int a = 1 中的 a 是左值，++a 是左值, func() 也可能是左值，而 a+1 是右值， 110 也是一个右值。 左值可以放在 = 的左边，右值只能放在 = 的右边，这其中隐含的意思就是左值也能放在 = 的右边，但是右值不能放在 = 的左边。 左值可以取地址，代表着内存中某个位置，可以存储数据，右值仅仅是一个值，不能取地址，或者它看起来是一个变量，但它是临时的无法取地址，例如一个函数的非引用的值返回。 以上规则从定义来看一点也不严谨，比如一个常量定义是可以赋值，后面就不行了，它也可以取地址，但是不能赋值的它到底是左值还是右值，这点其实不用纠结，心里知道这个情况就可以了。 再比如一个普通变量，它原本是一个左值，当用它给其他变量赋值的时候，它又化身为一个右值，这时它也可以取地址，好像与上面的说法相违背了，但是仔细想想真的是这样吗？它只是临时化身为右值，其实是一个左值，所以才可以取地址的。 其实你如果不做学术研究、不斤斤计较，那么完全可以把能够赋值的表达式作为左值，然后把左值以外的表达式看成右值，如果你不熟悉解左值和右值可能根本不会影响你平时的工作和学习，但是了解它有助于我们深入理解一些内置运算符和程序执行过程，以及在出现编译错误的时候及时定位问题。 具体的示例最简单的赋值语句1int age = 18; 这个赋值语句很简单，= 作为分界线，左边的 age 是左值，可以被赋值，可以取地址，它其实就是一个表达式，代表一个可以存储整数的内存地址；右边的 18 也是一个表达式，明显只能作为右值，不能取地址。 118 = age; 这个语句在编译时会提示下面的错误： 1error: lvalue required as left operand of assignment 错误提示显示：赋值语句的左边需要一个左值，显然 18 不能作为左值，它不代表任何内存地址，不能被改变。 如果程序中的表达式都这么简单就不需要纠结了，接着我们往下看一些复杂点的例子。 自增自减运算1++age++; 第一眼看到这个表达式，你感觉它会怎样运算，编译一下，你会发现编译失败了，错误如下： error: lvalue required as increment operand 加个括号试试： 1++(age++) 编译之后会出现相同的错误： error: lvalue required as increment operand 再换一种加括号的方式再编译一次： 1(++age)++ 这次成功编译了，并且输出值之后发现 age 变量增加了两次。 先不考虑左值右值的问题，我们可以从这个例子中发现自增运算的优先级，后置自增 age++ 的优先级要高于前置自增 ++age 的优先级。 现在回过头来看看之前的编译错误，为什么我们加括号改变运算顺序之后就可以正常执行了呢？这其实和自增运算的实现有关。 前置自增前置自增的一般实现，是直接修改原对象，在原对象上实现自增，然后将原对象以引用方式返回： 12345UPInt&amp; UPInt::operator++()&#123; *this += 1; // 原对象自增 return *this; // 返回原对象&#125; 这里一直操作的是原对象，返回的也是原对象的引用，所以前置自增表达式的结果是左值，它引用的是原对象之前所占用的内存。 后置自增后置自增的一般实现，是先将原对象的数据存储到临时变量中，接着在原对象上实现自增，然后将临时变量以只读的方式返回： 123456const UPInt UPInt::operator++(int)&#123; UPInt oldValue = *this; // 将原对象赋值给临时变量 ++(*this); // 原对象自增 return oldValue; // 返回临时变量&#125; 这里返回的是临时变量，在函数返回后就被销毁了，无法对其取地址，所以后置自增表达式的结果是右值，不能对其进行赋值。 所以表达式 ++age++; 先进行后置自增，然后再进行前置自增就报出编译错误了，因为不能修改右值，也不能对右值进行自增操作。 自增表达式赋值前面说到前置自增表达式是一个左值，那能不能对其赋值呢？当然可以！试试下面的语句： 1++age = 20; 这条语句是可以正常通过编译的，并且执行之后 age 变量的值为 20。 函数表达式函数可以作为左值吗？带着这个疑问我们看一下这个赋值语句： 1func() = 6; 可能有些同学会有疑问，这是正常的语句吗？其实它是可以正常的，只要 func() 是一个左值就可以，怎么才能让他成为一个左值呢，想想刚才的前置自增运算可能会给你启发，要想让他成为左值，它必须代表一个内存地址，写成下面这样就可以了。 1234567891011int g;int&amp; func()&#123; return g;&#125;int main()&#123; func() = 100;&#125; 函数 func() 返回的是全局变量 g 的引用，变量 g 是一个可取地址的左值，所以 func() 表达式也是一个左值，对其赋值后就改变了全局变量 g 的值。 那么我们注意到这里 func() 函数返回的是全局变量的引用，如果是局部变量会怎么样呢？ 12345678910int&amp; func()&#123; int i = 101; return i;&#125;int main()&#123; func() = 100;&#125; 上面的代码编译没有错误，但是会产生一个警告，提示返回了局部变量的引用: 1warning: reference to local variable ‘i’ returned [-Wreturn-local-addr] 运行之后可就惨了，直接显示段错误： 1Segmentation fault (core dumped) 改为局部变量之后，func() 函数虽然返回了一个值，但是这个值是一个临时值，函数返回之后该值被销毁，对应的内存空间也不属于它了，所以在最后赋值的时候才会出现段错误，就和我们访问非法内存是产生的错误时一样的。 总结 可以被赋值的表达式是左值，左值可以取地址。 右值应该是一个表示值的表达式，不是左值的表达式都可以看成是右值 后置自增操作符的优先级要高于前置自增操作符，它们是按照从右向左结合的 关于左值和右值的知识点还有很多，后续想到了再补充，我也是边学边总结，如果有错误也欢迎小伙伴们及时指出，我会及时改正的 时刻静下来想想当初为什么出发，不要在现实的汪洋中偏离航向]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>左值</tag>
        <tag>右值</tag>
        <tag>lvalue</tag>
        <tag>rvalue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（九）：替换带有等号=的字符串的子串]]></title>
    <url>%2Fblog%2F2020%2F04%2F18%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E6%9B%BF%E6%8D%A2%E5%B8%A6%E6%9C%89%E7%AD%89%E5%8F%B7-%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[前言今天写这篇记录要解决的问题来源于最近一名读者的提问，之前写过一篇名为《.bat批处理（六）：替换字符串中匹配的子串》的总结文章，结果有读者在评论区提问说，如果想要替换的子串中包含等号 =，那么就无法替换了，问有没有什么办法可以解决。遇到这个问题的第一感觉应该挺好处理的吧，如果批处理程序在替换操作中认为等号 = 比较特殊，那就加个转义字符应该就可以了，但事实却证明这种想法有些天真了。 在尝试多次失败之后，我意识到事情远没有想象的那么简单，开始在网上寻找解决方案，结果有些让人意外，绝大多数人都说这是 SET 命令的执行规则决定的，无法实现这种需求。当要替换的子串中包含 = 时，第一个 = 就会被认为是替换语法中的 =，进而导致无法得到正确的结果，即使是使用转义字符都无法完成正确替换，加入的转义字符会影响匹配，导致替换失败。还有一些人建议用其他工具来完整这种需求，比如记事本的替换功能 O(∩_∩)O。 遇到的问题看了上面的叙述，可能有些小伙伴对我所说的问题还没有太直观的认识，接下来我们举个例子来说一下这个问题究竟是怎样产生的。 0x00 带有 = 的字符串首先需要被替换的字符串中要包含等号，我们来定义一个这样的变量： 1set STR=abcdo=ocar12a=ajdjko=ot 变量的名字是 STR，变量的值是 abcdo=ocar12a=ajdjko=ot，其中包含了三个 =。 0x01 带有 = 的想要被替换的子串确定一下我们想要替换的子串 o=o，假如我们想把它替换成字母 A，按照一般的替换规则X:Y=Z，在 X 串中寻找到 Y 串之后把它替换成 Z 串，实现的代码如下： 1234567@echo offset STR=abcdo=ocar12a=ajdjko=otset RESULT=%STR:o=o=A%echo %RESULT%pause &gt; nul 运行之后的结果是： abcdo=A=o=Acar12a=ajdjko=A=o=At 和我们想法不一样，我们本来想把 o=o 替换成 A，但是从结果来看应该是把 o 替换成了 o=A，原因就是我们选择的被替换中的子串 o=o 包含一个 =，而这个 = 被当成了替换语法 X:Y=Z 中的 =，所以就不对了。 0x02 尝试用转义字符来处理很多语言中都有转义字符，比如 Markdown 语法中的反斜杠 \，在 Markdown 语法中被星号 * 包裹的文字是倾斜的，但是如果想正常的输出一个 * 怎么办呢？就需要在 * 前面加一个反斜杠 \，变成 \*，这样 * 原本的倾斜文字的作用就被转义了，变成了一个普通的输出字符。 在批处理中也有转义字符的概念，它就是 ^，我们知道在批处理中 &gt;、| 等符号都是有特殊用处的，所以不能简单的输出，比如 echo &gt; 是无法输出一个大于号的，要写成 echo ^&gt; 才能正常输出一个 &gt; 符号。 我们就利用这个转义字符来告诉替换命令，被替换的子串中的 = 是一个普通字符，不能作为替换规则的一部分，所以被替换的子串写成了 o^=o，我们实现下面的代码，看看能不能达到目的： 1234567@echo offset STR=abcdo=ocar12a=ajdjko=otset RESULT=%STR:o^=o=A%echo %RESULT%pause &gt; nul 运行之后结果如下： abcdo=ocar12a=ajdjko=ot 与替换前对比发现没有任何变化，看来转义字符的想法没能帮助我们解决问题，还是想想其他的办法吧。 稳扎稳打的解决方案既然 = 这么特殊，我们就先想办法干掉等号，直接替换的方式不好使，我们可以一个字符一个字符的判断啊，虽然麻烦一点，但是解决问题才是最重要的。 既然要一个个的字符去判断，就需要遍历原字符串，最简单的可以使用字符串分割啊，语法为 原串:~偏移,长度 就可以了，如果不太清楚可以参考一下 《.bat批处理（三）：变量声明、设置、拼接、截取》，截取第一个字符的语法是 原串:~0,1， 截取第二个字符的语法是 原串:~1,1，以此类推。 具体的思路就是我们先判断第一个字符，如果是 = 就进行替换，如果不是 = 就放到结果字符串里，然后继续判断第二个字符进行操作，最后所有的字符处理一遍就完成了替换。 需要使用 goto 语句来写一个循环，代码逻辑比较简单，就是遍历所有字符，是 = 就替换，不是 = 就保留，假设我们先把 = 替换成 #，实现的代码如下： 123456789101112131415161718@echo offset STR=abcdo=ocar12a=ajdjko=otset CURSTR=%STR%set RESULT=:nextif "%CURSTR%" equ "" goto endset a=%CURSTR:~0,1%if "%a%" equ "=" (set RESULT=%RESULT%#) else (set RESULT=%RESULT%%a%)set CURSTR=%CURSTR:~1%goto next:endecho source string is %STR%echo result string is %RESULT%pause &gt; nul :next 是循环的入口，每次截取第一个字符，判断是 = 就在结果中拼接 # 字符，相当于完成了替换，如果字符不是 = ，就将字符直接拼接到结果中，操作之后将原串的第一个字符删除形成新的原串，然后再判断第一个字符，以此类推，直到原串为空，运行结果如下： source string is abcdo=ocar12a=ajdjko=otresult string is abcdo#ocar12a#ajdjko#ot 最终方案事情到了这里好像还没完，在实际操作中有些情况不是替换一个 =，往往是替换的内容中包含 =，上面将 = 替换成 # 不具有通用型，如果是一开始的请求，将 o=o替换成 A 就不能这样写了，就应该是每次判断3个字符了，写起来有些麻烦，批处理中没有获得字符串长度的函数，需要自己实现一个，如果是100个字符的被替换串，那代码就很难写了。 既然 = 都能被我们替换掉，肯定有办法实现上面我们这种将 o=o替换成 A 的要求，下面我们就列举一种通用的处理方法。 0x00 首先将 = 替换成一个原串中不可能出现的字符或者序列这步替换可能最后需要还原的，所以要求我们替换成的目标序列不能在原串中出现，比如我们上面把 = 替换成了 #， 如果原串中有 # 就会弄混了，不能确定是原来字符串中就存在的 #，还是由 = 变成的 #。 这个序列我们可以定义的变态一点，比如把 = 替换成 ###i#am#happy###，我们把它记作 α。 0x01 用这个不能出现序列替换我们之前要查找替换子串中的 =我们之前要查找替换的子串是 o=o，那么替换之后形成 o###i#am#happy###o，我们把它记作 β。 0x02 将第1步结束获得的替换结果作为原串，将其中的 β 替换成 A其实就是把第1步替换完结果作为原串，把其中的 o###i#am#happy###o 也就是原来的 o=o 替换成 A。 0x03 将第3步结果的子串作为原串，将其中的 α 替换为 =这一步就是处理那些虽然是 =，但是这个 = 不是我要替换的结果子串中的，所以要还原 代码实现步骤梳理清楚了，下面来写代码，按照步骤一步步写就可以了： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@echo offrem 第一步set CORESTR=###i#am#happy###set STR=abcdo=ocar12a=ajdjko=otset CURSTR=%STR%set RESULT1=:next1if "%CURSTR%" equ "" goto end1set a=%CURSTR:~0,1%if "%a%" equ "=" (set RESULT1=%RESULT1%%CORESTR%) else (set RESULT1=%RESULT1%%a%)set CURSTR=%CURSTR:~1%goto next1:end1echo source1 string is %STR%echo result1 string is %RESULT1%pause &gt; nulrem 第 2 步set CORESTR=###i#am#happy###set STR=o=oset CURSTR=%STR%set RESULT2=:next2if "%CURSTR%" equ "" goto end2set a=%CURSTR:~0,1%if "%a%" equ "=" (set RESULT2=%RESULT2%%CORESTR%) else (set RESULT2=%RESULT2%%a%)set CURSTR=%CURSTR:~1%goto next2:end2echo source2 string is %STR%echo result2 string is %RESULT2%pause &gt; nulrem 第3步，需要开启延迟变量setlocal ENABLEDELAYEDEXPANSIONset RESULT3=!RESULT1:%RESULT2%=A!echo result3 string is %RESULT3%pause &gt; nulrem 第4步set RESULT4=!RESULT3:%CORESTR%==!echo finally result is %RESULT4% 运行之后的结果为： source1 string is abcdo=ocar12a=ajdjko=otresult1 string is abcdo###i#am#happy###ocar12a###i#am#happy###ajdjko###i#am#happy###otsource2 string is o=oresult2 string is o###i#am#happy###oresult3 string is abcdAcar12a###i#am#happy###ajdjkAtfinally result is abcdAcar12a=ajdjkAt 这次终于替换成功了，o=o 被成功替换成了字母 A，代码中用到了延迟变量，主要是为了实现被替换字符串是变量的情况，不清楚延迟变量的用法可以简单查询一下，至此文章开头提出的问题我们就成功解决了，虽然路途有些坎坷。 总结 批处理程序中的 = 比较特殊，使用常规的 X:Y=Z 的语法不能替换包含 = 的子串 遇到上述情况可以将字符串切割，采用逐个字符比较的方式，将 = 替换成其他字符再进行后续操作 有时候也不必非得使用批处理来替换包含 = 的字符串，随便一个文本工具，比如记事本都可以文本进行替换 如果非得用命令解决，也可以使用从 linux 的 sed 命令移植到 windows 的 sed.exe 程序来很方便的进行替换 使用 sed 命令的语法是 echo abcdo=ocar12a=ajdjko=ot | sed -e &quot;s/o=o/A/g&quot;，一步就可以完成了文章开头的需求了 如果你暂时没有 sed.exe 程序，可以点击这个链接 sed.exe程序 下载，若不是在同一目录使用，记得将命令目录添加到环境变量中 时间慢慢地磨去了年少轻狂，也渐渐地沉淀了冷暖自知。]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11中的时间库std::chrono（引发关于时间的思考）]]></title>
    <url>%2Fblog%2F2020%2F04%2F08%2FC-11%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4%E5%BA%93std-chrono%2F</url>
    <content type="text"><![CDATA[时间都去哪了？还没好好感受年轻就… 前言时间是宝贵的，我们无时无刻不在和时间打交道，这个任务明天下班前截止，你点的外卖还有5分钟才能送到，那个程序已经运行了整整48个小时，既然时间和我们联系这么紧密，我们总要定义一些术语来描述它，像前面说到的明天下班前、5分钟、48个小时都是对时间的描述，程序代码构建的程序世界也需要定义一些术语来描述时间。 今天要总结学习的是 std::chrono 库，它是 C++11 标准时从 boost 库中引入的，其实在 C++ 中还有一种 C 语言风格的时间管理体系，像我们常见的函数 time()、clock()、localtime()、mktime() 和常见的类型 tm、time_t、clock_t 都是 C 语言风格的时间管理体系。 std::chrono 这个库之前接触的不多，C++20 标准都出了，C++11 引入的这个库还没怎么用过，整天和 time()、 localtime()、 tm 打交道，最近工作中换了项目，代码中出现了 std::chrono 的使用，是时候好好学习总结一下了。 chrono 的概况 头文件 #include &lt;chrono&gt; 命名空间 std::chrono 这个库从 C++11 引入标准之后，每个版本都有所修改，不过核心内容变化不是太大，他定义了三种主要类型，分别是 durations、clocks 和 time points，以及围绕这些类型的一些工具函数和衍生的定义。 chrono 的核心内容duration这个模板类用来表示时间间隔，我们知道时间的基本单位是秒，这个类的对象所表示的时间间隔也是以秒为单位的，它的定义如下： 12template&lt;class Rep, class Period = std::ratio&lt;1&gt;&gt;class duration; Rep 表示一种数值类型，用来描述周期 Period 的数值类型，比如可以是 int、float 等，而 Period 的类型是 std::ratio，同样是一个模板类，实际表示的是一个有理数，像100、0、1/1000（千分之一）等等。 在 std 这个命名空间下有很多已经定义好的有理数，可以举几个常见的头文件 &lt;ratio&gt; 中的例子: 12345678nano std::ratio&lt;1, 1000000000&gt; // 十亿分之一micro std::ratio&lt;1, 1000000&gt; // 百万分之一milli std::ratio&lt;1, 1000&gt; // 千分之一centi std::ratio&lt;1, 100&gt; // 百分之一deci std::ratio&lt;1, 10&gt; // 十分之一deca std::ratio&lt;10, 1&gt; // 十hecto std::ratio&lt;100, 1&gt; // 百kilo std::ratio&lt;1000, 1&gt; // 千 比如我们想定义一个整数类型的100秒的时间间隔类型可以使用： 1typedef std::chrono::duration&lt;int, std::ratio&lt;100,1&gt;&gt; my_duration_type; 当然也可以简写成： 1typedef std::chrono::duration&lt;int, std::hecto&gt; my_duration_type; 如果我们想定义一个整数类型1分钟的时间间隔类型可以写成： 1typedef std::chrono::duration&lt;int, std::ratio&lt;60,1&gt;&gt; my_minute_type; 因为这种时、分、秒的时间表示在代码逻辑中很常用，所有在 std::chrono 命名空间下已经定义好了一些时间间隔类型: 123456std::chrono::nanoseconds duration&lt;/*signed integer type of at least 64 bits*/, std::nano&gt;std::chrono::microseconds duration&lt;/*signed integer type of at least 55 bits*/, std::micro&gt;std::chrono::milliseconds duration&lt;/*signed integer type of at least 45 bits*/, std::milli&gt;std::chrono::seconds duration&lt;/*signed integer type of at least 35 bits*/&gt;std::chrono::minutes duration&lt;/*signed integer type of at least 29 bits*/, std::ratio&lt;60&gt;&gt;std::chrono::hours duration&lt;/*signed integer type of at least 23 bits*/, std::ratio&lt;3600&gt;&gt; 另外还有一个很重要的成员函数 count()，用来获得指定的时间间隔对象中包含多少个时间周期，接下来可以写个例子理解一下，我们用 duration 这个模板类来表示一下5分钟和12小时，看看他应该怎么使用，对于5分钟你可以看成是 5 个 1 分钟或者 1 个 5 分钟，或者更变态你可以看成 2.5 个 2 分钟，而 12 小时一般会看成是 12个 1 小时，你当成 0.5 个 1 天也是可以的： 123456789101112131415161718192021222324#include &lt;chrono&gt;#include &lt;iostream&gt;int main()&#123; // 以下为5分钟表达 std::chrono::minutes minute1&#123;5&#125;; // 5个1分钟 std::chrono::duration&lt;int, std::ratio&lt;5*60, 1&gt;&gt; minute2&#123;1&#125;; // 1个5分钟 std::chrono::duration&lt;double, std::ratio&lt;2*60, 1&gt;&gt; minute3&#123;2.5&#125;; // 2.5个2分钟 std::cout &lt;&lt; "minutes1 duration has " &lt;&lt; minute1.count() &lt;&lt; " ticks\n" &lt;&lt; "minutes2 duration has " &lt;&lt; minute2.count() &lt;&lt; " ticks\n" &lt;&lt; "minutes3 duration has " &lt;&lt; minute3.count() &lt;&lt; " ticks\n"; // 一下为12小时表达 std::chrono::hours hours1&#123;12&#125;; // 12个1小时 std::chrono::duration&lt;double, std::ratio&lt;60*60*24, 1&gt;&gt; hours2&#123;0.5&#125;; // 0.5个1天 std::cout &lt;&lt; "hours1 duration has " &lt;&lt; hours1.count() &lt;&lt; " ticks\n" &lt;&lt; "hours2 duration has " &lt;&lt; hours2.count() &lt;&lt; " ticks\n"; // 使用 std::chrono::duration_cast&lt;T&gt; 将分钟间隔转化成标准秒间隔 std::cout &lt;&lt; "minutes1 duration has " &lt;&lt; std::chrono::duration_cast&lt;std::chrono::seconds&gt;(minute1).count() &lt;&lt; " seconds\n";&#125; 上述代码中还使用了 std::chrono::duration_cast&lt;T&gt;() 函数，用于各种时间间隔的换算，运行结果如下： 123456minutes1 duration has 5 ticksminutes2 duration has 1 ticksminutes3 duration has 2.5 tickshours1 duration has 12 tickshours2 duration has 0.5 ticksminutes1 duration has 300 seconds clock从名字可以看出这个类叫做时钟，时钟是用来看时间和计时的，常用的两个类是 system_clock 和 steady_clock，在 C++20 标准中又加入了多种内容，现在我们先来看看这两个常用类。 从这一部分开始类的定义让人有些迷糊，其实 clock 引用了 std::chrono::duration 和后面要说的 std::chrono::time_point， 而 std::chrono::time_point 又引用了 std::chrono::duration 和现在要讲的 std::chrono::system_clock、 std::chrono::steady_clock，如果只看定义很容易被绕晕，所以还是先做个练习实验一下。 system_clock这个类被称为系统内时钟，当修改系统时钟时可能会改变其单调递增的性质，静态成员函数有 now()、to_time_t()、from_time_t() 三个，关于它的单调性被修改举个例子，一般认为时间一直是递增的，但是当你现在调用一次函数 now()，然后把时间往过去调1天，然后再调用 now() 函数，就会发现新得到的时间“变小”了。 也因为这样它会受到 NTP（Network Time Protocol，网络时间协议）的影响，但是不会受时区和夏令时的影响（其实很多国家早就废除夏令时了）。 下面写个例子练习一下，例子中使用了 now()、to_time_t()、from_time_t() 三个函数，不清楚的时候可以对照一下： 12345678910111213141516171819202122232425262728293031#include &lt;chrono&gt;#include &lt;iostream&gt;int main()&#123; std::chrono::duration&lt;int, std::ratio&lt;60*60*24&gt; &gt; one_day(1); // 根据时钟得到现在时间 std::chrono::system_clock::time_point today = std::chrono::system_clock::now(); std::time_t time_t_today = std::chrono::system_clock::to_time_t(today); std::cout &lt;&lt; "now time stamp is " &lt;&lt; time_t_today &lt;&lt; std::endl; std::cout &lt;&lt; "now time is " &lt;&lt; ctime(&amp;time_t_today) &lt;&lt; std::endl; // 看看明天的时间 std::chrono::system_clock::time_point tomorrow = today + one_day; std::time_t time_t_tomorrow = std::chrono::system_clock::to_time_t(tomorrow); std::cout &lt;&lt; "tomorrow time stamp is " &lt;&lt; time_t_tomorrow &lt;&lt; std::endl; std::cout &lt;&lt; "tomorrow time is " &lt;&lt; ctime(&amp;time_t_tomorrow) &lt;&lt; std::endl; // 计算下个小时时间 std::chrono::system_clock::time_point next_hour = today + std::chrono::hours(1); std::time_t time_t_next_hour = std::chrono::system_clock::to_time_t(next_hour); std::chrono::system_clock::time_point next_hour2 = std::chrono::system_clock::from_time_t(time_t_next_hour); std::time_t time_t_next_hour2 = std::chrono::system_clock::to_time_t(next_hour2); std::cout &lt;&lt; "tomorrow time stamp is " &lt;&lt; time_t_next_hour2 &lt;&lt; std::endl; std::cout &lt;&lt; "tomorrow time is " &lt;&lt; ctime(&amp;time_t_next_hour2) &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 12345678now time stamp is 1586662332now time is Sun Apr 12 11:32:12 2020tomorrow time stamp is 1586748732tomorrow time is Mon Apr 13 11:32:12 2020tomorrow time stamp is 1586665932tomorrow time is Sun Apr 12 12:32:12 2020 steady_clock这是一个单调时钟，一旦启动之后就与系统时间没有关系了，完全根据物理是时间向前移动，成员函数只有一个 now()，通常可以用来计时，使用方法与 system_clock 相比简单许多，下面写个小例子。 123456789101112131415161718#include &lt;chrono&gt;#include &lt;iostream&gt;int main()&#123; // 先记录程序运行时间 std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); volatile int nDstVal, nSrcVal; for (int i = 0; i &lt; 1000000000; ++i) nDstVal = nSrcVal; // 做差值计算耗时 std::chrono::duration&lt;double&gt; duration_cost = std::chrono::duration_cast&lt; std::chrono::duration&lt;double&gt; &gt;(std::chrono::steady_clock::now() - start); std::cout &lt;&lt; "total cost " &lt;&lt; duration_cost.count() &lt;&lt; " seconds." &lt;&lt; std::endl; return 0;&#125; 运行结果如下：1total cost 1.9424 seconds. time point这个类与 duration 类似，同样是模板类，表示具体的时间点，比如今天 18:00 开饭，明天上午 10:00 发版本，今年 5 月 1 日可能因为疫情不让出去玩了，像这些具体的时间点可以使用 std::chrono::time_point 来表达，它的定义如下： 12template&lt;class Clock, class Duration = typename Clock::duration&gt;class time_point; 首先这个类是在 std::chrono 这个命名空间下，但是你会经常看到以下这种写法： 12std::chrono::system_clock::time_point today = std::chrono::system_clock::now();std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); 好像 time_point 又在 std::chrono::system_clock 和 std::chrono::steady_clock 范围内，实际上这两个范围内的 time_point 引用的是 std::chrono::time point，看看 std::chrono::system_clock 的定义能明白一些。 123456789101112class system_clock &#123;public: using rep = /*see description*/ ; using period = ratio&lt;/*unspecified*/, /*unspecified*/ &gt;; using duration = chrono::duration&lt;rep, period&gt;; using time_point = chrono::time_point&lt;system_clock&gt;; static constexpr bool is_steady = /*unspecified*/ ; static time_point now() noexcept; // Map to C API static time_t to_time_t (const time_point&amp; t) noexcept; static time_point from_time_t(time_t t) noexcept;&#125;; 对照上面的定义可以知道，std::chrono::system_clock::time_point 实际上 std::chrono::time_point&lt;system_clock&gt;，这几个时间类的定义相互引用，看到这一部分的时候一定不要烦躁，一步步推导分析其中的关系。 time_point 这个类有一个成员函数 time_since_epoch() 用来获得 1970-01-01 00:00:00 到 time_point 时间经过的 duration, 返回的 duration 的单位取决于 timepoint 定义时的 duraion 的单位，不过你也可以得到 duration 之后使用 std::chrono::duration_cast&lt;T&gt;() 函数来转化。 12345678910111213141516171819202122232425262728293031#include &lt;chrono&gt;#include &lt;iostream&gt;int main()&#123; // 获得epoch 和 now 的时间点 std::chrono::time_point&lt;std::chrono::system_clock&gt; epoch = std::chrono::time_point&lt;std::chrono::system_clock&gt;&#123;&#125;; std::chrono::time_point&lt;std::chrono::system_clock&gt; now = std::chrono::system_clock::now(); // 显示时间点对应的日期和时间 time_t epoch_time = std::chrono::system_clock::to_time_t(epoch); std::cout &lt;&lt; "epoch: " &lt;&lt; std::ctime(&amp;epoch_time); time_t today_time = std::chrono::system_clock::to_time_t(now); std::cout &lt;&lt; "today: " &lt;&lt; std::ctime(&amp;today_time); // 显示duration的值 std::cout &lt;&lt; "seconds since epoch: " &lt;&lt; std::chrono::duration_cast&lt;std::chrono::seconds&gt;(epoch.time_since_epoch()).count() &lt;&lt; std::endl; std::cout &lt;&lt; "today, ticks since epoch: " &lt;&lt; now.time_since_epoch().count() &lt;&lt; std::endl; std::cout &lt;&lt; "today, hours since epoch: " &lt;&lt; std::chrono::duration_cast&lt;std::chrono::hours&gt;(now.time_since_epoch()).count() &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 12345epoch: Thu Jan 1 08:00:00 1970today: Sun Apr 12 12:30:04 2020seconds since epoch: 0today, ticks since epoch: 1586665804624992500today, hours since epoch: 440740 从运行结果来看，epoch 的时间点是 Thu Jan 1 08:00:00 1970，为什么不是 1970-01-01 00:00:00 呢？那是因为我们在东8区，格林威治时间为1970-01-01 00:00:00 的时候，我们的时间就是 Thu Jan 1 08:00:00 1970，这样看来 std::ctime() 这个函数考虑了时区的影响，相同的代码如果在韩国同时运行得到的可能就是 epoch: Thu Jan 1 09:00:00 1970。 关于时间的思考思考一个问题，时间是不是一种不变的量，或者换一种说法，它是不是一种均匀的量。如果了解过《三体》中的部分章节，你就会发现时间总在被任意改变着。但是在现实生活中好像时间就是一个标准，我们认为它是一成不变的，总是感觉今天的1天和昨天的24小时在时间上是等同的，今年的这一年和去年的365天是等同的，但其实你了解一下闰年、闰秒、夏令时就会发现，前面提到的这些未必等同。 日常生活中对时间的描述只是为了理解和阐明一些事物，我们把太阳升到头顶叫做中午，把地球自转一圈叫做一天24小时，把地球围绕太阳公转一圈叫做1年365天，但是地球自转不是那么均匀的，也就是说每转一圈占用的绝对时间是不一样的，我们现在使用的时钟通常是滴答滴答一秒秒的走着，如果地球自转一圈的时间不是完全相同的，那么建立在这个滴答上的一切时间都是不准确的。 什么是建立在滴答滴答上的时间，我们以滴答一次作为1秒来计算，那么1分钟是60秒，也就是滴答60次，1小时是60分钟，滴答3600次，一天是24小时，滴答86400次，滴答的次数是均匀的，但是自转和公转是不均匀的，那么两个时间就对不上了，所以出现了闰秒、闰年等方法来调整时间，使得我们用来描述生活的时间和周围的环境现象可以一致，不然大约几千年以后就会出现中午12点天上出现月亮的奇观，那时的人们在史书中会发现我们这个时代中午12点挂在天上的是太阳，简直太玄幻。 有没有一种计时可以描述这种不均匀的自转呢？其实我们伟大的古人早已经发明出来了，你一定听说过日晷这种计时工具，它是观测日影记时的仪器，主要是根据日影的在日晷面上的位置，以指定当时的时辰或刻数，是我国古代较为普遍使用的计时仪器。为什么它没有时间不一致的问题？因为它本身就是不均匀的，它是根据自然现象来规定生活中每天的时间的，其实对照现在来说就是每个时辰的滴答数实际上是不一样的。 日晷这种不均匀的计时其实是为了适应天文现象，方便人们的生产生活，所以说现在地球自转一圈是一天，但不一定是86400秒，地球公转一圈是一年，但不一定是365天，后来人们使用电子设备计时，按道理来说应该非常准确，但是因为地球自转、公转的速率都不稳定，这种差距渐渐地会给生活带来困扰，于是又发明了一个折中的协调世界时，会在适当的时候闰秒、闰天，以弥补这种差距。假如你买了一个绝对精准的不联网的电子计时器，但是几年之后你就会发现你的计时器肯定和大家使用的标准时间不一致了。 其实还有一种基于特定铯原子的振荡周期来确定的国际原子时，主要是在时间精度要求较高的航天、通讯、电子等领域，为了保持系统的连续性而使用的，在日常生活中基本不会使用，但是这个时间是相对恒定的，不会去计较天文现象，每一秒都“准确”的流逝着。 时间函数思考现在回过头来再来看这些时间函数，是不是感觉有点不一样了，比如 time(NULL) 这个函数，它返回的是从 1970-01-01 00:00:00 到现在时间的秒数，回忆一下上面关于时间的思考，这个秒数真的是准确的吗？其实你如果理解了上面的内容就能得出结论，它肯定和国际原子时是有出入的。 再考虑下闰秒的影响，假如你实现了一个函数，第一次执行是在0点执行，执行之后你设置了一个86400秒的倒计时，也就是1天的时间，到第二天0点的时候正好又执行，你又设置了一个86400秒的倒计时，但今天正好是闰秒的日子，也就是今天会比昨天多1秒，那么今天的时间到23:59:59的时候就经过了86400秒，也就是说在23:59:59的时候就会执行你写的函数，如果碰到秒杀就尴尬了… 一般的程序开发不用太考虑闰秒的影响，但是如果这一秒的误差出现的宇宙飞船的飞行中，可能会导致几十公里的误差，所以程序员们一定要理解闰秒的可能带来的问题，评估自己所写的代码需不需要处理这种情况。曾经的一次闰秒直接导致了芬兰航空系统的瘫痪，所以一些大型项目还是会提前很长时间就把即将到来的闰秒处理写入到自己的系统中，以应对它带来的危险。 当你认为时间不会倒流的时候，它确实就发生了。我们一般假定时间不会倒流，但是如果你过分依赖这个特性，可能就会导致一些问题，这种情况常常出现设定了自动校准时间的电脑上，电脑的时间走快了，然后到达一定的差距后会触发校准程序，这时就会出现“时间倒流”的现象，比如 time(NULL) 这种依赖于电脑时间的函数，在这种情况下函数返回值就会变小，出现不单调性。 总结 关于时间的操作真的太多了，我居然发现一种名为 operator&quot;&quot;h 的操作符，与数字连用表示小时，有兴趣的话可以自己扩展学习一下。 durations、clocks 和 time points 三种有关时间操作的定义相互之间是有引用的，需要理清其中的关系。 需要了解闰秒、闰年、天文时、原子时、协调时产生的原因，这样就可以做到熟悉原理，心里不慌。 在测试的例子中出现了时区的概念，其实是人们为了生产生活主动创造出来以适应自然现象的。 这里抛出一个疑问，我之前刚接触时晕乎了很久，后来渐渐才明白，有些时间函数的说明中会提到与时区无关，比如 time(NULL)、还有今天学习的 system_clock，但是当我修改电脑时区的时候会发现，这些函数的返回值会发生突变，大家有探究过其中的原因吗？ 我们都是追逐时间奔跑的蝼蚁，改变世界的同时也被时间改变着。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>chrono</tag>
        <tag>时间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10通过带命令行的安全模式清除顽固的广告弹窗文件]]></title>
    <url>%2Fblog%2F2020%2F04%2F02%2FWin10%E9%80%9A%E8%BF%87%E5%B8%A6%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E6%B8%85%E9%99%A4%E9%A1%BD%E5%9B%BA%E7%9A%84%E5%B9%BF%E5%91%8A%E5%BC%B9%E7%AA%97%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言最近电脑开机后偶尔会出现一个弹窗，这种广告弹窗见的多了也就麻木了，本来也没放在心上，随手一关就准备去做其他事情了，但是点击关闭按钮后这个广告弹窗居然还弹出了二次确认框，想想也忍了，毕竟广告商做半天就是为了让你多看几眼，当我用鼠标的光标接近这个确认按钮时，确认框消失了，整个广告页面居然还在！ 一开始我还以为自己手滑点错了，后来试了3、4次之后发现，这个二次确认框从出来到消失不到1秒钟，以我的手速试了这么多次就没点到，这次暴脾气上来了，再也不忍了，我必须找到你是哪个软件的广告页，然后把你干掉！ 战斗经历本以为找到软件直接卸载就完事了，没想到碰上硬茬了，这个软件大有来头，真不是随随便便能搞定的。 查找广告来源这一步比较简单，这个打开的广告页在任务栏上有个图标，鼠标光标放到图标上会显示缩略图，就像下面这样： 在图标上单击右键，然后在弹出菜单中将光标移到最上面的选项继续单击右键，这时会在弹出一个菜单，如果一次不行就多试几次： 这时点击菜单上的属性按钮会弹出这个广告页对应程序的属性页面： 开始删除程序让我找到你了吧，目录是 D:\Program Files (x86)\MyDrivers\DriverGenius\ksoft，看来是驱动精灵软件带来的广告页，直接进入目录删除： 尴尬！提示“你需要提供管理员权限才能删除此文件”，点击“继续”按钮试试： 依旧不行，提示“你需要权限执行此操作”、“你需要Administrators提供的权限才能对此文件进行更改”，真是有点诡异，一个普通软件居然还要管理员权限才能删。 找管理员帮忙没办法了，提权吧！我把沉睡的管理员账号的都开启了，再试一次还是不行，我可是 Administrator 啊，在这个电脑中还有什么是我不能干的吗？ 微软：“你能干什么你说了不算，我说了算！”。 右键单击软件查看属性是不是只读了呢？没有啊！这时我还没意识到它究竟有多难缠，以为简简单单设置几个属性就能把它删掉，于是一拍脑袋决定，这种情况下一般需要修改权限啊，然后在属性面板中点击了“安全”选项卡： 然后点击“高级按钮”，弹出了很多教程中都给出的界面，长成下面这个样子： 这时要点击“更改”开始修改权限了，接着神奇的一幕发生了，当前界面一闪没有弹出修改界面，而原来的“更改”两个字也变成灰色不能再使用了。 有点慌了啊，试试命令行吧，一个 del 强制删除试试，丝毫未动，删除请求被拒绝了： 再试试别的文件，删除一个失败，再试一个又失败，最后发现这个文件夹中，我连个日志文件都删不掉，不仅发出了灵魂拷问，我真的是管理员吗？我的 Administrator 不会是假的吧？打开文件夹左看看、右看看没有发现什么可疑的地方，忽然我发现文件夹外面一层有个齿轮，难道被当成系统配置了，这是什么骚操作？ 看来我的电脑已经被这个软件给控制住了，一个做驱动的，在操作系统启动时早早的被唤醒，牢牢的控制住了局面，设置了一道道钩子，将可能影响到的它生存的途径全部堵死，这可能就是我修改权限时，界面闪了一下就再也修改不了的原因吧。 之前还碰到过一个软件比较坑，也是不让删除，在任务管理器强行关闭时会提示拒绝访问，最后发现一个名称比较相似的服务一直在启动着，然后尝试关闭这个服务，结果有趣的事情发生了，只要我点禁用服务一刷新，它又会重启，不知道还有哪个匿名的进程在默默的帮助它。 进入安全模式没办法了，本来想快速解决战斗，改改权限后直接删除了，哪里想到它这么顽强，既然是涉及到了驱动，谁知道你在系统启动时搞了什么鬼，我就在另一个世界把你搞掉吧，从安全模式启动，让你原来的小算盘只能在硬盘里乖乖的躺着了，说到这怎么有一种从四维空间看三维世界的感觉。 关于怎么进入安全模式的命令行，之前在XP和Win7上好像是开机就可以选，在Win10上开机没有看到，之前也没操作过，不过网络上有大量的教程，我发现其中有两个比较有意思的，一个是要我用U盘做系统盘，然后假装给电脑做系统，在配置界面打开命令行进行设置然后重启，这有点太麻烦了吧。 还有一种更好玩就是让你在电脑启动的时候直接按电源键关机，反复尝试2-3次等电脑感觉到自己异常了，就能看到安全模式的选项了，这就好比让你借梯子你借不到，就在家里放了把火，结果借来了消防队的云梯，有可能损失惨重啊。 这里说一个我感觉最简单的方法吧，使用 Win+R 组合键，调出 windows 运行窗口，然后输入 shutdown /r /o，回车等着电脑重启就可以了。 接着电脑会出现下面这个画面，选择其中的“疑难解答”选项： 然后界面变化进入下面展示的“高级选项”界面，选择其中的“启动设置”选项： 最后在启动设置的详细界面上选择“重启”按钮，短暂运行之后，电脑上开始出现下面的选项： 这时就可以选择进入系统的模式了，使用 F1~F9 来进行选择，F4 就是进入安全模式，不加载多余的驱动，F6 是带命令提示符的安全模式： 我们可以按键盘上的 F6 选择带命令提示符的安全模式，然后界面上就出现了下面这个“黑框框”： 彻底删除文件有了黑框框就可以删除文件，先通过 cd 命令进入待删除文件所在目录，然后使用 del 命令删除文件： 12345C:\Windows\System32&gt;d:D:\&gt;cd "Program Files (x86)\MyDrivers\DriverGenius\ksoft"D:\Program Files (x86)\MyDrivers\DriverGenius\ksoft&gt;del /f znb.exe 没有任何报错，世界都安静了，输入 shutdown /r /t 0 重启电脑，正常进入操作系统，这时就会发现刚刚统治了我的电脑的可执行程序，已经被我干掉不存在了。 总结 这些顽固广告真是厉害，一般删除文件的办法还真删不掉它们。 不光普通方法删不掉，连一些“流氓卫士”的文件粉碎功能都拿它们没办法。 不过电脑毕竟在用户手中，总有一些非常规办法可以干掉这些不正常的文件。 修理电脑时没有什么是重启电脑不能解决的，如果真的有，那就请你重装系统。 别放弃，坚持朝着目标一步一步的走就好了~]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>del</tag>
        <tag>exe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git stash帮你在切换分支前暂存不想提交的修改]]></title>
    <url>%2Fblog%2F2020%2F03%2F25%2Fgit-stash%E5%B8%AE%E4%BD%A0%E5%9C%A8%E5%88%87%E6%8D%A2%E5%88%86%E6%94%AF%E5%89%8D%E6%9A%82%E5%AD%98%E4%B8%8D%E6%83%B3%E6%8F%90%E4%BA%A4%E7%9A%84%E4%BF%AE%E6%94%B9%2F</url>
    <content type="text"><![CDATA[前言偶然间发现这个命令，正好解决了最近遇到的问题，使用 Git 管理代码时有这样一种场景，你正在分支 branch2 上开发新功能，突然刚刚提交测试的 branch1 分支上报了严重的BUG，需要尽快修改，这时候就需要切换到 branch1 分支上去修复BUG，但是你刚刚在分支 branch2 修改的文件还没有提交，接下来该怎么办？ 如果本地的修改正好到达一个比较完整的阶段，可以直接提交，然后切换分支改BUG，那再好不过了。可是发生这种情况的时候往往是函数写了一半，或者功能大致写完但是还没来得及测试，这样的代码你敢提交吗？我感觉最好还是不要提交吧，那么如果这时候切换分支会有什么后果呢？一般会遇到两种情况：第一种是你在 branch2 分支上所做的修改与 branch1 上做过的修改不冲突，这时切换分支会将本地修改带到 branch1 分支，如果冲突了就是第二种情况，git checkout branch1 命令会被拒绝，当然你可以添加 -f 参数强行切换分支是能成功切换的，代价就是你会丢掉本地的所有修改。 git stash上面提到的切换分支时遇到的两种情况一般都不是我们想要的，之前说过“在 Git 中没有真正的方法来做任何事情，这就是它的妙处！”，但是关于切换分支有这样一个建议，那就是在切换分支时尽量保证你的工作区和暂存区是干净的，而 git stash 命令就是用来做这件事的。 当我们遇到这种状况，本地的修改我不能提交，不想带到新切换的分支，更不想直接丢掉，只想把他们暂存到一个地方，等我切换完分支修改好BUG，再切换回来迎接他们。使用 SVN 想保存本地修改可以使用 patch，而使用 Git 想要解决这种情况更加方便，那就是利用 git stash 命令。 使用方法这个命令的使用方法非常简单，最常用的 git stash push 和 git stash pop 就能应付大部分情况了，其中 push 这个单词还可以省略，使用起来可以说是相当方便了，接下来尝试一下具体用法。 本地有修改时切换分支的两种情况之前提到过这两种情况，一种是将当前分支修改带到要切换的分支，另一种是切换会导致冲突，本次切换操作被拒绝，下面具体操作一下。 将当前分支修改带到要切换的分支首先以 dev 分支为基础新建 feature 分支 12345678910albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ git checkout -b featureSwitched to a new branch 'feature' 在 feature 分支上修改文件，再切换回 dev 分支，可以正常切换，git status 查看状态，发现修改的文件被带到了 dev 分支上 123456789101112131415161718192021222324252627282930albert@homepc MINGW64 /d/gitstart (feature)$ echo "test checkout"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (feature)$ git checkout devSwitched to branch 'dev'M README.mdYour branch is up to date with 'origin/dev'.albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a") 这里要注意一点，在切换到 dev 分支的时候，有一行 M README.md的内容，表示这个文件在切换过来的时候就是修改的。 如果你想要的效果就是这样，就可以直接提交了，比如修改了代码发现分支弄错了，可以这样带着修改的内容切换分支，假设就是这种情况，我们直接在 dev 分支提交修改。 123456789101112131415albert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git commit -m"add comments"[dev 5f4181e] add comments 1 file changed, 1 insertion(+)gitalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree clean 切换分支操作被拒绝上面一种情况，在 feature 分支的修改被带到 dev 分支提交，我们在此基础上切换回 feature 分支看一下： 12345678albert@homepc MINGW64 /d/gitstart (dev)$ git checkout featureSwitched to branch 'feature'albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featurenothing to commit, working tree clean 发现此时 feature 分支上没有任何修改了，我们再改一次，然后切换到 dev 分支上试试： 12345678910111213141516171819albert@homepc MINGW64 /d/gitstart (feature)$ echo "second try"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (feature)$ git checkout deverror: Your local changes to the following files would be overwritten by checkout: README.mdPlease commit your changes or stash them before you switch branches.Aborting 看到了吧，切换分支的操作被拒绝了，原因是这次切换可能导致本地的修改被覆盖，你可以在切换分支前尝试 commit 你的修改或者 stash 你的修改，等等，这里出现了 stash 这个单词，其实之前我都没注意到，不是修改好提交了就是直接加 -f 参数放弃了所做的修改，没想到还有这样神奇 stash 命令帮我渡过难关。 stash 一般操作接下来展示一下 git stash 最常用的操作，也就是标题中提到的——在切换分支前暂存不想提交的修改，继续在上面的环境下操作，现在 feature 分支上修改了 README.md 文件，切换到 dev 分支时因为可能产生冲突而被拒绝，我们先来看一下文件状态。 12345678910111213141516171819202122albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (feature)$ git diffwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directorydiff --git a/README.md b/README.mdindex 76a124a..4c2bfb8 100644--- a/README.md+++ b/README.md@@ -1,2 +1,3 @@ learn git branch command m2+second try 存储临时修改对比显示我们增加了一行，然后执行 git stash 命令，再查看一下文件状态： 12345678910albert@homepc MINGW64 /d/gitstart (feature)$ git stashwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directorySaved working directory and index state WIP on feature: 6ae97d0 Revert "modify README 1"albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featurenothing to commit, working tree clean 执行完 git stash 之后我们发现，刚才的修改不见了，本地状态提示为 nothing to commit, working tree clean，这时我们再来切换分支： 12345678910111213albert@homepc MINGW64 /d/gitstart (feature)$ git checkout devSwitched to branch 'dev'Your branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree clean 这次切换就没有被拒绝，成功的切换到了 dev 分支，你可以在 dev 分支上进行想要的操作，全部操作完成后再切换回 feature 分支，我们这里就不操作了，直接切回 feature 分支查看一下状态： 12345678albert@homepc MINGW64 /d/gitstart (dev)$ git checkout featureSwitched to branch 'feature'albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featurenothing to commit, working tree clean 还原临时修改我们看到工作区很干净，这时如果想还原刚才在 feature 分支的修改，可以使用 git stash pop 命令，我们执行一下然后查看状态： 1234567891011121314151617181920albert@homepc MINGW64 /d/gitstart (feature)$ git stash popOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")Dropped refs/stash@&#123;0&#125; (bd500adb74d57a3d916a89ff2cd4536cf4eaf6ae)albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a") 刚刚被临时暂存的修改又恢复了，我们可以在 feature 分支上继续愉快地开发了。 stash 进阶操作作为这么神奇的命令，stash 肯定不止这么一点点用法，接下来再列举几个较为常用的参数组合： 查看临时存储的所有条目 git stash list当你使用几次 git stash 命令之后就会发现，这个命令有点像建立还原点，所以暂存命令不止可以用一次，当使用多次暂存命令之后就会形成一个暂存列表，这时可以使用 git stash list 命令查看所有的暂存操作，执行命令后大概就是下面的样子： 1234albert@homepc MINGW64 /d/gitstart (feature)$ git stash liststash@&#123;0&#125;: WIP on feature: 84dfd79 add test filestash@&#123;1&#125;: WIP on feature: 6ae97d0 Revert "modify README 1" 这个列表的构成很像一个栈，stash@{0} 是栈顶元素，stash@{1} 是栈顶下面的一个元素，当使用 git stash 命令时会把新的临时存储信息压入栈顶，原来的信息向栈底移动，当使用 git stash pop 命令的时候又会把栈顶的元素弹出，恢复到工作区和暂存区。 临时存储未追踪的新文件 git stash -u在开发过程中新添加的文件不属于任何一个分支，在不冲突的文件情况下也可以在切换分支的时候带到新的分支，默认在使用 git stash 命令的时候不会把这些文件临时存起来，如果想要存起来加上 -u 参数就可以了，执行之后你会发现这个新加的文件在工作区中消失了。 临时存储被忽略的文件 git stash -a被忽略的文件在默认情况下也不会被 git stash 命令存储，想要临时存储这部分文件只要使用 -a 参数就可以了，这样不仅会把忽略的文件临时存储，连未追踪的文件也存储了起来。 stash 操作的标号前面的 git stash list 命令也提到了，使用 git stash 命令的结果会形成一个栈形式的列表，其中 stash@{n} 就是每次临时存储对应的标号，针对于这些标号的操作也有很多，如果不加这些标号默认使用 stash@{0} ，也就是栈顶元素。 查看临时修改的具体内容 git stash show stash@{0}这个查询过程和查询提交日志的形式有点像，主要展示了某次临时存储时改了哪些内容： 123456789albert@homepc MINGW64 /d/gitstart (feature)$ git stash show stash@&#123;0&#125; test.txt | 1 + 1 file changed, 1 insertion(+)albert@homepc MINGW64 /d/gitstart (feature)$ git stash show stash@&#123;1&#125; README.md | 1 + 1 file changed, 1 insertion(+) 恢复指定标号的临时修改 git stash apply stash@{0}在恢复临时存储的修改时不仅可以使用 git stash pop 命令来恢复栈顶那一次修改，也可以按照标号恢复指定的某次修改，测试如下： 123456789101112131415161718192021albert@homepc MINGW64 /d/gitstart (feature)$ git stash apply stash@&#123;1&#125;On branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (feature)$ git stash liststash@&#123;0&#125;: WIP on feature: 84dfd79 add test filestash@&#123;1&#125;: WIP on feature: 6ae97d0 Revert "modify README 1"albert@homepc MINGW64 /d/gitstart (feature)$ git stash apply stash@&#123;1&#125;error: Your local changes to the following files would be overwritten by merge: README.mdPlease commit your changes or stash them before you merge.Aborting 这个命令在执行后，指定标号的修改会被恢复到工作区和暂存区，但是临时存储的列表不会被删除，这时可以尝试再次恢复相同标号的修改到工作区，你会发现本次操作因为修改了相同的文件而被拒绝。 刪除指定标号的临时存储的修改 git stash drop stash@{0}可以在临时存储列表中删除指定标号的一些修改，可以测试一下看看效果： 123456789101112albert@homepc MINGW64 /d/gitstart (feature)$ git stash liststash@&#123;0&#125;: WIP on feature: 84dfd79 add test filestash@&#123;1&#125;: WIP on feature: 6ae97d0 Revert "modify README 1"albert@homepc MINGW64 /d/gitstart (feature)$ git stash drop stash@&#123;1&#125;Dropped stash@&#123;1&#125; (8408e56305fabcd82c1d05db18e177c89c47c5ac)albert@homepc MINGW64 /d/gitstart (feature)$ git stash liststash@&#123;0&#125;: WIP on feature: 84dfd79 add test file 利用临时存储的修改内容新建分支 git stash branch &lt;branchname&gt; [&lt;stash&gt;]一般这种情况就是使用过多次 git stash push 命令，而本地分支还修改了其他内容，直接恢复之前的修改不太合适，所以利用这个命令新建一个分支，分支的内容以指定的存储标号 &lt;stash&gt; 对应的提交 commit-id 为基础，然后应用 &lt;stash&gt; 的修改，实际上就是新建了一个对应 &lt;stash&gt; 的分支，继续之前未完成的工作，&lt;stash&gt; 默认为 stash@{0}，测试如下： 12345678910111213141516albert@homepc MINGW64 /d/gitstart (feature)$ git stash branch feature1Switched to a new branch 'feature1'M README.mdOn branch feature1Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.md modified: test.txtno changes added to commit (use "git add" and/or "git commit -a")Dropped refs/stash@&#123;0&#125; (2922b3eeeda44c98316453b93fcf07c1fcfffca4)albert@homepc MINGW64 /d/gitstart (feature1)$ git stash list 这个操作会消耗掉对应 &lt;stash&gt; 标号的临时存储的内容，将这些内容从存储列表中移除。 stash 的注意事项这个命令不仅可以在同一分支上存储和还原，也可用于不同分支之间，这时就可以有一个应用，当我们发现在错误的分支上开发了代码，可以先 git stash push 将这些修改临时存储起来， 然后切换到正确的分支，再执行 git stash pop 命令将刚才的修改引用到现在的分支上。 git stash push 命令默认是存储工作区和暂存区的修改内容的，但是 git stash pop 命令在还原是默认将所有的修改还原到工作区，如果想还原到对应的暂存区，需要加额外的参数，像这样 git stash pop --index。 总结 这个命令挺有用的，在合作开发的时候经常碰到临时问题需要处理，切换分支暂存一下很方便 感觉这个命令其实和 commit 也很像的，在操作过程中你会发现，它也有自己的 hash-id，但是不会放到 commit 列表中 这个命令参数也有好多个，不过记住常用的就可以面对大多数情况了，简单列举下 git stash push 会将当前本地的修改临时保存起来，push 可以省略 git stash list 查看当前stash push操作的记录 git stash pop 取出最近一次修改，并应用到本地 git stash apply stash@{n} 应用 stash@{n} 对应的修改，但是不删除这条记录 git stash show stash@{n} 展示 stash@{n} 对应的修改的实际修改内容]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>checkout</tag>
        <tag>stash</tag>
        <tag>merge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[没想到C++中的std::remove_if()函数历史还挺悠久]]></title>
    <url>%2Fblog%2F2020%2F03%2F19%2F%E6%B2%A1%E6%83%B3%E5%88%B0C-%E4%B8%AD%E7%9A%84std-remove-if-%E5%87%BD%E6%95%B0%E5%8E%86%E5%8F%B2%E8%BF%98%E6%8C%BA%E6%82%A0%E4%B9%85%2F</url>
    <content type="text"><![CDATA[前言看到 remove 这个单词的第一反应是什么意思？我的第一感觉是删除、去掉的意思，就像一个程序员看到 string 就会说是字符串，而不会说它是线、或者细绳的意思，可是C++里居然有个函数叫 std::remove()，调用完这个函数什么也没删除，这我就奇怪了，打开有道词典查询一下： 不查不要紧，一查吓一跳，以下是词典给出的三个释义： vt. 移动，迁移；开除；调动 vi. 移动，迁移；搬家 n. 移动；距离；搬家 及物动词、不及物动词、名词给出的含义都是移动，只有一个开除的意思和删除有点像，难道我穿越了？我之前一直以为它是删除的意思啊，很多函数还是用它命名的呢！ 赶紧翻翻其他的字典，给高中的英语老师打个电话问问，最终还是在一些释义中找到了删除的意思，还有一些用作删除的例句，有趣的是在有道词典上，所有的单词解释都和移动有关，所有的例句都是和删除有关。 remove_if的历史为什么要查单词的 remove 的意思，当然是被它坑过了，本来想从 std::vector&lt;T&gt; 中删除指定的元素，考虑到迭代器失效的问题，放弃了循环遍历的复杂处理，选择直接使用算法函数 std::remove_if()来进行删除，之前对于 std::remove() 和 std::remove_if() 有过简单的了解，不过记忆还是出现了偏差。 一直记得 std::remove() 函数调用之后需要再使用 erase() 函数处理下，忘记了 std::remove_if() 函数也要做相同的处理，于是在出现问题的时候一度怀疑这个函数的功能发生了变更，开始找这个函数历史迭代的版本，这里推荐一个网站 C++标准函数查询 - std::remove_if()，用来查询函数的定义、所在头文件和使用方法非常方便。 文档中有这样两句： 1) Removes all elements that are equal to value, using operator== to compare them.3) Removes all elements for which predicate p returns true. 解释函数作用时用到的单词都是 remove ，你说神不神奇，这里应该都是取的移动的意思。 这两句话对应的函数声明应该是： 12345template&lt; class ForwardIt, class T &gt;ForwardIt remove( ForwardIt first, ForwardIt last, const T&amp; value ); // (until C++20)template&lt; class ForwardIt, class UnaryPredicate &gt;ForwardIt remove_if( ForwardIt first, ForwardIt last, UnaryPredicate p ); // (until C++20) 这两个函数后面都有相同的说明—— (until C++20) ，意思大概就是说这两个函数一直到 C++20 版本都存在，在我的印象中 std::remove_if() 函数比较新，最起码得比 std::remove() 函数年轻几岁，可是他们到底是哪个版本添加到c++标准的的呢？中途的功能有没有发生变更，继续回忆！ 第一次看到这两个函数应该是在看《Effective STL》这本书的时候，大概是5年前了，正好这个本书就放在手边，赶紧翻目录查一下，打开对应章节发现其中确实提到了删除 std::vector&lt;T&gt; 中的元素时，在调用了这两个函数之后都需要再调用 erase() 函数对待删除的元素进行擦除。 看看书的出版时间是2013年，难道是 C++11 的标准加上的，不对，看一下翻译者写得序，落款时间2003年，不能是 C++03 的标准吧？不过这是一本翻译书籍，再看看原作者 Scott Meyers 写的前言，落款时间2001年，好吧，看来这两个函数肯定在 C++98的版本中就已经存在了，我有点惊呆了，这确实颠覆了我的记忆和认知。 造成这种认知错误主要有两方面原因，第一方面就是受到了开发环境的限制，从一开始学习的时候Turob C 2.0、VC++ 6.0、VS2005、VS2008、VS2010就很少接触 C++11 的知识，Dev-C++ 和 Code::Blocks 也是在特定的情况下使用，没有过多的研究，结果在刚开始工作的时候开发工具居然是VS2003，这个版本我之前都没听说过，还好一步步升级到了08、13、17版本。 第二方面就是这两个函数常常与 Lambda 表达式，auto 关键字一起用，这都是 C++11 里才有的，让人感觉好像这个 std::remove_if() 函数也是 C++11 版本中的内容，造成了错觉。总来说还是用的少，不熟悉，以后多看多练就好了。 remove_if的实现要想更深入的学习 std::remove_if() 函数， 那这个函数实现的细节有必要了解一下，这有助于我们理解函数的使用方法，下面给出两个版本可能的实现方式，也许下面的实现与你查到的不一样，但是思想是相通的，有些实现细节中使用了 std::find_if() 函数，这里没有列举这个版本，下面这两个版本的代码更容易让人明白，它究竟做了哪些事情。 123456789101112131415// C++98 版本template &lt;class ForwardIterator, class UnaryPredicate&gt; ForwardIterator remove_if (ForwardIterator first, ForwardIterator last, UnaryPredicate pred)&#123; ForwardIterator result = first; while (first!=last) &#123; if (!pred(*first)) &#123; *result = *first; ++result; &#125; ++first; &#125; return result;&#125; 123456789101112131415// C++11 版本template &lt;class ForwardIterator, class UnaryPredicate&gt; ForwardIterator remove_if (ForwardIterator first, ForwardIterator last, UnaryPredicate pred)&#123; ForwardIterator result = first; while (first!=last) &#123; if (!pred(*first)) &#123; *result = std::move(*first); ++result; &#125; ++first; &#125; return result;&#125; 对比两段代码有没有发现区别——只改了半行代码，将赋值语句中的 *first 在 C++11 版本中替换成了 std::move(*first)，这只能发生在 C++11 之后，因为 std::move() 函数是 C++11 才加入的。 这代码乍一看挺唬人的，其实仔细分析一下还挺简单的，只是这些符号看起来有些生疏，其实可以把 ForwardIterator 看成一个指针类型，UnaryPredicate 是一个函数类型，我们改写一下: 12345678910111213int* remove_if (int* first, int* last, func_type func)&#123; int* result = first; for (;first!=last;++first) &#123; if (!func(*first)) &#123; *result = *first; ++result; &#125; &#125; return result;&#125; 这代码是不是就比较接地气了，想象一下，一个是包含10个元素的数组，让你删除其中的偶数怎么做？其实就是遍历一遍数组，从开始位置到结束位置逐个判断，如果不是偶数就不进行操作，如果是偶数就把当前的偶数向前移动到结果指针上就好了，结果指针向后移动准备接受下一个奇数，这个判断是不是偶数的函数就是上面代码中的 func()。 最后结果指针 result 停在有效元素后面一个位置上，这个位置到结尾指针 last 的位置上的元素都应该被删除，这就是为什么常常将 std::remove_if() 函数的返回值作为 erase() 函数的第一个参数，而将 last 指针作为 erase() 函数的第二个参数，实际作用就是将这些位置上的元素擦除，从头擦到尾，达到真正删除的目的。 具体使用说了这么多，接下来看看具体怎么用，我们将 std::remove_if() 函数和 erase() 函数分开使用，主要看一下调用 std::remove_if() 函数之后的 vector 中元素的值是怎么变的。 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;bool isEven(int n) // 是否是偶数&#123; return n % 2 == 0;&#125;int main()&#123; std::vector&lt;int&gt; vecTest; for (int i = 0; i &lt; 10; ++i) vecTest.push_back(i); for (int i = 0; i &lt; vecTest.size(); ++i) std::cout &lt;&lt; vecTest[i] &lt;&lt; " "; std::cout &lt;&lt; std::endl; // 移动元素 std::vector&lt;int&gt;::iterator itor = std::remove_if(vecTest.begin(), vecTest.end(), isEven); // 查看移动后的变化 for (int i = 0; i &lt; vecTest.size(); ++i) std::cout &lt;&lt; vecTest[i] &lt;&lt; " "; std::cout &lt;&lt; std::endl; // 删除元素 vecTest.erase(itor, vecTest.end()); for (int i = 0; i &lt; vecTest.size(); ++i) std::cout &lt;&lt; vecTest[i] &lt;&lt; " "; return 0;&#125; 运行结果为： 0 1 2 3 4 5 6 7 8 91 3 5 7 9 5 6 7 8 91 3 5 7 9 从结果可以看出，第二步调用 std::remove_if() 函数之后，vector 中的元素个数并没有减少，只是将后面不需要删除的元素移动到了 vector 的前面，从第二行结果来看，调用 std::remove_if() 函数之后返回的结果 itor 指向5，所以擦除从5所在位置到结尾的元素就达到了我们的目的。 这段代码在 C++98、C++11、C++14 环境下都可以编译运行，在这里推荐一个在线编译器 C++ Shell，可以测试各个版本编译器下运行结果，界面简洁明了，方便测试。 上面的代码其实写得有些啰嗦，如果使用 C++11 语法之后，可以简写为： 1， 运行结果： 0 1 2 3 4 5 6 7 8 91 3 5 7 9 总结 对于模糊的知识要花时间复习，避免临时用到的时候手忙脚乱出问题 对于一些心存疑虑的函数可以看一下具体的实现，知道实现的细节可以让我们更加清楚程序都做了哪些事情 对于新的技术标准可以不精通，但是必须花一些时间进行了解，比如新的 C++ 标准 对于违反常识的代码，先不要否定，即使在你的运行环境中报错，说不定人家是新语法呢？ 曾经看到一段在类的定义时初始化非静态变量的代码，一度认为编译不过，但后来发现在 C++11 中运行的很好]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>历史</tag>
        <tag>删除</tag>
        <tag>remove</tag>
        <tag>remove_if</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python操作Excel工作簿(\*.xlsx)]]></title>
    <url>%2Fblog%2F2020%2F03%2F14%2FPython%E6%93%8D%E4%BD%9CExcel%E5%B7%A5%E4%BD%9C%E7%B0%BF%2F</url>
    <content type="text"><![CDATA[前言Excel 作为流行的个人计算机数据处理软件，混迹于各个领域，在程序员这里也是常常被处理的对象，可以处理 Excel 格式文件的 Python 库还是挺多的，比如 xlrd、xlwt、xlutils、openpyxl、xlwings 等等，但是每个库处理 Excel 的方式不同，有些库在处理时还会有一些局限性。 接下来对比一下几个库的不同，然后主要记录一下 xlwings 这个库的使用，目前这是个人感觉使用起来比较方便的一个库了，其他的几个库在使用过程中总是有这样或那样的问题，不过在特定情况下使用也是挺不错的。 EXCEL文件Excel 被称为电子表格，其实际可以保存的格式分为很多种，但是“Excel 工作簿(*.xlsx)”和“Excel 97-2003 工作簿(*.xls)”是其中比较常用的两种，可以认为 .xls 格式的表格是 03版Excel 之前常用的格式，而 .xlsx 是 03版之后，一般指 07版Excel 之后常用的格式。 一般的 Excel 程序对于上述的两种格式都可以打开编辑，也可以相互转化存储，不过还是建议在没有特殊要求的情况下使用新版本的格式，一方面新的稳定版本可能会修复之前的一些BUG，同时也会带来进行一些优化。 我也是在写这篇总结之前才发现，一个空的 .xlsx 格式的文件大小有 7KB，而一个空的 .xls 格式的文件大小有 24KB，当我分别写入一个相同的汉字后，两个文件大小变成了 10KB 和 30KB，差距还是不小的，还有一个问题就是在将 .xlsx 格式的文件另存为 .xls 格式时还会有兼容性提示，提醒用户有些设置可能会丢失，所以能选新版本还是尽量用新版本吧。 测试环境因为很多应用程序是不断迭代的，相对应的 Python 库也是不断迭代的，这里尽可能的给出版本号，不同的版本可能会有不同的问题： 操作系统: Windows 10 随意版 Python: 3.75 xlrd: 1.2.0 xlwt: 1.3.0 xlutils: 2.0.0 openpyxl: 3.0.3 xlwings: 0.18.0 以上各个程序库使用之前自行安装就行，安装方法就不赘述了，不过可以提供一个可以快速安装镜像源，使用 pip install -i https://pypi.doubanio.com/simple 库名 可以尽可能解决下载安装缓慢的问题。 Excel具体操作关于使用 Python 具体操作 Excel 的方法可以分为三组，配合使用 xlrd、xlwt、xlutils 操作作为第一组，使用库 openpyxl 作为第二组，而 xlwings 作为第三组，这篇总结重点总结 xlwings 的使用，其他两组简单了解。 xlrd、xlwt、xlutils这一组操作 Excel 的库名字很形象，一个读、一个写、一个小工具，凑到一起就可以对 Excel 肆意妄为了，下面做个小练习，打开一个 Excel 文件然后修改第一个单元格的值，再另存为一个新文件，代码如下: 123456789101112131415import xlrdimport xlwtimport xlutils.copydef save_as_new_file(file_name, new_file_name): # 打开Excel文件 rb = xlrd.open_workbook(file_name) # 创建一个可写入的副本 wb = xlutils.copy.copy(rb) # 获得第一个sheet页签 ws = wb.get_sheet(0) # 第一个单元格写入测试值 ws.write(0, 0, 'test value') # 另存为一个新文件 wb.save(new_file_name) 上述代码无论是操作 .xlsx 文件还是操作 .xls 文件都不会报错，但是另存为的 .xlsx 格式的文件会打不开，同时你会发现正常存储的 .xls 文件打开后格式全都没了，怎么办，改个参数试试，将打开文件的代码修改如下： 1rb = xlrd.open_workbook(file_name, formatting_info=True) 其中参数 formatting_info=True 就表示打开Excel时保留原有的格式，但是这是相对于 .xls 格式的文件，对于 .xlsx 格式的文件直接跑出异常 raise NotImplementedError(&quot;formatting_info=True not yet implemented&quot;)，就因为处理不了 .xlsx 格式的文件，我暂时没有使用这几个库操作 Excel。 还有一点，这几个库操作单元格时，行和列的索引是从0开始的。 openpyxl首先说这个库主要用来操作 .xlsx 格式的文件，对于 .xls 格式的文件无法打开，会报 openpyxl does not support the old .xls file format 这样的错误，但是可以存储成这样的格式，再次打开时会有格式不匹配的警告，但是基础的数据还在，所以还是优先用来操作 .xls 格式的文件吧。 写一个新文件的常见用法： 1234567891011121314151617181920from openpyxl import Workbookfrom openpyxl import load_workbookfrom openpyxl.styles import Font, Fill, Alignment, PatternFilldef write_new_excel(file_name): # 创建一个excel文档 wb = Workbook() # 获得当前激活的sheet对象 ws = wb.active # 给A2单元格赋值 ws['A2'] = 'This is A2 cell' # 一行添加多列数据 ws.append([1, 2, 'hello']) # 添加新的sheet ws = wb.create_sheet(title='NewInfo',index=0) # 设置单元格的值 ws['A1'] = 'This is new sheet' # 保存excel wb.save(file_name) 读取和改写一个原有文件的常见用法： 12345678910111213141516171819202122232425262728def read_update_excel(file_name): # 加载Excel表 wb = load_workbook(file_name) # 打印sheet数量 print('sheet count:', len(wb.sheetnames)) # 打印所有sheet名字 print('sheet name list:', wb.sheetnames) # 获取第一个sheet对象 ws = wb[wb.sheetnames[0]] # 打印sheet表行数和列数 print('rows count:', ws.max_row, 'cols count:', ws.max_column) # 更新单元格A1的内容 ws['A1'] = 'this is A1' # 在第二行位置插入一行 ws.insert_rows(2) # 删除第五行 ws.delete_rows(5) # 获取单元格对象，对应B2单元格 cell = ws.cell(2,2) # 设置单元格内容 cell.value = 'this is B2' # 修改字体格式为粗体 cell.font = Font(bold=True) # 修改单元格格式 cell.fill = PatternFill("solid", fgColor="F0CDCD") # 保存原文件或另存一个文件 wb.save(file_name) 使用这个库遇到的情况，存储带有样式的数据没有发现问题，但是当加入一个计算公式后，另存为一个文件时明显文件尺寸变小了，但是数据和公式没有发现有问题。 有资料说处理速度真的很慢，因为我处理的文件比较小，但是没有发现这方面的问题，还有一个问题就是说Excel中的宏全部丢失，这个测试的时候确实是丢了，只不过这个好像和文件格式有关，要想保存宏需要存储为 .xlsm 格式，但是 openpyxl 使用来操作 .xlsx 文件的，存储时会导致宏丢失，强行存储为 .xlsm 格式会导致最终的文件打不开。 还有一点，这个库操作单元格时，行和列的索引是从1开始的。 xlwings这个库在操作的首先要创建一个 App，通过这个创建出来的 App 对象来操作 Excel，非常像把 Excel 的各种操作 api 封装到一起，然后通过这个 App 对象来调用，如果在创建 App 的时候不设置隐藏参数，是会正常打开 Excel 程序的。 使用 xlwings 的基本方式：1234567891011import xlwings as xw# 设置Excel程序不可见app = xw.App(visible=False, add_book=False)# 通过 app 操作 Excel文件# app.bala bala bala .....# app.bala bala bala .....# 优雅的退出app.quit() 创建一个新的 Excel 文件并写入数据：12345678910111213141516def write_new_excel(app, file_name): # 创建新的 Excel 表 wb = app.books.add() # 获取当前活动的sheet ws = wb.sheets.active # 初始化二维区域的值 arr_data = [[1, 2, 3], [4, 5, 6], [7, 8, 'end']] # 设置到新建的Excel中 ws.range('A1:B3').value=arr_data # 设置单独一个单元格的值 ws.range('A4').value='this is A4' # 设置单独一个单元格的值 ws[3,1].value='this is B4' # 保存Excel文件 wb.save(file_name) wb.close() 需要注意的是通过行索引和列索引修改单元格时，起始索引是0。 读入已有 Excel 表格并修改1234567891011121314151617181920212223242526272829303132333435363738394041def read_update_excel(app, file_name): # 加载已有的表格 load_wb = app.books.open(file_name) # 获取Excel表中第一个sheet load_ws = load_wb.sheets[0] # 打印sheet的名字 print(load_ws.name) # 根据sheet名字获取sheet对象 load_ws = load_wb.sheets[load_ws.name] # 获取当前活动的sheet load_ws = load_wb.sheets.active # 获取存在数据的行数和列数 rows = load_ws.api.UsedRange.Rows.count cols = load_ws.api.UsedRange.Columns.count print('rows count:', rows, 'cols count:', cols) # 修改指定单元格数据（A1单元格） load_ws[0,0].value='this is A1' # 有空行或空列时获取准确的行列数量 print(load_ws.used_range.shape) # 从A1单元格开始扩展到非空行空列，最后的行数和列数 print((load_ws.range('A1').expand().last_cell.row, load_ws.range('A1').expand().last_cell.column)) # 从A1单元格开始扩展到非空行空列，最后的行数和列数 print((load_ws.range('A1').expand().last_cell.row, load_ws.range('A1').expand().last_cell.column)) # 从A1单元格开始扩展到非空行空列，最后形状 print(load_ws.range(1,1).expand().shape) # 从A1单元格开始扩展到非空行空列，最后的行数和列数 print((load_ws.range('A1').expand('table').rows.count, load_ws.range('A1').expand('table').columns.count)) # 保存修改后的Excel load_wb.save(file_name) load_wb.close() Excel 增加删除行和列1234567891011121314151617181920def insert_delete_rowscols(app, file_name): # 加载已有的表格 load_wb = app.books.open(file_name) # 获取当前活动的sheet load_ws = load_wb.sheets.active # 从第2行开始插入4行，也就是说2-5行变成新插入的空行 load_ws.api.rows('2:5').insert # 删除第6行和第7行 load_ws.api.rows('6:7').delete # 插入一个单元格，实际测试效果是B列从B2开始向下移动，B2为新添加的单元格 load_ws.range('B2').api.insert # 插入新的一列 load_ws.api.columns('B').insert # 删除一列 load_ws.api.columns('C').delete # 保存修改后的Excel load_wb.save(file_name) load_wb.close() 单元格宽高查询设置与合并12345678910111213141516171819202122232425262728293031def cell_operation(app, file_name): # 加载已有的表格 load_wb = app.books.open(FILE_PATH_ROOT + file_name) # 获取当前活动的sheet load_ws = load_wb.sheets.active # 合并单元格 load_ws.range('A2:A3').api.merge #获取单元格 cell = xw.Range('B2') # 打印单元格所在的行和列 print("row is:", cell.row, "col is:", cell.column) # 打印当前格子的高度和宽度 print("cell.width:", cell.width, "cell.height:", cell.height) # 设置当前格子的高度和宽度 cell.row_height = 32 cell.column_width = 64 # 指定单元格的高度和宽度自适应 cell.columns.autofit() cell.rows.autofit() # 再次打印当前格子的高度和宽度 print("cell.width:", cell.width, "cell.height:", cell.height) # 保存修改后的Excel load_wb.save(file_name) load_wb.close() 几个库支持情况对比虽然前面写了这么多方法，但是遇到一个实际的问题时还是会犹豫，到底用哪种方式呢？下面做一个简单的对比，只是根据我做的实验来简单对比，如果有不准确甚至是错误的地方，欢迎大家指出来，我会尽快改正的。 情景/库 xlrd、xlwt、xlutils openpyxl xlwings 读取.xls 可以带有样式读取 不支持 可以读取 保存.xls 可以带有样式保存 可以保存，但是提示文件扩展名不匹配，可以看到原始数据 可以保存，但是提示文件扩展名不匹配，可以看到原始数据 读取.xlsx 可以读取，但没有样式 可以带有样式读取 可以带有样式读取 保存.xlsx 保存后打不开 可以带有样式保存 可以带有样式保存 读取.xlsm 可以读取，但没有样式和宏 可以读取，但没有宏 可以读取包含宏的表格 保存.xlsm 保存后打不开，存成 .xls 格式宏丢失 保存后打不开，存成 .xls想 格式宏丢失 存储后宏还在 增删行和列 没有直接方法 支持 支持 另存后大小 .xls 文件没有变化 .xlsx 文件会变小 .xls、.xlsx 文件没有变化 使用建议 只操作.xls文件可以考虑 只操作.xlsx文件可以考虑，不能带有宏 一个比较好的选择，使用时感觉速度稍微有点慢 总结 Excel 表格程序经过版本的更替发生了很大的变化，出现了相同内容时 .xls 比 .xlsx 格式的文件大很多的情况 基于上一点考虑，如果能使用的新版的表格，那么就放弃旧的格式的吧 还有一个神奇的情况，一个带有少量数据的 .xlsx 格式的表格要比一个空表格还要小，这是什么情况，暂时没弄明白怎么回事，求知道的大神告知一二]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Excel</tag>
        <tag>xlrd</tag>
        <tag>xlutils</tag>
        <tag>xlwings</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git checkout/git reset/git revert/git restore常用回退操作]]></title>
    <url>%2Fblog%2F2020%2F03%2F03%2Fgit-checkout-git-reset-git-revert-git-restore%E5%B8%B8%E7%94%A8%E5%9B%9E%E9%80%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[在 Git 中没有真正的方法来做任何事情，这就是它的妙处！ 前言经常会听到别人说，如果时光可以倒流，我将会如何如何，可是现阶段的科技还达不到时光倒流的目的，或许在《三体》世界的四维裂缝里可以试一下。现实的世界中找不到后悔药，但是在代码的世界里却可以轻松实现，错误的BUG修改、砍掉的做了一半的功能都可以轻松回退，不留一丝痕迹，回滚之后一切又可以重新开始了。 代码回退大型编程项目的开发往往伴随着版本工具的使用，其实引入代码版本控制工具，有一部分原因也是为了方便回退，回退操作每天都发生的，只是有时是我们显式的操作，有时却自然而然的进行着，我们切换着分支很可能就是从开发版本回退到一个稳定版本，我们查询日志，实际上是在记忆上回退我们整个的开发过程，找寻其中的问题和修改的内容。 Git管理下的各种文件状态Git的使用中，由于一个文件存在好几种状态的变化，所以处理起回退要分情况进行，有些各式各样的命令最终分析起来其实作用是一样的。 说起Git常常会提到工作区、暂存区、版本库的概念，这是很通用的说法，其实工作区一般就是指我们能看到的文件、本地操作文件所在的目录，我们正常编写的代码文件、管理的资源文件都是在工作区里操作，这里的文件也不是全都平等的，又细分为受版本控制的文件和不受版本控制的文件。 提到暂存区就和index文件建立起了联系，工作区的新文件和已经修改的受版本控制的文件，使用 git add file_name 就可以加到暂存区，相当于登记报个名，以后提交到版本库的时候会把这些登记的文件都带上，实际上执行了 git add 命令的文件都生成了对应的 object 对象，放在.git/objects目录下，状态变成了 staged， 当提交到版本库时，分支会引用这些对象。 版本库就是文件修改的目的地了，最终的修改会提交到版本库，这时提交的文件状态变成 committed，其实也是一种 unmodified 状态，一路走来，版本库中记录了你的每一次提交，可以追溯你每一次修改的内容。 其实还有一个远程仓库的概念，一般确定本地仓库的修改没有问题了，或者要将本地代码远程备份时，可以将自己修改的分支推送到远程仓库，因为有时候我们也想回退已经推送到远程仓库的修改，所以这里先提一下远程仓库。 总结起来一个文件的状态通常可以分为： 不受版本控制的 untracked 状态 受版本控制并且已修改的 modified 状态 受版本控制已修改并提交到暂存区的 staged 状态 从暂存区已经提交到本地仓库的 committed 状态 提交到本地仓库未修改或者从远程仓库克隆下来的 unmodified 状态 Git回退命令上面提到了在 Git 这个版本控制工具下文件的各种状态，其实回退操作就是通过命令实现这些文件状态的“倒退”，进而达到回退操作的目的，下面一起先来了解下这些可以实现回退的命令。 git checkout这个命令又出现了，上次是总结 git branch 分支操作的时候，git checkout 可以用来新建或者切换分支，这次总结回退版本的命令，git checkout 也可以用来回退文件版本，很神奇吧。 其实这个命令的作用就是它单词的本义——检出，他的常用操作也取自这个意思，比如 git checkout branch_name 切换分支操作，实际上就是把指定分支在仓库中对应的所有文件检出来覆盖当前工作区，最终表现就是切换了分支。 而针对于文件的检出可以使用 git checkout -- file_name，当不指定 commit id 就是将暂存区的内容恢复到工作区，也就可以达到回退本地修改的作用。 不过，这个身兼数职的 git checkout 命令现在可以轻松一些了，从 Git 2.23 版本开始引入了两个新的命令： git switch 用来切换分支，git restore用来还原工作区的文件，这个后面还会提到。 git revertrevert 这个词的意思是：归还，复原，回退，它和后面即将提到的 restore 在意思上简直无法区分，为了区别他们两个这里可以把 git revert 看成归还的意思，对某次提交执行 git revert 命令就是对这次修改执行一个归还操作，其实就是反向再修改一次。 要理解 git revert 就要从反向修改的含义来看，当我们再一个文件中添加一行内容，并提交到版本库后，产生一个提交id——commit-id-a，如果这时使用 git revert commit-id-a 命令，就相当于在工作区中的那个文件将刚在新加的一行内容删除掉，然后再进行一个提交。 注意，这个操作是会改变分支记录的，因为产生了新的提交。 git restore这个命令是 Git 2.23 版本之后新加的，用来分担之前 git checkout 命令的功能，作用就是用暂存区或者版本库中的文件覆盖本地文件的修改可以达到回退修改的目的，同时也可以使用版本库中的文件覆盖暂存区的文件，达到回退git add 命令的目的。 注意，这个操作是不会影响分支记录的，就是相当于之前的 git checkout 命令重新检出一份文件来覆盖本地的修改。 git resetreset 重新设置的意思，其实就是用来设置分支的头部指向，当进行了一系列的提交之后，忽然发现最近的几次提交有问题，想从提交记录中删除，这是就会用到 git reset 命令，这个命令后面跟 commit id，表示当前分支回退到这个 commit id 对应的状态，之后的日志记录被删除，工作区中的文件状态根据参数的不同会恢复到不同的状态。 --soft: 被回退的那些版本的修改会被放在暂存区，可以再次提交。 --mixed: 默认选项，被回退的那些版本的修改会放在工作目录，可以先加到暂存区，然后再提交。 --hard: 被回退的那些版本的修改会直接舍弃，好像它们没有来过一样。 这样来看，git set 命令好像是用来回退版本的，但是如果使用 git rest HEAD file_name 命令就可以将一个文件回退到 HEAD 指向版本所对应的状态，其实就是当前版本库中的状态，也就相当于还原了本地的修改。 git rm临时插播的命令，本来删除不能算是回退，但是如果它和某些命令反着来就是一种回退，比如对一个新文件使用 git add newfile_name 命令，然后再使用 git rm --cached newfile_name 就可以将这个文件从暂存区移除掉，但是在工作区里没有消失，如果不加 --cached 参数，就会从工作区和版本库暂存区同时删除，相当于执行了 rm newfile_name 和 git add new_file 两条命令。 具体回退操作说了这么多肯定有点懵，特别是一个相同的需求可以使用很多命令来实现的时候，接下来看一些具体需求，整个测试过程用上一篇总结《git branch常用分支操作》使用的 git 仓库来进行，远程地址是 git@gitee.com:myname/gitstart.git，下面测试开始，我们看一下这些情况怎么进行还原： 初始状态12345678910111213141516171819202122albert@homepc MINGW64 /d/gitstart (dev)$ lsREADME.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master remotes/origin/dev remotes/origin/masteralbert@homepc MINGW64 /d/gitstart (dev)$ git branch -vv* dev 3226b63 [origin/dev] add readme file master 3226b63 [origin/master] add readme file 还原00：工作区中未加到暂存区和版本库的文件，还原今天所做的修改实话实说，办不到，没有加到过暂存区就没有被追踪，它的任何修改是没有办法回退的，可是使用 Ctrl+Z 碰碰运气，没准就退回到了你想要的状态。 还原01：工作区中未加到暂存区和版本库的文件，执行了 git add 操作这种情况可以使用git rm --cached newfile、git restore --staged newfile 或者 git reset HEAD newfile 命令，使用后两个命令的时候不能是版本库的第一个文件。 git rm1234567891011121314151617181920212223242526272829303132albert@homepc MINGW64 /d/gitstart (dev)$ echo "test data"&gt;new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git add new.txtwarning: LF will be replaced by CRLF in new.txt.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) new file: new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git rm --cached new.txtrm 'new.txt'albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) new.txtnothing added to commit but untracked files present (use "git add" to track) git restore12345678910111213141516171819202122232425262728albert@homepc MINGW64 /d/gitstart (dev)$ git add new.txtwarning: LF will be replaced by CRLF in new.txt.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) new file: new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git restore --staged new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) new.txtnothing added to commit but untracked files present (use "git add" to track) git reset12345678910111213141516171819202122232425262728albert@homepc MINGW64 /d/gitstart (dev)$ git add new.txtwarning: LF will be replaced by CRLF in new.txt.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) new file: new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git reset HEAD new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) new.txtnothing added to commit but untracked files present (use "git add" to track) 还原02：版本库中的文件，修改或删除后未执行 git add 操作我们直接修改 README.md 文件吧，删除刚才添加的未受版本管理的 new.txt，在 README.md 文件中添加内容，然后试着还原，这种情况常常出现在修改一个功能还未提交，但是先不要求修改了，可以直接还原。 这种情况可以使用git restore file_name、git checkout -- file_name 或者 git reset --hard HEAD 命令，最后的git reset 命令带有 --hard 参数不能再加文件目录，只能将工作区全还原。 git restore123456789101112131415161718192021222324albert@homepc MINGW64 /d/gitstart (dev)$ echo "new line"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git restore README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree clean git checkout123456789101112131415161718192021222324albert@homepc MINGW64 /d/gitstart (dev)$ echo "new line"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git checkout -- README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree clean git reset1234567891011121314151617181920212223242526272829albert@homepc MINGW64 /d/gitstart (dev)$ echo "new line"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git reset --hard HEAD README.mdfatal: Cannot do hard reset with paths.albert@homepc MINGW64 /d/gitstart (dev)$ git reset --hard HEADHEAD is now at 3226b63 add readme filealbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree clean 还原03：版本库中的文件，修改或删除后执行了 git add 操作使用了 git add 命令之后，文件的改变就放到了暂存区，这种情况可以使用git restore --staged file_name 或者 git reset HEAD file_name 命令。 git restore执行 git restore --staged file_name 实际上是使用版本库中的文件覆盖暂存区中的数据，执行结束后文件状态变成了 &lt;还原02&gt; 中的情况。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ echo "test add"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) modified: README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git restore --staged README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a") git resetgit reset 命令如果加上 --hard 参数不能再加文件目录，只能将工作区全还原，如果不加默认参数为 --mixed，执行之后修改的文件状态变成了 &lt;还原02&gt; 中的情况。 12345678910111213141516171819202122232425262728293031323334353637383940414243albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) modified: README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git reset HEAD README.mdUnstaged changes after reset:M README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a") 还原04：版本库中的文件，修改或删除后执行了 git add、git commit 操作git commit 命令一旦执行了之后就形成了“历史”，我们叫做提交日志，要想回退就得有篡改历史的能力，很幸运 Git 给了我们这种能力，其实提交之后我们可以把本地文件反向修改，然后再提交一次，但是我们说的还原，一般都是只倒退，既然是错误的提交，我们就像把这段“历史”抹去，这时就要用到 git reset HEAD^ 命令。 执行这个命令之后，刚刚的提交记录就被抹掉了，文件状态就回到了 &lt;还原02&gt; 的情况，如果加上参数 --soft 就会回到 &lt;还原03&gt; 的情况，如果加上参数 --hard ，就不能添加 file_name 这个文件名，然后整个工作区倒退到上一次修改之前，其他两种参数 --mixed 和 --soft 就可以指定添加名字。 这里的 HEAD^ 表示最新版本的前一版，也就是倒数第二版本，可以类推，HEAD^^ 表示倒数第三版本，HEAD^^^ 表示倒数第四版本。 另外还有另一种写法 HEAD~1 表示最新版本的前一版，也就是倒数第二版本，HEAD~2 表示倒数第三版本，HEAD~3 表示倒数第四版本。 其中 ^ 和 ~ 的含义并不相同，涉及到合并分支的概念，有兴趣的话可以多了解下，这里就不展开了，继续还原当前这种情况，我们选择 git reset HEAD^ 命令，先提交看下： 12345678910111213141516171819202122232425262728293031323334353637383940414243albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git commit -m"modify readme 1"[dev 8a40f22] modify readme 1 1 file changed, 1 insertion(+)albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ git log -2commit 8a40f229881da037ff99070fa205d7819ba9f51b (HEAD -&gt; dev)Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 15:46:32 2020 +0800 modify readme 1commit 3226b63185a16398a02d5eaea47c95309ba49588 (origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme file 然后再还原试试： 123456789101112131415161718192021222324albert@homepc MINGW64 /d/gitstart (dev)$ git reset HEAD^Unstaged changes after reset:M README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git log -2commit 3226b63185a16398a02d5eaea47c95309ba49588 (HEAD -&gt; dev, origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme file 怎么样，历史被我们抹除了，需要注意的是，如果想还原“历史”，那么 git set 命令后面不能跟文件名，也就是说必须整个还原到上一版本，否则就相当于将单个文件简单反向修改添加到暂存区，而之前对文件的修改保留在本地，文件的日志并没有回退，具体的文件状态还得你自己操作感受一下。 还原05：版本库中的文件，修改或删除后执行了 git add、git commit、git push 操作这种情况就是还原远程仓库的日志记录了，实际上操作步骤先按照 &lt;还原04&gt; 来处理，然后将本地分支情况推送到远程分支即可。 我们先把刚才的修改提交，然后推送到远程分支，使用 git status 可以看到本地分支已经领先远程分支了(Your branch is ahead of ‘origin/dev’ by 1 commit.)， git push 操作之后两个分支同步了。 1234567891011121314151617181920212223242526272829303132albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ git logcommit a5b6c18db71a0487f6316f5db4304a99984f2ab3 (HEAD -&gt; dev)Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 15:51:56 2020 +0800 modify readme 1commit 3226b63185a16398a02d5eaea47c95309ba49588 (origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme filealbert@homepc MINGW64 /d/gitstart (dev)$ git pushWarning: Permanently added the ECDSA host key for IP address '180.97.125.228' to the list of known hosts.Enumerating objects: 5, done.Counting objects: 100% (5/5), done.Writing objects: 100% (3/3), 286 bytes | 286.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)remote: Powered by GITEE.COM [GNK-3.8]To gitee.com:myname/gitstart.git 3226b63..a5b6c18 dev -&gt; dev 这时通过远程仓库的管理软件，你可以看到远程分支已经有了最新的提交，然后我们可以参考 &lt;还原04&gt; 的情况，先将本地日志还原，再推送到远程仓库。 1234567891011121314151617181920212223242526272829303132333435albert@homepc MINGW64 /d/gitstart (dev)$ git reset HEAD^Unstaged changes after reset:M README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is behind 'origin/dev' by 1 commit, and can be fast-forwarded. (use "git pull" to update your local branch)Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git logcommit 3226b63185a16398a02d5eaea47c95309ba49588 (HEAD -&gt; dev, origin/master, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme filealbert@homepc MINGW64 /d/gitstart (dev)$ git pushTo gitee.com:myname/gitstart.git ! [rejected] dev -&gt; dev (non-fast-forward)error: failed to push some refs to 'git@gitee.com:myname/gitstart.git'hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details. 和想象的不太一样的，这种情况是远程仓库的记录领先，无法直接推送，此时可以添加 -f 参数，用本地提交记录覆盖远程分支记录： 123456albert@homepc MINGW64 /d/gitstart (dev)$ git push -fTotal 0 (delta 0), reused 0 (delta 0)remote: Powered by GITEE.COM [GNK-3.8]To gitee.com:myname/gitstart.git + a5b6c18...3226b63 dev -&gt; dev (forced update) 这次再查询远程分支记录，发现也被回退了，目的达成。 还原06：两次git commit 之后产生两条日志，只还原第一次提交这种情况其实发生了两次修改和两次提交，和 &lt;还原05&gt; 情况不同的是要还原的提交不是最后一次，如果使用 git reset 命令必然将最后一次修改也还原了，虽然不能直接完成，但是给我们提供了解决问题的思路： 第一种方法：直接使用 git reset HEAD^^ 命令还原两次提交，然后在工作区将文件按第二次修改再改一次进行提交，这种方法适用于想要抹除第一次提交历史的情况。 第二种方法：如果你不在意提交历史，只是想还原第一次修改，那么可以使用 git revert HEAD^ 命令来反向修改那一次变化，修改之后会自动添加到暂存区，等待提交。 先来修改提交两次，产生两次记录： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ echo "m1"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git commit -m"modify README 1"[dev e570df1] modify README 1 1 file changed, 1 insertion(+)albert@homepc MINGW64 /d/gitstart (dev)$ echo "m2"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git commit -m"modify README 2"[dev 140547f] modify README 2 1 file changed, 1 insertion(+)gialbert@homepc MINGW64 /d/gitstart (dev)$ git logcommit 140547f8d0b10d9a388beaf2ce522c38c878a839 (HEAD -&gt; dev)Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:26:17 2020 +0800 modify README 2commit e570df134b39ee7424bc8c48c1067e72c3fb9637Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:26:07 2020 +0800 modify README 1commit 3226b63185a16398a02d5eaea47c95309ba49588 (origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme filealbert@homepc MINGW64 /d/gitstart (dev)$ cat README.mdlearn git branch commandm1m2 然后使用 git revert HEAD^ 还原第一次修改记录： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647albert@homepc MINGW64 /d/gitstart (dev)$ git revert HEAD^Auto-merging README.mdCONFLICT (content): Merge conflict in README.mderror: could not revert e570df1... modify README 1hint: after resolving the conflicts, mark the corrected pathshint: with 'git add &lt;paths&gt;' or 'git rm &lt;paths&gt;'hint: and commit the result with 'git commit'albert@homepc MINGW64 /d/gitstart (dev|REVERTING)$ vi README.mdalbert@homepc MINGW64 /d/gitstart (dev|REVERTING)$ git add README.mdalbert@homepc MINGW64 /d/gitstart (dev|REVERTING)$ git commit[dev 6ae97d0] Revert "modify README 1" 1 file changed, 1 deletion(-)albert@homepc MINGW64 /d/gitstart (dev)$ git logcommit 6ae97d0e136abc1ed241854298037ca9d1c4460c (HEAD -&gt; dev)Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:31:50 2020 +0800 Revert "modify README 1" This reverts commit e570df134b39ee7424bc8c48c1067e72c3fb9637.commit 140547f8d0b10d9a388beaf2ce522c38c878a839Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:26:17 2020 +0800 modify README 2commit e570df134b39ee7424bc8c48c1067e72c3fb9637Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:26:07 2020 +0800 modify README 1commit 3226b63185a16398a02d5eaea47c95309ba49588 (origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme file 因为修改了同一个文件，还原的时候还产生了冲突，解决冲突之后才提交，看日志发现这是一条新的记录，在实际操作的过程中可能会发生比这还要麻烦的场景，多练就好了。 常用集合使用 Git 进行版本管理时，遇到的回退情况远不止这么多，这只是我目前常见的，之后遇到还会补充，每种情况我们其实不止有一种解决方式，接下来对于每种情况给一个我个人常用的处理方式，因为 git checkout 的作用被逐渐拆分成更具体的 git switch 和 git restore，我们尽量选择功能明确的命令： 还原00：工作区中未加到暂存区和版本库的文件，还原今天所做的修改 尝试下Ctrl+z吧，不行就找找自动保存的缓存文件，看看能不能找到之前版本 还原01：工作区中未加到暂存区和版本库的文件，执行了 git add 操作 直接使用 git restore --staged file_name 命令，如果版本不支持则使用 git rm --cached file_name 还原02：版本库中的文件，修改或删除后未执行 git add 操作 直接使用 git restore file_name 命令，如果版本不支持则使用 git checkout -- file_name 还原03：版本库中的文件，修改或删除后执行了 git add 操作 直接使用 git restore --staged file_name 命令，按 &lt;还原02&gt; 情况处理 还原04：版本库中的文件，修改或删除后执行了 git add、git commit 操作 直接使用 git reset HEAD^ 命令，按 &lt;还原02&gt; 情况处理，或者使用 git reset --soft HEAD^ 命令，按 &lt;还原03&gt; 情况处理 还原05：版本库中的文件，修改或删除后执行了 git add、git commit、git push 操作 先按照 &lt;还原04&gt; 情况处理，然后使用 git push -f 命令 还原06：两次git commit 之后产生两条日志，只还原第一次提交 使用 git revert HEAD^ 命令，解决冲突后提交，revert 后面跟具体的 commit id 也可以。 总结 参考这些具体的例子你会发现，很多操作选择在使用 git status 之后都有列举 所以说 git status 是一个可以提示你做选择的强大帮手，不知所措时可以试试它 Git 2.23版本之后学会用 git switch 和 git restore 命令，因为之前 git checkout 背负了太多了 最后放一幅图吧，只画了主要的，没有画出全部情况，否则会很乱，可以对照着练习一下]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>checkout</tag>
        <tag>reset</tag>
        <tag>revert</tag>
        <tag>restore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git branch常用分支操作]]></title>
    <url>%2Fblog%2F2020%2F02%2F25%2Fgit-branch%E5%B8%B8%E7%94%A8%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言最近代码的版本控制工具由SVN换成了Git，只用管理个人项目常用的灵魂三步git add、git commit、git push看来是行不通了，之前虽然也一直在用 git，但是用法很有限，主要集中在前面提到的三步，所以为了更好的工作，我决定还是好好总结一下。 分支在Git的操作里有着很重要的地位，代表了不同的开发线路，创建一个分支，也就多了一个索引文件，相比于SVN分支拷贝全部文件来说来方便的多，所以Git使得按功能分支的开发模式变得非常简单，在开发过程中常常需要对分支进行操作。 远程仓库本来就几个分支，操作上也没有太麻烦，但是加入了远程仓库以后，事情变得复杂起来。有了远程仓库一般意味着代码开发需要多人合作了，这时候常常会产生冲突，分支合并时也变得不那么容易了。 远程仓库其实也很好理解，就是放在远处用来保存代码资源的一个仓库，其实和本地的代码库没有什么区别，这个远程仓库主要是为了把大家修改的代码都合并到一起，给大家提供一个统一的目标点。 远程仓库究竟有多远，常见的代码托管平台：github、gitlab、码云都可以提供远程仓库，如果你在月球上放置一台可以联网的代码仓库服务器，那么距离就是38.4万千米，但是远程仓库也可以很近，你也可以把本机电脑的D盘里的代码仓库作为E盘的代码仓库的远程仓库，或许远程仓库可能只和你隔了一个文件夹。 由于网络的原因，github 和 gitlab 访问常常很慢，所以为了做练习测试推送，我在码云创建了一个仓库 gitstart，它的地址大概是这个样子：git@gitee.com:myname/gitstart.git，创建的方法一搜一大把，上面提到的几个托管平台，在哪创建都可以，一定要记住地址，因为后面还要用到。 建立联系本地创建文件夹并进入12345678albert@homepc MINGW64 /d$ mkdir gitstartalbert@homepc MINGW64 /d$ cd gitstart/albert@homepc MINGW64 /d/gitstart$ 这里的文件夹名字可以和远程仓库不同，但是为了看起来方便对应，还是取相同的名字好一点。 初始化仓库123456albert@homepc MINGW64 /d/gitstart$ git initInitialized empty Git repository in D:/gitstart/.git/albert@homepc MINGW64 /d/gitstart (master)$ 临时插播好奇心（不在流程中）目前这个状态有点意思，初始化完之后，(master) 这个字符串表示当前是在 master分支，查一下日志看看： 123456albert@homepc MINGW64 /d/gitstart (master)$ git logfatal: your current branch 'master' does not have any commits yetalbert@homepc MINGW64 /d/gitstart (master)$ 提示也是正确的，说 master分支没有任何提交，但是我们查询一下分支看看： 12345albert@homepc MINGW64 /d/gitstart (master)$ git branch -aalbert@homepc MINGW64 /d/gitstart (master)$ 居然是空的，没有分支，查询 .git\HEAD 文件发现里面有一行 ref: refs/heads/master，说明当前分支时 master，但是为什么查询分支没有结果呢？ 打开 .git\refs\heads 目录，发现这个文件夹下根本没有 master文件，其实想想也对，Git 中的分支其实对应着 commit id，现在什么都没有提交，master 也就找不到 commit id，所以就是有 master 文件，里面也不知道写什么。 查询远程仓库12345albert@homepc MINGW64 /d/gitstart (master)$ git remote -valbert@homepc MINGW64 /d/gitstart (master)$ 依旧什么内容都没有，说明还没有和远程仓库建立联系。 与远程仓库建立对应关系12345678910albert@homepc MINGW64 /d/gitstart (master)$ git remote add origin git@gitee.com:myname/gitstart.gitalbert@homepc MINGW64 /d/gitstart (master)$ git remote -vorigin git@gitee.com:myname/gitstart.git (fetch)origin git@gitee.com:myname/gitstart.git (push)albert@homepc MINGW64 /d/gitstart (master)$ 这一步需要注意，origin看起来就是一个远程仓库的别名，代表着 git@gitee.com:myname/gitstart.git 这个代码仓库，刚刚提到过，这个远程仓库也可以是本地的，所以你添加git remote add origin d:/test 也是可以的，就表明 gitstart 的远程仓库是本地的 test 仓库。 第一个分支刚刚说过，现在本地库的状态有些特殊，实际上刚刚在码云上创建的 git@gitee.com:myname/gitstart.git 库也很特殊，他们都没有真正的分支，这时只要我们成功提交一次，创建一个commit id，就相当于初始化了master分支。 添加README文件12345678910111213albert@homepc MINGW64 /d/gitstart (master)$ echo "learn git branch command"&gt;README.mdalbert@homepc MINGW64 /d/gitstart (master)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (master)$ git commit -m"add readme file"[master (root-commit) 3226b63] add readme file 1 file changed, 1 insertion(+) create mode 100644 README.md 查询当前分支123albert@homepc MINGW64 /d/gitstart (master)$ git branch -a* master 这次可以是出现了，分支为 master，前面的 * 表示为当前分支。 将分支推送到远程仓库12345678910albert@homepc MINGW64 /d/gitstart (master)$ git push -u origin masterEnumerating objects: 3, done.Counting objects: 100% (3/3), done.Writing objects: 100% (3/3), 248 bytes | 248.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)remote: Powered by GITEE.COM [GNK-3.8]To gitee.com:myname/gitstart.git * [new branch] master -&gt; masterBranch 'master' set up to track remote branch 'master' from 'origin'. 至此，本地仓库和远程仓库就建立了联系，下面可以开始学习 Git 分支命令了。 分支操作新建分支新建分支可以使用 git branch branch_name 命令，以下就是一个创建名为 release 分支的命令： 12albert@homepc MINGW64 /d/gitstart (master)$ git branch release 也可以使用 git checkout -b branch_name 来创建一个新分支，创建完会自动切换到新分支： 123456albert@homepc MINGW64 /d/gitstart (master)$ git checkout -b devSwitched to a new branch 'dev'albert@homepc MINGW64 /d/gitstart (dev)$ 切换分支这是一个很奇怪的命令，命令格式为 git checkout branch_name，总感觉 checkout 子命令包揽了不属于自己的工作，如果在git branch的基础上加一个参数会更合理的一点，但这和切换分支的实际含义可能还有关系，切换分支其实就是修改HEAD文件中的 commit id，而没有真正的发生切换。 12345678910albert@homepc MINGW64 /d/gitstart (dev)$ git checkout releaseSwitched to branch 'release'albert@homepc MINGW64 /d/gitstart (release)$ git checkout devSwitched to branch 'dev'albert@homepc MINGW64 /d/gitstart (dev)$ 查看本地分支像刚才我们创建的 release 分支和 dev 分支都是在本地创建的，这样的分支通过 git branch 命令就可以查看 12345albert@homepc MINGW64 /d/gitstart (dev)$ git branch* dev master release 这样就列举了本地的所有分支，在当前分支名字 dev 前面哈还有一个 * 作为标记 查看远程分支只要在上面的命令基础上加上 -r 参数就行了 123albert@homepc MINGW64 /d/gitstart (dev)$ git branch -r origin/master 查询到的分支只有 origin/master 一个，这个分支是一开始我们进行第一次提交产生 master 分支之后，通过 git push -u origin master 推送到远程仓库的，所以现在只有一个。 查看所有分支所有分支包括本地分支和远程分支，将 -r 参数换成 -a 参数就可以了 123456albert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master release remotes/origin/master 将本地分支推送到远程仓库其实之前已经操作过了，可以试着复习一下，git push -u origin branch_name，其实这是一个简写，-u 可以写成 --set-upstream 表示设置上游分支，其实就是和远程仓库的分支建立联系。 branch_name 也是 local_branch_name:remote_branch_name的一种简写，冒号前表示本地分支，冒号后面表示远程分支，如果只写一个就表示两个分支名相同，远程仓库中如果没有这个分支就会新建一个。 也就是说 git push -u origin dev 和 git push--set-upstream origin dev:dev 是一样的，下面来试一下，然后查看一下分支： 1234567891011121314151617albert@homepc MINGW64 /d/gitstart (dev)$ git push -u origin devTotal 0 (delta 0), reused 0 (delta 0)remote: Powered by GITEE.COM [GNK-3.8]remote: Create a pull request for 'dev' on Gitee by visiting:remote: https://gitee.com/myname/gitstart/pull/new/myname:dev...myname:masterTo gitee.com:myname/gitstart.git * [new branch] dev -&gt; devBranch 'dev' set up to track remote branch 'dev' from 'origin'.albert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master release remotes/origin/dev remotes/origin/master 冒号前后的米名字是不是一定相同呢？完全没有必要，我们可以让本地的 release 分支对应远程的 master 分支，只不过这样怪怪的，但是操作上完全可以的。 12345678albert@homepc MINGW64 /d/gitstart (dev)$ git checkout releaseSwitched to branch 'release'albert@homepc MINGW64 /d/gitstart (release)$ git push -u origin release:masterEverything up-to-dateBranch 'release' set up to track remote branch 'master' from 'origin'. 查看本地分支与远程分支对应关系这个也是刚刚知道的，可以使用 git branch -vv 命令，注意是两个 v: 12345albert@homepc MINGW64 /d/gitstart (release)$ git branch -vv dev 3226b63 [origin/dev] add readme file master 3226b63 [origin/master] add readme file* release 3226b63 [origin/master] add readme file 执行这个命令之后可以看出，本地的 master 和 release 分支都对应着远程的 master 分支 删除本地分支我们先复习一下新建分支，然后把它推送到远程仓库，再使用 git branch -d branch_name 命令进行删除 12345678910111213141516171819202122albert@homepc MINGW64 /d/gitstart (release)$ git checkout -b feature_testSwitched to a new branch 'feature_test'albert@homepc MINGW64 /d/gitstart (feature_test)$ git push origin feature_testTotal 0 (delta 0), reused 0 (delta 0) remote: Powered by GITEE.COM [GNK-3.8]remote: Create a pull request for 'feature_test' on Gitee by visiting:remote: https://gitee.com/myname/gitstart/pull/new/myname:feature_test...myname:masterTo gitee.com:myname/gitstart.git * [new branch] feature_test -&gt; feature_testalbert@homepc MINGW64 /d/gitstart (feature_test)$ git branch -a dev* feature_test master release remotes/origin/dev remotes/origin/feature_test remotes/origin/master 开始删除分支，删除之前记得切换到别的分支，否则删除不成功 1234567891011121314151617albert@homepc MINGW64 /d/gitstart (feature_test)$ git checkout devSwitched to branch 'dev'Your branch is up to date with 'origin/dev'.albert@homepc MINGW64 /d/gitstart (dev)$ git branch -d feature_testDeleted branch feature_test (was 3226b63).albert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master release remotes/origin/dev remotes/origin/feature_test remotes/origin/master 删除远程分支通过上面的操作我们发现只删除了本地的分支，远程的分支还在，要想删除远程分支，需要使用 git push origin --delete branch_name 命令 12345678910111213albert@homepc MINGW64 /d/gitstart (dev)$ git push origin --delete feature_testremote: Powered by GITEE.COM [GNK-3.8]To gitee.com:myname/gitstart.git - [deleted] feature_testalbert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master release remotes/origin/dev remotes/origin/master 这次再查看时发现远程分支也被删掉了。 获取远程主分支到本地其实 Git 的克隆命令默认就是把远程仓库的主分支下载到本地，我们可以使用 git clone 远程地址 本地文件夹 命令来克隆一个仓库，如果本地文件夹省略，则默认新建一个与仓库名相同的文件夹： 1234567891011121314151617albert@homepc MINGW64 /d$ git clone https://gitee.com/myname/gitstart.git gitstartcopyCloning into 'gitstartcopy'...remote: Enumerating objects: 3, done.remote: Counting objects: 100% (3/3), done.remote: Total 3 (delta 0), reused 0 (delta 0)Unpacking objects: 100% (3/3), done.albert@homepc MINGW64 /d$ cd gitstartcopy/albert@homepc MINGW64 /d/gitstartcopy (master)$ git branch -a* master remotes/origin/HEAD -&gt; origin/master remotes/origin/dev remotes/origin/master 获取远程其他分支到本地从上面命令执行后的结果来看，当前本地仓库中只有 master 分支，其他的分支都是在远程仓库上，这时可以用 git checkout branch_name 命令来下载远程分支： 1234567891011121314151617albert@homepc MINGW64 /d/gitstartcopy (master)$ git checkout devSwitched to a new branch 'dev'Branch 'dev' set up to track remote branch 'dev' from 'origin'.albert@homepc MINGW64 /d/gitstartcopy (dev)$ git branch -a* dev master remotes/origin/HEAD -&gt; origin/master remotes/origin/dev remotes/origin/masteralbert@homepc MINGW64 /d/gitstartcopy (dev)$ git branch -vv* dev 3226b63 [origin/dev] add readme file master 3226b63 [origin/master] add readme file 看到这里可能会疑惑了，git checkout branch_name 不是切换分支的命令吗？实际上当 branch_name 分支在本地不存在而远程仓库存在时，这个命令与 git checkout -b &lt;branch&gt; --track &lt;remote&gt;/&lt;branch&gt; 含义相同，会在本地新建一个分支，并与远程分支建立联系。 常用集合 新建分支：git checkout -b branch_name 切换分支：git checkout branch_name 查看分支：git branch -a 删除分支：git branch -d branch_name 推送分支到远程：git push origin branch_name 删除远程的分支：git push origin --delete branch_name 拉取远程分支到本地：git checkout branch_name 查询分支的对应关系：git branch -vv 总结 以上这些命令都是在本地测试过的，可能考虑的不太全面，不过没关系，以后的分支操作还会补充到这里。 这些命令在有些特殊的情况下使用可能会遇到问题，如果大家发现了问题请及时指出，我会尽快修改的。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>branch</tag>
        <tag>checkout</tag>
        <tag>push</tag>
        <tag>remote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挥一挥衣袖，开始一段新的旅程]]></title>
    <url>%2Fblog%2F2020%2F02%2F16%2F%E6%8C%A5%E4%B8%80%E6%8C%A5%E8%A1%A3%E8%A2%96%EF%BC%8C%E5%BC%80%E5%A7%8B%E4%B8%80%E6%AE%B5%E6%96%B0%E7%9A%84%E6%97%85%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[悄悄的我走了，正如我悄悄的来；我挥一挥衣袖，不带走一片云彩。 前言钱钟书老先生在《围城》中说道:“天下只有两种人。比如一串葡萄到手，一种人挑最好的先吃，另一种人把最好的留到最后吃。照例第一种人应该乐观，因为他每吃一颗都是吃剩的葡萄里最好的；第二种人应该悲观，因为他每吃一颗都是吃剩的葡萄里最坏的。不过事实却适得其反，缘故是第二种人还有希望，第一种人只有回忆”。 而我在反思自己时却发现，无法将自身完全归于这两种人的一类，如果非要选一种，我更像是老先生提到的第二种人，总喜欢把最好的留到最后。按理说这样的人应该总是向前充满希望的，但是我却热衷于收集回忆，记录生活中的点点滴滴，认真记下生活中的每一笔支出，写下人生中一次次感动… 其实一开始我并没有这方面的爱好，不知从何时起，儿时的记忆渐渐和梦境中的画面杂糅在了一起，有些事情已经分不清是之前确实发生的，还是曾经在梦境中悄悄的来到过，所以慢慢地我养成了这样的习惯，记录生活中一切想要被回忆的事情，期待着有一天能与对此感兴趣的人一起分享这些点点滴滴。 缘由技术博客中很少写自己的生活，这一次其实也和技术有关，在一个工作岗位上勤勤恳恳工作六年之后，今年终于鼓足勇气决定出来找找新的机会，确定了新的目标之后回头看看这六年收获了很多，同时在这段找工作的经历中也学会了不少东西，新的工作基本定下来了，现在总结一下工作以来的经历以及面试中遇到的问题，方便后续复盘时能有个参照。 懵懵懂懂的6年前走出校门从大四学期便开始走出校门，一切按照教学大纲进行着，大四这年是企业实训，我们被安排到一家机构进行学习，学习结束已经13年的深冬，我们一群小伙伴作为花朵开始走出曾经的温室。 其实从实训的后半段我们已经开始在北京各大高校“流窜”，参加了很多不请自来的校招，进行了一轮轮的笔试，但很少有人从中得到满意的工作机会，我也尝试过几次，但是感觉自己真的很渺小，经过努力得到了一个大厂的面试邀请，我怀着激动的心情前去面试，走的时候还换上了自己都觉得怪异的正装，好在面试官并不在意我的这份不自然，完全投入身心开始进行面试。 很幸运我通过了一面，但是在去参加二面的路上我才发现后背已经被汗水浸湿了，之前也参加过几次面试，但是这一次确实是让我身心俱疲，从中也渐渐体会到了不同公司之间的技术差距，我一心想加入这个团体，但是二面的结果又把我拉回了现实，二面的过程很糟糕，有一道题我至今还记得，那就是关于数据库的连接，但是回答的很模糊，由于自己的知识储备不足，我怀疑自己当时连题目都没有弄懂。 走进社会大厂失利后，开始寻找其他的机会，毕竟工作是现在的第一要务，放弃了保研机会一心想着早点参加工作，如果连工作都找不到岂不是让人笑话了，时间不久便通过了几家面试，其中有意向可以试试的有两家，一家是做偏硬件的软件，另一家就是做游戏开发的，工资待遇差不多，相比较而言第一家要高一些，但是当时沉迷于Dota的我经过“深思熟虑”之后，委婉的谢绝了第一家的邀请，进入了这家游戏公司，也就是后来我工作了6年的公司。 开始工作我的人生很幸运，我一直这样觉得，在这里我碰到了我工作的中的第一位导师，都说师傅领进门，修行在个人，那么他就是我进入社会环境的那个师傅，但是他比我也大不了几岁，我更愿意称呼他为兄长。 时光荏苒，岁月如梭，即便转眼已经过去了6年多，但是工作第一天他让我修改的第一个BUG我至今还记得，那是一把“罪恶坑的钥匙”，BUG具体的细节并不重要，而是他处理的方式让我记忆犹新。 开发环境配置好之后，兄长便指给我一个BUG，让我尝试修改，便是那个“罪恶坑的钥匙”，第二天他就过来询问BUG的修改情况，我告诉他我的修改思路A，他说可以这样改，但是这种修改方式可能会给后面带来一些不利于扩展的问题，然后在他的电脑上给我看了他建议的修改思路B，然后让我按照这个思路去修改，我比较之后确实思路B更好一些。问题的关键是这个BUG他已经想好了修改方案，并且尝试修改过，他指定让我修改完全是为了帮我熟悉问题的处理方式而非完成工作。 之后也和一些其他领导沟通过工作，但是能这样带我入门的兄长就只此一个，其他人大多是就是完成工作即可，很少有人再想教我的什么东西了。我是幸运的，在我懵懂的年纪碰上了这样一位领路人，之后我们在项目组之间分分合合，但始终工作在同一个屋檐下。 勤勤恳恳的6年初入职场刚刚参见工作，一切都显得那么新鲜，经常会有这样的感叹：原来游戏中的这个功能是这样实现的！开始的时候对于工作的状态还是有点不适应，印象最深的就是下午的时候总是昏昏沉沉的，当时可以用“熬”这个字来形容，但是随着后面工作内容的铺开，大脑在紧张的处理这些问题时，犯困的毛病就改掉了。 工作之前写的项目很多是个人完成的，就是几个人合伙做一个项目，基本上也不太大，所有的代码也都很了解，但是刚接触这个游戏项目时感觉它太大了，所有的代码只能不断的搜索才能找到，仿照已有的功能开发了两个新功能之后，渐渐的找到了感觉。 很长一段时间之后再回过头来看自己的代码时会发出这样的感叹：这段代码是我写的吗？现在整个流程我已经清楚了，但是当时写这段代码的时候是怎么找到这里的。其实一开始写代码完全是照葫芦画瓢，很多语句不知道什么意思，但是功能类似，这样写完就可以用了。 当时还有一个情况就是开发环境是没有网络的，有问题不能上网去查，好在分配给我的没有太复杂的功能，依照原来的系统都可以完成，并且我喜欢做笔记，常用的那些代码实现都让我记在了本子里，有些还记了不止一遍，这些笔记我至今还留着，现在看起来显得过于幼稚，但却是我工作以来的痕迹。 渐入佳境工作一年以后，对整个游戏已经比较熟悉了，可以独自完成很多功能，稳定下来的游戏也逐渐对接多个平台开始蓬勃发展，那时的我真的是干劲十足，每天像打了鸡血一样，作为服务器开发的我开始偶尔“插手”客户端的开发工作。 期间还养成了每天读书的习惯，其实这个习惯的养成是被动的，原因是分配给我的电脑比较卡，我提过几次但是一直没有换的机会，每天早上开机至少得10分钟左右才能正常顺畅的工作，所以后来我一般会早来一会，开机这期间我就会把旁边同事的书拿过来看看，后来同事的书看完了，我就买一些相同类型的技术书籍来看，再后来开始扩展知识面，买一些流行技术的书，这个习惯就一直保留了下来，一直到现在每年都会看几本技术书籍，有些是不朽的经典，有些是新进的技术。 其实很多书的内容我只是有大概的印象，具体的内容早就忘记了，偶有几本书感觉有意思会回过头来再次翻看，每次看都会有不同的感受，我喜欢在纸质书上做笔记，想到什么就写什么，有些章节会被我划的很乱。在我看书的时候总有同事问我，你看那么多书都记得吗？都学会了吗？这时我常常会自嘲一般的回答：“看着玩而已，早都不记得了”。 实际上记不记得重要吗？今天吃了有营养的东西，明天依旧会饿，你会因为明天还会吃饭就放弃今天的美食吗？我想不会的吧，我感觉看书也是一样，我今天看了明白了一些事情，或者读到一个故事感动了很久，明天忘了就忘了，毕竟我曾经学会过，曾经也感动过。这些东西会消失的无影无踪吗？我想也不会的吧，吃过的美食总会有一部分营养进入了我们的细胞，成为了肉体的一部分，而曾经读过的书会忘得一干二净吗？当然不会，那些使我们印象深刻的文字总会在未来的某个深刻，在我们的脑子中再次迸发出来。 再入蛮荒天下没有不散的筵席，参加工作时就参与开发的这个项目终于到了最后的维护阶段，这个阶段距离我刚进公司时已经过去了2年半的时间，此时原项目不再进行新的开发只进行必要的维护，原项目组的人也被分成了两部分，现在的有两个新项目，一个是相同技术栈不同玩法的端游项目，一个是紧追潮流的手游项目。我当时想去做手游，最终也确实分到了手游组，就是从这时起，我与之前的兄长分到了不同的项目。 事实证明这个公司向手游进军的项目确实是一条蛮荒的道路，整个技术链遇到了前所未有的挑战，我们一步步探索着前进的道路，试图越过一个个技术的深坑，而真实情况却是多少次我都陷在了里面。 在这个项目组我遇到了很多新的伙伴，有的乐观、有的开放、有的乐于奉献、有的精益求精，在这我看到了相同而又不同的服务器程序，之前的程序被改的面目全非，我又得重新适应，面对全新的客户端也有太多的新知识需要学习，每天必须打起十二分精神来应对工作。 一次次的否定自我，一次次的推到重建，在项目的紧要关头，升级引擎、重建UI、优化逻辑，最终还是把这款游戏送上了线，但事实却如昙花一般，一闪而过，失败了，我们没有做出成功的产品，仅仅是做了一次失败的尝试，此时距离进入这个项目组过了1年半的时间。 并入源头手游组的失败尝试使我有了到外面大世界看看的想法，就在这时，和我们一同开始的另一个项目组已经完成了一次轮回，很明显他们成功了，作为同时开始的两个项目，一成一败的比较对于我们的打击很大，而那个成功的项目组也进入维护阶段，领导决定合并两个项目组继续完成手游的开发，这使我又打消了出门找工作的念头。 两个项目虽然一成一败，但是各有优势，因为最终做的是手游，所以原来的手游组有技术优势，而另一个成功的项目有成功的游戏内容，两者一合并应该很快就能出一个产品，更重要的是，这两拨人有很多都是曾经的好友，好友联手打造一个游戏也是很有意思的事情。 可是理想很丰满，现实很骨感，事实证明做出一款游戏是多么的不容易，虽然两部分好友合并到一起没有什么磨合的问题，但是游戏内容的一次次修改不断冲击着之前制定的开发计划，整个开发计划不断修改，时间节点在不断修改的需求面前显得那么渺小，常常被无情的践踏。 终于看到胜利的曙光了，在不断调整了2年之后，游戏迎来了上线的的一天，之后开始根据线上反馈进行调整，在我看来游戏开发到这已经基本完成，虽然达不到爆款的要求，但终究是一款中规中矩的游戏，没有大的问题，也没有太闪光的点，我在这的修行也要告一段落了。 离开这里的一个导火索是游戏内容一次大的调整，本来现阶段不可能大面积修改功能了，可是在计划中还是出现了太多看不懂的修改内容，因为之前有了完成游戏就离开的想法，看到这里仿佛又要开启一个新游戏了，我也就没有再留下的必要了，是时候到外面的世界去看看了。 信心满满的6年后外出求索19年底，在第一家也是唯一一家公司呆了6年之后，我开始外出面试了，从一开始的信心满满到后面的发奋图强，我逐渐认识到了，我必须出来闯闯了，我在一个安逸的地方待了太久，虽然每天都在学习，但事实上优秀的人比你还要努力，以下简单介绍下面试过程，对于需要掌握的知识进行一个梳理，便于日常复习警醒自己，大概面了几家，以下按一面时间先后排列以下T、D、Y、W、Z、H：，全部以字母代替就不列出公司名了，有兴趣的可以进一步交流下。 T公司 很抱歉一开始把公司名看错了，当时还在想一家旅游公司怎么还做游戏，但是毕竟是第一家在招聘APP上给我发面试邀请的公司，怎么也要去看看，后来了解了一下这是一家主营棋牌类的公司，面试当天早早就来到了这家公司，顺便再楼下吃个了饭，面试开始先填个表格，内容跟查户口一样，我只填了其中的必要信息，接着做笔试题，包括后面几家面试，这是我唯一做的一套笔试题，内容不难，可能就是一个简单了解。 我还没写完面试官就来了，我看他特别像我初中的化学老师，整个过程很轻松，聊聊笔试题、曾经的项目，面试官还介绍了他们公司的情况，他表示了对我的肯定，问我有没有兴趣转Golang语言，我内心是拒绝的，其实嘴上也拒绝了，因为我一直使用C++，之后又是其他一个组的负责人来面试，他们使用的Python，整个过程依旧轻松加愉悦，还向他请教了分布式服务器的知识，最终因为我不想放弃C++而结束，他问原因是什么？我开玩笑说：可能是情怀吧！ 涉及到笔试面试部分内容，列举在此主要为了重温复盘，如果你想做游戏开发也可以看看这些知识： 不同类型数据内存占用大小 估算PC机上1秒钟可能执行的空的for循环次数 linux下常用搜索文件命令 常用的设计模式 字符串翻转 扑克牌中挑最长顺子 回旋排列矩阵 分布式服务器设计 linux中的lvs 服务器横向扩展 从头实现一个服务期 websocket 玩家背包怎样设计 D公司 这个公司完全是抱着学习的心态去的，因为公司本身很大并且不是做游戏的，来这家公司完全是因为他们的技术总监在招聘APP给我发了面试邀请，我本来觉得不合适，人家说可以来试试，抱着学习的态度我就去了，为了这次面试还看了好几个调度算法，最终也没用上。 本来10点半的面试，7点多我就出发了，期间地铁还坐反了，还好出门早，来到西二旗发现手机都没有信号，出门都骑不了自行车，走了很久才找到一辆，开锁出发一气呵成，9点多就到公司楼下了，旁边便利店买了个菜团子，对于干吃的我来说太大了，10点左右进入公司，大公司就是不一样，进门登记后还要贴一个签，这是怕我乱跑啊。 面试不久后进行，来了一个小哥哥，年纪应该不大，很沉稳的样子，带了一台笔记本电脑，这个好像是标配，提倡无纸化办公吧，我的一切反馈他都会记录在上面，整个过程对于他来说应该是轻松的，但是对于我来说有些窘迫，整个过程对我的评价就是，很多东西用的很熟，但是对于原理掌握的还不够，算是没有达到他们的要求，这也在我的意料之中，毕竟就是来学习的。涉及到的面试内容大概有如下问题： 开源项目源码的阅读情况 动态库加载路径 编译的过程 线程崩溃为什么会导致进程挂掉？一定会挂掉吗？ 加权最短路径 打印过程中出现中断 中断信号怎么处理 怎么理解多态 编译时多态和运行时多态 模板和基础类型的效率比较 gdb调试 为什么先构造基类 析构函数的调用顺序 非阻塞的write什么时候返回 如果连不上服务器会有那些情况 注意wireshark的使用 listen的backlog参数 Y公司 这个公司有自己成熟的产品线，涉及到卡牌、MMORPG等等，同样是在招聘APP上收到面试邀请，但这次招人的是一个SLG游戏组，整个给人的感觉无论是公司的氛围还是项目的情况与我当前公司很像，一共来公司面了两次，第一次两个技术Leader分别进行面试，然后又和HR聊了一下，技术面主要围绕曾经的项目，后来第二次面试跟游戏制作人聊了一下，感觉和之前的公司更像了，当时就打了退堂鼓，最终婉言谢绝了这家公司的Offer，面试主要技术内容： 技能设计 redis缓存 指针用法 二分法思想 项目熟练度 W公司 这个公司是游戏开发中的大厂了，首先是在招聘APP上，HR和我沟通之后要去了简历想要看看，后来收到了面试电话确定了面试时间，面试当天也是早早的来到了公司，这天在周围没有找到吃饭的地方，要饿着肚子了，等待了一会被HR小姐姐带去二楼等了半小时，后来她跑过来告诉我位置错了，确实有点尴尬。 之后我被带去了正确的位置，然后开始了面试过程，面试官是一个小哥哥，整个面试的过程感觉表现的不是很好，有些问题回答的不太完整，但是却从中学习到了很多东西，临走时问了几个面试问题的正解，并且冒昧的问了小哥哥的工作年限，得知才比我大两岁就已经在游戏大厂当主程之后，深感我们之间的差距还很大，同时也激起了我努力学习的意志。 面试后好几天也没有消息，本来我感觉这次面试可能失败了，但是几天后我又收到了该公司的二面邀请，收到邀请时挺高兴的，当时还有另外几家面试，之前已经约好了时间，所以这次二面不得不向后推了几天，因为是第二次去，路线熟悉了很多，又是早早来到公司，本来以为还是技术面，但是交流几个问题之后发现问的都是之前的项目，和人员之间的沟通的问题，后来对方主动说明他是项目负责人，整个聊天过程比较轻松，谈过之后让回去等消息。 第二天收到HR视频面试的邀请，本来想约晚一点回家好好面试，但是因为HR小姐姐还有其他安排，我只得将面试时间提前，在公司旁边找了个安静的地方进行视频面试，主要聊了一下目前的薪资待遇以及项目情况，能够入职的时间等等，整个过程很愉快，并且得知其实是一个工作室在招聘，我问了一些相关的问题，面试结束回到家我仔细考虑了这个机会，第二天又找该项目的负责人了解了项目的详细情况，觉得这是一个很好的学习机会，与目前的工作内容有很强的互补性，可以试一试。 Gitflow使用方法 gcc编译过程 extern和static的作用 多态、虚函数、多继承虚函数 大根堆创建和插入 排序找出接近当前数的较大数 迭代器的理解、迭代器都是指针吗？ 字符编码、unicode、utf8 指针数组、数组指针、函数指针 引用和指针的区别 网络4次挥手、为什么要4次？ 函数阻塞是否占用资源–挂起不占用 Z公司 这个公司不是游戏公司，近两年异常火爆，有专门的游戏部门，但是我面试的职位不是游戏岗位，而是时下非常火的中台岗位，其实是在尝试新的领域。起初是猎头在招聘APP上要了我的简历，然后接到了公司HR小姐姐的电话，约定了面试时间，相互加了微信，面试之前和HR小姐姐交流了不少，知道公司技术面大概有3面，因为心里没底，抱着学习的态度考虑能过两面就行，如果实在太难能过一面也行，作为一个求知者，知道各个公司都需要哪些知识也就有了学习的目标。 可现实总是太残酷，这个面试我算通过了半面，什么叫半面，由于我的“出色”表现，我感觉正常的一轮面试并没有结束就被礼貌的请出来了，因为几个问题之后我也感觉出来了，我之前学的技术和他们开发思想差的有点多，所以出于礼貌，面试官也没说什么，还给出了一些建议，人真的不错，大公司的涵养还是有的。 你开发的最满意的系统 –道具系统 map和hashmap的区别 stl的使用 vector的扩容，是不是线程安全的？ 遍历删除vector元素，迭代器失效 有没有用过redis的有序集合 redis中hash插入的时间复杂度 设计一个红包系统 –评价为原始的面向对象方式，有些过时 给出建议这个红包系统必须要考虑redis、分布式、容灾、备份 建议如果想转向互联网需要准备的很多，可以先看下现有的解决方案 H公司 这个公司的面试机会是猎头推荐的，主要做战争题材的游戏比较多，现在也有卡牌和休闲，本来约的面试时间比较早，但是由于个人原因回了次老家，结果这个面试不得不向后推了，面试当天来到公司，前台居然一个人都没有，之后电话联系到面试官，首先表达了之前改约的歉意开始了面试过程。 面试主要围绕之前的项目进行，对具体的系统实现问的很详细，通过对细节的了解，对我之前的工作内容有很多不理解，感觉有很多内容不符合他的认知，整个过程倒还轻松，没有太多的技术问题，总体感觉不是一路人，很可能走不到一起。 聊了大概一小时，换HR继续聊，还是问了之前项目、期望薪资以及入职时间等等，确定了是卡牌组再招人，问了一些当前公司情况之后，按照流程回去等消息，但个人觉得可能不太合适。 面向对象要求比较高，C+Class的方式不被认可 着重问了一个游戏系统的实现方式（押镖） 面相对象设计技能系统 强调游戏充值实现的重要性，以及可能出现的多种情况 认为只有DBA才有权利修改数据库结构 准备离开出去面试一圈基本确定了新的工作，是时候离开了，先跟带自己入门的兄长道个别，我们两个聊了很久，对于我离开去学习新知识，兄长表示支持，他不仅是我工作上的领路人，同时脾气特别好，平时处理问题也很妥当，一直是我学习的榜样。 紧接着便向老大提出了离职，准备年前离职后去新公司入职，而老大的意思是再等等，年前太仓促了，先看看现在公司的情况，年后回来如果还想走再办离职吧，考虑到还有一段时间的就要放假了，为了更好的完成了交接工作，我答应了老大的请求。 即将离开过年期间考虑了好久，还是准备出去闯一闯，今年春节的新型冠状病毒疫情非常严重，很多公司都推迟了上班时间，虽然2号之后就回来上班了，但是很多同事由于封路的原因都还没回来，离职手续也一直没有办成，年后又找老大聊了一次，毕竟工作了6年，虽然不舍，但是确实该离开了，期待下周的情况能好一些，能顺利办完手续开始新的旅程。 挥挥手再出发 更新于2020年2月15日22:45:51 挥手告别事情办得比较顺利，经过前期的准备，周一便完成了工作交接，上传了交接文档，周二开始办理离职手续，由于新型冠状病毒疫情的原因，公司依旧没有什么人，好在办理离职的人员都在，签字、签字、再签字，成功在下午拿到离职证明，不过唯一遗憾的是，工牌和门禁卡同时上交了，不能给我留个纪念了，毕竟是在身上装了6年的工牌，6年了几乎没有离开过…… 因为很多同事也没来，加之疫情的严重性，散伙饭并没有吃成，前一天下班的时候专门去旁边的簋街转了一圈，发现除了几家仅有的外卖之外，都是黑着灯的，这可是簋街啊，是让人们可以排队等到凌晨2点的簋街，现在居然这样冷冷清清的，找不到吃饭的地方，散伙饭只能作罢。 下班前跟仅有的几个来上班的好友道了别，当然其中还有我那位可敬的兄长，当所有人都在关心你飞的高不高时，只有朋友关心你累不累，兄长就是这样的朋友，临走了还关心地问我社保能不能接上，只因为我之前和他提过一次担心社保断缴的问题。因为很多人还没来上班，剩下的关系好的小伙伴，在我晚上回家后，通过微信开始了与他们的远程云分别。 还看今朝告别了过去的工作，自然要步入新的旅程，为尽量避免人员接触，新的公司在周五为我在线办理了入职手续，两位帮忙办理入职的新同事真的非常友好，整个流程遇到不懂的都会及时解答，这让我非常期待下周一正式工作后的生活，新的旅程即将开始，又要在一个地方生根发芽了~]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用__declspec(dllexport)和__declspec(dllimport)在Windows平台编写和使用DLL的小例子]]></title>
    <url>%2Fblog%2F2020%2F02%2F05%2F%E5%88%A9%E7%94%A8-declspec-dllexport-%E5%92%8C-declspec-dllimport-%E5%9C%A8Windows%E5%B9%B3%E5%8F%B0%E7%BC%96%E5%86%99%E5%92%8C%E4%BD%BF%E7%94%A8DLL%E7%9A%84%E5%B0%8F%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[前言关于 __declspec(dllexport) 和 __declspec(dllimport) 这两个关键字在上大学期间就没见过几次面，直到毕业后在公司项目的代码中又遇到过几次，每次也是绕着走，生怕和它产生什么联系，只知道它和动态链接库 DLL 有关，但是当前这个项目中几乎没有用到自己写的动态链接库，所以我也就心安理得的躲了它这么久。 最近看一些开源项目的源码时又发现了这两个关键字，此时凭借自己掌握的知识和学习方法再来看这两个关键字，发现也没有什么值得害怕的地方，其实简单来说就是 __declspec(dllexport) 是用来说明指定类和函数需要从 DLL 中导出的，而 __declspec(dllimport) 是用来说明指定的类和函数是从DLL中导入的。 说明 __declspec(dllexport) 和 __declspec(dllimport) 只在 Windows 平台才有，用来说明类或函数的导出和导入。 在 Linux 平台上源文件中的所有函数都有一个的visibility属性，默认导出。如果要隐藏所有函数导出，则需要在GCC编译指令中加入 -fvisibility=hidden 参数。 生成 dll 的同时还会生成对应的 lib 文件，一般是一些索引信息，记录了 dll 中函数的入口和位置，这在之前还真的不知道，原来一直以为 lib 只是静态库文件呢。 疑问 为什么要导入导出，直接把代码拿过来一起编译不好吗？ 想要一起编译前提是你得有源代码，如果人家就给你一个动态库或者静态库，你想把源代码放到一起编译的愿望根本实现不了。 为什么要分为静态库和动态库？搞这么麻烦，还要导入导出。 这具体的就要查查他们两者的优缺点了，每种事务的产生必要有其产生的原因，比如静态库，很可能就是一个程序员今天在A工程写了一个读取文件的类，过一段时间又在B工程写了一个读取文件的类，代码都差不多，不久又在C工程中直接把代码复制过来改一改又写了一份，这时想到干脆了写个“静态库”这种东西吧，相同的代码直接封装到库中，哪个工程需要就直接拿过来编译，也不需要再复制代码了。 又比如动态库，前面的静态库解决了代码重复开发和维护的问题，但是读取文件的静态库中的代码在A、B、C三个工程中都存在一份，导致每个可执行程序都很大，可不可以共用一份呢？结果又发明了动态库，在编译时只指定函数的入口地址，运行时才加载动态库，这样就使得可执行程序体积大大缩小。 以上内容纯粹我个人想像的，真正发明静态库和动态库是由于什么原因，大家可以自行去了解… 动态库要比静态库好吗？ 个人感觉合适的才是最好的，不存在动态库要比静态库好的说法，最起码不是全都好，动态库的代码是在可执行程序运行时才载入内存的，在编译过程中仅简单的引用，因此代码体积较小，但是运行时要去加载库会花费一定的时间，执行速度相对会慢一些，总的来说静态库是牺牲了空间换时间，而动态库是牺牲了时间换空间。 .h（头文件） .lib（库文件） .dll（动态链接库文件） 之间的联系和区别 .h 文件是编译时需要的， .lib 是链接时需要的， .dll 是运行时需要的。如果有 .dll 文件，那么 .lib 一般是一些索引信息，记录了 .dll 中函数的入口和位置，.dll 中是函数的具体的执行内容。如果只有 .lib 文件，那么这个 .lib 文件是静态编译出来的，索引和实现都在文件中。 DLL的编写与使用前面说了这么多，其实就是想带大家先了解一下动态链接库 DLL ，接下来开始编写一个DLL并在另一个工程中使用它，前提是你已经会使用开发工具VS，如果不会先查查教程。 测试环境 VS2013随意版（个人感觉这个版本启动能快一点） Win10畅想版（我也不知道啥版本） 编写DLL编写 DLL 的方法不知一种，这里只简单介绍一种，对于直接写 .def 文件的方法这里不会展开，尽量依靠开发工具一步步向下执行就好，其实当你理解了开发工具的是怎样工作的，一切就没有那么神秘了，有些步骤直接修改配置文件也是可以实现的，只不过开发工具给我们提供了界面，操作起来更加方便了而已，下面我们开始编写： 打开VS新建项目，选择Win32项目，项目名称GenDLL，解决方案名称DLLExample，点击确定： 直接下一步，应用程序类型选择DLL，点击完成： 项目会自动创建一个GenDLL.cpp文件，我们在手动创建一个GenDLL.h文件，两个文件中编写如下代码： 123456789// GenDLL.h#ifdef GENDLL_EXPORTS#define TEST_API __declspec(dllexport)#else#define TEST_API __declspec(dllimport)#endifTEST_API int add(int a, int b); 123456789// GenDLL.cpp : 定义 DLL 应用程序的导出函数。#include "stdafx.h"#include "GenDLL.h"TEST_API int add(int a, int b)&#123; return a + b;&#125; 这段代码中有一个 TEST_API 是我在头文件中自定义的，当存在GENDLL_EXPORTS宏时， TEST_API 代表 __declspec(dllexport) 也就是导出函数，当不存在GENDLL_EXPORTS宏时， TEST_API 代表 __declspec(dllimport) 表示导入函数，而 GENDLL_EXPORTS 这个宏是与项目名相关的，自动生成的宏，在 DLL 项目中存在格式为 “大写项目名_EXPORTS”。 也就是说同一个头文件中计算加法的函数 add 在 GenDLL 这个生成 DLL 的项目中表示导出函数，在其他使用这个 DLL 的项目中表示导入函数。 编译看输出发现有GenDLL.lib和GenDLL.dll两个文件： 123456781&gt;------ 已启动生成: 项目: GenDLL, 配置: Debug Win32 ------1&gt; stdafx.cpp1&gt; dllmain.cpp1&gt; GenDLL.cpp1&gt; 正在创建库 c:\users\administrator\documents\visual studio 2013\Projects\DLLExample\Debug\GenDLL.lib 和对象 c:\users\administrator\documents\visual studio 2013\Projects\DLLExample\Debug\GenDLL.exp1&gt; GenDLL.vcxproj -&gt; c:\users\administrator\documents\visual studio 2013\Projects\DLLExample\Debug\GenDLL.dll========== 生成: 成功 1 个，失败 0 个，最新 0 个，跳过 0 个 ========== 使用DLL 在DLLExample这个解决方案下添加一个新项目，命名为UseDLL，然后点击确定： 直接下一步，应用程序类型选择“控制台应用程序”，点击完成： 在文件UseDLL.cpp文件中引用之前GenDLL项目的头文件，编写使用 add 函数的代码： 123456789101112// UseDLL.cpp : 定义控制台应用程序的入口点。#include "stdafx.h"#include &lt;iostream&gt;#include "../GenDLL/GenDLL.h"int _tmain(int argc, _TCHAR* argv[])&#123; std::cout &lt;&lt; "100+1=" &lt;&lt; add(100, 1) &lt;&lt; std::endl; system("pause"); return 0;&#125; 编译代码发现报错，提示有一个无法解析的外部命令： 1234567891&gt;------ 已启动生成: 项目: UseDLL, 配置: Debug Win32 ------1&gt; UseDLL.cpp1&gt; stdafx.cpp1&gt; 正在生成代码...1&gt;UseDLL.obj : error LNK2019: 无法解析的外部符号 &quot;__declspec(dllimport) int __cdecl add(int,int)&quot; (__imp_?add@@YAHHH@Z)，该符号在函数 _wmain 中被引用1&gt;c:\users\administrator\documents\visual studio 2013\Projects\DLLExample\Debug\UseDLL.exe : fatal error LNK1120: 1 个无法解析的外部命令========== 生成: 成功 0 个，失败 1 个，最新 0 个，跳过 0 个 ========== 提示这个错误本意就是说链接没有找到函数实现，链接需要什么文件，前面提到需要lib文件，那么我们设置一下，让UseDLL工程能够找到GenDLL.lib文件。 打开UseDLL工程的属性，在“配置属性-&gt;链接器-&gt;输入-&gt;附加依赖项”中添加GenDLL.lib: 然后在“配置属性-&gt;链接器-&gt;常规-&gt;附加库目录”中添加GenDLL.lib所在路径“../Debug”即可成功编译： 直接运行就可以看到调用DLL的结果，因为这两个工程在同一解决方案下，所以最终UseDLL.exe和GenDLL.dll在同一目录下，这样不会报找不到DLL的错误 如果是不同的目录就会像下图那样，提示找不到GenDLL.dll，只要把GenDLL.dll复制到和UseDLL.exe相同目录即可： 加载DLL上面提到当运行程序找不到 DLL时可以把 DLL 放到可执行程序程序的目录，有时运行大型软件找不到 DLL 时，我们也会下载一个放到System32目录，其实程序在加载 DLL 的时候是会按照一定顺序的，这些目录包括：包含exe文件的目录、进程的当前工作目录、Windows系统目录、Windows目录、Path环境变量中的一系列目录等等，这些目录的搜索顺序还会受到安全 DLL 搜索模式是否启用的影响。 所以说如果不是对DLL 放置的位置有特殊要求，那么直接放在exe文件所在的目录就好了，一般也是会优先搜索的。 总结 Windows上才有 __declspec(dllexport) 和 __declspec(dllimport) .h 文件是编译时需要的， .lib 是链接时需要的， .dll 是运行时需要的 程序运行时加载 DLL 一般优先从exe文件的所在目录优先加载]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>dllexport</tag>
        <tag>dllimport</tag>
        <tag>DLL</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020年的春节，我们一起抗击新型冠状病毒]]></title>
    <url>%2Fblog%2F2020%2F01%2F29%2F2020%E5%B9%B4%E7%9A%84%E6%98%A5%E8%8A%82%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%8A%97%E5%87%BB%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92%2F</url>
    <content type="text"><![CDATA[终于到了什么都不用做，在家躺着就能为国家做贡献的时候了！ 前言新型冠状病毒，一个看起来陌生的词语，使得原本最热闹的春季变得异常冷清，随着疫情范围的扩大，这个本来陌生的词语一次次冲击着人们的认知。这个病毒到底是什么，为什么扩散起来这么凶猛？ 2019-nCoV新型冠状病毒，在各种媒体上还会以“2019-nCoV”的名字出现，人感染了冠状病毒后常见体征有呼吸道症状、发热、咳嗽、气促和呼吸困难等。在较严重病例中，感染可导致肺炎、严重急性呼吸综合征、肾衰竭，甚至死亡。 引用百度百科中文字，对其描述为： 2019新型冠状病毒，即“2019-nCoV”，因2019年武汉病毒性肺炎病例而被发现，2020年1月12日被世界卫生组织命名。冠状病毒是一个大型病毒家族，已知可引起感冒以及中东呼吸综合征（MERS）和严重急性呼吸综合征（SARS）等较严重疾病。新型冠状病毒是以前从未在人体中发现的冠状病毒新毒株。 目前掌握的情况： 传染源: 野生动物，可能为中华菊头蝠 传播途径: 经呼吸道飞沫传播，亦可通过接触传播 易感人群: 人群普遍易感。老年人及有基础疾病者感染后病情较重，儿童及婴幼儿也有发病 潜伏期: 1 ~ 14 天，平均 10 天，潜伏期内存在传染性 2019-nCoV与SARS这个新型冠状病毒导致的肺炎传播速度如此之快，很多人拿它和03年的非典（SARS）相比，确实这两个病毒有很多相似的地方，同样都是新型冠状病毒，同样都会引起肺炎，甚至连发生的时间都非常相似，非典是02年11月出现病例，而2019-nCoV出现的时间大概是12月。 那它们两个这么像，能不能用相同的方法和药物治疗呢？目前来看是办不到的，两者虽然很相似，但是毕竟都是新型病毒，我们知道一种病毒变异后原来的药物很可能就起不到作用了，更何况这是两种不同的病毒，但是也有好的一面，毕竟在抗击非典时我们积累了宝贵的经验，对于防控类似的疾病能够提供很大的帮助。 比如当年非典时期,北京市在小汤山就建立起了一座封闭式的医院,就是小汤山医院，而武汉参照北京“小汤山模式” 神速建造了“火神山医院”，从开工到投入使用预计会花费10天左右，这个速度也是令人惊叹了，没有之前的经验积累是很难办到的。 为什么传播的这么快其实一开始我们都没有重视这场战斗，导致这个新型冠状病毒钻了空子，传播速度之快达到了让人心惊的地步，感觉主要有下面几方面的原因吧： 华南海鲜市场存在大量新型冠状病毒，源头上就很广 起初没有得到足够重视，认为不存在人传人的可能，导致接触者甚至医务人员被感染 病毒在潜伏期也有可能传播，这是与非典不同的，导致一些携带病毒的人在无意识的情况下成了传染源 正好赶上春节返乡高峰，而武汉作为九省通衢的枢纽，反倒为病毒散播提供了便捷的条件，扩散范围很快就达到了全国 目前的形式新型冠状病毒疫情已经开始进入初期扩散阶段，并呈上升趋势，但是应对措施也已经铺开，延长假期，控制人员流动，积极研制疫苗，组织医疗救援队赶赴武汉等等，相信不久疫情就能够控制住。 本来春节是一年中最忙碌的日子，今年却异常的冷清了，为了大家的健康，今年周围的人都取消了拜年聚会活动，村口也安排了人专门劝返探亲人员，因为这样我们反而多了一些陪伴家人的时间，而我居然有时间来码字了，往年不是在这喝酒就是在那聚会的，现在这样安安静静的待在家里感觉也不错。 引用网上一段顺口溜，写的不错与大家分享： 国家有难，咱不添乱。坐在家里，就是贡献。亲戚不走，来年还有。朋友不聚，回头再叙。利人利己，互不传染。吃好睡好，悠闲过年。坚持几天，你我平安。 新型冠状病毒最新消息目前新型冠状病毒处于蔓延的趋势，各种消息满天飞，真真假假难以辨认，所以我单独建了一个项目用来收集最新的消息，尽可能保证消息的准确，其中包含最新的疫情地图、最新疫情新闻、以及正规的捐助渠道等等，有兴趣的小伙伴可以一起舔砖加瓦。 抗击2019-nCoV最新情报-ChineseVictory 更新于2020年1月29日21:25:30 今天看到一个同类型的记录武汉抗击新型冠状病毒的项目，已经有3000多的star了，我们两个项目创建的时间很接近，再看看我的项目情况有点惨淡啊！贴个图，大家感兴趣可以来逛逛，不过人家那个项目确实很规范，我还有很多东西可以学习。 从star数为0来看确实惨淡，再放一遍项目地址-ChineseVictory，感兴趣可以来看看;)]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>病毒</tag>
        <tag>武汉</tag>
        <tag>春节</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019！一份迟到的年终总结]]></title>
    <url>%2Fblog%2F2020%2F01%2F03%2F2019%EF%BC%81%E4%B8%80%E4%BB%BD%E8%BF%9F%E5%88%B0%E7%9A%84%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[长大了就要为小时候吹过的牛而奋斗~ 前言2019即将过去，瞅一眼桌面右下角的时间，距离2020年还有63分51秒，这是我第一次这么强烈地想在一年结束之际写下点什么。本身是一个喜欢收集和总结知识的性格，但对自己的人生却少有总结，一方面感觉没什么可写，一方面也确实对自己太过宽容。 每年年初总是在朋友圈和各大平台浏览着一个个新年展望，而每年年末总是在相同的位置看着他们把年初展望的日期向后推一年，这是个段子，同样也是事实，很多人的生活过得平平淡淡，我们都是人群中的大多数，平庸而碌碌无为。 迟到之所以在年终总结前冠以“迟到”二字，是因为我突然意识到这份总结早就应该做了，对于参加工作已经6年的我来说，刚刚意识到需要做年终总结确实有些不应该。作为普通人的我来说记忆能力有限，小时候吹过的牛早就忘了，如果能及时的做年终总结，或许我还可以为了去年吹过的牛奋斗一把，可是我没有，我连去年的想法也忘得差不多了，此时此刻才刚刚意识到曾经的失误。 得与失既然是总结就要回想一下在过去的一年中我得到了什么，失去了什么，而在新的一年中我想获得什么，回想即将过去的2019年发现，今年确实发生了很多往年没有发生过的事情，这可能也是我突然非常想写点东西，记录下来的原因。 回顾2019这一年发生的事情太多，相互之间纠葛不断，不过还是从最简单的分类：工作、学习、生活这三个方面来聊聊吧，虽然很多事情不能完全归为某一类，但是贴一个标签总能清楚一点。 工作上依旧是踏踏实实，勤勤恳恳的一年，做了整整一年的游戏开发几乎颗粒无收，这已经不是第一年没有收成了，有时候真的有点后悔为了工作侵占了陪家人的时间，特别是看不到回报的时候。 从年初就开始做收尾工作，几次上线几次调整，不知不觉我们又过了一个年，在我心中这款游戏的开发工作已经接近尾声，这样的状态不应该再持续下去了。 学习上作为一个好学的程序猿，深知“学如逆水行舟，不进则退”的道理，今年在CSDN上写了42篇原创博客，算是高产的一年了，也终于迈进了总排名前一万名的大关，截个图记录一下： 有点小遗憾，访问量差几百才到50万，不过新年第一天应该差不多啦，不仅仅是知识的总结，由于加了CSDN的博客群，今年还认识了许多有意思的小伙伴，比如：铁柱同学（一个冒充小白的大佬）、第三女神（粉丝炸裂式增长）、TRHX（网站做的特漂亮），还有很多小伙伴就不一一列举啦。 关于读书，我只喜欢读纸质的书籍，喜欢那种在书上乱画，随便记笔记的方式，当然有一点不好，就是想查一个知识点，知道是哪一本书，不得不翻一翻才能找到，好希望纸质书能有个搜索按钮，不过这个问题找个电子版就能解决了。 今年一共读了7本关于编程技术的书籍： Redis入门指南(第2版) 图解HTTP 自动化平台测试开发 ——Python测试开发实践 图解密码技术 图解TCP/IP 漫画算法 ——小灰的算法之旅 MySQL必知必会 推荐这本《小灰的算法之旅》，可以把学知识当做一种乐趣，绝对能达到事半功倍的效果。 附上 我的完整书单 生活上今年在生活上发生的事情好像比之前几年加起来都要多，年中得到了一个特别可爱的宝宝，为了解决宝宝上学问题，之前从没考虑买房的我到了售楼处就买了一套，几乎都没挑就定下了，当然这么冲动的行为必须要付出代价，因此背上了近百万的债务，从此变成了一个给银行打工按月还款的房奴。 说实话宝宝刚出生时并不好看，可是越长越可爱，现在已经7个月大了，开始会爬了，真想不去上班一直陪她玩，有时候确实有一种为了她放弃全世界的冲动，宝宝今天有点发烧，凌晨一点了还没有睡，陪我一起跨年总结了，好在这会儿烧退了一些，快点好起来吧！ 2019年生活上发生的另外一件很重要的事情就是投资，入市有风险，投资需谨慎，这不是一句玩笑话，今年年初股市行情一片大好，正当我们陶醉其中的时候，贸易大棒直接挥下，给准备一飞冲天的行情当头一棒，还好跑得快，不然年初那波行情的盈利在贸易战初期就会飞烟灭了。 年初的基金行情也异常火爆，在支付宝买了点指数基金，贸易战开始之后就抛掉了，并没有多少盈利： 说完赚钱的接下来就是赔钱的，P2P暴雷给我炸的遍体鳞伤，从5月份出事到现在毫无音信，真应了那句话，你看上了人家的高息，而人家看上的是你的本金，P2P今年可谓损失惨重。 因为赚钱心切，今年还投资了一点数字货币，结果因爆仓而结束，每次都是到了爆仓点位迅速反弹，好像在提示我压根就不是我应该玩的，不过从这次投资来看，我才明白为什么有钱的人越来越有钱，而穷人一辈子很难翻身。 一句话，穷人没有东山再起的资本和承担风险的能力，举个例子：我和一个富有的人同时买一只数字货币，而我们都只花了500块来买相同的点位，不同的是富有的人保证金更多一些，这样当行情来到我的爆仓点位时，我和富有的人损失相同的钱，但是我爆仓了，而他没有，待到行情迅速反弹，他却赚的盆满钵满。 或者换一种情况，我们两个同时爆仓，他立马在低位投入2倍的钱，迅速赚回损失，而我只能眼睁睁的看着行情反弹却没有投资的砝码，忽然觉得T+1好像真的是帮助我们这些散户的。 展望20202019已经过去，面对着已经到来的2020年，我们需要踏上新的征程，我还没有适应给自己定出量化的目标，不过可以暂时写下大致的方向，也算是给自己一个时刻的提醒。 对工作新的一年不能再碌碌无为，真的需要去闯一闯了，最近和一些互联网公司的员工沟通过，仿佛我们不是生活在一个地球，外面的世界真的很大，外面的机会真的很多，是时候出去看看了，浏览一下世界的另一面，当然，脚踏实地的工作风格不能丢弃。 对学习经过一段时间的与大牛们的沟通，我渐渐的明白了自己的差距，也大致了解了需要重点学习哪些知识，所以简单列举如下： 巩固基础知识，对于一些函数不仅要会用，还应该花时间探究实现的方式，往深处挖掘，比如listen函数backlog参数意义。 阅读 redis 源码，这是很多人都提到的一点，适当可以看一下 STL 源码 看两本有关分布式知识的图书 对生活 尽最大可能陪陪家人 投资达到2019的水平（只看赚的，不看赔的） 总结接触了一些大牛之后备受打击，可是以往的岁月已经无法改变，只要认清了自己从现在开始就不晚，2019悄然离开，2020已经隆重登场！加油~]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单继承、多继承、菱形继承的虚函数表]]></title>
    <url>%2Fblog%2F2020%2F01%2F03%2F%E5%8D%95%E7%BB%A7%E6%89%BF%E3%80%81%E5%A4%9A%E7%BB%A7%E6%89%BF%E3%80%81%E8%8F%B1%E5%BD%A2%E7%BB%A7%E6%89%BF%E7%9A%84%E8%99%9A%E5%87%BD%E6%95%B0%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[前言最近被问到一个关于多继承虚函数表的问题，当时回答是可能存在多个虚函数表，应该是顺序排列的，但具体怎么排列还是有些疑惑的，回答的时候到有点儿心虚。之后查了资料，做了简单的实验，可以确定的是对于继承了多个含有虚函数基类的子类来说，指向虚函数表的指针应该不止一个。 问题虚函数表的问题是从C++多态的概念引出的，要想实现多态有3个条件： 存在继承：没有继承就没有多态（运行时），在多态中必须存在有继承关系的父类和子类。 重写函数：父类中需要定义带有 virtual 关键字的函数，而在子类中重写一个名字和参数与父类中定义完全相同的函数。 向上转型：将父类的指针和引用指向子类的对象。 满足以上三个条件，当使用父类的指针调用带有 virtual 关键字的函数时，就会产生多态行为。 实现这种多态表现的核心内容就是虚函数表，对于带有 virtual 关键字的函数地址会被放入一个表格，而在类中会有一个指向虚函数表的指针指向这个表格，表明这个表格属于类的一部分。 对于父类来说，这个表格中都是自己类的虚函数，而对于子类来说，首先这个虚函数表包含父类中所有的虚函数，当子类重写某个虚函数时就会用子类重写后的函数地址替换原来父类中定义的函数地址，同时在子类的虚函数表中还会包含子类独有的虚函数。 由此可见虚函数表的不同和复杂性还是体现在子类上，所以之后会分别测试单继承、多继承、菱形继承三种情况下虚函数表的不同，主要看一下虚函数表的个数和内存布局情况。 测试环境首先来说明一下测试环境，测试工具是VS2013，对于int *p; sizeof(p)的结果是4，说明编译环境是32位的，这个对后面查看内存结构非常关键。 开始测试使用VS2013查看类的内存布局非常方便，因为类的大小在编译期间就已经确定了，不用运行就可以通过添加编译选项知道类的大小和布局，而指向虚函数表的指针也会占用类的大小，如果说编译的时候确定了类的大小，那从侧面也说明了在编译期间虚函数表实际上也确定了。 使用VS2013查看类的布局时，可以在项目的属性页：“配置属性”–&gt;“C/C++”–&gt;“命令行”中输入以下任意一个命令， /d1reportAllClassLayout ：这个选项可以在VS的输出窗口显示所有相关联的类结构，因为一些外部类也会显示，最终的内容会非常多，需要自己辨别有用的信息。 /d1reportSingleClassLayoutXXX ：这个选项只会在输出窗口显示指定的类结构，只需要将XXX替换成想显示的类的名字即可，缺点就是无法同时显示多个想查看的类。 无虚函数简单类结构在查看虚函数表的结构之前，先使用之前的编译参数来查看一下简单的类结构，排除虚函数的干扰，能更清楚的了解类成员在类中的布局情况，有一点需要提一下，成员变量会占用类的大小，但是成员函数不会，如果有虚函数，所有的虚函数会被放入一个表格，而在类中放置一个指向虚函数表的指针，来看一下简单代码： 123456789101112131415class CBase&#123;public: void func() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase&#123;public: void func() &#123;&#125;public: int m_var2;&#125;; 编译输出的类的内存布局为： 1234567891011121&gt; class CBase size(4):1&gt; +---1&gt; 0 | m_var11&gt; +---1&gt;1&gt; class CDerived size(8):1&gt; +---1&gt; | +--- (base class CBase)1&gt; 0 | | m_var11&gt; | +---1&gt; 4 | m_var21&gt; +--- 从上面的输出内容来看，很清楚的可以看到基类 CBase 的大小 size(4) 占用4个字节，只有一个成员变量 m_var1，在类中偏移量为0的位置，而派生类 CDerived 占用8个字节大小，第一个成员继承自基类 CBase 的 m_var1，在类中偏移量为0的位置，还有一个子类独有的成员变量 m_var2，在类中偏移量为4的位置。 掌握着这种简单类的查看类结构的方法，接下来开始看一下包含虚函数的类的内存布局。 包含虚函数的类结构查看包含虚函数的类结构相对来说麻烦一点，先来说两个符号，免得一会看见结构发懵，vfptr 表示类中指向虚函数表的指针，通常放在类的起始位置，比成员变量的位置都要靠前， vftable 表示类中引用的虚函数表，在具体分析是还有有一些修饰符，用来表明是谁的虚函数表。 单继承这种情况的下的子类的虚函数表很简单，在该子类的内存布局上，最开始的位置保存了一个指向虚函数表的指针，虚函数表中包含了从父类继承的虚函数，当子类中重写父类虚函数时会将虚函数表中对应的函数地址替换，最后添加上自己独有的虚函数地址，下面上代码分析一下： 12345678910111213141516171819class CBase&#123;public: void func0() &#123;&#125; virtual void func1() &#123;&#125; virtual void func2() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase&#123;public: virtual void func2() &#123;&#125; virtual void func3() &#123;&#125; void func4() &#123;&#125;public: int m_var2;&#125;; 上面这两个类的内存布局情况如下： 123456789101112131415161718192021222324252627282930313233341&gt; class CBase size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var11&gt; +---1&gt;1&gt; CBase::$vftable@:1&gt; | &amp;CBase_meta1&gt; | 01&gt; 0 | &amp;CBase::func11&gt; 1 | &amp;CBase::func21&gt;1&gt; CBase::func1 this adjustor: 01&gt; CBase::func2 this adjustor: 01&gt;1&gt;1&gt; class CDerived size(12):1&gt; +---1&gt; | +--- (base class CBase)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | m_var11&gt; | +---1&gt; 8 | m_var21&gt; +---1&gt;1&gt; CDerived::$vftable@:1&gt; | &amp;CDerived_meta1&gt; | 01&gt; 0 | &amp;CBase::func11&gt; 1 | &amp;CDerived::func21&gt; 2 | &amp;CDerived::func31&gt;1&gt; CDerived::func2 this adjustor: 01&gt; CDerived::func3 this adjustor: 0 看起来是不是比没有虚函数时复杂多了，不过不要着急，从上到下慢慢分析就好了，这次的基类 CBase 大小是8个字节，首先是{vfptr}这个指向虚函数表的指针，在类中的偏移量是0，接下来是成员变量 m_var1，在类中偏移量是4。 然后是 CBase::$vftable@ 表示基类 CBase 的虚函数表，其中第一行 &amp;CBase_meta 看起来怪怪的，这里我们不展开（因为我也没弄太懂），应该是和虚函数表相关的元数据，第二行是一个0，看起来是一个偏移量，这里没有偏移，当出现偏移时我们再试着分析（相信我，马上就会出现），第三行内容 &amp;CBase::func1 是自己类的虚函数，前面有一个0，应该是指该虚函数在虚函数表中索引，第四行也是相同的情况。 接下来出现了两行非常相似的内容，看一下CBase::func1 this adjustor: 0，这句代码中的关键是 adjustor，其实有是一个偏移量，据说涉及到thunk技术，据说“thunk其实就是一条汇编指令，操作码是0xe9，就是jmp，后面紧跟操作数”，这里我们就不展开了，如果后面弄明白了可以单独写一篇总结，到此为止基类的内存结构就分析完了。 继续看派生类 CDerived，它的大小是12个字节，内部结构首先是 {vfptr} 一个指向虚函数表的指针，偏移量为0，m_var1 是从父类继承的成员变量，偏移量为4，而 m_var2 是自己类独有的成员变量，偏移量是8。 然后看派生类对应的虚函数表 CDerived::$vftable@，跳过前两行直接看一下后面几个函数，发现只有 func1 是基类的，而函数 func2 和 func3 都是派生类的，出现这种情况的原因是子类重写了函数 func2 和 func3 ，所以用重写后的函数地址替换了从基类继承的虚函数，造成了目前看到的状况。 最后又出现了两行 adjustor，很奇怪为什么 func1 函数没有 adjustor，貌似这个 adjustor 只对当前类有效，先留个疑问，接下来看一下多继承。 多继承当多个父类中都包含虚函数的时候，和子类关联的虚函数表就不止一个了，这个情况是可以通过使用sizeof(子类)来简单验证的： 这一部分是在没有VS的情况下预先写下的，本来考虑使用VS展开布局后，这一段就没有什么必要了，但是后来想想还是留着吧，因为这一段使用的g++编译器，64位环境，每个指针占用8个字节，通过不同的环境调试，更加可以证明，多继承下的多个虚函数表的存在性： 1234567class W&#123;public: long n;public: void func()&#123;&#125;&#125;; 对于这样的一个简单类，sizeof(W) = 8，类的大小等于成员变量的大小。 123456789101112131415class W1&#123;public: long n1;public: virtual void func1()&#123;&#125;&#125;;class W2&#123;public: long n2;public: virtual void func2()&#123;&#125;&#125;; 对于上面这两个简单的包含虚函数的类，sizeof(W1) = 16，sizeof(W2) = 16，因为每个类都除了一个 long 类型的成员变量以外，还包含了指向虚函数的一个指针，所以类的大小是16个字节。 1234567class WW : public W1, public W2&#123;public: long nn;public: virtual void func()&#123;&#125;&#125;; 而继承了 W1 和 W2 这两个父类的子类 WW 在继承了两个成员变量 n1 和 n2 之外，还有自己的成员变量 nn，三个变量占用字节24个，而计算类 WW 的的大小 sizeof(W1) = 40，也就是说除了成员变量24个字节，还剩余了16个字节的空间没有着落，我们知道它至少包含一个指向虚函数表的指针，占用8个字节的大小，还剩8个字节没有找到用处，从此处分析应该还有一个指向虚函数表的指针，具体的情况可以看一下内存分布。 接下来和单继承的分析方法一样，写代码编译查看布局： 123456789101112131415161718192021222324252627282930313233class CBase0&#123;public: void func0() &#123;&#125; virtual void func1() &#123;&#125; virtual void func2() &#123;&#125; virtual void func3() &#123;&#125;public: int m_var0;&#125;;class CBase1&#123;public: void func0() &#123;&#125; virtual void func2() &#123;&#125; virtual void func3() &#123;&#125; virtual void func4() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase0, public CBase1&#123;public: virtual void func1() &#123;&#125; virtual void func2() &#123;&#125; virtual void func4() &#123;&#125; virtual void func5() &#123;&#125; void func6() &#123;&#125;public: int m_var2;&#125;; 上面3个类描述了一个简单的多继承的情况，之所以写这么多函数就是构建一种，既有虚函数覆盖，又有单独不被覆盖的情况，下面展示了这段代码的内存布局。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566671&gt; class CBase0 size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var01&gt; +---1&gt;1&gt; CBase0::$vftable@:1&gt; | &amp;CBase0_meta1&gt; | 01&gt; 0 | &amp;CBase0::func11&gt; 1 | &amp;CBase0::func21&gt; 2 | &amp;CBase0::func31&gt;1&gt; CBase0::func1 this adjustor: 01&gt; CBase0::func2 this adjustor: 01&gt; CBase0::func3 this adjustor: 01&gt;1&gt;1&gt; class CBase1 size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var11&gt; +---1&gt;1&gt; CBase1::$vftable@:1&gt; | &amp;CBase1_meta1&gt; | 01&gt; 0 | &amp;CBase1::func21&gt; 1 | &amp;CBase1::func31&gt; 2 | &amp;CBase1::func41&gt;1&gt; CBase1::func2 this adjustor: 01&gt; CBase1::func3 this adjustor: 01&gt; CBase1::func4 this adjustor: 01&gt;1&gt;1&gt; class CDerived size(20):1&gt; +---1&gt; | +--- (base class CBase0)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | m_var01&gt; | +---1&gt; | +--- (base class CBase1)1&gt; 8 | | &#123;vfptr&#125;1&gt; 12 | | m_var11&gt; | +---1&gt; 16 | m_var21&gt; +---1&gt;1&gt; CDerived::$vftable@CBase0@:1&gt; | &amp;CDerived_meta1&gt; | 01&gt; 0 | &amp;CDerived::func11&gt; 1 | &amp;CDerived::func21&gt; 2 | &amp;CBase0::func31&gt; 3 | &amp;CDerived::func51&gt;1&gt; CDerived::$vftable@CBase1@:1&gt; | -81&gt; 0 | &amp;thunk: this-=8; goto CDerived::func21&gt; 1 | &amp;CBase1::func31&gt; 2 | &amp;CDerived::func41&gt;1&gt; CDerived::func1 this adjustor: 01&gt; CDerived::func2 this adjustor: 01&gt; CDerived::func4 this adjustor: 81&gt; CDerived::func5 this adjustor: 0 内容很多，前面两个基类 CBase0 和 CBase1 的布局很简单，参照之前的分析很容易看懂，直接从派生类看起吧。 我们发现派生类 CDerived 中确实有两个指向虚函数表的指针，接下来看一下这两个虚函数表，这个虚函数表和前面遇到的格式一样，除了第一行的元数据，第二行的诡异偏移量0，剩下的虚函数指针有的是从基类继承来的，有的是被当前派生类覆盖的，还有派生类自己独有的。 而第二个虚函数表就有点意思了，首先是少了 &amp;CDerived_meta 这一行，然后偏移量终于不是0了，而是-8，从派生类 CDerived 的内存布局上来看，以下开始大胆假设，至于小心求证的部分放到以后来做（看自己的进步状态了）。 第二个指向虚函数表的指针是不是距离类的起始偏移量是8，我猜这个-8的意思就是指的这个偏移量，这个值有可能被后面使用，第二行出现了 &amp;thunk: this-=8; goto CDerived::func2，其中包含 thunk 字样，表示这个 func2 不归我管，你去-8偏移量的那个虚函数表里找一找。 还有一点你有没有发现 func5 这个函数只在第一个虚函数表中出现，而没有出现在第二个虚函数表中，这也是一个规则，自己独有的虚函数放到第一个虚函数表中，这可能也是为什么只有第一个虚函数表包含元数据行。 最后一点，我们发现对于函数 func4 来说 adjustor 终于不是0了，而值变成了8，仿佛在说这个虚函数只在偏移量的为8的位置。 菱形继承对于这一部分，并没有太多新的内容，只是简单的菱形继承中，最初的基类在最终的子类中会包含两份，而虚函数的样子并没有太大的不同，接下来简单看一下代码和对应的内存布局即可，因为菱形继承并不被提倡，所以也不用花太多时间来分析这个问题。 123456789101112131415161718192021222324252627282930313233343536class CSuper&#123;public: virtual void func0() &#123;&#125; virtual void func1() &#123;&#125;public: int m_var;&#125;;class CBase0 : public CSuper&#123;public: virtual void func1() &#123;&#125; virtual void func2() &#123;&#125;public: int m_var0;&#125;;class CBase1 : public CSuper&#123;public: virtual void func1() &#123;&#125; virtual void func3() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase0, public CBase1&#123;public: virtual void func1() &#123;&#125; virtual void func3() &#123;&#125; virtual void func4() &#123;&#125;public: int m_var2;&#125;; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091921&gt; class CSuper size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var1&gt; +---1&gt;1&gt; CSuper::$vftable@:1&gt; | &amp;CSuper_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CSuper::func11&gt;1&gt; CSuper::func0 this adjustor: 01&gt; CSuper::func1 this adjustor: 01&gt;1&gt;1&gt; class CBase0 size(12):1&gt; +---1&gt; | +--- (base class CSuper)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | m_var1&gt; | +---1&gt; 8 | m_var01&gt; +---1&gt;1&gt; CBase0::$vftable@:1&gt; | &amp;CBase0_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CBase0::func11&gt; 2 | &amp;CBase0::func21&gt;1&gt; CBase0::func1 this adjustor: 01&gt; CBase0::func2 this adjustor: 01&gt;1&gt;1&gt; class CBase1 size(12):1&gt; +---1&gt; | +--- (base class CSuper)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | m_var1&gt; | +---1&gt; 8 | m_var11&gt; +---1&gt;1&gt; CBase1::$vftable@:1&gt; | &amp;CBase1_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CBase1::func11&gt; 2 | &amp;CBase1::func31&gt;1&gt; CBase1::func1 this adjustor: 01&gt; CBase1::func3 this adjustor: 01&gt;1&gt;1&gt; class CDerived size(28):1&gt; +---1&gt; | +--- (base class CBase0)1&gt; | | +--- (base class CSuper)1&gt; 0 | | | &#123;vfptr&#125;1&gt; 4 | | | m_var1&gt; | | +---1&gt; 8 | | m_var01&gt; | +---1&gt; | +--- (base class CBase1)1&gt; | | +--- (base class CSuper)1&gt; 12 | | | &#123;vfptr&#125;1&gt; 16 | | | m_var1&gt; | | +---1&gt; 20 | | m_var11&gt; | +---1&gt; 24 | m_var21&gt; +---1&gt;1&gt; CDerived::$vftable@CBase0@:1&gt; | &amp;CDerived_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CDerived::func11&gt; 2 | &amp;CBase0::func21&gt; 3 | &amp;CDerived::func41&gt;1&gt; CDerived::$vftable@CBase1@:1&gt; | -121&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;thunk: this-=12; goto CDerived::func11&gt; 2 | &amp;CDerived::func31&gt;1&gt; CDerived::func1 this adjustor: 01&gt; CDerived::func3 this adjustor: 121&gt; CDerived::func4 this adjustor: 0 虚继承解决菱形继承的一个常用的办法就是改为虚继承，实际上虚继承中就是将从最基类中继承的公共部分提取出来放在最子类的末尾，然后在提取之前的位置用一个叫做vbptr的指针指向这里。 之前看到过一种说法： 虚继承内部实现也相当复杂，似乎破坏了OO的纯洁性 至于复杂不复杂，看看后面的内存布局就很清楚了，那是相当复杂，其中出现了各种偏移，简单了解下就行了，如果不是维护老代码，谁现在还写这样的结构。 123456789101112131415161718192021222324252627282930313233343536class CSuper&#123;public: virtual void func0() &#123;&#125; virtual void func1() &#123;&#125;public: int m_var;&#125;;class CBase0 : virtual public CSuper&#123;public: virtual void func1() &#123;&#125; virtual void func2() &#123;&#125;public: int m_var0;&#125;;class CBase1 : virtual public CSuper&#123;public: virtual void func1() &#123;&#125; virtual void func3() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase0, public CBase1&#123;public: virtual void func1() &#123;&#125; virtual void func3() &#123;&#125; virtual void func4() &#123;&#125;public: int m_var2;&#125;; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281&gt; class CSuper size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var1&gt; +---1&gt;1&gt; CSuper::$vftable@:1&gt; | &amp;CSuper_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CSuper::func11&gt;1&gt; CSuper::func0 this adjustor: 01&gt; CSuper::func1 this adjustor: 01&gt;1&gt;1&gt; class CBase0 size(20):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | &#123;vbptr&#125;1&gt; 8 | m_var01&gt; +---1&gt; +--- (virtual base CSuper)1&gt; 12 | &#123;vfptr&#125;1&gt; 16 | m_var1&gt; +---1&gt;1&gt; CBase0::$vftable@CBase0@:1&gt; | &amp;CBase0_meta1&gt; | 01&gt; 0 | &amp;CBase0::func21&gt;1&gt; CBase0::$vbtable@:1&gt; 0 | -41&gt; 1 | 8 (CBase0d(CBase0+4)CSuper)1&gt;1&gt; CBase0::$vftable@CSuper@:1&gt; | -121&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CBase0::func11&gt;1&gt; CBase0::func1 this adjustor: 121&gt; CBase0::func2 this adjustor: 01&gt;1&gt; vbi: class offset o.vbptr o.vbte fVtorDisp1&gt; CSuper 12 4 4 01&gt;1&gt;1&gt; class CBase1 size(20):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | &#123;vbptr&#125;1&gt; 8 | m_var11&gt; +---1&gt; +--- (virtual base CSuper)1&gt; 12 | &#123;vfptr&#125;1&gt; 16 | m_var1&gt; +---1&gt;1&gt; CBase1::$vftable@CBase1@:1&gt; | &amp;CBase1_meta1&gt; | 01&gt; 0 | &amp;CBase1::func31&gt;1&gt; CBase1::$vbtable@:1&gt; 0 | -41&gt; 1 | 8 (CBase1d(CBase1+4)CSuper)1&gt;1&gt; CBase1::$vftable@CSuper@:1&gt; | -121&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CBase1::func11&gt;1&gt; CBase1::func1 this adjustor: 121&gt; CBase1::func3 this adjustor: 01&gt;1&gt; vbi: class offset o.vbptr o.vbte fVtorDisp1&gt; CSuper 12 4 4 01&gt;1&gt;1&gt; class CDerived size(36):1&gt; +---1&gt; | +--- (base class CBase0)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | &#123;vbptr&#125;1&gt; 8 | | m_var01&gt; | +---1&gt; | +--- (base class CBase1)1&gt; 12 | | &#123;vfptr&#125;1&gt; 16 | | &#123;vbptr&#125;1&gt; 20 | | m_var11&gt; | +---1&gt; 24 | m_var21&gt; +---1&gt; +--- (virtual base CSuper)1&gt; 28 | &#123;vfptr&#125;1&gt; 32 | m_var1&gt; +---1&gt;1&gt; CDerived::$vftable@CBase0@:1&gt; | &amp;CDerived_meta1&gt; | 01&gt; 0 | &amp;CBase0::func21&gt; 1 | &amp;CDerived::func41&gt;1&gt; CDerived::$vftable@CBase1@:1&gt; | -121&gt; 0 | &amp;CDerived::func31&gt;1&gt; CDerived::$vbtable@CBase0@:1&gt; 0 | -41&gt; 1 | 24 (CDerivedd(CBase0+4)CSuper)1&gt;1&gt; CDerived::$vbtable@CBase1@:1&gt; 0 | -41&gt; 1 | 12 (CDerivedd(CBase1+4)CSuper)1&gt;1&gt; CDerived::$vftable@CSuper@:1&gt; | -281&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CDerived::func11&gt;1&gt; CDerived::func1 this adjustor: 281&gt; CDerived::func3 this adjustor: 121&gt; CDerived::func4 this adjustor: 01&gt;1&gt; vbi: class offset o.vbptr o.vbte fVtorDisp1&gt; CSuper 28 4 4 0 总结 虚函数表是用来实现多态的核心内容。 多继承很强大但是不要滥用，当多个基类都含有虚函数时，派生类会有多个指向虚函数表的指针。 忘记菱形继承吧，为了取消二义性引入虚继承，结果造成内存分布复杂而又难以理解，大道至简，回归本质吧！]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>继承</tag>
        <tag>多态</tag>
        <tag>虚函数表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ STL中map的[]操作符使用时的一个坑]]></title>
    <url>%2Fblog%2F2019%2F12%2F14%2FC-STL%E4%B8%ADmap%E7%9A%84-%E6%93%8D%E4%BD%9C%E7%AC%A6%E4%BD%BF%E7%94%A8%E6%97%B6%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9D%91%2F</url>
    <content type="text"><![CDATA[前言学习C++，自从发现了map这个结构以后，就深深的被这种键值对的方式吸引了，写代码时也渐渐离不开这种结构了，一次偶然的机会发现这个map还有个 [] 运算符，仿佛又发现了新大陆一样，写代码更加方便了，殊不知一个深深的大坑正在前面等着我。 问题一开始学到map的时候还是中规中矩的使用函数插入删除，比如定义一个map，先引入头文件和命名空间： 1234#include &lt;map&gt;using namespace std;map&lt;int, int&gt; mapTest; 上面就轻松定义了一个map结构对象，是一个整数到另一个整数的映射，这种映射有什么用呢？举个简单的例子，这个映射可以作为学生的学号和成绩的对应关系，这样只要知道学号，就可以从map中直接获得对应的成绩很方便。 最开始学习插入时通常有以下两种方式： 12mapTest.insert(map&lt;int, int&gt;::value_type(1001, 100));mapTest.insert(make_pair(1002, 98)); 但是学了 map 的 [] 操作符以后，上述代码可以写成： 12mapTest[1001] = 100;mapTest[1002] = 98; 查找一个元素的时候需要用到find()函数，一般写成 123map&lt;int, int&gt;::const_iterator itor = mapTest.find(1001);if (itor != mapTest.end()) return itor-&gt;second; 但是学了 map 的 [] 操作符以后，上述代码就可以简写成： 1return mapTest[1001]; 特别的在插入一个元素的时候，比如用来计数，每次给一个键对应的值加1时，可以直接写成： 1mapTest[1001] += 1; 根本不用检查 1001 这个键是否存在，使用 [] 操作符，在使用前会先默认成0，然后执行+1操作，这个比先使用find()查找，然后+1操作后再插入方便多了。 其实这只是使用map结构的一种语法糖，但是这语法糖简直太好使了，太甜了，让人欲罢不能，所以我就含着这块糖掉进了坑里，因为调用 map 的有时会产生副作用，如果查找一个键不在 map 中，则会在map中对应的这个键的位置插入默认值，接下来看一下例子就明白了。 测试过程测试代码在VS2015中编译运行，C++11标准，如果编译不正确可以看一下环境是否不同，尝试修改代码实现即可，测试的例子也是上面提到的，使用 map 来存储学生学号和成绩的对应关系，下面来简单实现一个类，描述这种关系： 编写测试类1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;map&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;class CReportCard&#123;public: CReportCard() &#123; m_mapStuNo2Score.clear(); &#125; ~CReportCard() &#123; m_mapStuNo2Score.clear(); &#125;public: void LoadScores(); // 模拟录入成绩 int GetScoreByStudentNo(const int nStudentNo); // 根据学号查询成绩 void PrintReportCard(); // 打印成绩单private: map&lt;int, int&gt; m_mapStuNo2Score;&#125;;void CReportCard::LoadScores()&#123; m_mapStuNo2Score[1001] = 99; m_mapStuNo2Score[1002] = 94; m_mapStuNo2Score[1004] = 89; m_mapStuNo2Score[1005] = 92; m_mapStuNo2Score[1007] = 80;&#125;int CReportCard::GetScoreByStudentNo(const int nStudentNo)&#123; return m_mapStuNo2Score[nStudentNo];&#125;void CReportCard::PrintReportCard()&#123; cout &lt;&lt; "show report card start-----&gt;" &lt;&lt; endl; std::for_each(m_mapStuNo2Score.begin(), m_mapStuNo2Score.end(), [](std::map&lt;int, int&gt;::reference socrepair) &#123; std::cout &lt;&lt; socrepair.first &lt;&lt; "'s score = " &lt;&lt; socrepair.second &lt;&lt; "\n"; &#125;); cout &lt;&lt; "show report card end&lt;------" &lt;&lt; endl;&#125; 这个类的内容很简单，使用 map 类型的对象 m_mapStuNo2Score 来存储学号和成绩的对应关系，LoadScores()函数中使用 [] 操作符向 map 中插入元素，模拟成绩录入过程；GetScoreByStudentNo()函数同样使用了 [] 操作符模拟成绩查询过程；PrintReportCard()函数遍历 map 打印成绩单信息。 看似正常的调用接下来编写一个函数来使用这个类，测试如下： 123456789101112int main(int argc, char* argv[])&#123; CReportCard obj; obj.LoadScores(); cout &lt;&lt; "student no = 1001, score = " &lt;&lt; obj.GetScoreByStudentNo(1001) &lt;&lt; endl; cout &lt;&lt; "student no = 1004, score = " &lt;&lt; obj.GetScoreByStudentNo(1004) &lt;&lt; endl; obj.PrintReportCard(); return 0;&#125; 首先调用 LoadScores()函数来加载数据，然后通过 GetScoreByStudentNo() 函数来查找学号为 1001 和 1004 的两个学生的成绩，最后打印一下成绩单，接下来看一下运行结果： student no = 1001, score = 99student no = 1004, score = 89show report card start—–&gt;1001’s score = 991002’s score = 941004’s score = 891005’s score = 921007’s score = 80show report card end&lt;—— 以上结果正常的打印出了查询的分数和成绩单，一切看起来毫无问题，如果查询的学号不存在又会怎么样呢？ 出现问题的调用修改上面的测试函数，将学生学号改成不存在的数值，修改如下： 12345678910111213int main(int argc, char* argv[])&#123; CReportCard obj; obj.LoadScores(); cout &lt;&lt; endl; cout &lt;&lt; "student no = 1011, score = " &lt;&lt; obj.GetScoreByStudentNo(1011) &lt;&lt; endl; cout &lt;&lt; "student no = 1014, score = " &lt;&lt; obj.GetScoreByStudentNo(1014) &lt;&lt; endl; obj.PrintReportCard(); return 0;&#125; 大部分的内容并没有发生变化，只将学号改成了不存在的情况，测试结果如下： student no = 1011, score = 0student no = 1014, score = 0show report card start—–&gt;1001’s score = 991002’s score = 941004’s score = 891005’s score = 921007’s score = 801011’s score = 01014’s score = 0show report card end&lt;—— 不存在的学号对应的分数是0，这应该也说的过去，因为键不存在，所以对 map 使用 [] 操作符查找时，寻找的键不存在则返回了整型的默认值0，但是在打印成绩单的时候居然多了两项，这充分暴露了 [] 操作符可能产生的副作用。 在查找返回时，[] 操作符并不是找不到返回对应类型默认值就完了，还会把查找的键和默认值作为一对，插入到待查的 map，这种操作一般是我们不需要的，所以在你明确不需要这个副作用时，查找 map 元素不要使用 [] 操作符。 亡羊补牢上面说到，[] 操作符查找不到就插入的副作用一般我们不使用，所以在查找时还是使用 find() 函数更规范一些，修改 GetScoreByStudentNo() 函数如下： 123456int CReportCard::GetScoreByStudentNo(const int nStudentNo)&#123; //return m_mapStuNo2Score[nStudentNo]; map&lt;int, int&gt;::const_iterator itor = m_mapStuNo2Score.find(nStudentNo); return itor != m_mapStuNo2Score.end() ? itor-&gt;second : 0;&#125; 此时再运行上面的例子就正常了，成绩单中也不会插入无效值了。 总结 map 的 [] 操作符会有副作用，当查找的键不存在时，会在对应键位置插入默认值 时刻保持清醒的头脑，过分的方便或许会给你自己埋下深深的坑 敬畏自然、敬畏生命、敬畏你写下的每一行代码]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>find</tag>
        <tag>中括号</tag>
        <tag>insert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL中全局变量、会话变量、用户变量和局部变量的区别]]></title>
    <url>%2Fblog%2F2019%2F12%2F03%2FMySQL%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E3%80%81%E4%BC%9A%E8%AF%9D%E5%8F%98%E9%87%8F%E3%80%81%E7%94%A8%E6%88%B7%E5%8F%98%E9%87%8F%E5%92%8C%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言之前在项目的存储过程中发现有通过 DECLARE 关键字定义的变量如DECLARE cnt INT DEFAULT 0;，还有形如 @count 这样的变量，存储过程中拿过来直接就进行设置，像这样set @count=1;，这两种类型的变量究竟有什么区别却弄不清楚，赶紧上网查询资料，发现还有@@sql_mode这样的变量，这一个圈俩圈的到底是什么啊？会不会出现三个圈的情况？ 变量分类与关系经过一段时间学习和测试，再配合官方的文档，现在大致弄清楚了这些变量的区别，一般可以将MySQL中的变量分为全局变量、会话变量、用户变量和局部变量，这是很常见的分类方法，这些变量的作用是什么呢？可以从前往后依次看一下。 首先我们知道MySQL服务器维护了许多系统变量来控制其运行的行为，这些变量有些是默认编译到软件中的，有些是可以通过外部配置文件来配置覆盖的，如果想查询自编译的内置变量和从文件中可以读取覆盖的变量可以通过以下命令来查询: 1mysqld --verbose --help 如果想只看自编译的内置变量可以使用命令： 1mysqld --no-defaults --verbose --help 接下来简单了解一下这几类变量的应用范围，首先MySQL服务器启动时会使用其软件内置的变量（俗称写死在代码中的）和配置文件中的变量（如果允许，是可以覆盖源代码中的默认值的）来初始化整个MySQL服务器的运行环境，这些变量通常就是我们所说的全局变量，这些在内存中的全局变量有些是可以修改的。 当有客户端连接到MySQL服务器的时候，MySQL服务器会将这些全局变量的大部分复制一份作为这个连接客户端的会话变量，这些会话变量与客户端连接绑定，连接的客户端可以修改其中允许修改的变量，但是当连接断开时这些会话变量全部消失，重新连接时会从全局变量中重新复制一份。 其实与连接相关的变量不只有会话变量一种，用户变量也是这样的，用户变量其实就是用户自定义变量，当客户端连接上MySQL服务器之后就可以自己定义一些变量，这些变量在整个连接过程中有效，当连接断开时，这些用户变量消失。 局部变量实际上最好理解，通常由DECLARE 关键字来定义，经常出现在存储过程中，非常类似于C和C++函数中的局部变量，而存储过程的参数也和这种变量非常相似，基本上可以作为同一种变量来对待。 变量的修改先说全局变量有很多是可以动态调整的，也就是说可以在MySQL服务器运行期间通过 SET 命令修改全局变量，而不需要重新启动 MySQL 服务，但是这种方法在修改大部分变量的时候都需要超级权限，比如root账户。 相比之下会话对变量修改的要求要低的多，因为修改会话变量通常只会影响当前连接，但是有个别一些变量是例外的，修改它们也需要较高的权限，比如 binlog_format 和 sql_log_bin，因为设置这些变量的值将影响当前会话的二进制日志记录，也有可能对服务器复制和备份的完整性产生更广泛的影响。 至于用户变量和局部变量，听名字就知道，这些变量的生杀大权完全掌握在自己手中，想改就改，完全不需要理会什么权限，它的定义和使用全都由用户自己掌握。 测试环境以下给出MySQL的版本，同时使用root用户测试，这样可以避免一些权限问题。 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 变量查询与设置全局变量这些变量来源于软件自编译、配置文件中、以及启动参数中指定的变量，其中大部分是可以由root用户通过 SET 命令直接在运行时来修改的，一旦 MySQL 服务器重新启动，所有修改都被还原。如果修改了配置文件，想恢复最初的设置，只需要将配置文件还原，重新启动 MySQL 服务器，一切都可以恢复原来的样子。 查询查询所有的全局变量： 1show global variables; 一般不会这么用，这样查简直太多了，大概有500多个，通常会加个like控制过滤条件： 12345678910111213141516171819mysql&gt; show global variables like 'sql%';+------------------------+----------------------------------------------------------------+| Variable_name | Value |+------------------------+----------------------------------------------------------------+| sql_auto_is_null | OFF || sql_big_selects | ON || sql_buffer_result | OFF || sql_log_off | OFF || sql_mode | STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION || sql_notes | ON || sql_quote_show_create | ON || sql_safe_updates | OFF || sql_select_limit | 18446744073709551615 || sql_slave_skip_counter | 0 || sql_warnings | OFF |+------------------------+----------------------------------------------------------------+11 rows in set, 1 warning (0.00 sec)mysql&gt; 还有一种查询方法就是通过select语句： 1select @@global.sql_mode; 当一个全局变量不存在会话变量副本时也可以这样 1select @@max_connections; 设置设置全局变量也有两种方式： 1set global sql_mode=''; 或者 1set @@global.sql_mode=''; 会话变量这些变量基本来自于全局变量的复制，与客户端连接有关，无论怎样修改，当连接断开后，一切都会还原，下次连接时又是一次新的开始。 查询类比全局变量，会话变量也有类似的查询方式，查询所有会话变量 1show session variables; 添加查询匹配，只查一部分会话变量： 1show session variables like 'sql%'; 查询特定的会话变量，以下三种都可以： 123select @@session.sql_mode;select @@local.sql_mode;select @@sql_mode; 设置会话变量的设置方法是最多的，以下的方式都可以： 123456set session sql_mode = '';set local sql_mode = '';set @@session.sql_mode = '';set @@local.sql_mode = '';set @@sql_mode = '';set sql_mode = ''; 用户变量用户变量就是用户自己定义的变量，也是在连接断开时失效，定义和使用相比会话变量来说简单许多。 查询直接一个select语句就可以了： 1select @count; 设置设置也相对简单，可以直接使用set命令： 12set @count=1;set @sum:=0; 也可以使用select into语句来设置值，比如： 1select count(id) into @count from items where price &lt; 99; 局部变量局部变量通常出现在存储过程中，用于中间计算结果，交换数据等等，当存储过程执行完，变量的生命周期也就结束了。 查询也是使用select语句： 12declare count int(4);select count; 设置与用户变量非常类似： 1234declare count int(4);declare sum int(4);set count=1;set sum:=0; 也可以使用select into语句来设置值，比如： 12declare count int(4);select count(id) into count from items where price &lt; 99; 其实还有一种存储过程参数，也就是C/C++中常说的形参，使用方法与局部变量基本一致，就当成局部变量来用就可以了 几种变量的对比使用 操作类型 全局变量 会话变量 用户变量 局部变量（参数） 文档常用名 global variables session variables user-defined variables local variables 出现的位置 命令行、函数、存储过程 命令行、函数、存储过程 命令行、函数、存储过程 函数、存储过程 定义的方式 只能查看修改，不能定义 只能查看修改，不能定义 直接使用，@var形式 declare count int(4); 有效生命周期 服务器重启时恢复默认值 断开连接时，变量消失 断开连接时，变量消失 出了函数或存储过程的作用域，变量无效 查看所有变量 show global variables; show session variables; - - 查看部分变量 show global variables like &#39;sql%&#39;; show session variables like &#39;sql%&#39;; - - 查看指定变量 select @@global.sql_mode、select @@max_connections; select @@session.sql_mode;、 select @@local.sql_mode;、 select @@sql_mode; select @var; select count; 设置指定变量 set global sql_mode=&#39;&#39;;、 set @@global.sql_mode=&#39;&#39;; set session sql_mode = &#39;&#39;;、 set local sql_mode = &#39;&#39;;、 set @@session.sql_mode = &#39;&#39;;、 set @@local.sql_mode = &#39;&#39;;、 set @@sql_mode = &#39;&#39;;、 set sql_mode = &#39;&#39;; set @var=1;、 set @var:=101;、 select 100 into @var; set count=1;、 set count:=101;、 select 100 into count; 相信看了这个对比的表格，之前的很多疑惑就应该清楚了，如果发现其中有什么疑惑的地方可以给我留言，或者发现有什么错误也可以一针见血的指出来，我会尽快改正的。 总结 MySQL 中的变量通常分为：全局变量、 会话变量、 用户变量、 局部变量 其实还有一个存储过程和函数的参数，这种类型和局部变量基本一致，当成局部变量来使用就行了 在表格中有一个容易疑惑的点就是无论是全局变量还是会话变量都有select@@变量名的形式。 select@@变量名这种形式默认取的是会话变量，如果查询的会话变量不存在就会获取全局变量，比如@@max_connections 但是SET操作的时候，set @@变量名=xxx 总是操作的会话变量，如果会话变量不存在就会报错]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>全局变量</tag>
        <tag>会话变量</tag>
        <tag>用户变量</tag>
        <tag>局部变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库导入、导出、复制表、重命名表]]></title>
    <url>%2Fblog%2F2019%2F11%2F30%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%BC%E5%85%A5%E3%80%81%E5%AF%BC%E5%87%BA%E3%80%81%E5%A4%8D%E5%88%B6%E8%A1%A8%E3%80%81%E9%87%8D%E5%91%BD%E5%90%8D%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[前言提前说明这是一篇小白总结，高手勿喷请绕行，写这篇总结的原因是发觉自己有时候确实眼高手低了，大道至简，花了很多时间去看索引、缓存、主从等等，等到出现实际问题的时候却发现自己磨磨蹭蹭写出的SQL语句居然有语法错误，看来还得稳扎稳打从基础入手，因为实际工作的用到的SQL并不多，现在把常用的几条总结一下，即使下次不能立马写出来，也能在这篇文章中的快速找到想要的。 正如标题中的提到的这些，数据库的导入和导出在紧急处理线上数据时很常用，而复制表基本上也是为了不影响原数据的情况下进行问题排查，重命名表是为了导入多份备份数据时原数据不被覆盖，比如想对比两天的A表数据，可以先把第一天的数据导入，然后将A表名修改成Aold，接着直接再导入第二天的数据库数据，这样就可以将数据库中表Aold和A进行对比了，可以避免两个数据库中的同一个表进行对比时写很长的SQL。 测试环境 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 11Server version: 5.7.28-log MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 测试过程为了说明实现这些要求的具体SQL，我们先建立一个测试数据库，然后创建测试表格，插入测试数据，最后在这个数据库上依次实现这些要求。 创建测试数据创建测试数据库和表格1234567891011121314151617181920mysql&gt; create database dbtest;Query OK, 1 row affected (0.00 sec)mysql&gt; use dbtestDatabase changedmysql&gt; create table a(id int, num int);Query OK, 0 rows affected (0.02 sec)mysql&gt; create table b(id int, name varchar(32));Query OK, 0 rows affected (0.01 sec)mysql&gt; show tables;+--------------+| Tables_in_zz |+--------------+| a || b |+--------------+2 rows in set (0.00 sec) 插入测试数据1234567891011121314151617181920212223242526272829mysql&gt; insert into a values(1, 100);Query OK, 1 row affected (0.02 sec)mysql&gt; insert into a values(2, 200);Query OK, 1 row affected (0.01 sec)mysql&gt; select * from a;+------+------+| id | num |+------+------+| 1 | 100 || 2 | 200 |+------+------+2 rows in set (0.01 sec)mysql&gt; insert into b values(1, 'albert');Query OK, 1 row affected (0.01 sec)mysql&gt; insert into b values(2, 'tom');Query OK, 1 row affected (0.01 sec)mysql&gt; select * from b;+------+--------+| id | name |+------+--------+| 1 | albert || 2 | tom |+------+--------+2 rows in set (0.00 sec) 数据库导出数据库导出时使用的最基础的工具叫mysqldump，这是单独的工具不是mysql命令，刚学MySQL的时候居然在MySQL的命令行中使用mysqldump，现在只能当笑话看了。 导出指定数据库中所有表结构和数据在系统的命令行工具下输入以下命令，敲入回车输入密码，再回车就可以将数据库dbtest的结构和数据导出到dbtest.sql文件中： 1&gt;mysqldump -uroot -h192.168.1.101 -p dbtest &gt; dbtest.sql 打开dbtest.sql文件，显示如下：文件内容比较长，里面包含了数据库的表结构和其中的数据信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273-- MySQL dump 10.13 Distrib 5.7.21, for Win64 (x86_64)---- Host: localhost Database: dbtest-- -------------------------------------------------------- Server version 5.7.21-log/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;/*!40101 SET NAMES utf8 */;/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;/*!40103 SET TIME_ZONE='+00:00' */;/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;---- Table structure for table `a`--DROP TABLE IF EXISTS `a`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `a` ( `id` int(11) DEFAULT NULL, `num` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;---- Dumping data for table `a`--LOCK TABLES `a` WRITE;/*!40000 ALTER TABLE `a` DISABLE KEYS */;INSERT INTO `a` VALUES (1,100),(2,200);/*!40000 ALTER TABLE `a` ENABLE KEYS */;UNLOCK TABLES;---- Table structure for table `b`--DROP TABLE IF EXISTS `b`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `b` ( `id` int(11) DEFAULT NULL, `name` varchar(32) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;---- Dumping data for table `b`--LOCK TABLES `b` WRITE;/*!40000 ALTER TABLE `b` DISABLE KEYS */;INSERT INTO `b` VALUES (1,'albert'),(2,'tom');/*!40000 ALTER TABLE `b` ENABLE KEYS */;UNLOCK TABLES;/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;-- Dump completed on 2019-11-30 11:32:23 只导出指定数据库中所有表的结构只导出表结构的方法和上面是一样的，只是加上 -d 选项就可以了，运行下面命令就可以将dbtest数据库中的所有表结构导出到 dbteststructure.sql 中，因为和上面类似，文件中的内容就不贴了，只比 dbtest.sql 文件少了插入数据的内容： 1&gt;mysqldump -uroot -h192.168.1.101 -p -d dbtest &gt; dbteststructure.sql 只导出指定数据库中的一个表只导出数据库中指定表，可以是一个也可以是多个，在数据库名字后面跟表的名字就可以了，比如导出表a： 1&gt;mysqldump -uroot -h192.168.1.101 -p dbtest a &gt; dbtest_a.sql 导出多个数据库数据出多个数据库数据需要加上 --databases 选项，然后在后面依次跟上数据库名字就行： 1&gt;mysqldump -uroot -h192.168.1.101 -p --databases dbtest dbtest2 &gt; db_more.sql 导出所有数据库数据导出所有的数据库时不需要加数据库的名字，加上 --all-databases 选项就可以了 1&gt;mysqldump -uroot -h192.168.1.101 -p --all-databases &gt; db_all.sql 数据库导入数据库的导入比较简单，实际上就是把sql文件在MySQL中执行一下，可以使用以下两种方式： 系统命令行导入一般需要指定导入的数据库dbtest和sql文件的路径，在Linux上举例： 1&gt;mysql -uroot -h192.168.1.101 -p dbtest &lt; /home/albert/dbtest.sql --default-character-set=utf8 在Windows上举例，主要是路径需要注意，Windows上使用正斜杠/和反斜杠\都可以，默认是反斜杠，如果路径中包含空格可以用双引号将整个路径包起来： 1&gt;mysql -uroot -h192.168.1.101 -p dbtest &lt; D:\albert\dbtest.sql --default-character-set=utf8 注意--default-character-set=utf8是指定默认的字符集，主要是防止导入时出现编码错误，之前总结过，在此复习一下。 MySQL命令行导入首先连接MySQL服务器进行登陆： 1&gt;mysql -uroot -h192.168.1.101 -p --default-character-set=utf8 输入密码登陆后再使用source命令直接导入sql文件就可以： 1mysql&gt; source D:\albert\dbtest.sql 数据表复制数据表的复制可以分为结构复制和完全复制，其中完全复制时可以先复制结构，再将数据复制到新表中： 只复制表结构 使用LIKE语句，只不过5.0版本之后才支持，之前的版本无法使用 1CREATE TABLE new_table LIKE old_table; 1234567891011121314151617181920212223mysql&gt; select * from a;+------+------+| id | num |+------+------+| 1 | 100 || 2 | 200 |+------+------+2 rows in set (0.01 sec)mysql&gt; create table a2 like a;Query OK, 0 rows affected (0.04 sec)mysql&gt; desc a2;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.02 sec)mysql&gt; select * from a2;Empty set (0.00 sec) 使用 SELECT 语句加不成立的条件实现 1CREATE TABLE new_table SELECT * FROM old_table WHERE FALSE; 123456789101112131415mysql&gt; create table a3 select * from a where false;Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc a3;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.01 sec)mysql&gt; select * from a3;Empty set (0.01 sec) 复制表结构和数据 可以先按照上面的语句复制结构，然后再讲数据复制过去： 12CREATE TABLE new_table SELECT * FROM old_table WHERE FALSE;INSERT INTO new_table SELECT * FROM old_table; 123456789101112mysql&gt; insert into a2 select * from a;Query OK, 2 rows affected (0.07 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from a2;+------+------+| id | num |+------+------+| 1 | 100 || 2 | 200 |+------+------+2 rows in set (0.00 sec) 直接将结构和数据全部复制 1CREATE TABLE new_table SELECT * FROM old_table; 123456789101112131415161718192021mysql&gt; create table a4 select * from a;Query OK, 2 rows affected (0.06 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; desc a4;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.01 sec)mysql&gt; select * from a4;+------+------+| id | num |+------+------+| 1 | 100 || 2 | 200 |+------+------+2 rows in set (0.00 sec) 数据表重命名使用 ALTER 命令实现1ALTER TABLE old_table RENAME [TO|AS] new_table; 这个语句中的TO和AS是可选的，加不加都行，也可以选择其中一个，效果是一样的，测试如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || b |+------------------+5 rows in set (0.02 sec)mysql&gt; alter table b rename c;Query OK, 0 rows affected (0.04 sec)mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || c |+------------------+5 rows in set (0.00 sec)mysql&gt; alter table c rename to d;Query OK, 0 rows affected (0.02 sec)mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || d |+------------------+5 rows in set (0.00 sec)mysql&gt; alter table d rename as e;Query OK, 0 rows affected (0.02 sec)mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || e |+------------------+5 rows in set (0.00 sec) 使用RENAME命令1RENAME TABLE old_table TO new_table; 这个语句中TO就不能省略了，否则会报语法错误，测试如下： 123456789101112131415161718192021mysql&gt; show tables -&gt; ;+------------------+| Tables_in_dbtest |+------------------+| a || e |+------------------+5 rows in set (0.00 sec)mysql&gt; rename table e to f;Query OK, 0 rows affected (0.11 sec)mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || f |+------------------+5 rows in set (0.01 sec) 总结 数据库的导出、导入、数据表的复制、重命名都是MySQL操作的基础，需要熟练掌握 数据库导出：mysqldump -uroot -h192.168.1.101 -p dbtest &gt; dbtest.sql 数据库导入：mysql -uroot -h192.168.1.101 -p dbtest &lt; /tmp/dbtest.sql --default-character-set=utf8 数据表复制：CREATE TABLE new_table SELECT * FROM old_table; 表格重命名：RENAME TABLE old_table TO new_table;]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>导入</tag>
        <tag>导出</tag>
        <tag>复制表</tag>
        <tag>重命名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql导入数据库时报错ERROR: Unknown command ' ']]></title>
    <url>%2Fblog%2F2019%2F11%2F20%2FMysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%B6%E6%8A%A5%E9%94%99ERROR-Unknown-command-0%2F</url>
    <content type="text"><![CDATA[前言之前查询数据问题时多次使用过数据库导出导入命令，从来没发生过这种错误，那是一个风和日丽的上午，忽然来了一个紧急的任务，线上数据出问题了，需要马上处理一下，连上数据库备份服务器，找到备份数据直接下载下来，优雅（cong mang）地处理着这一切，本打算在Windows上直接导入查询处理一下算了，结果忙中添乱，导入数据库时居然报了一大堆错误，其中最扎眼的就是一连串的ERROR: Unknown command ‘\0’，没办法了，先找一台Linux服务器，上传导入数据分析处理一气呵成，处理完线上问题终于有时间回头来看看这个问题了。 测试环境数据库版本 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 9Server version: 5.7.28-log MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 系统版本 Win10马马虎虎版本来打算用来快速处理的，结果添了不少乱 问题出现过程直接使用cmd命令行输入mysql -uroot -h192.168.1.101 -p，然后输入密码后成功登录，接着选择数据库use dbtest，导入数据库文件source E:\onlinedb.sql，结果意外发生了，出现了一大堆的如下错误： …………ERROR:Unknown command ‘\0’.ERROR:Unknown command ‘\0’.ERROR:Unknown command ‘\0’.ERROR:Unknown command ‘\0’.ERROR 1064 (42000): You have an error in your SQL syntax; check the manual … 命令行界面中出现了多次错误提醒ERROR: Unknown command &#39;\0&#39;.，最后有一个常见的语法错误提醒，看到这个&#39;\0&#39;，这个字节中的0，程序中nullptr，我猜到可能是编码问题，而最后的语法错误也是由于编码不同而部分解析导致的，于是查了一些资料发现果然是编码问题，只要在客户端连接Mysql服务器时指定UTF8编码就可以了。 问题结果过程导入的流程不变，只要在客户端连接Mysql服务器时指定编码就可以避免前面遇到的错误，连接时的命令修改为mysql -uroot -h192.168.1.101 -p --default-character-set=utf8就没问题了，上面提到的语法错误也不存在了。 其实这种跨平台的坑有很多，因为平台之间的哲学思想不同，导致对一些默认值的处理不太一样，同样的文件在Windows平台上导入报错，但是我换到Linux服务器上就没有任何问题，接触多了自然就释然了。 总结 导入sql文件时报错ERROR: Unknown command &#39;\0&#39;需在Mysql命令行客户端连接服务器时指定编码 连接时指定编码的格式为mysql -uroot -h192.168.1.101 -p --default-character-set=utf8]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>ERROR</tag>
        <tag>source</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中Blob类型字段的插入、查看、截取和拼接]]></title>
    <url>%2Fblog%2F2019%2F11%2F20%2FMysql%E4%B8%ADBlob%E7%B1%BB%E5%9E%8B%E5%AD%97%E6%AE%B5%E7%9A%84%E6%8F%92%E5%85%A5%E3%80%81%E6%9F%A5%E7%9C%8B%E3%80%81%E6%88%AA%E5%8F%96%E5%92%8C%E6%8B%BC%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[前言本来并没有太注意到Blob这个类型，在游戏的开发中存储数据常常使用这个类型，这里的使用其实是“机械”的使用，因为应用程序和Mysql数据库之间的逻辑已经封装好了，我只要把对应的数据扔到接口里就行了，可是最近发生了点问题，所以决定深入研究一下Blob类型的操作方法。 问题是这样的，由于应用程序的一个逻辑错误，导致Mysql数据库中有一个Blob类型的字段的前几个字节被写入了错误的值，当然这个问题，我们可以通过应用程序处理，在逻辑中读出Blob字段的值，修改为正确值以后再写回到数据库中，可是这样有些麻烦，并且这些处理逻辑与业务无关。 为了更方便的解决问题，决定使用SQL语句直接修改数据库，将错误的数据恢复正常，因为之前没有直接用SQL修改过Blob类型的字段，所以多花了一点时间用来测试，现在把整个过程记录一下，方便下次直接操作。 在整个处理的过程中用到了查看、截取和拼接三种操作，为了让例子看起来更加精炼，我们把插入也测一下，然后创造出我们想要的精简后的数据，首先还是来看一下数据库版本。 数据库版本 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.28-log MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 创建测试表测试的表格结构很简单，只需要带有一个Blob类型的字段就尅可以了，为了操作方便再添加一个id，操作的SQL语句如下： 1234567891011mysql&gt; create table bloboperation(id int, data blob);Query OK, 0 rows affected (0.36 sec)mysql&gt; desc bloboperation;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || data | blob | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.07 sec) 插入数据因为知道Blob是二进制数据，所以首先插入两条用十六进制表示的字节串试一下，提示成功插入，插入两条一样的数据是为了之后修改的时候对比方便： 12345mysql&gt; insert into bloboperation values(1, 0x01020304FFFFFFFF0000000CAACB0000);Query OK, 1 row affected (0.06 sec)mysql&gt; insert into bloboperation values(2, 0x01020304FFFFFFFF0000000CAACB0000);Query OK, 1 row affected (0.04 sec) 插入字节串没有问题，那插入字符串和数字看看会有什么结果，测试语句如下，最后发现均可以正常插入： 12345mysql&gt; insert into bloboperation values(3, 'hellworld');Query OK, 1 row affected (0.04 sec)mysql&gt; insert into bloboperation values(4, 0);Query OK, 1 row affected (0.03 sec) 查看数据上面插入了4条不同类型的数据都成功了，我们简单来查一下看看数据和我们插入的是否一样： 12345678910mysql&gt; select * from bloboperation;+------+------------------+| id | data |+------+------------------+| 1 | ÿÿÿÿ ? || 2 | ÿÿÿÿ ? || 3 | hellworld || 4 | 0 |+------+------------------+4 rows in set (0.00 sec) 这究竟是什么鬼，除了第3、4条和我们插入的数据一样，前两条数据看起来和我们之前插入数据时完全不一样，其实这时候需要用到一个hex()函数来看Blob类型的数据，查询结果如下： 12345678910mysql&gt; select id,hex(data),length(data) from bloboperation;+------+----------------------------------+--------------+| id | hex(data) | length(data) |+------+----------------------------------+--------------+| 1 | 01020304FFFFFFFF0000000CAACB0000 | 16 || 2 | 01020304FFFFFFFF0000000CAACB0000 | 16 || 3 | 68656C6C776F726C64 | 9 || 4 | 30 | 1 |+------+----------------------------------+--------------+4 rows in set (0.02 sec) 这回前两条数据正常了，可是后两条数据为什么又看起来不一样了呢，如果你产生了这样的疑问，就需要好好理解一下内存值和表现值的对应关系了，第4条插入语句的中数据0，实际上是被当做字符串存储的，而字符’0’的ASCII码是十进制的48，表示成十六进制就是0x30，也就是上面查到的这样，同理这个打错了的字符串’hellworld’也是这样存储的。 截取数据本来以为截取数据需要一个特别的函数，没想到用的是字符串截取函数substring(str,startpos,length)，第一个参数是需要截取的字符串或字节串，第二个参数起始位置从1开始，第三个参数就是截取的长度。 以第一条数据为例，截取第4到第8个一共5个字节，测试如下： 1234567mysql&gt; select id,hex(substring(data,4,5)) from bloboperation where id=1;+------+--------------------------+| id | hex(substring(data,4,5)) |+------+--------------------------+| 1 | 04FFFFFFFF |+------+--------------------------+1 row in set (0.00 sec) 拼接数据看到上一个函数之后，你应该有所察觉，这个Blob类型的数据处理起来并不麻烦，那么拼接函数会不会用的是concat()这个处理字符串的函数呢？恭喜你，答对了，就是使用这个函数，我们来把前四个字节和最后四个字节拼接到一起，测试如下： 1234567mysql&gt; select id,hex(concat(substring(data,1,4),substring(data,13,4))) from bloboperation where id=1;+------+-------------------------------------------------------+| id | hex(concat(substring(data,1,4),substring(data,13,4))) |+------+-------------------------------------------------------+| 1 | 01020304AACB0000 |+------+-------------------------------------------------------+1 row in set (0.00 sec) 进制转换我们看到id为1的数据有16个字节，实际上在应用程序的内存中对应了4个int类型，每个int类型占用四个字节，为了修改数据，我们需要知道原数据在程序中代表的数字是多少，这就用到进制转换函数conv，可以先进行一个简单转换，16进制转10进制的例子： 1234567mysql&gt; select conv('FF',16,10);+------------------+| conv('FF',16,10) |+------------------+| 255 |+------------------+1 row in set (0.00 sec) 通过上面的转换十六进制的FF被转换成了十进制的255，应用到Blob字段也是一样，我们看下id为1的数据第一个int保存的数据是多少: 12345678mysql&gt; select id,conv(hex(concat(substring(data,4,1),substring(data,3,1),substring(data,2,1),substring(data,1,1))),16,10) as firstint from bloboperation where id=1;+------+----------+| id | firstint |+------+----------+| 1 | 67305985 |+------+----------+1 row in set (0.01 sec) 现在我们就得到了第一个int类型的值是67305985，可能有的同学会有疑惑，为什么不直接截取前4个字节，而要一个一个的拼接呢？这就涉及到大端数据和小端数据知识了，我们使用的PC机通常是小端的，数据的地位存储在低内存，数据的高位存储在高内存，所以需要把四个字节反过来拼接在一起再进行转换。 实际处理理解了上面的知识，就可以处理之前遇到的问题了，假设这16个字节代表的4个int类型分别是A，B，C，D，需要处理的问题是当变量D的值是52138的时候把变量B清0。 通过分析判断D变量的值之前有类似的，按照刚才第一个变量那样处理，把B变量清零可以通过A变量拼接0，然后再拼接C变量和D变量得到，具体的执行语句如下： 12345678910111213141516171819mysql&gt; update bloboperation set data=concat(substring(data,1,4), 0x00000000, substring(data,9,8))whereconv( hex(concat(substring(data,16,1),substring(data,15,1),substring(data,14,1),substring(data,13,1))), 16,10)=52138and id=1;Query OK, 1 row affected (0.06 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select id,hex(data),length(data) from bloboperation;+------+----------------------------------+--------------+| id | hex(data) | length(data) |+------+----------------------------------+--------------+| 1 | 01020304000000000000000CAACB0000 | 16 || 2 | 01020304FFFFFFFF0000000CAACB0000 | 16 || 3 | 68656C6C776F726C64 | 9 || 4 | 30 | 1 |+------+----------------------------------+--------------+4 rows in set (0.00 sec) 执行更新后查询发现，第5到8个字节对应的变量B确实被清0了，也就是我们的目标达到了。 总结 Blob类型字段的处理常用到的函数hex()、substring()、concat()、conv() 注意conv()函数的第一个参数需要是十六进制表示的字符串，不需要带0x]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Blob substring</tag>
        <tag>hex</tag>
        <tag>concat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（八）：各种形式的变量%0、%i、%%i、var、%var%、!var!的含义和区别]]></title>
    <url>%2Fblog%2F2019%2F11%2F08%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E5%90%84%E7%A7%8D%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%8F%98%E9%87%8F-0%E3%80%81-i%E3%80%81-i%E3%80%81var%E3%80%81-var-%E7%9A%84%E5%90%AB%E4%B9%89%E5%92%8C%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言最近使用批处理程序处理文件的时候，发现这 bat中的变量形式真是“变化多端”，有时候加1个百分号%，有时候加2个百分号%%，还有的时候加感叹号!，真是让初学者一头雾水，于是查询资料做了一些小测试，终于大致弄清楚了这些变量的含义，接下来一一列举出来。 变量对比下面通过一些具体的例子来看下标题中提到的这些变量什么时候使用，使用的时候有哪些注意事项。 %0这个是批处理程序中的固定用法，类似于C++程序main函数中argv变量数组，类比可以知道，argv[0]表示exe程序的文件名，argv[1]表示启动程序的第1个参数，后面依次类推。而在批处理程序中%0表示这个批处理程序的文件名，%1表示调用这个批处理时传入的第1个参数，%2表示调用这个批处理时传入的第2个参数，最大可以到%9，具体用法可以参考之前的总结《.bat批处理（二）：%0 %1——给批处理脚本传递参数》，简单测试如下： 12345@echo offecho param0=%0echo param0=%1echo param0=%2 将上述代码保存在文件testparams.bat中，从cmd命令行运行批处理文件，只传入一个参数，运行结果如下： C:\Users\Administrator\Downloads&gt;testparams.bat “hello world”param0=testparams.batparam1=”hello world”param2= %i在题目所列的这些变量中，这一个比较特殊，因为它不是批处理文件中的变量，只能用于cmd命令行下的for循环中，在命令行中for循环的语法是for %variable in (set) do command [command-parameters]，其中的variable只能是单字母或者非特殊含义的字符，同样的for循环语句如果写在批处理文件中variable之前就要加两个%%了，先来看看%i的用法，直接在命令行中遍历集合打印输出： C:\Users\Administrator\Downloads&gt;for %i in (1,3,5,8) do echo %iC:\Users\Administrator\Downloads&gt;echo 11C:\Users\Administrator\Downloads&gt;echo 33C:\Users\Administrator\Downloads&gt;echo 55C:\Users\Administrator\Downloads&gt;echo 88 如果将其中的%i改成%%i，就会报语法错误，测试结果如下： C:\Users\Administrator\Downloads&gt;for %%i in (1,3,5,8) do echo %%i此时不应有 %%i。 %%i这种类型也是for循环中特有的，与%i相对，属于批处理程序的用法，换句话说就是在for循环中遍历的索引变量，如果在命令行中定义需要一个%，如果相同的语句定义在批处理文件中需要2个%%，语法为for %%variable in (set) do command [command-parameters]，variable同样只能是单个字母或者普通字符，至于为什么同样含义的变量在批处理中要多加一个%，至今也没有找到官方的说法，查找MSDN也没有发现说明，不过就我个人理解可能就像我们在命令行中打印一个%，可以正常打印输出，如果通过printf()想输出%就需要2个%的原理一样吧，测试如下： 1for %%i in (1,3,5,8) do echo %%i 运行结果： C:\Users\Administrator\Downloads&gt;testfor.batC:\Users\Administrator\Downloads&gt;for %i in (1 3 5 8) do echo %iC:\Users\Administrator\Downloads&gt;echo 11C:\Users\Administrator\Downloads&gt;echo 33C:\Users\Administrator\Downloads&gt;echo 55C:\Users\Administrator\Downloads&gt;echo 88 观察运行结果发现，运行批处理文件的时候，实际上去掉了%%i变量的1个%，将文件中代码改为1个%试下： 1for %i in (1,3,5,8) do echo %i 运行结果： C:\Users\Administrator\Downloads&gt;testfor.bat此时不应有 i。 var这个变量看起来挺正常的，也没有那么多奇奇怪怪的字符，和Lua、Python等语言中的变量长得挺像，实际上变量的这种形式很“短暂”，一般只能出现在给变量赋值的时候，也就是set语句之后，作为左值接受赋值时，或者在等号右测可评估的表达式中，举个例子，编写下面代码保存在normalVar.bat中： 1234567@echo offset var1=1set /a var2=var1+1echo var1echo var2 运行之后的结果为: C:\Users\Administrator\Downloads&gt;normalVar.batvar1var2 看完结果之后觉得很神奇是不是，为什么和我学的其他语言不一样呢，我使用set分别为var1和var2赋了值，但是输出的时候居然不是数字而是变量名，其实这就引出了之后%var%这种用法，接着往下看。 %var%在批处理中除了上面所说的在set语句后面的两种情况，再要想引用变量就需要在变量两端各加一个百分号%，明确的告诉引用者这是一个变量，使用时需要评估一下值，而不要当成字符串，上一个例子中echo后面想要输出的变量没有加%，那就被当成一个字符串处理，原样输出了，修改上个例子如下： 12345678910@echo offset var1=1set /a var2=var1+1set var3=%var2%echo %var1%echo %var2%echo %var3% 运行之后运行结果入下： C:\Users\Administrator\Downloads&gt;normalVar.bat122 看了这次的结果感觉正常多了，有一点需要注意，set var3=%var2%这一句中var2变量中的%不能省略，因为它既不属于左值也不属于被评估值的表达式，如果不加%，赋值后var3的值会变成“var2”这个字符串。 !var!这是最后一种常见的变量形式，同时也是一种不太好理解的形式，需要记住一点，这种变量与延迟环境变量扩展有关，如果没开启延迟环境变量扩展，那么!var!就是一个普通的包含5个字母的字符串，如果开启了延迟环境变量扩展，那么它就是变量var的实际值，可能说到这有的人会产生疑惑，引用变量var的值不是使用%var%吗？那么在开启延迟环境变量扩展的情况下，%var%和!var!有什么区别呢？下面举个例子测试下，编写如下代码保存在extVar.bat文件中： 1234@echo offset var1=110set var1=120&amp;echo %var1% 运行之后的结果为： C:\Users\Administrator\Downloads&gt;extVar.bat110 看到结果的时候是不是再次怀疑了世界，在打印变量var1之前明明重新赋值了120，为什么打印出来还是110呢？其实这是批处理脚本执行机制导致的，它会按行执行，在执行之前会先预处理，当执行set var1=110之后，变量var1变成了110，在执行set var1=120&amp;echo %var1%之前先预处理，将变量%var1%替换成了110，然后语句变成了set var1=120&amp;echo 110，所以就得到了我们上面测试的结果。 想要解决这个问题就需要开启延迟环境变量扩展，语句为setlocal enabledelayedexpansion，然后将引用变量的形式由%var1%改为!var1!即可，所以可以修改代码如下： 12345@echo offsetlocal enabledelayedexpansionset var1=110set var1=120&amp;echo !var1! 运行之后的结果为： C:\Users\Administrator\Downloads&gt;extVar.bat120 这回输出的结果符合预期了，开启了延迟环境变量扩展之后，!var!形式的变量在用之前才会评估确切的值，这是一个知识点，也是一个易错点，特别是在for循环中要格外注意，因为for循环语句的循环体括号中，所有的操作被看成是同一行，所以经常会用到延迟环境变量扩展。 总结 for循环在cmd命令行中的固定用法for %i in (set) do (...)，循环变量格式为%i for循环在bat处理程序中的固定用法for %%i in (set) do (...)，循环变量格式为%%i 至于为什么for语法在批处理中需要多写一个%，希望知道的小伙伴能给出答案和参考资料，不胜感激 想要变量被使用的时候再评估值需要开启延迟环境变量扩展，语法为setlocal enabledelayedexpansion，同时使用!var!形式的变量]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下常用的打包、压缩、解压命令（tar、gzip、bzip2、zip）]]></title>
    <url>%2Fblog%2F2019%2F11%2F06%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%B8%B8%E7%94%A8%E7%9A%84%E6%89%93%E5%8C%85%E3%80%81%E5%8E%8B%E7%BC%A9%E3%80%81%E8%A7%A3%E5%8E%8B%E5%91%BD%E4%BB%A4%EF%BC%88tar%E3%80%81gzip%E3%80%81bzip2%E3%80%81zip%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言经常使用电脑的人常常会接触到压缩文件，不管是软件、数据还是资料，下载之后通常就是一个压缩包，在Windows平台上如果安装了WinRAR或者360压缩，不管是什么格式的压缩文件，一般点击压缩文件右键选择解压选项即可，非常地方便。正因为长时间在Windows平台上方便的解压文件，导致我对打包、压缩的概念理解错误，结果在linux操作压缩文件时有很多疑问，今天终于明白了一点，专门总结一下，同时列举常用的压缩、解压命令，方便日后查找使用。 linux上操作压缩文件也是通过命令实现的，但是压缩文件的后缀有很多，比如.tar.gz、.tar.bz2、.gz、.zip、.Z等等，而生成和解压这些文件的命令同样很多，比如tar、gzip、bzip2、zip、unzip等，看得人眼花缭乱，记忆的过程中也常常出现偏差，不是命令不对应就是参数错误，特别是一些不常用的压缩格式，经常需要查询尝试，浪费了不少时间，其实造成这些问题的原因还是由于对打包压缩的概念不太清楚，接下来先了解一下这些概念。 基础概念在Windows上经常直接在图形化界面上操作压缩和解压文件，导致我将这种操作行为带到了linux上，而实际上在linux上压缩和解压文件之外还有一个操作就是“打包”，原因就是linux的压缩和解压通常作用在一个文件上，如果想将一大堆文件压缩最终成为一个文件，需要先打成一个包，然后对这个包文件进行压缩。 打包/归档打包或者叫归档，就是将多个文件和目录（也可以是一个文件）就变成了一个总的文件，但不是将所有文件进行融合，使用tar命令。 压缩压缩是将一个大的文件通过特定的压缩算法尽可能变成一个小文件，可以减少存储空间，加快网络传输效率，使用gzip、bzip2、zip等命令。 解压解压是将压缩生成的最终的小文件还原为压缩之前的大文件，可以使用gzip、gunzip、bunzip2、unzip等命令。 打包压缩通过上面的概念解析我们可以知道，我们之前所说的压缩操作通常是指打包和压缩两个步骤，由于linux大部分的压缩命令都是只能压缩一个文件，所以在压缩之前需要将待压缩的所有文件先进行打包，生成一个文件后再进行压缩操作。 明白了打包和压缩操作的含义，我们可以通过一些约定俗成的命名规则，选择合适的压缩和解压方法，比如下面这些文件： xxx.tar：这是一个归档文件，也就是只通过tar进行了打包操作 xxx.tar.gz：这是一个压缩文件，打包之后，以gzip方式进行了压缩 xxx.tar.bz2：这是一个压缩文件，打包之后，以bzip2方式进行了压缩 xxx.gz：这是一个压缩文件，没有经过打包操作，只是gzip方式进行了压缩 如果能按照这些命名规则生成压缩文件，那么解压文件的时候会方便很多，但有时压缩文件的扩展名是不标准的，可以通过file命令查看文件实际的格式，使用方法如下： 12345[albert@localhost#15:03:05#/home/albert/compress]$file test.tar.bz2test.tar.bz2: bzip2 compressed data, block size = 900k[albert@localhost#15:03:24#/home/albert/compress]$file test.tar.gztest.tar.gz: gzip compressed data, from Unix, last modified: Wed Nov 6 12:02:05 2019 压缩解压命令压缩文件的格式和命令真的是太多，所以在此总结一份常用命令表格，方便日后需要的时候直接拿来就用，加快解决问题的速度。假设原始文件是a.log和b.txt，当前目录下还有一个output目录，可以作为解压后存放文件的目录，那么常用压缩和解压命令如下： 文件格式 压缩命令 命令备注 解压命令 命令备注 xxx.tar tar -cvf test.tar a.log b.txt - tar -xvf test.tar -C ./output 不使用-C则解包在当前目录 xxx.tar.gz tar -zcvf test.tar.gz a.log b.txt - tar -zxvf test.tar.gz -C ./output 不使用-C则解压在当前目录 xxx.tar.bz2 tar -jcvf test.tar.bz2 a.log b.txt - tar -jxvf test.tar.bz2 -C ./output 不使用-C则解压在当前目录 xxx.tar.Z tar -Zcvf test.tar.Z a.log b.txt - tar -Zxvf test.tar.Z -C ./output 不使用-C则解压在当前目录 xxx.gz gzip -c a.log &gt; test.gz、gzip a.log 前者保留a.log，后者直接删除a.log gzip -d test.gz、gunzip test.gz 不能指定解压文件存储目录 xxx.bz2 bzip2 -c a.log &gt; test.bz2、bzip2 a.log 前者保留a.log，后者直接删除a.log bzip2 -d test.bz2、bunzip2 test.bz2 不能指定解压文件存储目录 xxx.Z compress -c a.log &gt; test.Z、compress a.log 前者保留a.log，后者直接删除a.log compress -d test.Z、uncompress test.Z 不能指定解压文件存储目录 xxx.rar rar a test.rar a.log - unrar e test.rar 将e选项换成x可以指定目录 xxx.zip zip test.zip a.log b.txt - unzip test.zip -d ./output 不使用-d则解压在当前目录 分析对比上面的压缩也解压命令可以发现，tar这个命令可以将打包和压缩合并到一起，也可以将解压和解包合并到一起，只需要修改选项中的参数就可以调用不同的程序压缩或者解压，比如-cvf表示只打包不压缩，而-zcvf表示打包后使用gzip压缩，改为-jcvf表示打包后使用bzip2压缩，其实还有很多的压缩方式，可以参考一下tar命令的帮助文档，具体压缩选项如下。 压缩选项: -a, –auto-compress 使用归档后缀名来决定压缩程序 -I, –use-compress-program=PROG 通过 PROG 过滤(必须是能接受 -d 选项的程序) -j, –bzip2 通过 bzip2 过滤归档 -J, –xz 通过 xz 过滤归档 –lzip 通过 lzip 过滤归档 –lzma 通过 lzma 过滤归档 –lzop –no-auto-compress 不使用归档后缀名来决定压缩程序 -z, –gzip, –gunzip, –ungzip 通过 gzip 过滤归档 -Z, –compress, –uncompress 通过 compress 过滤归档 总结 上述这些命令只是基础用法，还有很多参数选型没有提到，比如tar -tf test.tar可以不解压直接查看归档文件中的内容。 gzip命令只能压缩一个文件，如果在命令后面添加多个文件，则会分别压缩生成多个文件。 据说compress命令是一个相当古老的 unix 档案压缩指令，现在基本被gzip命令取代了。 由于文中涉及的命令较多，难免有些笔误，为了不传播错误用法，我也进行了多次检查，如果大家还发现其他错误，欢迎批评指正。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>tar</tag>
        <tag>gzip</tag>
        <tag>bzip2</tag>
        <tag>zip</tag>
        <tag>打包</tag>
        <tag>压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb调试程序时跳进函数和跳出函数]]></title>
    <url>%2Fblog%2F2019%2F11%2F01%2Fgdb%E8%B0%83%E8%AF%95%E7%A8%8B%E5%BA%8F%E6%97%B6%E8%B7%B3%E8%BF%9B%E5%87%BD%E6%95%B0%E5%92%8C%E8%B7%B3%E5%87%BA%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言说实话平时在Windows平台上开发，gdb调试用的并不是很多，但是一些在linux平台才会出现的BUG，或者在linux运行时宕机产生了core文件，这些还是需要使用gdb调试的，之前的文章《linux环境下服务器程序的查看与gdb调试》列举了常用的gdb命令，基本上调试一些core文件和简单bug使用这些命令足以了，但是新的需求总是会出现。 新的需求也很常见，就是跳进一个函数，调试一部分代码后还要跳出这个函数，一般情况就是这个函数特别长，调试前几行已经明白函数的逻辑和用意，如果使用 next 命令逐行运行需要花费较多时间，所以需要跳出函数回到调用的位置，这两个操作在Visual Studio中的快捷键分别是F11和Shift+F11，使用起来非常的方便，其实在gdb调试的过程中也有对应的命令，分别是step(s)和finish(fin)，括号中的内容为命令的简写，此外还有一个return命令也可以使函数返回，接下来可以看一下它们的区别。 测试代码测试的代码很简单，只需要写一个简单的函数，并且在主函数中调用这个函数即可，代码如下： 12345678910111213141516171819#include &lt;iostream&gt;using namespace std;int son_func()&#123; int a = 100; int b = 1; return a + b;&#125;int main()&#123; int i = 10; cout &lt;&lt; i &lt;&lt; endl; int result = son_func(); cout &lt;&lt; result &lt;&lt; endl;&#125; 代码编译编译代码时只需要注意一点，那就是加上-g选项，否则可能会影响调试： 1albert@localhost#11:56:18#/home/albert/gdbtest]$g++ -g stepfinish.cpp -o stepfinishtest step/finish组合这个组合不会影响函数的运行结果，简单的调试过程如下： 1234567891011121314151617181920212223242526272829303132333435363738394041[albert@localhost#11:32:17#/home/albert/gdbtest]$gdb stepfinishtestGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/gdbtest/stepfinishtest...done.(gdb) startTemporary breakpoint 1 at 0x400809: file stepfinish.cpp, line 13.Starting program: /home/albert/gdbtest/stepfinishtestTemporary breakpoint 1, main () at stepfinish.cpp:1313 int i = 10;Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-23.el6.x86_64(gdb) n14 cout &lt;&lt; i &lt;&lt; endl;(gdb) n1016 int result = son_func();(gdb) stepson_func () at stepfinish.cpp:66 int a = 100;(gdb) finishRun till exit from #0 son_func () at stepfinish.cpp:60x0000000000400831 in main () at stepfinish.cpp:1616 int result = son_func();Value returned is $1 = 101(gdb) n18 cout &lt;&lt; result &lt;&lt; endl;(gdb) n10119 &#125;(gdb) cContinuing.Program exited normally.(gdb) 首先使用start命令启动程序，然后使用next(n)命令让程序运行到调用函数son_func()所在的行，使用step命令进入函数，我们看到a的值为100，假设这时我们想回到这个函数被调用的位置，直接敲finish命令就可以，函数完整的执行并返回结果101，最后连续执行next(n)命令，程序正常退出，整个过程只是调试查看数据，并没有改变程序运行结果。 step/return组合这个组合有可能会影响函数的运行结果，具体要看return命令使用的位置和返回的参数： 12345678910111213141516171819202122232425262728293031323334353637383940414243[albert@localhost#11:53:42#/home/albert/gdbtest]$gdb stepfinishtestGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/gdbtest/stepfinishtest...done.(gdb) startTemporary breakpoint 1 at 0x400809: file stepfinish.cpp, line 13.Starting program: /home/albert/gdbtest/stepfinishtestTemporary breakpoint 1, main () at stepfinish.cpp:1313 int i = 10;Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-23.el6.x86_64(gdb) n14 cout &lt;&lt; i &lt;&lt; endl;(gdb) n1016 int result = son_func();(gdb) stepson_func () at stepfinish.cpp:66 int a = 100;(gdb) n7 int b = 1;(gdb) return 119Make son_func() return now? (y or n) y#0 0x0000000000400831 in main () at stepfinish.cpp:1616 int result = son_func();(gdb) n18 cout &lt;&lt; result &lt;&lt; endl;(gdb) print result$1 = 119(gdb) cContinuing.119Program exited normally.(gdb) 首先同样使用start命令启动程序，然后使用next(n)命令让程序运行到调用函数son_func()所在的行，使用step命令进入函数，我们看到a的值为100，这时再敲入next(n)命令让程序运行一步，可以看到b的值为1，假设这时我们想返回一个自定义值而不返回a+b的结果，可以直接敲命令return 119，表示直接返回119这个值，再打印返回值变量result发现是值119，跳出函数的同时，程序运行结果也已经被我们改变了。 总结 gdb中跳入函数的命令是step，相当于Visual Studio中的快捷键F11 gdb中跳出函数的命令是finish，相当于Visual Studio中的快捷键Shift+F11，函数完整执行后返回 gdb中还有一个直接返回的命令是return，它会跳过当前函数后面的语句直接返回，返回值可以自定义，紧跟在return命令后面即可]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>step</tag>
        <tag>finish</tag>
        <tag>return</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python使用scatter函数绘制点在线的上层]]></title>
    <url>%2Fblog%2F2019%2F10%2F30%2FPython%E4%BD%BF%E7%94%A8scatter%E5%87%BD%E6%95%B0%E7%BB%98%E5%88%B6%E7%82%B9%E5%9C%A8%E7%BA%BF%E7%9A%84%E4%B8%8A%E5%B1%82%2F</url>
    <content type="text"><![CDATA[前言前几天在QQ群里发现有人问这样一个问题，使用Python的matplotlib库绘制图形时，函数 scatter() 绘制的点总是在 plot() 函数绘制的线下边，看起来样子很丑，大概就是下图这个样子，问有没有方法让点显示到线的上面。 看到这个问题一开始以为是绘制顺序的原因，调整 scatter() 函数和 plot() 函数的调用顺序并没有达到预想的效果，点还是在线的下面，原来一直没注意这个问题，是因为我一直把线和标注用的点使用了同一种颜色，所有看不出来是谁覆盖了谁，现在抛出这个问题居然让人有点不知所措，直觉上认定肯定有个属性可以设置这个显示顺序，但究竟是图的属性？坐标轴的属性？还是函数的参数呢？这个还需要查一查。 解决办法最后经过一顿查找发现函数 scatter() 和 plot() 都有个参数 zorder，这时候才恍然大悟，做游戏界面开发时常常使用这个参数来控制UI的层级，现在怎么突然忘了呢，赶紧设置一下发现与原来在游戏开发中的参数含义相同，zorder这个整数越大，显示的时候越靠上，所以写了下面的测试代码： 1234567891011121314import numpy as npimport matplotlib.pyplot as pltdef draw_point_on_line(): plt.title("draw point on line") plt.xlabel("x-axis") plt.ylabel("y-axis") X = np.linspace(1, 100, 10, endpoint=True) Y = np.random.randint(60, 100, len(X)) plt.plot(X, Y, color ='blue', linewidth=3.5, zorder=1) # 在第一层画线 plt.scatter(X, Y, 50, color ='red', zorder=2) # 在第二层画点 plt.show() 效果展示这次终于正常了，因为scatter()函数中的zorder值较大，所以放到上面绘制，效果如下： 总结 帮助他人解决问题是一个自我提升的好机会 有很多我们认为很简单的事情，其实并不是我们想的那样 代码文件前后图例代码对比-传送门]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>matplotlib</tag>
        <tag>plt</tag>
        <tag>scatter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python切割超大日志文件、保留文件最后几行]]></title>
    <url>%2Fblog%2F2019%2F10%2F24%2FPython%E5%88%87%E5%89%B2%E8%B6%85%E5%A4%A7%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E3%80%81%E4%BF%9D%E7%95%99%E6%96%87%E4%BB%B6%E6%9C%80%E5%90%8E%E5%87%A0%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[前言关于日志这个东西的存在，主要是为了记录发生的事情，编程的过程中也常常用到，记得我们在刚刚学习编程的时候，常常会出现程序错误，这时候就需要输出一下，其实这个输出也是日志的一种体现，随着编程水平的提升，各种调试工具和方法渐渐进入我们的视线，但是输出一下这种方法却一直被使用，特别是一些偶发性问题，调试工具很难捕捉到他们，这时候往往需要将中间过程输出到日志文件中，这些日志文件就是我们分析问题的基础。 随着程序规模的渐渐扩大，出现问题时需要打印的日志也越来越多，最近就出现这样一个情况，游戏程序总是莫名的崩溃，看代码找不到问题的原因，所以采用了打印日志文件的方法，有时候大约跑半天就能出现崩溃，日志文件大概600M，Windows系统自带的记事本很难打开，但是使用Notepad++等几秒钟是可以看到内容的。 比较变态的是最近一次跑了2天才崩溃，导出日志文件发现大概有3G，这次使用Notepad++打开时也卡死了，使用sublime打开时进度卡在了80%左右，据说非常强大的GVim打开文件时也毫无反应了，这就尴尬了，崩溃之前的日志内容就在文件中，可是我们却看不见。 问题出现问题很明显摆在这了，文件由于太大无法看到其中的内容，得想个办法。很直接的一个想法就进入了脑海，把文件拆开成几份，这样每个文件缩小了就可以看到了啊，所以我们找到了一个解决问题的办法，接下来使用Python来简单写一下切分文件。 分割日志文件按照文件大小分割分割文件的规则需要先确定一下，可以很简单的按照文件大小分割，一个源文件大小为10M的日志文件，可以切分成10个大小为1M的日志文件，分割的大小不用太绝对，每一份近似相等就可以，整体思路就是先获得源文件的大小，然后计算出分割结束每个文件的大小，接着不断从源文件中读内容，往目标文件中写内容，达到之前计算的字节大小时，再生成新的目标文件，简单代码如下： 12345678910111213141516171819202122import osDATA_LEN_PER_READ = 1024 * 1024def split_file_by_size(file_name, parts=3): file_size = os.path.getsize(file_name) per_file_size = file_size//parts file_size_list = [] for x in range(parts-1): file_size_list.append(per_file_size) file_size_list.append(file_size-per_file_size*(parts-1)) output_file, ext = os.path.splitext(file_name) with open(file_name, 'rb') as rfile: for n in range(parts): with open('&#123;0&#125;_part&#123;1&#125;&#123;2&#125;'.format(output_file, n+1, ext), 'wb') as wfile: read_size = 0 while read_size &lt; file_size_list[n]: want_read = min(DATA_LEN_PER_READ, file_size_list[n] - read_size) wfile.write(rfile.read(want_read)) read_size += want_read 按照文件行数分割以上按照文件大小分割的日志文件有一点小问题，不能保证行是完整的，当遇到汉字这种占用多字节的字符，甚至都不能保证汉字是完整的，所以我们可以换一个思路，尝试使用按照行数分割，整体思路就是先获得文件的行数，然后计算出分割结束每个文件的行数，接着不断从源文件中一行行读内容，往目标文件中一行行写内容，达到之前计算的文件行数时，再生成新的目标文件，简单代码如下： 12345678910111213141516171819202122232425import osdef get_file_line_count(file_name): line_count = 0 for index, line in enumerate(open(file_name, 'r')): line_count += 1 return line_countdef split_file_by_line(file_name, parts=3): total_line = get_file_line_count(file_name) per_file_line = total_line//parts file_line_list = [] for x in range(parts-1): file_line_list.append(per_file_line) file_line_list.append(total_line-per_file_line*(parts-1)) output_file, ext = os.path.splitext(file_name) with open(file_name, 'rb') as rfile: for n in range(parts): with open('&#123;0&#125;_part&#123;1&#125;&#123;2&#125;'.format(output_file, n+1, ext), 'wb') as wfile: read_line = 0 while read_line &lt; file_line_list[n]: wfile.write(rfile.readline()) read_line += 1 获取日志文件尾部内容对于分析奔溃这类问题，出现问题的日志往往就在最后几行，所以没有必要非得打开整个日志文件，也不一定需要将整个文件分割成几部分，只需将日志文件的最后一部分读出来写到新的文件中供我们分析就可以了，这时候开头几个字符的不完整也是可以接受的，所以没必要按行读取，只需按照经验，从尾部截取指定字节大小的内容就可以了，比如我们的日志，最后有用的部分也就10M左右，一般的文本文件就都能打开了。 代码思路就是先打开文件，然后将读指针定位到尾部往前10M左右，然后读取所有内容保存到新的文件中，简单的代码示例如下： 12345678import osdef tailslice(file_name, data_size= 1024): output_file, ext = os.path.splitext(file_name) with open(file_name, 'rb') as rfile: rfile.seek(-data_size, 2) with open('&#123;0&#125;_tail&#123;1&#125;'.format(output_file, ext), 'wb') as wfile: wfile.write(rfile.read()) 总结 有时候要问题需要变通一下，如果一个文件大到打不开的地步，我们就把它分割成可以接受的范围 当文件中的内容只有一部分可用时，我们完全可以不分割，直接取出可用部分即可。 代码传送门 分割超大日志文件 获取日志文件尾部内容]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>日志文件</tag>
        <tag>split</tag>
        <tag>切割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中查询当前用户、当前数据库等基础信息]]></title>
    <url>%2Fblog%2F2019%2F10%2F14%2FMysql%E4%B8%AD%E6%9F%A5%E8%AF%A2%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7%E3%80%81%E5%BD%93%E5%89%8D%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AD%89%E5%9F%BA%E7%A1%80%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言生活中有时会出现灵魂三问：我是谁？我在哪？我在做什么？特别的喝醉酒的第二天，完全不记得昨天发生了什么。而在数据库操作中也会出现这种灵魂拷问，我用的是哪个用户，为什么会没有权限？我操作的是哪个数据库，刚刚不会把线上正式服务器数据删了吧？ 上面描述的问题常常出现在切换数据库处理问题的时候，通过一个客户端连接到Mysql数据库服务器，操作数据库1，然后切换再操作数据库2，这时如果中间有人打扰，很容易忘记刚刚操作的是哪个数据库，或者中途处理个其他紧急的事情，回来连操作的用户都忘了，这时就需要一些基础信息的查询命令，帮助你来恢复记忆。 数据库基础信息查询数据库的基础信息涉及到方方面面，这里只列举几个常用的查询命令，用来回答上面的灵魂拷问，其他命令还有很多，用到了再总结吧。 查询当前操作的用户1234567mysql&gt; select user();+----------------+| user() |+----------------+| root@localhost |+----------------+1 row in set (0.07 sec) 查询当前操作的数据库1234567mysql&gt; select database();+------------+| database() |+------------+| sqltest2 |+------------+1 row in set (0.09 sec) 查询当前数据库端口1234567mysql&gt; show variables like 'port';+---------------+-------+| Variable_name | Value |+---------------+-------+| port | 3306 |+---------------+-------+1 row in set (0.07 sec) 查询当前数据库版本1234567mysql&gt; select version();+------------+| version() |+------------+| 5.7.21-log |+------------+1 row in set (0.06 sec) 数据库结构信息查询这里的结构我指的是DDL中定义的那些元素，比如表、存储过程等，有一些常用的查询命令，要是一段时间不使用还是会忘记，比如查询一个数据库中的存储过程，每次查询时都要上网搜一下，所以今天总结在一起方便查找。 查询当前数据库中的所有表1234567891011121314mysql&gt; show tables;+--------------------+| Tables_in_sqltest2 |+--------------------+| a || b || c || d || m || p || tb_test || tb_with_index |+--------------------+14 rows in set (0.11 sec) 查询创建表的sql语句12345678910mysql&gt; show create table a;+-------+----------------------------------------+| Table | Create Table |+-------+----------------------------------------+| a | CREATE TABLE `a` ( `id` int(11) DEFAULT NULL, `num` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+----------------------------------------+1 row in set (0.11 sec) 查询指定表中的所有字段12345678mysql&gt; desc a;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.12 sec) 查询当前数据库中的所有存储过程这个命令我得吐槽一下，为什么不能像查询当前数据库的中所有表一样，搞个show procedures;命令，非得通过where子句指定数据库呢，具体的原因还不知道，等我弄明白了再回来补充。123456789mysql&gt; show procedure status where db='sqltest2';+----------+-----------------------+-----------+----------------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+| Db | Name | Type | Definer | Modified | Created | Security_type | Comment | character_set_client | collation_connection | Database Collation |+----------+-----------------------+-----------+----------------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+| sqltest2 | fill_slow_query_test | PROCEDURE | root@localhost | 2019-03-25 11:14:01 | 2019-03-25 11:14:01 | DEFINER | | utf8mb4 | utf8mb4_general_ci | utf8_general_ci || sqltest2 | fill_tb_without_index | PROCEDURE | root@localhost | 2019-03-18 09:53:32 | 2019-03-18 09:53:32 | DEFINER | | utf8mb4 | utf8mb4_general_ci | utf8_general_ci || sqltest2 | fill_tb_with_index | PROCEDURE | root@localhost | 2019-03-18 09:53:33 | 2019-03-18 09:53:33 | DEFINER | | utf8mb4 | utf8mb4_general_ci | utf8_general_ci |+----------+-----------------------+-----------+----------------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+3 rows in set (0.17 sec) 查询创建存储过程的sql语句1234567891011121314151617mysql&gt; show create procedure fill_tb_with_index;+--------------------+----------------------------------------------------------------+-------------------+----------------------+----------------------+--------------------+| Procedure | sql_mode | Create Procedure | character_set_client | collation_connection | Database Collation |+--------------------+----------------------------------------------------------------+-------------------+----------------------+----------------------+--------------------+| fill_tb_with_index | STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |CREATE DEFINER=`root`@`localhost` PROCEDURE `fill_tb_with_index`()BEGIN DECLARE i int default 1; WHILE i &lt;= 100000 do insert into tb_with_index values(i, i, i); set i = i + 1; END WHILE;END | utf8mb4 | utf8mb4_general_ci | utf8_general_ci |+--------------------+----------------------------------------------------------------+-------------------+----------------------+----------------------+--------------------+1 row in set (0.09 sec) 查询指定表上的索引123456789mysql&gt; show index from tb_with_index;+---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| tb_with_index | 1 | id_index | 1 | id | A | 100035 | NULL | NULL | YES | BTREE | | || tb_with_index | 1 | num_index | 1 | num | A | 98715 | NULL | NULL | YES | BTREE | | || tb_with_index | 1 | money_index | 1 | money | A | 100035 | NULL | NULL | YES | BTREE | | |+---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+3 rows in set (0.03 sec) 查询当前用户连接的权限1234567mysql&gt; show grants;+---------------------------------------------------------------------+| Grants for root@localhost |+---------------------------------------------------------------------+| GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' WITH GRANT OPTION || GRANT PROXY ON ''@'' TO 'root'@'localhost' WITH GRANT OPTION |+---------------------------------------------------------------------+ 查询指定用户连接的权限1234567mysql&gt; show grants for 'guest';+---------------------------------------------------------------------------------------------------------------+| Grants for guest@% |+---------------------------------------------------------------------------------------------------------------+| GRANT ALL PRIVILEGES ON *.* TO 'guest'@'%' IDENTIFIED BY PASSWORD '*6C8DE74065898C44C21EF74D67A834C5256BFA1C' |+---------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 总结 以上总结的查询语句都是我经常用到，相比Mysql所有查询语句来说简直是冰山一角，总结到一起主要是方便日后查找，同时也希望给他人带来帮助 查看这些语句会发现，有些是select开头，有些是show开头，实际上很多show开头的都是对information_schema数据库数据的封装 information_schema 数据库是Mysql系统自带的数据库，记录了整个数据库实例上所有数据结构信息，更像是记录数据库的数据库，包含表结构、字符集，权限等太多的信息，有机会后续找时间聊聊这个数据库，在此就不展开了 正因为很多show开头的都是对information_schema数据库数据的封装，所以这些查询语句基本都可以通过在information_schema数据库查询得到，比如show procedure status where db=&#39;sqltest2&#39;;就可以改写成select routine_name from information_schema.routines where routine_schema=&#39;sqltest2&#39;; 此时此刻，我正在距离天安门500多米的现场等待阅兵仪式的开始，一边学习一边为祖国庆生的感觉真好！（注：500多米多了5公里，现场是卧室床前的电视机旁，这么近的距离不知道一会能不能看见接受检阅的飞机o(\￣︶￣*)o）*]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>用户</tag>
        <tag>当前信息</tag>
        <tag>表结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下C++代码打印函数堆栈调用情况]]></title>
    <url>%2Fblog%2F2019%2F09%2F06%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8BC-%E4%BB%A3%E7%A0%81%E6%89%93%E5%8D%B0%E5%87%BD%E6%95%B0%E5%A0%86%E6%A0%88%E8%B0%83%E7%94%A8%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[前言关于什么是函数调用堆栈在上篇文章《windows环境下C++代码打印函数堆栈调用情况》中已经介绍过了，简单的来说就是可以展现出函数之间的调用关系，上篇文章展示了如何在windows上打印出函数调用堆栈，其中用到了windows系统上的API，这些接口在linux上是无法使用的，因为工作的关系，也常常需要在linux的调试程序，所以本文介绍一下如何在linux上打印出C++程序的调用堆栈。 实现打印堆栈信息的函数在linux系统上想打印函数调用堆栈信息，需要引用头文件&lt;execinfo.h&gt;，然后利用函数backtrace、backtrace_symbols来获取当时的函数调用堆栈信息，以下的代码实现了一个简单的打印堆栈新的函数，堆栈深度最大同样设置为12层。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;execinfo.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#define STACK_INFO_LEN 1024void ShowTraceStack(const char* szBriefInfo)&#123; static const int MAX_STACK_FRAMES = 12; void *pStack[MAX_STACK_FRAMES]; static char szStackInfo[STACK_INFO_LEN * MAX_STACK_FRAMES]; char ** pStackList = NULL; int frames = backtrace(pStack, MAX_STACK_FRAMES); pStackList = backtrace_symbols(pStack, frames); if (NULL == pStackList) return; strcpy(szStackInfo, szBriefInfo == NULL ? "stack traceback:\n" : szBriefInfo); for (int i = 0; i &lt; frames; ++i) &#123; if (NULL == pStackList[i]) break; strncat(szStackInfo, pStackList[i], STACK_INFO_LEN); strcat(szStackInfo, "\n"); &#125; printf("%s", szStackInfo); // 输出到控制台，也可以打印到日志文件中&#125;void func2()&#123; bool isError = true; if (isError) &#123; ShowTraceStack("error in func2\n"); &#125; else &#123; printf("this is func2\n"); &#125;&#125;void func1()&#123; int sum = 0; for (int i = 0; i &lt; 100; ++i) sum += i; func2();&#125;int main(int argc, char* argv[])&#123; printf("hello world\n"); func1(); return 0;&#125; 显示堆栈调用信息上面的测试代码中函数的调用逻辑为：main()函数调用func1()函数，然后func1()函数调用func2()函数，当func2()中发生问题的时候打印当时的堆栈信息，然后我们编译一下查看运行结果 12345678910[albert@localhost#10:59:03#/home/albert/test/backtrace]$g++ -rdynamic linuxtraceback.cpp -o linuxtraceback[albert@localhost#10:59:05#/home/albert/test/backtrace]$./linuxtracebackhello worlderror in func2./linuxtraceback(_Z14ShowTraceStackPKc+0x25) [0x400a19]./linuxtraceback(_Z5func2v+0x1c) [0x400b06]./linuxtraceback(_Z5func1v+0x32) [0x400b46]./linuxtraceback(main+0x1e) [0x400b66]/lib64/libc.so.6(__libc_start_main+0xfd) [0x7fbbcae43d1d]./linuxtraceback() [0x400939] 上面的运行结果已经展示了程序函数的调用关系，其中编译选项中的-rdynamic是很重要的，它实际上是一个链接选项，作用是把所有符号（而不仅仅只是程序已使用到的外部符号）都添加到动态符号表里，以便那些通过 dlopen() 或 backtrace()这样的函数使用，换句话说就是如果不加这个选项在调用堆栈中就可能看不到函数名。 上面的调用堆栈中函数名大致能看出来，但是有些奇怪的字母，可以通过工具c++fileter来处理，处理之后就可以看到正常的函数名了，具体使用方式如下：123456789[albert@localhost#10:59:12#/home/albert/test/backtrace]$./linuxtraceback | c++filthello worlderror in func2./linuxtraceback(ShowTraceStack(char const*)+0x25) [0x400a19]./linuxtraceback(func2()+0x1c) [0x400b06]./linuxtraceback(func1()+0x32) [0x400b46]./linuxtraceback(main+0x1e) [0x400b66]/lib64/libc.so.6(__libc_start_main+0xfd) [0x7f73aea34d1d]./linuxtraceback() [0x400939] 编译时无法添加-rdynamic选项如果是自己写的小项目或者小程序，编译选项是可以随便改的，没有什么关系，需要查看堆栈信息加上-rdynamic就可以了，但是如果是公司的大型项目，编译选项是不会随便改的，可能是直接使用automake生成的，先来看一下不添加-rdynamic选项编译之后的运行结果 12345678910[albert@localhost#11:22:15#/home/albert/test/backtrace]$g++ linuxtraceback.cpp -o linuxtraceback[albert@localhost#11:22:18#/home/albert/test/backtrace]$./linuxtracebackhello worlderror in func2./linuxtraceback() [0x4007d9]./linuxtraceback() [0x4008c6]./linuxtraceback() [0x400906]./linuxtraceback() [0x400926]/lib64/libc.so.6(__libc_start_main+0xfd) [0x7f4e12ba2d1d]./linuxtraceback() [0x4006f9] 可以看到不添加-rdynamic选项编译之后运行虽然能显示出调用堆栈，但都是一些函数地址，无法看到函数名，这时可以通过工具addr2line帮助我们定位问题，这个工具的作用就是将函数地址转换成函数所在的行，使用方法就是在命令行运行addr2line 0x4008c6 -e ./linuxtraceback，具体使用时替换函数地址和可运行程序的名字即可 说实话这个小程序中使用运行addr2line 0x4008c6 -e ./linuxtraceback没有看到我想要的，只显示了??:0，仿佛被优化掉了，但是我在正式的项目中使用这个方法是可以得到函数所在行的，这也帮助我查到了一个隐藏很深的BUG。 总结 linux平台下可以利用函数backtrace、backtrace_symbols、backtrace_symbols_fd来获取当时的函数调用堆栈信息 使用上述函数时，需要引用头文件&lt;execinfo.h&gt;，编译时最好加上-rdynamic选项 如果实在无法添加-rdynamic，可以通过addr2line辅助查找问题 程序源码打印堆栈信息–源码传送门]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>函数</tag>
        <tag>linux</tag>
        <tag>堆栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows环境下C++代码打印函数堆栈调用情况]]></title>
    <url>%2Fblog%2F2019%2F09%2F03%2Fwindows%E7%8E%AF%E5%A2%83%E4%B8%8BC-%E4%BB%A3%E7%A0%81%E6%89%93%E5%8D%B0%E5%87%BD%E6%95%B0%E5%A0%86%E6%A0%88%E8%B0%83%E7%94%A8%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[前言程序运行的过程中，函数之间的是会相互调用的，在某一时刻函数之间的调用关系，可以通过函数调用堆栈表现出来，这个调用堆栈所展现的就是函数A调用了函数B，而函数B又调用了函数C，这些调用关系在代码中都是静态的，不需要程序运行就可以知道。 既然函数之间的调用关系可以通过分析代码就可以知道，那么查看函数调用的堆栈是不是作用不大了呢？事实上恰恰相反，查看函数调用堆栈的作用非常大。因为在较大型的项目中，函数之间的调用不是简单的一条线，常常会出现复杂的网状结构，这时如果函数C被调用了，可能不是仅仅是B函数调用过来的，也有可能是D、E、F等函数调用了C函数，所以知道在程序运行时究竟是哪个函数调用了C函数显得很重要，特别是有众多函数会调用C函数的时候。 查看函数堆栈的作用举个例子就明白了，假如C函数中逻辑的执行需要一些特殊条件状态，理论上执行C函数时这些条件都应该满足的，但是程序在运行的过程中有时运行C函数时条件就是不满足的，那就说明有些调用C函数的逻辑分支有问题，无法满足C函数中逻辑所需条件，这时候知道是谁调用C函数导致条件不满足就是确定问题的关键。 如果是在VS调试状态下，在C函数不满足条件的逻辑中打一个断点，然后运行程序等待断点触发时，就可以通过VS工具自带的调用堆栈窗口，就可以看到程序从主函数main()开始怎样一步步调用的出错的函数C的。 可实际项目中，出错的时候不总是在VS的调试状态下，也有可能发生在程序实际的工作环境中，这时没有办法通过加断点来查看调用堆栈，如果此时有一个函数，可以打印当前的函数调用堆栈那就太好了，这样我就可以在需要调试的逻辑中，调用这个函数，将当时的函数调用堆栈信息打印到文件中，方便查找程序逻辑问题，这篇文章要做的就是在Windows环境下，利用现有的API实现这样一个函数。 实现打印堆栈信息的函数在Windows系统上想打印函数调用堆栈信息，需要引用头文件&lt;dbghelp.h&gt;，添加库引用DbgHelp.Lib，然后利用函数CaptureStackBackTrace、SymFromAddr、SymGetLineFromAddr64来获取当时的函数调用堆栈信息，以下的代码实现了一个简单的打印堆栈新的函数，堆栈深度最大设置为12层，实际情况肯定是越深越好，设置为12一般就可以查到问题了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include &lt;windows.h&gt;#include &lt;dbghelp.h&gt;#include &lt;stdio.h&gt;#if _MSC_VER#define snprintf _snprintf#endif#define STACK_INFO_LEN 1024void ShowTraceStack(char* szBriefInfo)&#123; static const int MAX_STACK_FRAMES = 12; void *pStack[MAX_STACK_FRAMES]; static char szStackInfo[STACK_INFO_LEN * MAX_STACK_FRAMES]; static char szFrameInfo[STACK_INFO_LEN]; HANDLE process = GetCurrentProcess(); SymInitialize(process, NULL, TRUE); WORD frames = CaptureStackBackTrace(0, MAX_STACK_FRAMES, pStack, NULL); strcpy(szStackInfo, szBriefInfo == NULL ? "stack traceback:\n" : szBriefInfo); for (WORD i = 0; i &lt; frames; ++i) &#123; DWORD64 address = (DWORD64)(pStack[i]); DWORD64 displacementSym = 0; char buffer[sizeof(SYMBOL_INFO)+MAX_SYM_NAME * sizeof(TCHAR)]; PSYMBOL_INFO pSymbol = (PSYMBOL_INFO)buffer; pSymbol-&gt;SizeOfStruct = sizeof(SYMBOL_INFO); pSymbol-&gt;MaxNameLen = MAX_SYM_NAME; DWORD displacementLine = 0; IMAGEHLP_LINE64 line; line.SizeOfStruct = sizeof(IMAGEHLP_LINE64); if (SymFromAddr(process, address, &amp;displacementSym, pSymbol) &amp;&amp; SymGetLineFromAddr64(process, address, &amp;displacementLine, &amp;line)) &#123; snprintf(szFrameInfo, sizeof(szFrameInfo), "\t%s() at %s:%d(0x%x)\n", pSymbol-&gt;Name, line.FileName, line.LineNumber, pSymbol-&gt;Address); &#125; else &#123; snprintf(szFrameInfo, sizeof(szFrameInfo), "\terror: %d\n", GetLastError()); &#125; strcat(szStackInfo, szFrameInfo); &#125; printf("%s", szStackInfo); // 输出到控制台，也可以打印到日志文件中&#125;void func2()&#123; bool isError = true; if (isError) &#123; ShowTraceStack("error in func2\n"); &#125; else &#123; printf("this is func2\n"); &#125;&#125;void func1()&#123; int sum = 0; for (int i = 0; i &lt; 100; ++i) sum += i; func2();&#125;int main(int argc, char* argv[])&#123; printf("hello world\n"); func1(); return 0;&#125; 显示堆栈调用信息上面的测试代码中函数的调用逻辑为：main()函数调用func1()函数，然后func1()函数调用func2()函数，当func2()中发生问题的时候打印当时的堆栈信息，然后我们查看一下打印结果 hello worlderror in func2 ShowTraceStack() at e:\vs2013projects\trackback\windowstrackback\windowstrackback.cpp:24(0xe01440) func2() at e:\vs2013projects\trackback\windowstrackback\windowstrackback.cpp:59(0xe01840) func1() at e:\vs2013projects\trackback\windowstrackback\windowstrackback.cpp:74(0xe017c0) main() at e:\vs2013projects\trackback\windowstrackback\windowstrackback.cpp:82(0xe018c0) __tmainCRTStartup() at f:\dd\vctools\crt\crtw32\dllstuff\crtexe.c:626(0xe01d40) mainCRTStartup() at f:\dd\vctools\crt\crtw32\dllstuff\crtexe.c:466(0xe020c0) error: 487 error: 487 error: 487 总结 Windows平台下可以利用函数CaptureStackBackTrace、SymFromAddr、SymGetLineFromAddr64来获取当时的函数调用堆栈信息 使用上述函数时，需要引用头文件&lt;dbghelp.h&gt;，添加库引用DbgHelp.Lib 程序源码打印堆栈信息–源码传送门]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>函数</tag>
        <tag>堆栈</tag>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（七）：PC端从手机内复制文件到本地]]></title>
    <url>%2Fblog%2F2019%2F08%2F21%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9APC%E7%AB%AF%E4%BB%8E%E6%89%8B%E6%9C%BA%E5%86%85%E5%A4%8D%E5%88%B6%E6%96%87%E4%BB%B6%E5%88%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[前言解决方案都是在实际工作中遇到问题时想出来解决方法，很多方法乍一看根本用不上，但实际操作中发现真的很有用，今天提到的这个方法就是这种类型的。 游戏开发中常常会将一些关键信息或者调试信息写入到日志文件中，这样可以在出现BUG的情况时，通过分析日志文件来进一步定位问题的原因，在真机上跑游戏时就需要将手机中的日志文件导出到电脑上，方便查看，这就是这篇文章所讲的内容。 可能有人会说，现在手机连接电脑很方便，直接插一根数据线，在“我的电脑”里找到手机，然后就可以像从其他文件夹复制一下，从手机中把文件复制下来，可事实上并不是这样的，手机连接电脑有个缓存的毛病。 这种问题就是第一次连接的时候查看文件是正常，但是复制删除几次文件以后就会出现缓存的现象，我明明新建了一个文件就是找不到，比如产生了新的日志文件，通过数据线连接电脑以后，在文件夹中看不到，这时可以通过adb命令复制出来，虽然看不到，但是文件是确实存在的。 准备条件 需要电脑安装adb，常用来调试手机的电脑一定会安装过这个东西，有些版本直接可以使用，具体怎么安装，网上的教程有很多。 手机需要打开USB调试模式，打开模式前可能需要开启开发者选项，同样开启USB调试的教程也有很多。 实现代码1234567891011121314151617@SET LOG_FILE_NAME=project_%date:~0,4%%date:~5,2%%date:~8,2%%time:~0,2%%time:~3,2%%time:~6,2%.logadb pull /storage/emulated/0/project2.log ./%LOG_FILE_NAME%@echo offecho running result:if %errorlevel%==0 goto endSuccess:endFailecho Copy data from phone to pc falied!!!pauseexit /b 1:endSuccessecho Copy data from phone to pc success!!!pauseexit /b 0 代码分析其实这一大段中核心的代码只有一句adb pull /storage/emulated/0/project2.log ./%LOG_FILE_NAME%，之前的变量LOG_FILE_NAME是为了解决多次导出文件时同名会覆盖的问题，加上时间字符串可以防止重名出现，adb pull 手机中路径+文件名 本地PC路径+文件名就是实际复制的过程 如果复制过程中不报错就会走到:endSuccess代码段，如果报错就会走到:endFail代码段，两段代码会返回不同的值供调用者判断，整个代码文件加了一些提示消息，如果嫌麻烦的话直接使用adb pull /storage/emulated/0/project2.log ./%LOG_FILE_NAME%也是可以的。 代码测试直接在cmd命令行中运行就可以，假设以上的bat文件名为CopydataPhone2PC.bat，手机根目录下有文件project.log，我们可以尝试拷贝project.log和project2.log两个文件到手机看看效果，当然project2.log文件是不存在的肯定会失败 拷贝成功1234567E:\batTool&gt;CopydataPhone2PC.batE:\batTool&gt;adb pull /storage/emulated/0/project.log ./project_20190822102324.log124 KB/s (1284 bytes in 0.010s)running result:Copy data from phone to pc success!!!请按任意键继续. . . 拷贝失败1234567E:\batTool&gt;CopydataPhone2PC.batE:\batTool&gt;adb pull /storage/emulated/0/project2.log ./project_20190822102422.logremote object '/storage/emulated/0/project2.log' does not existrunning result:Copy data from phone to pc falied!!!请按任意键继续. . . 总结有些领域真的很奇妙，如果你之前没有接触过，直接告诉你，手机里有个很普通的文件，但是你就是看不到，你会不会觉得很奇怪，针对于这些奇怪的问题其实别人可能早就有了解决方案，百思不得其解时不妨浏览一下。]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[std::bind（二）：包装成员函数]]></title>
    <url>%2Fblog%2F2019%2F08%2F15%2Fstd-bind%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8C%85%E8%A3%85%E6%88%90%E5%91%98%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言关于std::bind()对普通函数的包装作用，在之前的总结文章《std::bind（一）：包装普通函数》已经举例说明过了，后来发现丢下了普通函数嵌套包装的情况，所以在这篇文章中继续说明一下，然后重点总结std::bind()函数对成员函数的包装，在面向对象的大潮还未褪去的今天，还是成员函数见到的更多一些，所以讲讲对它的包装。 普通函数嵌套包装实际上就是普通函数包装的变形和组合，直接写个例子吧，如果test1_1()、test1_2()、test1_3()三个函数的输出结果都答对了就说明已经掌握了。12345678910111213141516171819202122232425262728293031323334353637void func(int n1, int n2, int n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl;&#125;int calc_value(int c1)&#123; return c1 * c1;&#125;void calc_value2(int c1)&#123; int result = c1 * c1;&#125;void test1_1()&#123; auto f1 = std::bind(func, placeholders::_1, 101, std::bind(calc_value, placeholders::_2)); f1(11, 2); // same as call func(11, 101, calc_value(2))&#125;void test1_2()&#123; int n = 2; auto f1 = std::bind(func, placeholders::_1, 101, std::bind(calc_value, std::ref(n))); n = 4; f1(11, 2); // same as call func(11, 101, calc_value(44)) 多出的参数2无人使用&#125;void test1_3()&#123; auto f1 = std::bind(func, placeholders::_1, 101, std::bind(calc_value2, placeholders::_2)); //f1(11, 2); // 编译出错，无法将参数 3 从“void”转换为“int”&#125;// 11 101 4// 11 101 16 第一个test1_1函数的逻辑应该很容易理解，就是把函数calc_value(2)的返回值作为函数func的第三个参数，而函数test1_2中利用了std::ref()传递引用的功能，将变量n作为引用变量进行传递，在包装调用之前可以感知到参数n的变化。 其实难点在第三个函数test1_3，可能大家知道这里会报错，因为我们需要返回值但是却包装了一个没有返回值的函数，但其实把第二行注释掉之后，程序就可以成功编译，也就是说包装错误的函数如果不被调用，是不会报错的，这一点和模板函类不使用就不会创建很相似，最终是相同的。 包装类成员在深入学习std::bind()这个函数之前一直以为它只能用来包装函数，后来通过进一步了解发现它还能用来包装成员变量，我们一起来看一下简单的实现方法。 成员函数的包装这里我们不考虑静态成员函数，因为静态函数没有this指针，和普通的函数基本一样，在用法上也没有很大的差异，所以此处的包装只考虑成员非静态函数，可以尝试分析以下几个例子。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class CTest&#123;public: CTest() &#123;&#125; ~CTest() &#123;&#125;public: void func1(int n1, int n2) &#123; cout &lt;&lt; "func1 " &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; endl; &#125; int n_public;private: void func2(int n1, int n2) &#123; cout &lt;&lt; "func2 " &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; endl; &#125; int n_private;&#125;;void test2_1()&#123; CTest testObj; auto f2 = std::bind(&amp;CTest::func1, testObj, 101, placeholders::_1); f2(1); // same as call testObj.func1(101, 1)&#125;void test2_2()&#123; CTest testObj; auto f2 = std::bind(&amp;CTest::func1, &amp;testObj, 101, placeholders::_1); f2(2); // same as call testObj.func1(101, 2)&#125;void test2_3()&#123; CTest testObj; CTest&amp; obj = testObj; auto f2 = std::bind(&amp;CTest::func1, obj, 101, placeholders::_1); f2(3); // same as call testObj.func1(101, 3)&#125;void test2_4()&#123; CTest testObj; auto f2 = std::bind(&amp;CTest::func1, placeholders::_1, placeholders::_2, 101); f2(testObj, 4); // same as call testObj.func1(4, 101)&#125;void test2_5()&#123; CTest testObj; // auto f2 = std::bind(&amp;CTest::func2, &amp;testObj, 101, placeholders::_1); // 编译错误，func2不可访问&#125;//func1 101 1//func1 101 2//func1 101 3//func1 4 101 前三个函数tes2_1()、tes2_2()、tes2_3()的作用基本一致，就是将一个类的非静态成员函数和对象绑定，并且可以动态绑定一些参数，三种调用方式都可以，暂时没有发现什么问题，大家知道区别的可以指导我一下，我补充上来，需要注意的是函数std::bind()参数个数需要在原函数参数个数的基础上加两个，第一个很明显就是函数名，而第二个必须是调用这个函数的对象，至于传递的是指针还是引用都没有什么问题，这两个参数过后才是真正的原函数的参数。 函数test2_4()相对于前三个来说更加灵活，将对象也最为参数在调用时传入，这就相当于把一个成员函数看成，一个普通函数然后在第一个参数前加this指针的形式，后面这种调用方式在查看C++调用堆栈时应该很容易看到，本质上是一样，其实这里还有一个对象传递的问题，我们在成员变量时再测试一下。 函数test2_5()出现了编译错误，原因是在使用函数std::bind()的时候也要考虑到原函数的访问权限，在测试函数中访问对象的私有函数显然是不可以的。 成员变量的包装1234567891011121314151617181920212223242526272829303132333435void test3_1()&#123; CTest testObj; auto f3 = std::bind(&amp;CTest::n_public, testObj); f3(1) = 10; cout &lt;&lt; f3(1) &lt;&lt; endl; cout &lt;&lt; testObj.n_public &lt;&lt; endl;&#125;void test3_2()&#123; CTest testObj; auto f4 = std::bind(&amp;CTest::n_public, placeholders::_1); f4(testObj) = 4; cout &lt;&lt; f4(testObj) &lt;&lt; endl; cout &lt;&lt; testObj.n_public &lt;&lt; endl;&#125;void test3_3()&#123; CTest testObj; auto f3 = std::bind(&amp;CTest::n_public, std::ref(testObj)); f3(1) = 11; cout &lt;&lt; f3(1) &lt;&lt; endl; cout &lt;&lt; testObj.n_public &lt;&lt; endl;&#125;//10//-858993460//4//4//11//11 这个成员变量的绑定测试结果，有没有让人意想不到呢？或者说这种f3(1) = 10;写法已经让人很惊讶了，其实我在写例子的时候就是简单试试，没想到这样写居然可以，看起来好像把一个值赋值给了一个函数一样。 函数test3_1()的第二个输出可能有点想不到，但是看到结果是有些人可能就明白了，因为在上一篇里提到“std::bind()函数中的参数在被复制或者移动时绝不会以引用的方式传递，除非你使用了std::ref()或者std::cref()包装的参数”。 因为没有使用std::ref()函数包装，所以std::bind()函数绑定的testObj对象实际上是原对象的副本，那么针对于副本的操作和修改自然就不会反应到原对象上，这也就是打印testObj.n_public会输出随机值的原因。 函数test3_2()在绑定时并没有具体到特定的对象，而是使用了placeholders::_1占位符，这样生成的函数，在调用的时候再传入操作对象，那么此时修改对象属性就可以起作用了。 函数test3_3()是针对于函数test3_1()的，添加了std::cref()包装的原对象，可以通过绑定后的函数修改。 总结 std::bind()函数可以嵌套绑定 std::bind()函数绑定成员函数时，函数名参数后面需要紧跟着类的对象作为参数 std::bind()不仅可以绑定普通函数、成员函数、还可以绑定成员变量 完整代码代码传送门]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>bind</tag>
        <tag>placeholders</tag>
        <tag>class</tag>
        <tag>object</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[雷电模拟器一键宏实现循环点击]]></title>
    <url>%2Fblog%2F2019%2F08%2F09%2F%E9%9B%B7%E7%94%B5%E6%A8%A1%E6%8B%9F%E5%99%A8%E4%B8%80%E9%94%AE%E5%AE%8F%E5%AE%9E%E7%8E%B0%E5%BE%AA%E7%8E%AF%E7%82%B9%E5%87%BB%2F</url>
    <content type="text"><![CDATA[前言今天在使用雷电模拟器测试游戏的时候，有一个领奖界面需要点击领奖100次，程序猿作为解放劳动力的先锋，必须想个办法解决这个事情，按键精灵是个好东西，但是重装系统之后还没有安装，然后发现这个雷电模拟器里除了简单的按键映射，还有一键宏的功能，那就用它解决了。 解决过程关于雷电模拟器的按键操控功能，官方论坛-帮助教程已经写得很清楚了，一键宏怎么设置教程里也写的很清楚，唯一的缺点就是各个截图中的代码太模糊了，根本看不清，所以我在尝试的过程中还花了点时间，其中遇到了几个坑和大家分享下。 我用的模拟器版本是3.54，这个版本在编写一键宏的时候可以直接在界面上获得对应点的坐标，非常的方便，在开始写一键宏的时候有一个误区，就是怎么控制我写的这段宏代码的开始与结束，起初我还控制按键的按下和抬起，发现没有什么用，最后测试发现，就是设置的按键按下时执行，如果存在循环就一直执行，按键抬起时执行结束，简单粗暴。 代码编写其实这个一键宏也算不上代码，最多也就算个伪代码，然后通过模拟器解析一下，要实现循环点击的功能需要的指令不多，只有4个： size：指定模拟器的分辨率 loop：说明以下的指令开始循环 touch：点击屏幕上的像素点 wait：休息一下，防止点击过快，单位是毫秒 其中坑人最深的就是size这个命令，我一开始以为是设置分辨率的，而我玩游戏默认的分辨率是1600X900，所以设置的点击位置也是在这个分辨率下取的点，然后就没加这个指令，结果一键宏一直不生效，后来加上了size 1600 900这一句才好使，这时我才明白这个指令不是设置分辨率的，而是告诉以下指令，当前的分辨率是多少，在操作像素点时不至于选错，宏的内容很简单，只有四句： 1234size 1600 900looptouch 1400 350wait 1000 这个宏组合的意思也很清楚，在分辨率1600X900的情况下，循环点击(1400, 350)这个像素点，每次点击间隔1s，防止点击过快。 总结 一键宏设置完成后，按键按下执行，如果是循环指令，则按键抬起结束 size指令并不是设置分辨率，而是说明现在的分辨率]]></content>
      <categories>
        <category>game</category>
      </categories>
      <tags>
        <tag>雷电</tag>
        <tag>模拟器</tag>
        <tag>按键操控</tag>
        <tag>一键宏</tag>
        <tag>loop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[std::bind（一）：包装普通函数]]></title>
    <url>%2Fblog%2F2019%2F08%2F01%2Fstd-bind%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8C%85%E8%A3%85%E6%99%AE%E9%80%9A%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言不知道大家在做项目写程序的过程中有没有遇到这样的情况，别的模块类提供了一个拥有很多参数接口函数，但是我这个功能只用到其中几个，其他的参数都是固定的，可是为了调用这个接口函数，不得不将所有的参数写一遍，每次写一堆固定参数都感觉在浪费生命。 有的人可能想到默认参数，的确，默认参数可以解决部分问题，因为默认参数只能出现参数列表的尾部，如果4个参数中，我需要传递的参数是第4个，而前3个参数想默认的话，默认参数是做不到这种效果的，并且别人的接口函数也不一定会有默认参数。 函数封装，这是一个办法，我们在自己的模块中添加一个对接口函数进行包装后的函数，将不变的参数进行固定，然后只留可变的参数供我们自己调用，如果我们有3种常用的调用方式可能就需要定义3个函数，这种方法可行，不过比较麻烦，而std::bind()函数就是为了包装函数而生的，使用起来更加方便。 std::bind()的作用std::bind()的作用就是对原函数进行包装，可以将参数值固定，调换顺序然后生成新的函数供我们调用。举个例子，一块铁片你可以拿它来做很多事情，打造一下可以做成一把刀，敲敲打打可以做成一个桶，甚至直接拿来就可以挡窗户上的洞。std::bind()的作用就是把这块铁的作用固定，比如给她安上一个刀把，这样我们每次使用就可以把这块铁片当成菜刀来使用了。 std::bind()可以包装各种函数，但是这篇文章只总结一下包装普通函数的方法，因为在学习的过程中我发现单单是包装普通函数也会遇到很多问题，所以为了列举出诸多可能，说明各种注意事项，本文还是只关注于普通函数的包装，至于成员函数的包装还是放到以后的文章，给自己埋下一个坑。 在包装普通函数时，std::bind()的第1个参数就是原函数的名字，当然也可以是指向函数的指针，或者函数引用，从第2个参数开始，填写的内容依次对应原函数中的各个参数，所以说如果原函数是3个参数，如果想包装它，那么std::bind()需要传入4个参数，如果原函数是8个参数，那么包装它的std::bind()就需要传入9个参数，这里为了将原函数和包装后的函数参数建立联系，需要引入命名空间std::placeholders。 placeholders的作用std::placeholders的命名空间下有多个参数占位符，比如placeholders::_1、placeholders::_2等等，最大为placeholders::_20，在包装普通函数时，固定的参数很好说，就是填写固定值就可以，但是要想原函数的参数和包装后函数的参数建立联系就需要用到刚刚提到的占位符， placeholders::_1就表示包装后函数的调用时的第1个参数，同理placeholders::_2就表示包装后函数的调用时的第2个参数。 有了占位符的概念，我们就可以推断出，包装后的函数与原函数相比，不但可以减少函数参数，也可以增加函数参数，虽然暂时没有想到什么实际的使用场景，但是理论上是可行的。 std::bind()使用测试首先需要先引入头文件，免得找不到命名空间和函数定义123#include &lt;iostream&gt;#include &lt;functional&gt;using namespace std; 固定参数、调换顺序1234567891011121314151617181920void func1(int n1, int n2, int n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl;&#125;void test1_1()&#123; auto f1 = std::bind(func1, placeholders::_1, 101, placeholders::_2); f1(11, 22); // same as call func1(11, 101, 22)&#125;void test1_2()&#123; auto f1 = std::bind(func1, placeholders::_2, 101, placeholders::_1); f1(11, 22); // same as call func1(22, 101, 11)&#125;// 输出//11 101 22//22 101 11 函数test1_1()展示了std::bind()函数最常见的用法，其中参数n2被固定为101，参数n1使用占位符placeholders::_1表示，表示包装后函数的第1个参数会传给形参n1使用，同理包装后函数的第2个参数会传给形参n3使用，所以调用函数f1(11, 22) 就等同于调用函数 func1(11, 101, 22)，test1_2()函数简单展示了调换参数顺序的方法，只要明白了placeholders的作用，这两个例子也就明白了。 包装后函数的参数个数可增可减123456789101112131415161718192021222324252627void func2(int n1, int n2, int n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl;&#125;void test2_1()&#123; auto f2 = std::bind(func2, placeholders::_3, 101, placeholders::_1); f2(11, 22, 33); // same as call func2(33, 101, 11)&#125;void test2_2()&#123; auto f2 = std::bind(func2, placeholders::_1, 101, placeholders::_1); f2(11); // same as call func2(11, 101, 11)&#125;void test2_3()&#123; auto f2 = std::bind(func2, placeholders::_1, 101, placeholders::_2); f2(11); // 报错，因为没有参数传给placeholders::_2&#125;// 输出//33 101 11//11 101 11//编译错误 其实在理解了placeholders的作用之后，这个测试结果也能想到的，函数test2_1()中使用了placeholders::_3，所以包装后函数的参数至少要传3个才不会报错，而test2_2()函数中使用了placeholders::_1，所以被包装函数调用时只需要传入一个参数，最后是函数test2_3()，绑定时引用了placeholders::_2，而在调用时只传了一个参数，所以出现编译错误。 bind()绑定时参数个数固定，类型需匹配12345678910111213141516171819202122void func3(int n1, int n2, int n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl;&#125;void test3_1()&#123; auto f3 = std::bind(func3, placeholders::_1, 101); //f3(11); // 编译错误，因为bind函数中少了一个参数&#125;void test3_2()&#123; auto f3 = std::bind(func3, placeholders::_1, 101, 102, 103); //f3(11); // 编译错误，因为bind函数中多了一个参数&#125;void test3_3()&#123; auto f3 = std::bind(func3, placeholders::_1, "test", placeholders::_1); //f3(11); // 编译错误，第二个参数类型不匹配，无法将参数 2 从“const char *”转换为“int”&#125; 看了之前的测试之后，是不是觉得参数的个数很随意，可以随便增加和减少，所以在绑定的时候也不好好写了，结果发现上述3个函数全部编译错误，test3_1()函数中因为绑定时少了一个参数而报错，test3_2()函数中因为绑定时多了一个参数而报错，而test3_3()函数中因为绑定时第二个参数的类型不匹配而报错，所以参数个数的增减只能是包装后的函数，而绑定时必须严格与原函数的参数个数以及类型相匹配。 普通函数的参数中有引用类型弄明白上面的例子之后，可能会产生一种我会了的错觉，想象一下如果原函数参数中包含引用类型应该怎样写，可以自己先想一下，然后看看下面的例子1234567891011121314151617181920212223242526272829303132333435363738394041424344void func4(int n1, int n2, int&amp; n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl; n3 = 101;&#125;void test4_1()&#123; int n = 10; auto f4 = std::bind(func4, 11, 22, n); n = 33; f4(); // same as call func4(11, 22, 10) cout &lt;&lt; "n = " &lt;&lt; n &lt;&lt; endl;&#125;void test4_2()&#123; const int n = 30; auto f4 = std::bind(func4, 11, 22, n); f4(); // same as call func4(11, 22, 30)&#125;void test4_3()&#123; int n = 30; auto f4 = std::bind(func4, 11, 22, ref(n)); n = 33; f4(); // same as call func4(11, 22, n) cout &lt;&lt; "n = " &lt;&lt; n &lt;&lt; endl;&#125;void test4_4()&#123; const int n = 30; auto f4 = std::bind(func4, 11, 22, ref(n)); //f4(); // 编译错误，无法将参数 3 从“const int”转换为“int &amp;”&#125;// 输出//11 22 10//n = 33//11 22 30//11 22 33//n = 101 如果能准确说出test4_1()函数的输出结果，那么后面的内容应该是不需要看了，如果只回答对了部分内容，或者干脆全错了，那么我们还有很长的路要走。 在std::bind()的官方文档中有这样一句话，std::bind()函数中的参数在被复制或者移动时绝不会以引用的方式传递，除非你使用了std::ref()或者std::cref()包装的参数，如果知道了这个限定，就很容易明白函数test4_1()函数的输出结果了。 在函数test4_1()中std::bind(func4, 11, 22, n)就相当于std::bind(func4, 11, 22, 10)，所以输出结果为11 22 10，可是函数func4()中还有一句 n3 = 101;，这就很让人奇怪了，我们知道常数是没办法作为参数传递给可变引用变量的，如果说把10作为参数传递给参数int&amp; n3肯定会报错，而函数test4_1()却正常执行，没有任何问题。 我们猜测常数10到参数int&amp; n3并不是直接传递，而是发生了拷贝，而函数func4()中修改的n3变量也是修改的拷贝内容，所以我们做了test4_2()这个实验，发现将变量n改为常量也是可以正常执行的，甚至直接写成std::bind(func4, 11, 22, 10)也是没问题的，这也验证了我们上面的想法。 既然文档了提到了std::ref()和std::cref()函数，那么我们想传递引用给原函数只能使用它们了，看下函数test4_3()的实现，这才是正确传递引用变量的方式，变量n被函数 std::ref() 包装之后，既能够感受到本函数中变量n的变化，也能够传入到原函数中被原函数的逻辑改变，并将结果反映回来。 函数test4_4()只是一个常量传递的简单测试，将一个常量作为可变引用变量来传递肯定是无法通过编译的，这在函数调用时很明确，但是在std::bind()加入之后显得有些混乱，只要记住一点，常量不应该被改变，如果传递之后内容可能会变化，那么很可能这种写法就是错误的。 总结 其实std::bind()函数测试到现在远远没有结束，配合std::ref()和std::cref()函数会产生多种组合情况，不过主要的问题上面都提到了一些，出现问题的时候对照着定义和概念看看应该就能理解了。 需要理解std::placeholders的占位作用，它们是std::bind()函数最基本的用法。 完整代码代码传送门]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>auto</tag>
        <tag>bind</tag>
        <tag>placeholders</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中char和varchar的区别]]></title>
    <url>%2Fblog%2F2019%2F07%2F27%2FMysql%E4%B8%ADchar%E5%92%8Cvarchar%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言这个标题起的过于俗套，前一段时间我还写了一篇总结《Mysql5.7版本中数据表字段可用的类型》来批判这种对比，原因是对比时没有指明数据库，内容写的是char、varchar和nvarchar的对比，结果我测试了半天发现Mysql当前版本根本没有nvarchar，浪费来了不少时间。 问题起因真香定律来的总是这么快，这才过了几天，我也来写写Mysql中char和varchar究竟有什么区别，起因是看到CSDN好友“铁柱同学”一篇关于innodb主键长度最大为767字节的讲解，里面涉及到一个char类型最大存储255个字符，按照utf8编码来看最大的字节数应该是255*3=765个字节的知识点。现在来看767的来源好像并不是256*3-1，而是255*3+2，这个2就是存储char类型字段中实际有多少个字节的。 有点跑题了，实际上是在研究索引长度的过程中，我发现我对char和varchar这两个类型一直存在着误解，因为一直做游戏开发的缘故，游戏数据的存储一般使用varbinary来存，导致我把字符和字节有点弄混了，所以我一直认为在utf8编码下char(9)可以存储9个英文字符，或者3个中文汉字，实际我做完实验后发现char(9)也可以正常存储9个汉字。 提到字符和字节，初学者可能会有点蒙，实际上它们两者之间是需要通过编码来转换的，之前做过游戏的多语言版本，所以对这一块还是比较熟的，字节就是计算机中的8个二进制位，而字符是每个语言中的不可分割的单元，字符转换成字节需要依赖编码，实际上编码就是一本大字典，里面对应了每个字符在当前编码下转换成字节是什么样的，ANSI是一本字典，UTF8也是一本字典，编码的类型还有很多，每种编码都记录了各自的转换结果。 举个例子，“中”这个字是汉字中的一个字符，在ANSI这本字典中对应的是2个字节，而在UTF8这本字典中对应的是3个字节，而C这个字母是英文中的一个字符，在ANSI这本字典中对应的是1个字节，而在UTF8这本字典中对应的同样是1个字节，从这个例子中可以简单理解下字节与字符的关系。 多说一句，不要认为UTF8编码中汉字转换成字节都是3个字节，实际情况是常用字一般都占用了3个字节，但是中国语言博大精深，光语言平面就单独占了好几个，有些汉字转换成UTF8编码可能需要4个字节，5个字节甚至是6个字节，这一点不要形成思维定式，认为汉字在UTF8编码下都是3个字节。 length 和 char_length今天之前我是不知道Mysql还有一个char_length函数的，发现这个函数后越发感觉Mysql的强大，这两个函数的区别就是length用来统计字段中的字节数，char_length用来统计字段中的字符数，接下来我们用一个例子来看看这两个函数以及char、varchar的区别。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 10Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 测试过程 首先创建一个带有char和varchar类型的测试表，查看表结构发现编码为utf8 1234567891011121314mysql&gt; create table diff(id int, s1 char(10), s2 varchar(10));Query OK, 0 rows affected (0.07 sec)mysql&gt; show create table diff;+-------+------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------+| diff | CREATE TABLE `diff` ( `id` int(11) DEFAULT NULL, `s1` char(10) DEFAULT NULL, `s2` varchar(10) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------+1 row in set (0.07 sec) 插入少于10个字符的测试数据，然后查看结果，发现字节数为10，字符数为6 12345678910mysql&gt; insert into diff values(1, "测试test", "测试test");Query OK, 1 row affected (0.01 sec)mysql&gt; select id, s1, length(s1), char_length(s1), s2, length(s2), char_length(s2) from diff;+----+----------+------------+-----------------+----------+------------+-----------------+| id | s1 | length(s1) | char_length(s1) | s2 | length(s2) | char_length(s2) |+----+----------+------------+-----------------+----------+------------+-----------------+| 1 | 测试test | 10 | 6 | 测试test | 10 | 6 |+----+----------+------------+-----------------+----------+------------+-----------------+1 row in set (0.06 sec) 增加插入长度后发现，可以超过10个字节，可以完整存储10个汉字 1234567891011mysql&gt; insert into diff values(2, "测试策划和开发做游戏", "测试策划和开发做游戏");Query OK, 1 row affected (0.01 sec)mysql&gt; select id, s1, length(s1), char_length(s1), s2, length(s2), char_length(s2) from diff;+----+----------------------+------------+-----------------+----------------------+------------+-----------------+| id | s1 | length(s1) | char_length(s1) | s2 | length(s2) | char_length(s2) |+----+----------------------+------------+-----------------+----------------------+------------+-----------------+| 1 | 测试test | 10 | 6 | 测试test | 10 | 6 || 2 | 测试策划和开发做游戏 | 30 | 10 | 测试策划和开发做游戏 | 30 | 10 |+----+----------------------+------------+-----------------+----------------------+------------+-----------------+2 rows in set (0.10 sec) 分别增加s1和s2字段长度后发现，均无法正常插入，Mysql给出报错信息 12345mysql&gt; insert into diff values(2, "测试策划和开发做游戏OK", "测试策划和开发做游戏");1406 - Data too long for column 's1' at row 1mysql&gt; insert into diff values(2, "测试策划和开发做游戏", "测试策划和开发做游戏OK");1406 - Data too long for column 's2' at row 1mysql&gt; 至此没有看出区别，在插入内容前后都加上空格测试一下 123456789mysql&gt; select id, s1, concat('#', s1, '$'), length(s1) as len_s1, char_length(s1) as clen_s1, s2, concat('#', s2, '$'), length(s2) as len_s2, char_length(s2) as clen_s2 from diff;+----+----------------------+------------------------+--------+---------+----------------------+------------------------+--------+---------+| id | s1 | concat('#', s1, '$') | len_s1 | clen_s1 | s2 | concat('#', s2, '$') | len_s2 | clen_s2 |+----+----------------------+------------------------+--------+---------+----------------------+------------------------+--------+---------+| 1 | 测试test | #测试test$ | 10 | 6 | 测试test | #测试test$ | 10 | 6 || 2 | 测试策划和开发做游戏 | #测试策划和开发做游戏$ | 30 | 10 | 测试策划和开发做游戏 | #测试策划和开发做游戏$ | 30 | 10 || 3 | OK | # OK$ | 3 | 3 | OK | # OK $ | 4 | 4 |+----+----------------------+------------------------+--------+---------+----------------------+------------------------+--------+---------+ 这一次出现了区别，char类型的字段去掉了尾部的空格，而varcahr了类型的字段原样存储，没有去掉尾部空格，两者对于头部的空格都是存储的，这导致两者显示的字节数和字符数都不相同了。 分别使用不带空格、带头部空格，头尾都带空格进行测试12345678910111213141516171819202122232425262728293031323334353637mysql&gt; select * from diff where s1 = 'OK ';Empty setmysql&gt; select * from diff where s2 = 'OK ';Empty setmysql&gt; select * from diff where s1 = ' OK ';+----+-----+------+| id | s1 | s2 |+----+-----+------+| 3 | OK | OK |+----+-----+------+1 row in set (0.05 sec)mysql&gt; select * from diff where s2 = ' OK ';+----+-----+------+| id | s1 | s2 |+----+-----+------+| 3 | OK | OK |+----+-----+------+1 row in set (0.05 sec)mysql&gt; select * from diff where s1 = ' OK';+----+-----+------+| id | s1 | s2 |+----+-----+------+| 3 | OK | OK |+----+-----+------+1 row in set (0.05 sec)mysql&gt; select * from diff where s2 = ' OK';+----+-----+------+| id | s1 | s2 |+----+-----+------+| 3 | OK | OK |+----+-----+------+1 row in set (0.04 sec) 测试结果可能让人出乎意料，虽然s1和s2中存储的内容不同（差一个空格），但是查找时的行为却完全一样，这说明查找时尾部的空格并不会被考虑。 char和varchar区别做了半天试验发现char和varchar还是没有多大区别，实际上有些区别通过表面数据是测试不出来的，具体区别整理如下： 行为 char字段 varchar字段 最大长度 255字符 65535个字节，所以括号中最大的字符数还得通过编码来算 是否定长 定长，不足的部分用隐藏空格填充 不定长 空间使用 会有浪费 更加节省 查找效率 高 低 尾部空格 插入时省略 插入时不会省略，查找时省略 like查找 语句中like后的’ ‘不会省 语句中like后的’ ‘不会省，字段结尾的空格也不会省 总结 char(n)中的n是字符数，范围是0~255（额外需要1到2个字节来存长度） varchar(n)中的n也是字符数，但是最大值需要通过编码来算，不能超过65535字节（从中还需要拿出1到2个字节来存长度） 一般定长的数据选用char类型，比如身份证号，手机号，电话等，长度变化很大的可以使用varchar类型 注意尾部空格的匹配，特别是插入时和使用like查找时]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>char</tag>
        <tag>Mysql</tag>
        <tag>varchar</tag>
        <tag>length</tag>
        <tag>char_length</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时分秒针在一天之内重合多少次]]></title>
    <url>%2Fblog%2F2019%2F07%2F23%2F%E6%97%B6%E5%88%86%E7%A7%92%E9%92%88%E5%9C%A8%E4%B8%80%E5%A4%A9%E4%B9%8B%E5%86%85%E9%87%8D%E5%90%88%E5%A4%9A%E5%B0%91%E6%AC%A1%2F</url>
    <content type="text"><![CDATA[前言分析问题之前先给出问题的答案：2次，送给急需要知道答案又不求甚解的朋友。 这个问题之前听过类似的，一直没有当回事，今天在解题的时候发现了这道题，于是动脑筋想了一下，从12点位置时分秒3个表针重合开始，第一次应该在1点5分之后，那是分针转了一圈快追上时针了，再稍微走一点就能追上，然后秒针再转过来就完成了第一次重合，同理在2点10分之后也有一次，在3点15之后还有一次，这样算下来12小时之内有11次，那么一天24小时就有22次。 正在为自己的想法得意时，查了一下参考答案发现我被幼稚的想法打败了，实际上一天24小时内，时分秒针只重合了2次，原因就是我设想从12点开始到1点5分，分针转了一圈快追上时针了，此刻时针与分针确实会有一次相遇，但是此时的秒针却没办法跟他们相逢，因为三个表针是联动的，针对于每个精确到秒的时间，三个针都有固定的位置，不是想重合就能重合的。 在1点5分附近的情况就是，时针和分针快要重合了，然后秒针匆匆赶来，然后时针和分针重合了，秒针还差一点才能到，然后秒针继续走，但是秒针走会继续带动分针和时针运动，然后秒针赶到了分针时针相遇的附近，却发现它俩已经“分手”了，秒针只能大步流星的一个个越过它们俩，期待着下次它们仨能相遇在一处。 时针和分针的相遇在考虑时分秒三针重合情况之前，我们可以先想一下一天24小时内，分针和时针相遇了多少次，其实这才是我刚才想的那个答案22次，知道了次数之后我们还想知道具体的时间，可不可以算出来呢？当然可以！ 接下来我们以一种通俗的方式来解这个问题，那就是列方程式求解，首先将时间作为连续值来看待，我们设时针的角速度是ω，因为时针走1格，分针会走1圈，也就是12格，所以分针的角速度是12ω，分针转了一圈追上时针用的是t，时针和分针转过角度差为1圈，也就是2π，那么此时可以列出方程： $$12ωt-ωt=2π$$ 关于角速度ω的值，因为1圈的角度是2π，转一圈需要花的时间是12小时，所以ω=2π/12小时，带入方程得到t=12小时/11，同理如果分针转两圈追上时针，那么方程式为： $$12ωt-ωt=4π$$ 可以求得t=24小时/11，由此我们就得到了，时针分针相遇时刻与分针转的圈数i的关系： $$t=i*12小时/11$$ 代码实现有了上面的分析，我们可以写代码计算一下一天之中时针和分针相遇具体时刻，因为开着lua编辑器，顺手就用lua写了，代码如下： 123456789101112function print_meet(id, meet) local h = math.floor(meet) local t = meet - h; local ts = t * 3600; local m = ts // 60; local s = ts - m * 60; print(string.format("%02dth meet, time = %02d:%02d:%05.2f", id, h, m, s));endfor i=1,24 do print_meet(i, i * 12 / 11)end 运行结果 01th meet, time = 01:05:27.2702th meet, time = 02:10:54.5503th meet, time = 03:16:21.8204th meet, time = 04:21:49.0905th meet, time = 05:27:16.3606th meet, time = 06:32:43.6407th meet, time = 07:38:10.9108th meet, time = 08:43:38.1809th meet, time = 09:49:05.4510th meet, time = 10:54:32.7311th meet, time = 12:00:00.0012th meet, time = 13:05:27.2713th meet, time = 14:10:54.5514th meet, time = 15:16:21.8215th meet, time = 16:21:49.0916th meet, time = 17:27:16.3617th meet, time = 18:32:43.6418th meet, time = 19:38:10.9119th meet, time = 20:43:38.1820th meet, time = 21:49:05.4521th meet, time = 22:54:32.7322th meet, time = 24:00:00.0023th meet, time = 25:05:27.2724th meet, time = 26:10:54.55 分析从上面的结果来看，处于一天内的时间相遇时刻只有前22次，12点之后第一次相遇是在01:05:27.27，此时虽然时针和分针相遇，但是秒针大概在27秒的位置，离他们还很远，同理分针时针第二次相遇时刻02:10:54.55，秒针也没有跟他们在一起，但是有两次例外，那就是12:00:00.00和24:00:00.00，这两次时针、分针、秒针完全重合了，所以我们也得到了本文标题中的答案。 总结 时分秒针在一天之内重合2次 从连续的时间来看，时针和分针在一天之内重合22次 有一种现实情况就是表盘上的时间是离散的，不连续的，最小的时间间隔是1秒，此时我们计算的第一次相遇时间01:05:27.27是不存在的，01:05:27的时候分针在时针之前，而01:05:28的时候分针在时针之后，它们也错过了，所以时针和分针考虑离散的情况，一天之后也只是重合2次。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Lua</tag>
        <tag>time</tag>
        <tag>interview</tag>
        <tag>clock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSI七层模型中各层协议及作用]]></title>
    <url>%2Fblog%2F2019%2F07%2F18%2FOSI%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B%E4%B8%AD%E5%90%84%E5%B1%82%E5%8D%8F%E8%AE%AE%E5%8F%8A%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言OSI七层模型在网络这门学科中占有很大的比重，最近在看《图解TCP/IP》这本书，其中对模型中的各个层的作用和对应的协议讲的很详细，而自己有时候总是记错，所以想总结一下，巩固记忆，毕竟好记性不如烂笔头嘛，现在烂笔头不好找了，应该说烂键盘吗？ 各层简析对比 名称 作用 常用协议或标准 相关设备 传输单位 应用层 特定应用对接收数据的处理 HTTP、FTP、SMTP 终端、服务器 - 表示层 设备数据格式与网络标准数据格式转换 LPP、GIF、JPEG 终端、服务器 - 会话层 通信管理，建立和断开通信连接 RPC、SSL、TLS 终端、服务器 - 传输层 管理两个网络终端之间的数据传输 TCP、UDP 终端、服务器 段 网络层 网络地址管理和路由选择 IP/IPv6、ICMP 路由器、三层交换机 分组、包 数据链路层 互联设备之间传送和识别数据帧 ARP、PARP 网桥、二层交换机 帧 物理层 比特流与电子信号之间的转换 IEEE 802.3/802.2 网卡、网线、集线器、中继器、调制解调器 比特位 总结 这里只是我看书之后的基础理解与总结，后续关系紧密的内容也会更新到这里。 本文很多内容中掺杂着个人的理解，如果有不正确的地方欢迎批评指正，我会尽快修改，这也是一种有效的学习方式。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>OSI</tag>
        <tag>七层模型</tag>
        <tag>网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++自定义全部替换函数replace]]></title>
    <url>%2Fblog%2F2019%2F07%2F16%2FC-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%85%A8%E9%83%A8%E6%9B%BF%E6%8D%A2%E5%87%BD%E6%95%B0replace%2F</url>
    <content type="text"><![CDATA[前言今天遇到一个问题，需要把源字符串中的所有A串替换成B串，可能是最近写脚本写的太多了，第一反应就是使用replace()函数就完成了，在 Lua 和 Python 中确实如此，但是我现在正在写C++啊，查询std::string发现确实有一个repalce()函数，但是查看定义后发现事情却不像想象的那样简单。 C++中的这个replace()函数显得过于“原始”，相比于其他脚本语言来说，用起来显得不太方便，不过很符合基础工具语言的特点，这个自带的repalce(pos, len, dst)函数的作用是从源字符串的第pos个字符开始，往后数len个字符，然后将这一部分替换成dst串。 有了这个替换函数，我们完全可以使用循环和查找函数完成全部替换，查找函数可以选择string::find()，从返回的找到的位置开始替换即可，若没有找到则会返回 string::npos，这时也就完成了所有的替换。 函数实现代码很简单，就是利用循环、string::find()函数、string::replace()函数来进行适当的组合，逻辑很清晰，代码如下：1234567891011121314#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;string replace(string&amp; base, string src, string dst)&#123; int pos = 0, srclen = src.size(), dstlen = dst.size(); while ((pos = base.find(src, pos)) != string::npos) &#123; base.replace(pos, srclen, dst); pos += dstlen; &#125; return base;&#125; 关于是否需要返回值完全看你自己定义，我这里加了返回值只要是为了测试输出方便。 测试函数12345678910111213141516int main()&#123; string base1 = "1.0.0.1"; cout &lt;&lt; replace(base1, ".", "[.]") &lt;&lt; endl; string base2 = "1【.】0【.】0【.】1"; cout &lt;&lt; replace(base2, "【.】", "[.]") &lt;&lt; endl; string base3 = "1【.】0【.】0【.】1"; cout &lt;&lt; replace(base3, "【.】", ".") &lt;&lt; endl; string base4 = "this is a book"; cout &lt;&lt; replace(base4, "is", "are") &lt;&lt; endl; return 0;&#125; 运行结果 1[.]0[.]0[.]11[.]0[.]0[.]11.0.0.1thare are a book 总结 注意string::replace()函数与脚本中常用替换函数的不同 使用string::find()函数查找不到待查串时会返回string::npos]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>字符串</tag>
        <tag>替换</tag>
        <tag>replace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb使用watch命令设置数据断点]]></title>
    <url>%2Fblog%2F2019%2F07%2F16%2Fgdb%E4%BD%BF%E7%94%A8watch%E5%91%BD%E4%BB%A4%E8%AE%BE%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%96%AD%E7%82%B9%2F</url>
    <content type="text"><![CDATA[前言“数据断点”这个说法是沿用在Visual Studio中学到的设置断点的方法，在gdb中一般被叫做“硬件断点”，算是断点调试中一种较为高级的调试方法了，这个方法起初是在VS中学会的，属于有需求必有响应的产物。刚开始调试程序的时候只会设置普通断点，就是在要调试的程序代码所在行设置断点，然后等程序运行到断点处可以单步执行，查看内存变量，遇到多个位置修改一个变量并且要查看是谁改变了变量的时候，就要设置多个断点，当时就想如果可以设置一个断点，当变量值被改变就触发这个断点那该多好啊。 当年果然是太年轻，后来发现这个功能就是VS中的数据断点，同样作用的还有gdb工具的中硬件断点，硬件断点不仅可以处理上面提到的需求，更是查找内存写超过的强大工具，要想知道一个正常的变量如何被“不正常”地修改了，硬件断点可以说是最佳工具了。 数据变化断点在gdb工具中设置普通断点的语法是b 变量名/函数名/文件位置，设置数据变化断点（硬件断点）语法也很简单，只需要一个watch命令即可，写法为watch 变量名，但是与普通断点不同的是，数据断点必须在程序运行时设置，在敲入r命令之前对变量设置数据断点会提示找不到符号。 编写测试程序代码 首先新建测试文件watchtest.cpp然后添加下面的代码： 123456789101112131415161718#include &lt;iostream&gt;using namespace std;int main()&#123; int k = 1; int n; n = 1; k = 2; cout &lt;&lt; n &lt;&lt; "," &lt;&lt; k &lt;&lt; endl; n = 3; k = 4; cout &lt;&lt; n &lt;&lt; "," &lt;&lt; k &lt;&lt; endl; return 0;&#125; 将C++源代码编译成可执行文件，为了调试记得加-O0 -g选项 1[albert@localhost#17:08:00#/home/albert/test]$g++ watchtest.cpp -O0 -g -o watchtest 加数据断点并调试 以下为gdb添加数据变化断点（硬件断点）并调试的整个过程，(gdb)后面的内容为敲入的命令： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[albert@localhost#17:52:47#/home/albert/test]$gdb watchtestGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/test/watchtest...done.(gdb) b watchtest.cpp : 6Breakpoint 1 at 0x40085c: file watchtest.cpp, line 6.(gdb) watch nNo symbol "n" in current context.(gdb) rStarting program: /home/albert/test/watchtestBreakpoint 1, main () at watchtest.cpp:66 int k = 1;Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-23.el6.x86_64(gdb) watch nHardware watchpoint 2: n(gdb) cContinuing.Hardware watchpoint 2: nOld value = 0New value = 1main () at watchtest.cpp:1010 k = 2;(gdb) cContinuing.1,2Hardware watchpoint 2: nOld value = 1New value = 3main () at watchtest.cpp:1414 k = 4;(gdb) cContinuing.3,4Watchpoint 2 deleted because the program has left the block inwhich its expression is valid.0x00007ffff72c6d1d in __libc_start_main () from /lib64/libc.so.6(gdb) qA debugging session is active. Inferior 1 [process 18567] will be killed.Quit anyway? (y or n) y[albert@localhost#17:55:04#/home/albert/test]$ 总结 设置数据断点需要在程序启动之后，在运行r命令之前设置断点给出信息：No symbol &quot;n&quot; in current context. 当程序运行到监控变量的作用域之外以后，断点自动被删除，这一点观察执行q命令之前的文字可以看出 添加数据变化断点（硬件断点）格式：watch 变量名]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>watch</tag>
        <tag>断点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql5.7版本中数据表字段可用的类型]]></title>
    <url>%2Fblog%2F2019%2F07%2F02%2FMysql5-7%E7%89%88%E6%9C%AC%E4%B8%AD%E6%95%B0%E6%8D%AE%E8%A1%A8%E5%AD%97%E6%AE%B5%E5%8F%AF%E7%94%A8%E7%9A%84%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[前言为什么会有这个总结，因为在测试Mysql的字符串函数时发现，char 和 varchar 有些不同，网上搜索一番发现了各种char、varchar、nvarchar 类型的对比，还有一些奇奇怪怪的这里就不说了，然后我就开始了对这几种类型字符串的测试，接着就悲剧了，测试多次之后发现创建为nvarchar类型的字段居然是varchar类型的，再查询官方文档后发现，当前版本（5.7.21）的Mysql根本就没有nvarchar类型的字段，白白浪费了时间，所以要把Mysql支持的字段列举在这里，方便后面查找使用。 从13年开始工作到现在，数据库主要使用Mysql，关于常使用的字段类型无非 int、char、varchar、blob、datetime 这几种，工作之前用的最多的是SqlServer，其次就是Oracle和db2了，当时数据库的规模也不大，也没有注意到字段都有哪些类型，基本也是使用上述几种，因为今天在Mysql中的数据类型这栽了跟头，所以查了下官方文档，看看到底都有哪些类型。 支持类型真是不查不知道，查询后发现当前版本（5.7.21-log MySQL Community Server）支持的数据类型居然有40种，这还是超出我的想象的，以字典排序列举在此方便查找： bigint，binary，bit，blob，char，date，datetime，decimal，double，enum，float，geometry，geometrycollection，int，integer，json，linestring，longblob，longtext，mediumblob，mediumint，mediumtext，multilinestring，multipoint，multipolygon，numeric，point，polygon，real，set，smallint，text，time，timestamp，tinyblob，tinyint，tibytext，varbinary，varchar，year。 类型简述数字类型 BIT[(M)]比特值类型，M默认为1，范围是[1,64]。 TINYINT[(M)] [UNSIGNED] [ZEROFILL]单字节整数，有符号时范围是[-128,127]，无符号时范围是[0,255]。 BOOL, BOOLEAN布尔值类型，需要注意的是创建表时如果指定这两种类型会被自动转为TINYINT类型，0代表false，非0代表true。 SMALLINT[(M)] [UNSIGNED] [ZEROFILL]两字节整数，有符号时范围是[-32768,32767]，无符号时范围是[0,65535]。 MEDIUMINT[(M)] [UNSIGNED] [ZEROFILL]三字节整数，有符号时范围是[-8388608,8388607]，无符号时范围是[0,16777215]，这个类型在编程语言中很少见。 INT[(M)] [UNSIGNED] [ZEROFILL]四字节整数，有符号时范围是[-2147483648,2147483647]，无符号时范围是[0,4294967295]，与INTEGER等价。 BIGINT[(M)] [UNSIGNED] [ZEROFILL]八字节整数，有符号时范围是[-9223372036854775808,9223372036854775807]，无符号时范围是[0, 18446744073709551615]。 SERIAL是BIGINT UNSIGNED NOT NULL AUTO_INCREMENT UNIQUE类型的别名，感觉可以直接拿来做主键。 DECIMAL[(M[,D])] [UNSIGNED] [ZEROFILL]用于存储精确小数，M表示有效数字位数，范围是[1,65]，默认是10，D表示小数点后位数，范围是[0,30]，默认是0。 NUMERIC[(M[,D])] [UNSIGNED] [ZEROFILL]是DECIMAL的别名，同样含义的还有DEC[(M[,D])] [UNSIGNED] [ZEROFILL]、FIXED[(M[,D])] [UNSIGNED] [ZEROFILL]。 FLOAT[(M,D)] [UNSIGNED] [ZEROFILL]单精度浮点数，M表示有效数字位数，D表示小数点后位数，范围有三部分[-3.402823466E+38,-1.175494351E-38]，0，[1.175494351E-38,3.402823466E+38]，该类型属于Mysql自己的扩展，依赖硬件和操作系统，指定UNSIGNED表示禁用负数。 FLOAT(p) [UNSIGNED] [ZEROFILL]单精度浮点数，p用来表示精度，取值为0-24等价于没有M和D的FLOAT，取值为25-53等价于没有M和D的DOUBLE。 DOUBLE[(M,D)] [UNSIGNED] [ZEROFILL]双精度浮点数，表示有效数字位数，D表示小数点后位数，范围有三部分[-1.7976931348623157E+308,-2.2250738585072014E-308]，0，[2.2250738585072014E-308, 1.7976931348623157E+308]，该类型属于Mysql自己的扩展，依赖硬件和操作系统，指定UNSIGNED表示禁用负数。等价于DOUBLE PRECISION[(M,D)] [UNSIGNED] [ZEROFILL]。 REAL[(M,D)] [UNSIGNED] [ZEROFILL]一般情况等价于DOUBLE[(M,D)] [UNSIGNED] [ZEROFILL]，但如果SQL mode指定了REAL_AS_FLOAT，那么它等价于FLOAT[(M,D)] [UNSIGNED] [ZEROFILL]。 日期和时间类型 DATE日期类型，展示格式为’YYYY-MM-DD’，支持的范围是[‘1000-01-01’ , ‘9999-12-31’]。 DATETIME[(fsp)]日期时间格式，展示格式为’YYYY-MM-DD hh:mm:ss[.fraction]，支持范围是[‘1000-01-01 00:00:00.000000’, ‘9999-12-31 23:59:59.999999’]，fsp表示小数位数，默认是0，取值范围是[0,6]。 TIMESTAMP[(fsp)]时间戳，范围是[‘1970-01-01 00:00:01.000000’ UTC, ‘2038-01-19 03:14:07.999999’ UTC]，注意到起始秒数从1开始，是因为0被保留用来代表’0000-00-00 00:00:00’了，fsp表示小数位数，默认是0，取值范围是[0,6]。 TIME[(fsp)]时间类型，展示格式为 ‘hh:mm:ss[.fraction]’，支持的范围是[‘-838:59:59.000000’, ‘838:59:59.000000’]，fsp表示小数位数，默认是0，取值范围是[0,6]。 YEAR[(4)]代表年份类型，展示格式为’YYYY’，支持的范围是[1901, 2155]和0000。 字符串类型 [NATIONAL] CHAR[(M)] [CHARACTER SET charset_name] [COLLATE collation_name]固定长度的字符串，M表示字符串最大长度，范围是(0,255]，若实际长度不足M，实际串右侧会填充空格，M默认为1。 [NATIONAL] VARCHAR(M) [CHARACTER SET charset_name] [COLLATE collation_name]可变长度的字符串，M表示字符串最大长度，范围是(0, 65535],当存储UTF8编码中文时，一般需要3个字节存储一个汉字。 BINARY[(M)]与CHAR类似，只是存储的是二进制字节串而非普通的字符串。 VARBINARY(M)]与VARCHAR类似，只是存储的是二进制字节串而非普通的字符串。 TINYBLOB字节串，最大长度是255。 TINYTEXT [CHARACTER SET charset_name] [COLLATE collation_name]字符串，最大长度是255。 BLOB[(M)]字节串，最大长度64K-1，若指定M，则会创建一个能存储M字节最小的BLOB类型，比如TINYBLOB。 TEXT[(M)] [CHARACTER SET charset_name] [COLLATE collation_name]字符串，最大长度64K-1，若指定M，则会创建一个能存储M字节最小的BLOB类型，比如TINYTEXT。 MEDIUMBLOB字节串，最大长度16M-1。 MEDIUMTEXT [CHARACTER SET charset_name] [COLLATE collation_name]字符串，最大长度16M-1。 LONGBLOB字节串，最大长度4G-1。 LONGTEXT [CHARACTER SET charset_name] [COLLATE collation_name]字符串，最大长度4G-1。 ENUM(‘value1’,’value2’,…) [CHARACTER SET charset_name] [COLLATE collation_name]枚举值，一个字符串代表一个值，内部通过整数实现，理论上最多可以有65535个不同的值，但实际上这个值小于3000。 SET(‘value1’,’value2’,…) [CHARACTER SET charset_name] [COLLATE collation_name]集合，包含一组字符串，其内部还是呈现为一个整数，最大可以有64个不同的字符串对象。 特殊数据类型 Mysql提供了GEOMETRY、POINT、LINESTRING、POLYGON等特殊类型来与OpenGIS类一一对应，用来存储一些图形数据，同时还有MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION可以表示这些集合，我感觉我是没机会用这些了，用到了再展开说吧。 Json数据类型 自从Mysql5.7.8之后添加的一种类型，可以存储{“k1”: “val”, “k2”: 110}形式的数据。 常用数据类型大小 类型 存储数据范围（只考虑无符号） 单位 TINYINT 0-255 整数 SMALLINT 0-65535 整数 MEDIUMINT 0-16777215 整数 INT 0-4294967295 整数 BIGINT 0-18446744073709551615 整数 DATETIME 1000-01-01 00:00:00.000000 -&gt; 9999-12-31 23:59:59.999999 时间点 TIMESTAMP 1970-01-01 00:00:01.000000 UTC -&gt; 2038-01-19 03:14:07.999999 UTC. 时间点 TIME -838:59:59.000000 -&gt; 838:59:59.000000 时间点 CHAR 0-255 字符数 VARCHAR 0-65535 字符数 BINARY 0-255 字节数 VARBINARY 0-65535 字节数 TINYBLOB 255 字节数 BLOB 65535(64K-1) 字节数 MEDIUMBLOB 16777215(16M-1) 字节数 LONGBLOB 4294967295(4G-1) 字节数]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>char</tag>
        <tag>Mysql</tag>
        <tag>varchar</tag>
        <tag>blob</tag>
        <tag>varbinary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逐步砍掉树杈的堆排序]]></title>
    <url>%2Fblog%2F2019%2F06%2F29%2F%E9%80%90%E6%AD%A5%E7%A0%8D%E6%8E%89%E6%A0%91%E6%9D%88%E7%9A%84%E5%A0%86%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言在实现堆排序之前，我们先来看看常见的数据结构，在网上我看到了一个特别全的版本：数组，栈，链表，队列，树，堆，图，散列表，本着鸡蛋里挑骨头的态度，我们来看看数组和链表，这两个到底算不算数据结构，貌似它们应该算是线性表这个结构，它们更应该被称作是一个实现结构的元素，比如通过数组和链表可以实现线性表、队列、栈，二叉树等等，可是看看数据结构的定义是计算机存储、组织数据的方式，貌似它们又算是数据结构，反正这个概念模模糊糊，不太清楚，要按我的理解常见结构应该只有线性表、栈、队列、树、图，其他的像堆其实是一种树，散列表很多的内部实现也是树。 好了，数据结构的事情也放一边，今天的目的是排序，主角是堆，那么究竟什么是堆呢？它的形状和山一样，只不过比山要“便宜”，比如土堆、煤堆、垃圾堆，这和金山、银山、绿水青山是没法比的，但是形状相似，只是小一点而已，上边小下边大，尖尖的，从侧面看就是第一个三角形。堆排序中的堆也是这种形状，上边窄下边宽呈现出一个三角形，其本质是一颗完全二叉树，一个n层的二叉树只有最后一层叶子节点可能不完整，但是都靠左排列，最多只有一个单叶子节点，如果说到这里你根本不知道什么是完全二叉树，甚至对树结构都是一头雾水，那么请去补补课，查询一下相关的定义就明白了。 一棵树要想被称为堆结构，光满足完全二叉树还不够，其中的元素也有要求，如果每个父节点都大于等于左右子节点被称为大根堆，如果每个父节点都小于等于左右子节点被称为小根堆，这样我们就知道在堆这个结构中，堆的顶端不是最大的值就是最小的值。 堆顶元素恰恰是堆排序的关键，试想一个有大根堆，我们把堆顶的数据拿下来放到一旁，把剩下的元素再调整成一个大根堆，然后再把堆顶数据拿下来放在刚才拿出那个元素的前面，再调整剩下的元素，反复这样操作，最后拿出来的这些元素就构成了一个有序序列，也就达到了排序的目的。 在进行升序排列时常使用大根堆，降序排列时常使用小根堆，这个知识点不是绝对的，也不需要记忆，只是这样操作更加方便，当你理解了堆排序的流程之后，很自然就能明白这样的用意，并且到那时候你完全可以反着操作来提升自己，不过效果和可读性可能会差一点。 堆排序树结构可以用数组表示出来，特别是完全二叉树用数组表示起来更加方便，从上往下，从左往右依次把数据放入数组，我们就把一颗完全二叉树塞进了一维数组里，本来打算一个图都不画的，但是突然良心发现了，不能对你们太残忍，还是画一个吧，下面这个图展示了完全二叉树与数组的对应关系，这可是本文中唯一的一个图了，可要珍惜一下，把这个图弄明白，之后我们就只操作数组，不再画树了，因为我太懒了。 1234567891011graph TB 2--&gt;3; 2--&gt;9; 3--&gt;4; 3--&gt;7; 9--&gt;12; 9--&gt;6; 4--&gt;1; 4--&gt;11; 7--&gt;5; 7--&gt;8; idx_0 idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 2 3 9 4 7 12 6 1 11 5 8 这个用数组表示的完全二叉树有一个性质，这个性质是我编的，你之前可能没有听过，那就是从后面删除n个节点之后，剩下的元素还是一颗完全二叉树，这一点隐含在堆排序的整个过程中，并且二叉树的父节点都排列在数组前面，叶子节点都排在数组后边，父节点和子节点对应的索引满足一定的关系： 假设父节点索引是i，左节点索引=2*i+1 假设父节点索引是i，右节点索引=2*i+2 假设子节点索引是i，父节点的索引=(int)((i-1)/2) 明白了上面的关系，先简单描述一下堆排序的整个过程，操作的数据源就是这个数组，长度n=11，先将整个数组表示的完全二叉树调整成一个大根堆，这时树的根节点也就是数组的第一个元素就是最大值，把它和数组的最后一个元素交换，之后不再管最后这个数据，相当于把这个树杈砍掉了，根据上段提到的性质，砍掉最后一个叶子节点的二叉树仍然是一颗完全二叉树，调整数据使得前n-1个节点再次成为一个大根堆，继续把根节点索引为0的元素，也就是这个次最大值，与倒数第二个元素交换，之后也放弃倒数第二个元素了，相当于再次砍掉了这棵二叉树的一个树杈，如此反复操作，当“砍”到根节点时，整个数组也就从小到大排好序了。 排序过程通过上面描述可能还是不太明白堆排序的过程，接下来可以通过一个例子来实际操作一次，看看数组是怎样在不断调整大根堆的过程中慢慢变成有序的，数组的初始状态是： idx_0 idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 2 3 9 4 7 12 6 1 11 5 8 先将数组代表的完全二叉树调整成一个大根堆 首先需要确认的是这个调整应该从下往上调整，先看最下边的子树，调整父节点和子节点的数据，使得父节点数据最大，然后再看前一个子树，继续把父子节点的数据调整好，这样一直调整到根节点时，整个完全二叉树的最大值就被调整到根节点了。这个过程有点像冒泡，从下往上，把最大的数据慢慢的冒到最上面。 反过来想想，如果是从上往下挨个子树来看，当从根节点调整到最后一个（最下最右）子树，并不能保证根节点数据最大，只是把较大数据向上整体移动了一次，所以还是要从下往上调整。 知道了这一点以后就要找到最后一个子树的位置，其实就是找到最后一个父节点，这个节点之前的数据都在非叶子节点上，这个节点之后的数据都在叶子节点上，只要调整这个最后父节点以及前面的所有节点就可以影响所有数据，关键是找到这个节点的位置。 要想找到最后一个父节点需要用到之前我们提到的公式，整个数组的元素个数n=11，最后一个元素的索引为n-1，那么其父节点就是最后一个子树的父节点，其索引应该为(int)((n-1-1)/2)，也就是n/2-1，这就是最后一个子树父节点的在数组中的索引，其数值为4，接着从这个节点开始从后往前调整子树： 子树父节点，索引i=4时，调整父节点和右孩子的值（从左右孩子中找一个较大的值，并且要大于父节点） |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9|4|7(parent)|12|6|1|11|5(left)|8(right)| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9|4|8(parent)|12|6|1|11|5(left)|7(right)| 子树父节点，索引i=3时，调整父节点和右孩子的值 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9|4(parent)|8|12|6|1(left)|11(right)|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9|11(parent)|8|12|6|1(left)|4(right)|5|7| 子树父节点，索引i=2时，调整父节点和左孩子的值 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9(parent)|11|8|12(left)|6(right)|1|4|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|12(parent)|11|8|9(left)|6(right)|1|4|5|7| 子树父节点，索引i=1时，调整父节点和左孩子的值，这时左孩子同时也是下面子树的父节点，所以还要调整一下该节点作为父节点的子树 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3(parent)|12|11(left)|8(right)|9|6|1|4|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|11(parent)|12|3(left)|8(right)|9|6|1|4|5|7| 左子树节点作为父节点进行调整 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|11|12|3(tmp parent)|8|9|6|1(tmp left)|4(tmp right)|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|11|12|4(tmp parent)|8|9|6|1(tmp left)|3(tmp right)|5|7| 子树父节点，索引i=0时，调整父节点和右孩子的值，这时右孩子同时也是下面子树的父节点，同样还要调整一下该节点作为父节点的子树 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2(parent)|11(left)|12(right)|4|8|9|6|1|3|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12(parent)|11(left)|2(right)|4|8|9|6|1|3|5|7| 右子树节点作为父节点进行调整 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12|11|2(tmp parent)|4|8|9(tmp left)|6(tmp right)|1|3|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12|11|9(tmp parent)|4|8|2(tmp left)|6(tmp right)|1|3|5|7| 到此为止整个大根堆就调整完成了，为了看的更加清楚。我不得不打脸再画一个图了，有时候还是看图更加方便，一图胜千言啊： |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12|11|9|4|8|2|6|1|3|5|7| 1234567891011graph TB 12--&gt;11; 12--&gt;9; 11--&gt;4; 11--&gt;8; 9--&gt;2; 9--&gt;6; 4--&gt;1; 4--&gt;3; 8--&gt;5; 8--&gt;7; 将大根堆根节点保存的最大值与当前大根堆最后一个节点进行交换，然后将这个节点“砍掉” 交换数据后，将剩余的这个完全二叉树继续调整成大根堆，既然已经打脸了，那就再画个图，这个砍树杈的动作已经和标题呼应了，每次生成大根堆后，将根节点和堆的最后一个结点交换，然后砍掉这个树杈，等整棵树被砍的只剩下根节点，排序也就完成了。 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12(head)|11|9|4|8|2|6|1|3|5|7(tail)| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||7(head)|11|9|4|8|2|6|1|3|5|12(tail)| 1234567891011graph TB 7--&gt;11; 7--&gt;9; 11--&gt;4; 11--&gt;8; 9--&gt;2; 9--&gt;6; 4--&gt;1; 4--&gt;3; 8--&gt;5; 8--&gt;|X|12; 重复上面的步骤，循环执行调整剩余元素为大根堆，首位交换，砍掉末尾元素这三步 这里需要注意的是除了第一次初始化成大根堆的过程比较麻烦，后面重复调整成大根堆的过程都很容易，因为只有这一个根节点不满足大根堆的定义，所以只从这个节点调整就可以，同时递归调整其不符合条件的子树即可。 每次交换之后都会“砍掉”树杈，所以大根堆每次都会减少元素，交换的索引也发生这变化，第一个是array[0]和array[n-1]，然后是array[0]和array[n-2]，最后一直交换到array[0]和array[1]，也就完成了整体的排序。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/*功能： 交换两个变量参数： element1--被交换的第一个元素的地址 element2--被交换的第二个元素的地址返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void swap_data(int* element1, int* element2)&#123; int middle_value = *element1; *element1 = *element2; *element2 = middle_value;&#125;/*功能： 从start到end构成最大堆，前提是start之后的部分已满足最大堆， 也就是说start存在左右子树的情况下，子树已经是最大堆参数： array--表示待排序的数组，此处会退化成指针 start--需要调整的父节点索引 end --最后一个可以被调整的节点索引，当形成最大堆后，第一个节点与当前最后节点交换后，那么这个当前最后节点下一轮就不能被调整了返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void max_heapify(int array[], int start, int end)&#123; int parent_index = start; int child_index = start * 2 + 1; while (child_index &lt;= end) &#123; if (child_index + 1 &lt;= end &amp;&amp; array[child_index] &lt; array[child_index + 1]) ++child_index; // 如果右边的孩子更大，选择右边的 if (array[parent_index] &gt; array[child_index]) break; swap_data(&amp;array[parent_index], &amp;array[child_index]); parent_index = child_index; child_index = parent_index * 2 + 1; &#125;&#125;/*功能： 堆排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 count--数组元素的个数返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void heap_sort(int array[], int count)&#123; for (int pos = count / 2 - 1; pos &gt;= 0; --pos) max_heapify(array, pos, count - 1); for (int target_pos = count - 1; target_pos &gt; 0; --target_pos) &#123; swap_data(&amp;array[0], &amp;array[target_pos]); max_heapify(array, 0, target_pos - 1); &#125;&#125; 代码分析堆排序其实是一种选择排序，但是堆排序的代码比选择排序要复杂一下，其实理解了算法思路这些代码还是很容易看懂的，函数heap_sort是堆排序的主体逻辑，第一个for循环是从最后一个父节点开始调整，将父节点与较大的子节点交换，一直调整到根节点，初始化成一个大根堆。 第二个for循环就是重复做交换首尾元素，然后调整剩余元素使其成为大根堆这两件事，重复n-1轮，排序过程也就完成了。 运行测试在线编辑器是一个很方便的测试代码的环境，如果想本地调试一下，也可以直接下载堆排序–源码，在本地编译后进行调试，其实单步调试是理解算法思路很有效的方式。]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
        <tag>堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[float的精度和取值范围]]></title>
    <url>%2Fblog%2F2019%2F06%2F27%2Ffloat%E7%9A%84%E7%B2%BE%E5%BA%A6%E5%92%8C%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4%2F</url>
    <content type="text"><![CDATA[前言关于float的精度和取值范围这个问题，我查询了很多次，每次都是用完就忘了，等到再使用的时候还需要再次查询，关键是这个问题大家给出的结果并不都是一致的，我得从众多的资料当中选择出正确的观点，这还要额外花一些时间，所以我决定也总结一次，方便我以后拿来直接用了，如果能给大家带来帮助那就更好了。下面提到一些说法很多都是我个人的理解，如果大家有疑义，欢迎讨论。 精度限制首先考虑下为什么会产生精度问题，是因为存储数据的空间有限，以一个四字节整数int n;为例，一共有32位，取值范围是 [-2147483648‬, 21474836487] ，一共是4,294,967,296种可能，它的精度可以说是小数点后一位都不保留，也就是只有整数，换句话说变量n可以表示实数范围内的4,294,967,296个数值。 如果换成float类型呢？一个变量float f所能表示多少个数呢？实际上由于存储空间未发生变化，同样是4字节32位，那么float类型也只能表示，或者说精确表示4,294,967,296个数值（实际上由于一些特殊的规则，最终所表示的数字个数还要少），说到这里很多人可能会疑惑，因为他知道float可以表示比4,294,967,296大的数，同时也能表示小数，如果只有4,294,967,296种可能，那究竟是怎么做到的呢？ 这里也就开始提到精度了，整数很好理解，每个数字的间隔都是1，int类型所表示的4,294,967,296个数字都是等间距的，步长为1。而float也只能表示4,294,967,296个数字，同时要表示比int还大的范围，一个很直观的想法就是把间距拉大，这样范围就大了，但是float还要表示小数，像0.2、0.4这样的数字间距明显要小于1啊，想要存储小数貌似要把间距缩小，这就和前面矛盾了啊。 实际上float类型存储数据的间隔不是等间距的，而是在0的附近间距小，在远离0的位置间距大，为什么会这样，一会我们看一下float类型数据的存储规则就明白了，这里先来看一下int类型和float类型所表示数字的范围对比，这只是一个示意图。 1234//int [ * * * 0 * * * ]//float[ * * * * * * * * * * * 0 * * * * * * * * * * * ] 上面的示意图就是两者表示数字范围的差异，每个星号*就表示一个数字，float通过这种不等间距的分布，既扩大了范围也表示了小数，那么有没有问题呢？ 当然有问题，饭就这么多，人多了自然不够吃了，因为远离0的位置间距越来越大，当要表示间距中间的一个数字时，只能找它附近离它最近的一个可以表示的数字来代替，这就导致了精度问题，比如我给一个float类型变量分别赋值为 4294967244 和 4294967295 ，再次输出时都变成了 4294967296，因为超过了精度，所以只能找最接近的数字代替。 float存储方式这部分内容基本上各篇文章说的都一致，我也简单描述下，后面根据这部分的定义来推算一下float的精度和取值范围。 首先我们知道常用科学计数法是将所有的数字转换成(±)a.b x $10^c$ 的形式，其中a的范围是1到9共9个整数，b是小数点后的所有数字，c是10的指数。而计算机中存储的都是二进制数据，所以float存储的数字都要先转化成(±)a.b x $2^c$，由于二进制中最大的数字就是1，所以表示法可以写成(±)1.b x $2^c$的形式，float要想存储小数就只需要存储(±)，b和c就可以了。 float的存储正是将4字节32位划分为了3部分来分别存储正负号，小数部分和指数部分的： Sign（1位）：用来表示浮点数是正数还是负数，0表示正数，1表示负数。 Exponent（8位）：指数部分。即上文提到数字c，但是这里不是直接存储c，为了同时表示正负指数以及他们的大小顺序，这里实际存储的是c+127。 Mantissa（23位）：尾数部分。也就是上文中提到的数字b。 三部分在内存中的分布如下，用首字母代替类型 S E E E E E E E E M M M M M M M M M M M M M M M M M M M M M M 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 float存储示例以数字6.5为例，看一下这个数字是怎么存储在float变量中的： 先来看整数部分，模2求余可以得到二进制表示为110。 再来看小数部分，乘2取整可以得到二进制表示为.1（如果你不知道怎样求小数的二进制，请主动搜索一下）。 拼接在一起得到110.1然后写成类似于科学计数法的样子，得到1.101 x $2^2$。 从上面的公式中可以知道符号为正，尾数是101，指数是2。 符号为正，那么第一位填0，指数是2，加上偏移量127等于129，二进制表示为10000001，填到2-9位，剩下的尾数101填到尾数位上即可 S E E E E E E E E M M M M M M M M M M M M M M M M M M M M M M 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 内存中二进制数01000000 11010000 00000000 00000000表示的就是浮点数6.5 float范围明白了上面的原理就可求float类型的范围了，找到所能表示的最大值，然后将符号为置为1变成负数就是最小值，要想表示的值最大肯定是尾数最大并且指数最大，那么可以得到尾数为 0.1111111 11111111 11111111，指数为 11111111，但是指数全为1时有其特殊用途，所以指数最大为 11111110，指数减去127得到127，所以最大的数字就是1.1111111 1111111 11111111 x $2^{127}$，这个值为 340282346638528859811704183484516925440，通常表示成 3.4028235E38，那么float的范围就出来了： [-3.4028235E38, 3.4028235E38] float精度float 类型的数据精度取决于尾数，相信大家都知道这一点，但是精度怎么算我也是迷糊了好久，最近在不断尝试的过程中渐渐的明白了，首先是在不考虑指数的情况下23位尾数能表示的范围是[0, $2^{23}-1$]，实际上尾数位前面还隐含了一个”1”，所以应该是一共24位数字，所能表示的范围是[0, $2^{24}-1$]（因为隐含位默认是”1”，所以表示的数最小是1不是0，但是先不考虑0，后面会特殊介绍，这里只按一般值计算），看到这里我们知道这24位能表示的最大数字为$2^{24}$-1，换算成10进制就是16777215，那么[0, 16777215]都是能精确表示的，因为他们都能写成1.b x $2^c$的形式，只要配合调整指数c就可以了。 16777215 这个数字可以写成1.1111111 11111111 1111111 $2^{23}$，所以这个数可以精确表示，然后考虑更大的数16777216，因为正好是2的整数次幂，可以表示1.0000000 00000000 00000000 $2^{24}$，所以这个数也可以精确表示，在考虑更大的数字16777217，这个数字如果写成上面的表示方法应该是 1.0000000 00000000 00000000 1 * $2^{24}$，但是这时你会发现，小数点后尾数位已经是24位了，23位的存储空间已经无法精确存储，这时浮点数的精度问题也就是出现了。 看到这里发现 16777216 貌似是一个边界，超过这个数的数字开始不能精确表示了，那是不是所有大于16777216的数字都不能精确表示了呢？其实不是的，比如数字 33554432 就可以就可以精确表示成1.0000000 00000000 00000000 * $2^{25}$，说道这里结合上面提到的float的内存表示方式，我们可以得出大于 16777216 的数字（不超上限），只要可以表示成小于24个2的n次幂相加，并且每个n之间的差值小于24就能够精确表示。换句话来说所有大于 16777216 的合理数字，都是[0, 16777215]范围内的精确数字通过乘以$2^n$得到的，同理所有小于1的正数，也都是 [0, 16777215] 范围内的精确数字通过乘以$2^n$得到的，只不过n取负数就可以了。 16777216 已经被证实是一个边界，小于这个数的整数都可以精确表示，表示成科学技术法就是1.6777216 * $10^{7}$，从这里可以看出一共8位有效数字，由于最高位最大为1不能保证所有情况，所以最少能保证7位有效数字是准确的，这也就是常说float类型数据的精度。 float小数从上面的分析我们已经知道，float可表示超过16777216范围的数字是跳跃的，同时float所能表示的小数也都是跳跃的，这些小数也必须能写成2的n次幂相加才可以，比如0.5、0.25、0.125…以及这些数字的和，像5.2这样的数字使用float类型是没办法精确存储的，5.2的二进制表示为101.0011001100110011001100110011……最后的0011无限循环下去，但是float最多能存储23位尾数，那么计算机存储的5.2应该是101.001100110011001100110，也就是数字 5.19999980926513671875，计算机使用这个最接近5.2的数来表示5.2。关于小数的精度与刚才的分析是一致的，当第8位有效数字发生变化时，float可能已经无法察觉到这种变化了。 float特殊值我们知道float存储浮点数的形式是(±)1.b x $2^c$，因为尾数位前面一直是个1，所以无论b和c取什么样的值，都无法得到0，所以在float的表示方法中有一些特殊的约定，用来表示0已经其他的情况。 float的内存表示指数位数有8位，范围是[0, 255]，考虑偏移量实际的指数范围是[-127,128]，但实际情况下指数位表示一般数字时不允许同时取0或者同时取1，也就是指数位的实际范围是[-126,127]，而指数取-127和128时有其特殊含义，具体看下面表格： 符号位 指数位 尾数位 数值 含义 0 全为0 全为0 +0 正数0 1 全为0 全为0 +0 负数0 0 全为0 任意取值f $0.f * 2^{-126}$ 非标准值，尾数前改为0，提高了精度 1 全为0 任意取值f $-0.f * 2^{-126}$ 非标准值，尾数前改为0，提高了精度 0 全为1 全为0 +Infinity 正无穷大 1 全为1 全为0 -Infinity 负无穷大 0/1 全为1 不全为0 NaN 非数字，用来表示一些特殊情况 总结 float的精度是保证至少7位有效数字是准确的 float的取值范围[-3.4028235E38, 3.4028235E38]，精确范围是[-340282346638528859811704183484516925440, 340282346638528859811704183484516925440] 一个简单的测试float精度方法，C++代码中将数字赋值给float变量，如果给出警告warning C4305: “=”: 从“int”到“float”截断，则超出了float的精度范围，在我的测试中赋值为16777216及以下整数没有警告，赋值为16777217时给出了警告。]]></content>
      <categories>
        <category>concepts</category>
      </categories>
      <tags>
        <tag>float</tag>
        <tag>精度</tag>
        <tag>取值范围</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中使用count加条件统计]]></title>
    <url>%2Fblog%2F2019%2F06%2F03%2FMysql%E4%B8%AD%E4%BD%BF%E7%94%A8count%E5%8A%A0%E6%9D%A1%E4%BB%B6%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[前言最近发现在处理Mysql问题时，count()函数频繁上镜，常常出现在分组统计的情景下，但是有时候并不是使用group by分好组就可以直接统计了，比如说一个常见的需求，统计每个班级男生所占的比例，这种情况一般会按照班级分组，但是分组内不但要统计班级的人数，还要统计男生的人数，也就是说统计是有条件的，之前确实没有考虑过怎样实心，后来查询了资料，总结在这里，方便日后查找使用。 Mysql中count()函数的一般用法是统计字段非空的记录数，所以可以利用这个特点来进行条件统计，注意这里如果字段是NULL就不会统计，但是false是会被统计到的，记住这一点，我们接下来看看几种常见的条件统计写法。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 准备工作 新建一个Mysql数据表a，包含id和num两个字段 12mysql&gt; create table a(id int, num int);Query OK, 0 rows affected (0.04 sec) 插入测试数据，为了看count()函数的效果，我们插入两个空数据 123mysql&gt; insert into a values (1,100),(2,200),(3,300),(4,300),(8,null),(9,null);Query OK, 6 rows affected (0.01 sec)Records: 6 Duplicates: 0 Warnings: 0 查询表a中的数据，与后面的统计做比较 123456789101112mysql&gt; select * from a;+----+------+| id | num |+----+------+| 1 | 100 || 2 | 200 || 3 | 300 || 4 | 300 || 8 | NULL || 9 | NULL |+----+------+6 rows in set (0.09 sec) 调用count()函数看效果，如果使用count(*)会查询出所有的记录数，但如果使用count(num)发现只有4条数据，num为NULL的记录并没有统计上 123456789101112131415mysql&gt; select count(*) from a;+----------+| count(*) |+----------+| 6 |+----------+1 row in set (0.03 sec)mysql&gt; select count(num) from a;+------------+| count(num) |+------------+| 4 |+------------+1 row in set (0.04 sec) 条件统计 count()函数中使用条件表达式加or null来实现，作用就是当条件不满足时，函数变成了count(null)不会统计数量 1234567mysql&gt; select count(num &gt; 200 or null) from a;+--------------------------+| count(num &gt; 200 or null) |+--------------------------+| 2 |+--------------------------+1 row in set (0.22 sec) count()函数中使用if表达式来实现，当条件满足是表达式的值为非空，条件不满足时表达式值为NULL; 1234567mysql&gt; select count(if(num &gt; 200, 1, null)) from a;+-------------------------------+| count(if(num &gt; 200, 1, null)) |+-------------------------------+| 2 |+-------------------------------+1 row in set (0.05 sec) count()函数中使用case when表达式来实现，当条件满足是表达式的结果为非空，条件不满足时无结果默认为NULL; 1234567mysql&gt; select count(case when num &gt; 200 then 1 end) from a;+---------------------------------------+| count(case when num &gt; 200 then 1 end) |+---------------------------------------+| 2 |+---------------------------------------+1 row in set (0.07 sec) 总结使用count()函数实现条件统计的基础是对于值为NULL的记录不计数，常用的有以下三种方式，假设统计num大于200的记录 select count(num &gt; 200 or null) from a; select count(if(num &gt; 200, 1, null)) from a select count(case when num &gt; 200 then 1 end) from a]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>if</tag>
        <tag>Mysql</tag>
        <tag>count</tag>
        <tag>条件统计</tag>
        <tag>casewhen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb启动多进程程序并切换调试进程]]></title>
    <url>%2Fblog%2F2019%2F05%2F24%2Fgdb%E5%90%AF%E5%8A%A8%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%A8%8B%E5%BA%8F%E5%B9%B6%E5%88%87%E6%8D%A2%E8%B0%83%E8%AF%95%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言gdb是linux环境下调试C/C++程序的强大工具，但是最近在使用gdb启动一个多进程程序的时候总是意外退出，显示信息中包含Detaching after fork from child process 25377.这一句，而用attach命令附加到正在运行的进程却没有问题，因为需要调试启动逻辑的部分代码，所以必须使用gdb启动多进程程序，后来发现可以通过gdb的follow-fork-mode选项来切换进程，达到调试指定进程的目的。 使用方法1set follow-fork-mode [parent|child] 这个命令只要gdb启动程序之后，在运行r命令之前敲入即可，如果不设置默认是parent模式，如果调试的child模式，就需要手动切换，我遇到的问题就是，程序启动使用fork()函数创建出子进程之后就把父进程退出了，gdb默认调试parent进程，也跟着结束了，所以出现了之前所说的Detaching after fork from child process 25377.信息，接下来可以写个简单的例子测试一下。 测试环境123456789101112131415[albert@localhost#20:15:45#/home/albert/gdbtest]$cat /etc/issueCentOS release 6.3 (Final)Kernel \r on an \m[albert@localhost#20:16:25#/home/albert/gdbtest]$gdb --versionGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.[albert@localhost#20:16:36#/home/albert/gdbtest]$^C 具体例子 先写一个简单的多进程程序，模拟我遇到的问题，父进程退出，子进程继续工作 1234567891011121314151617181920212223242526#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;int main ()&#123; pid_t pid; //pid表示fork函数返回的值，会根据不同进程返回不同值 pid = fork(); if (pid &lt; 0) &#123; exit(-1); &#125; else if (pid == 0) // 子进程返回pid为0 &#123; unsigned int u = 0; while(true) &#123; ++u; sleep(1); &#125; &#125; else // 父进程返回pid为子进程的id，大于0 &#123; exit(1); &#125; return 0;&#125; 将代码编译成可执行程序 1[albert@localhost#20:04:31#/home/albert/gdbtest]$g++ multiprocess.cpp -o multiprocess gdb启动程序并运行，其中我只输入了r和q两个gdb命令，发现程序运行r之后输出几行信息就退出了 1234567891011121314151617181920212223242526[albert@localhost#20:04:45#/home/albert/gdbtest]$gdb ./multiprocessGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/gdbtest/multiprocess...(no debugging symbols found)...done.(gdb) rStarting program: /home/albert/gdbtest/multiprocesswarning: the debug information found in "ld-2.12.so.debug" does not match "ld-linux-x86-64.so.2" (CRC mismatch).warning: the debug information found in "ld-2.12.so.debug" does not match "ld-linux-x86-64.so.2" (CRC mismatch).warning: the debug information found in "libm-2.12.so.debug" does not match "libm.so.6" (CRC mismatch).warning: the debug information found in "libm-2.12.so.debug" does not match "libm.so.6" (CRC mismatch).warning: the debug information found in "libc-2.12.so.debug" does not match "libc.so.6" (CRC mismatch).warning: the debug information found in "libc-2.12.so.debug" does not match "libc.so.6" (CRC mismatch).Detaching after fork from child process 25377.Program exited with code 01.Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-18.el6_9.2.x86_64(gdb) q[albert@localhost#20:05:03#/home/albert/gdbtest] 使用set follow-fork-mode child命令调试子进程，在r之前输入即可，这次发现程序停在了[New process 27522]，此时就可以打断点调试了 12345678910111213141516171819202122232425262728[albert@localhost#20:23:12#/home/albert/gdbtest]$gdb ./multiprocessGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/gdbtest/multiprocess...(no debugging symbols found)...done.(gdb) set follow-fork-mode child(gdb) rStarting program: /home/albert/gdbtest/multiprocesswarning: the debug information found in "ld-2.12.so.debug" does not match "ld-linux-x86-64.so.2" (CRC mismatch).warning: the debug information found in "ld-2.12.so.debug" does not match "ld-linux-x86-64.so.2" (CRC mismatch).warning: the debug information found in "libm-2.12.so.debug" does not match "libm.so.6" (CRC mismatch).warning: the debug information found in "libm-2.12.so.debug" does not match "libm.so.6" (CRC mismatch).warning: the debug information found in "libc-2.12.so.debug" does not match "libc.so.6" (CRC mismatch).warning: the debug information found in "libc-2.12.so.debug" does not match "libc.so.6" (CRC mismatch).[New process 27522]^CProgram received signal SIGINT, Interrupt.[Switching to process 27522]0x00007ffff7354c30 in __nanosleep_nocancel () from /lib64/libc.so.6Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-18.el6_9.2.x86_64(gdb) b multiprocess.cpp:17 总结 gdb调试多进程程序时使用set follow-fork-mode [parent|child]命令 默认调试parent进程，想调试child进程，使用set follow-fork-mode child命令切换]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>fork</tag>
        <tag>child</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql调优之Using filesort]]></title>
    <url>%2Fblog%2F2019%2F05%2F16%2FMysql%E8%B0%83%E4%BC%98%E2%80%94%E4%B9%8BUsing-filesort%2F</url>
    <content type="text"><![CDATA[前言在使用 explain 命令优化SQL语句的时候常常会在Extra列的描述中发现 Using filesort 选项，其实这个名字很容易造成误解，一开始我以为是“文件排序”的意思，进一步说可能就是使用了磁盘空间来进行排序，但是这个理解是错误的，Using filesort 真正含义其实只有 sort 这一个单词，和 file 没有什么关系，Mysql一般是通过内存进行排序的，不过，要是超过了配置中的限制，应该会生成临时表。 分析Using filesort 的含义很简单，就是使用了排序操作，出现这个选项的常见情况就是 Where 条件和 order by 子句作用在了不同的列上，这种情况还有优化的余地，有些场景由于数据量太小或者语句的简单性可能都不需要优化，既然说Using filesort是使用了排序的意思，那么是不是包含了 order by 子句的查询语句都会有这个选项呢？其实这个排序操作有时是可以避免的。 如果你想把一个表中的所有数据按照指定顺序输出，那么整个排序几乎是不可避免的，比如这个语句select * from a order by id，即使在id列上建立了索引，为了生成指定顺序的数据，那么整个数据的排序也是需要，不过个别时候这个排序还是可以省略的，比如id是该表的主键，并且是自增长的，数据本身就是有序的，那么直接返回数据就行了，相当于 order by id 这一部分被忽略了。 上面提到的常见情况，SQL语句通常写成这样select * from a where type = 5 order by id，这类语句一般会产生 Using filesort 这个选项，即使你在 type 和 id 上分别添加了索引。我们想一下它的工作过程，先根据type的索引从所有数据信息中挑选出满足 type = 5 条件的，然后根据id列的索引信息对挑选的数据进行排序，所以产生了Using filesort选项，想想怎样可以把后面排序的这个步骤省略掉？联合索引可以解决这个问题。 可以在 type, id 两列上建立一个联合索引，索引类型一般是 BTREE，根据Mysql索引的最左原则，可以知道一共建立了type_index和type_id_index两条索引，由于有了 type_id_index 这个联合索引，后面的排序步骤就可以省略了，在按照type = 5 条件挑选数据时，挂在type = 5 节点下的数据，其实按照id列的值也是有顺序的，我们只需要在挑选数据的同时，按照id从小到大的顺序挑选即可，最后得到的数据就是有序的，直接返回就行了，从这一点可以看出，“排序”操作并不是不存在了，只是隐含在了前面必要的步骤中，不需要单独操作了而已，下面举个简单例子，看看具体的效果。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 具体操作 先建立一个测试表格tb，一般为了加快查询速度，会在常用的字段上建立索引 12mysql&gt; create table tb(id int, type int, weight int, index t_index(type), index w_index(weight));Query OK, 0 rows affected (0.02 sec) 创建一个存储fill_test_data用来插入测试数据，创建完成调用一下 12345678910111213141516CREATE PROCEDURE `fill_test_data`()BEGIN DECLARE i int default 1; DECLARE w int default 100; DECLARE t int default 1; WHILE i &lt;= 100000 do insert into tb values(i, t, w); set i = i + 1; set w = (w + 10) % 1000; set t = (t + 1) % 10; END WHILE;ENDmysql&gt; call fill_test_data();Query OK, 1 row affected (25.36 sec) 查询数据，让 Where 条件和 order by 子句作用在不同的列上 1234567891011121314mysql&gt; select * from tb where type = 3 order by weight;+-------+------+--------+| id | type | weight |+-------+------+--------+| 193 | 3 | 20 || 293 | 3 | 20 || 393 | 3 | 20 |...| 99683 | 3 | 920 || 99783 | 3 | 920 || 99883 | 3 | 920 || 99983 | 3 | 920 |+-------+------+--------+10000 rows in set (2.22 sec) 使用 explain命令分析查询语句，就会发现Using filesort出现在了Extra条目中 123456789101112131415mysql&gt; explain select * from tb where type = 3 order by weight\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb partitions: NULL type: refpossible_keys: t_index key: t_index key_len: 5 ref: const rows: 17672 filtered: 100.00 Extra: Using index condition; Using filesort1 row in set, 1 warning (0.00 sec) 使用SQL命令给表tb的type列和id列添加联合索引 123mysql&gt; alter table tb add index tw_index(type, weight);Query OK, 0 rows affected (0.17 sec)Records: 0 Duplicates: 0 Warnings: 0 再次查询数据，看看与上一次的查询时间相比有没有变化 1234567891011121314mysql&gt; select * from tb where type = 3 order by weight;+-------+------+--------+| id | type | weight |+-------+------+--------+| 193 | 3 | 20 || 293 | 3 | 20 || 393 | 3 | 20 |...| 99683 | 3 | 920 || 99783 | 3 | 920 || 99883 | 3 | 920 || 99983 | 3 | 920 |+-------+------+--------+10000 rows in set (2.13 sec) 再次使用 explain命令分析查询语句，就会发现Using filesort选项已经消失了 123456789101112131415mysql&gt; explain select * from tb where type = 3 order by weight\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb partitions: NULL type: refpossible_keys: t_index,tw_index key: tw_index key_len: 5 ref: const rows: 17672 filtered: 100.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec)]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>查询</tag>
        <tag>Mysql</tag>
        <tag>Usingfilesort</tag>
        <tag>排序</tag>
        <tag>调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下查看C/C++程序的堆栈信息]]></title>
    <url>%2Fblog%2F2019%2F05%2F08%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9F%A5%E7%9C%8BC-C-%E7%A8%8B%E5%BA%8F%E7%9A%84%E5%A0%86%E6%A0%88%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言经常在Windows上开发的工程师们可能已经习惯了图形化的调试界面，在源代码的编辑框上点击就可以添加断点，在调用堆栈的窗口就可以看到程序运行的堆栈信息，但是在 linux 环境下，面对命令行的天下，我们需要掌握一些命令，才能够查看C/C++程序的堆栈信息。 测试环境1234567[albert@localhost#13:58:34#/home/albert]$cat /etc/issueCentOS release 6.3 (Final)Kernel \r on an \m[albert@localhost#13:58:43#/home/albert]$g++ --versiong++ (GCC) 4.4.7 20120313 (Red Hat 4.4.7-18)Copyright ?? 2010 Free Software Foundation, Inc. 查看方法 使用gdb程序调试core文件，格式为 gdb test_proc core.proc_id 使用gdb程序附加到调试程序的进程上，格式为 gdb attach proc_id 使用pstack程序输出调试程序的堆栈信息，格式为 pstack proc_id 使用strace程序打印调试程序的运行信息，格式为 strace -p proc_id 具体实践 一般查看堆栈信息时常常面对的都是多线程的程序，所以我们也来写一个简单的多线程小程序，代码如下： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;static void* thread_proc(void* arg)&#123; unsigned int sum = 3; while(true) &#123; for (int idx = 0; idx &lt; 1000000000; ++idx) sum += idx; printf("thread sum = %u\n", sum); sleep(2); &#125; return 0;&#125;int main()&#123; pthread_t thread_id; pthread_create(&amp;thread_id, NULL, thread_proc, NULL); unsigned int sum = 0; while(true) &#123; for (int idx = 0; idx &lt; 1000000000; ++idx) sum += idx; printf("main sum = %u\n", sum); sleep(1); &#125; return 0;&#125; 编译程序并运行，程序开始不断的打印计算结果 1234567891011[albert@localhost#15:06:54#/home/albert/test/threadtest]$g++ threadtest.cpp -O0 -pthread -o threadtest[albert@localhost#15:08:27#/home/albert/test/threadtest]$./threadtestthread sum = 3051657987main sum = 3051657984thread sum = 1808348675main sum = 1808348672main sum = 565039360thread sum = 565039363main sum = 3616697344thread sum = 3616697347... 现在可以通过上面描述的方法来查看threadtest程序堆栈信息了，几乎所有的命令都需要进程id，所以我们可以再开一个终端先通过pidof命令来获得： 12[albert@localhost#15:39:35#/home/albert/test/threadtest]$pidof threadtest21473 gdb调试core文件 通过kill命令产生core文件 使用命令 kill -11 21473可以将正在运行的程序杀死，并且产生core文件core.21473，-11表示段错误信号，通常是访问了无效的内存导致 通过gcore命令产生core文件 使用命令 gcore 21473可以产生core文件core.21473，但是不会杀死程序，适用于调试线上程序，又不影响用户使用的情况，可以测试一下： 12345678910111213[albert@localhost#15:39:43#/home/albert/test/threadtest]$gcore 21473warning: the debug information found in "/usr/lib/debug//lib64/libm-2.12.so.debug" does not match "/lib64/libm.so.6" (CRC mismatch)warning: the debug information found in "/usr/lib/debug/lib64/libm-2.12.so.debug" does not match "/lib64/libm.so.6" (CRC mismatch)warning: the debug information found in "/usr/lib/debug//lib64/libpthread-2.12.so.debug" does not match "/lib64/libpthread.so.0" (CRC mismatch)warning: the debug information found in "/usr/lib/debug/lib64/libpthread-2.12.so.debug" does not match "/lib64/libpthread.so.0" (CRC mismatch)[New LWP 21474][Thread debugging using libthread_db enabled]warning: the debug information found in "/usr/lib/debug//lib64/libc-2.12.so.debug" does not match "/lib64/libc.so.6" (CRC mismatch)warning: the debug information found in "/usr/lib/debug/lib64/libc-2.12.so.debug" does not match "/lib64/libc.so.6" (CRC mismatch)warning: the debug information found in "/usr/lib/debug//lib64/ld-2.12.so.debug" does not match "/lib64/ld-linux-x86-64.so.2" (CRC mismatch)warning: the debug information found in "/usr/lib/debug/lib64/ld-2.12.so.debug" does not match "/lib64/ld-linux-x86-64.so.2" (CRC mismatch)0x00000000004006eb in main ()Saved corefile core.21473 然后使用gdb调试core文件： 123456789101112131415161718192021222324252627[albert@localhost#15:47:13#/home/albert/test/threadtest]$gdb threadtest core.21473GNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/test/threadtest/threadtest...(no debugging symbols found)...done.[New Thread 21474][New Thread 21473]Missing separate debuginfo forTry: yum --enablerepo='*-debug*' install /usr/lib/debug/.build-id/80/1b9608daa2cd5f7035ad415e9c7dd06ebdb0a2Reading symbols from /usr/lib64/libstdc++.so.6...(no debugging symbols found)...done.Loaded symbols for /usr/lib64/libstdc++.so.6Reading symbols from /lib64/libm.so.6......省略无关信息(no debugging symbols found)...done.Loaded symbols for /lib64/ld-linux-x86-64.so.2Core was generated by `./threadtest'.#0 0x0000000000400691 in thread_proc(void*) ()Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-18.el6_9.2.x86_64(gdb) 显示所有线程信息，可以使用gdb命令thread apply all bt： 123456789(gdb) thread apply all btThread 2 (Thread 0x7f1b4e1d2720 (LWP 21473)):#0 0x00000000004006eb in main ()Thread 1 (Thread 0x7f1b4d270700 (LWP 21474)):#0 0x0000000000400691 in thread_proc(void*) ()#1 0x00007f1b4d60caa1 in start_thread () from /lib64/libpthread.so.0#2 0x00007f1b4d359bcd in clone () from /lib64/libc.so.6 gdb附加到进程可以通过 gdb attach pid 直接附加到正在运行的程序上，然后查看线程信息thread apply all bt：123456789101112131415161718192021222324252627282930313233343536[albert@localhost#15:54:59#/home/albert/test/threadtest]$gdb attach 21473GNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...attach: 没有那个文件或目录.Attaching to process 21473Reading symbols from /home/albert/test/threadtest/threadtest...(no debugging symbols found)...done.Reading symbols from /usr/lib64/libstdc++.so.6...(no debugging symbols found)...done.Loaded symbols for /usr/lib64/libstdc++.so.6Reading symbols from /lib64/libm.so.6......省略无关信息(no debugging symbols found)...done.Loaded symbols for /lib64/ld-linux-x86-64.so.20x00007f1b4d31dc4d in nanosleep () from /lib64/libc.so.6Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-18.el6_9.2.x86_64(gdb) thread apply all btThread 2 (Thread 0x7f1b4d270700 (LWP 21474)):#0 0x00007f1b4d31dc4d in nanosleep () from /lib64/libc.so.6#1 0x00007f1b4d31dac0 in sleep () from /lib64/libc.so.6#2 0x00000000004006b6 in thread_proc(void*) ()#3 0x00007f1b4d60caa1 in start_thread () from /lib64/libpthread.so.0#4 0x00007f1b4d359bcd in clone () from /lib64/libc.so.6Thread 1 (Thread 0x7f1b4e1d2720 (LWP 21473)):#0 0x00007f1b4d31dc4d in nanosleep () from /lib64/libc.so.6#1 0x00007f1b4d31dac0 in sleep () from /lib64/libc.so.6#2 0x0000000000400721 in main () pstack输出堆栈信息如果不需要调试，只想查看运行程序当前的堆栈信息，可以使用pstack命令，输出信息很简洁：123456789[albert@localhost#15:57:53#/home/albert/test/threadtest]$pstack 21473Thread 2 (Thread 0x7f1b4d270700 (LWP 21474)):#0 0x0000000000400683 in thread_proc(void*) ()#1 0x00007f1b4d60caa1 in start_thread () from /lib64/libpthread.so.0#2 0x00007f1b4d359bcd in clone () from /lib64/libc.so.6Thread 1 (Thread 0x7f1b4e1d2720 (LWP 21473)):#0 0x00007f1b4d31dc4d in nanosleep () from /lib64/libc.so.6#1 0x00007f1b4d31dac0 in sleep () from /lib64/libc.so.6#2 0x0000000000400721 in main () strace打印程序运行情况strace输出的不是堆栈信息，而是类似于程序的运行步骤，具体信息如下：123456789101112131415161718[albert@localhost#15:57:56#/home/albert/test/threadtest]$strace -p 21473Process 21473 attachedwrite(1, "main sum = 2580918016\n", 22) = 22rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0rt_sigaction(SIGCHLD, NULL, &#123;SIG_DFL, [], 0&#125;, 8) = 0rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0nanosleep(&#123;1, 0&#125;, 0x7fff56a49c50) = 0write(1, "main sum = 1337608704\n", 22) = 22rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0rt_sigaction(SIGCHLD, NULL, &#123;SIG_DFL, [], 0&#125;, 8) = 0rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0nanosleep(&#123;1, 0&#125;, 0x7fff56a49c50) = 0write(1, "main sum = 94299392\n", 20) = 20rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0rt_sigaction(SIGCHLD, NULL, &#123;SIG_DFL, [], 0&#125;, 8) = 0rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0nanosleep(&#123;1, 0&#125;, 0x7fff56a49c50) = 0^CProcess 21473 detached 总结 在解决实际问题的过程中，上述几种方法可以结合使用，选取合适的使用方法，比如面对程序突然崩溃，那么gdb proc core就是调试的首选方法。 如果只是想简单的查看堆栈信息，可以使用pstack pid这种方式，免去了生成巨大core文件的麻烦。 如果还想查看运行逻辑中的变量信息，那么gdb使我们可以帮助我们动态调试程序，查看一些程序运行时的状态。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>strace</tag>
        <tag>pstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python利用requests模块实现代理访问网络]]></title>
    <url>%2Fblog%2F2019%2F05%2F05%2FPython%E5%88%A9%E7%94%A8requests%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AE%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[前言代理相信很多人都听过，即使没有自己感受到，在无形之中可能也使用过，网络代理作为一项技术，在访问互联网时被广泛使用，那是因为使用代理有着诸多好处。 使用代理IP能够突破自身的访问限制，不要把突破限制看成是坏事情，有时后恰恰是为了网络安全才使用了代理，比如内网的一台服务器只针对特定的IP提供访问权限，这时如果给内部人员分配指定的代理就可以进行访问，不比对所有的IP地址都开放，代理IP还可以进行自主管理。 使用代理IP还提高访问速度，通常代理IP服务器都配置了一个较大的硬盘缓冲区，当缓冲区中保存有用户的请求信息时，则直接由缓冲区中取出信息，返回给用户，以提高访问速度。 测试环境12PS E:\&gt; python --versionPython 3.6.7 代码实现其实在Python 3中利用requests可以很方便的使用代理访问网络，比如下面这个简单的get方法：1requests.get(target_url, proxies=proxy_data) 其中需要注意的就是 proxies 参数的值，这里换成可以代理的ip就可以了，网上流传着众多的代理IP，只要可用就可以拿来代理IP访问，不过这些免费的IP失效性非常差，常常过几分钟就失效了，下面就给出一个完整的例子，检测代理IP是否可用： 1234567891011121314151617181920212223242526272829303132import requeststest_ip = '116.209.56.118'test_port = '9999'def test_proxy_request(ip, port): # 代理IP地址 proxy_data = &#123; 'http': 'http://' + ip + ':' + port, 'https': 'http://' + ip + ':' + port, &#125; # 客户端说明 head_data = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0', 'Connection': 'keep-alive' &#125; try: # 该返回当前的IP地址，http://icanhazip.com提供返回当前外网IP的服务 response = requests.get('http://icanhazip.com', headers=head_data, proxies=proxy_data) outer_ip = response.text.strip().replace('\n', '') return outer_ip == ip except: return Falseif __name__ == '__main__': test_result = test_proxy_request(test_ip, test_port) if test_result: print("IP代理成功 ==&gt; &#123;0&#125;:&#123;1&#125;".format(test_ip, test_port)) else: print("IP代理失败 ==&gt; &#123;0&#125;:&#123;1&#125;".format(test_ip, test_port)) 需要注意，其中只有这一句requests.get(&#39;http://icanhazip.com&#39;, headers=head_data, proxies=proxy_data)是代理的重点。 测试结果 IP代理成功 ==&gt; 116.209.56.118:9999]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>IP</tag>
        <tag>Python</tag>
        <tag>requests</tag>
        <tag>网络代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中explain命令简析]]></title>
    <url>%2Fblog%2F2019%2F04%2F27%2FMysql%E4%B8%ADexplain%E5%91%BD%E4%BB%A4%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言之前总结了Mysql慢查询日志的开启与配置方法，通过分析慢查询日志可以锁定执行效率差的SQL，但是这仅仅是发现了需要优化的部分，还要分析执行缓慢的原因，这时候就可以使用EXPLAIN命令去分析，所执行的操作究竟慢在哪里，是不是可以通过加索引或者改变查询方法来解决。 通过查询资料发现除了EXPLAIN命令，还有一个DESCRIBE命令，看起来很陌生是不是，但是如果写出简写desc应该很多人的就熟悉了，这不是查询表结构的时候常用的命令吗？实际上以上三个命令在mysql中是等价的，不过在使用时有些习惯性的偏向，通常使用 EXPLAIN 来分析SQL语句的执行缓慢的问题，而使用 DESCRIBE 或者 desc 来查看表的结构，就类似于惯用法，知道就好。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. EXPLAIN 的使用关于 EXPLAIN 使用其实很简单，就是在正常的执行语句之前加一个explain 就可以了，不过这里也存在一个疑问，就是发现很多篇文章，提到下面这种说法： explain 只能解释select查询，并不会对存储过程、insert、update、delete或其他语句做解释 但是我查阅了官方文档发现，EXPLAIN 后面可以跟 SELECT、 DELETE、 INSERT、 REPLACE、和 UPDATE语句，另外之前使用的 EXPLAIN EXTENDED 选项现在也默认开启，EXTENDED 关键字后续会在 Mysql 8.0 版本删除，应该是版本问题导致了explain语句使用的差异，所以请记住在使用新版本的Mysql时，需要分析语句执行情况的，只需要在语句前面添加一个 explain关键字即可。 不过在分析 select 语句时，explain命令会给出额外的提示信息，帮助我们优化查询语句，这也是我们需要学习的重点，先来简单看一下使用方法： 普通的查询语句 123456789mysql&gt; select * from a;+----+-----+| id | num |+----+-----+| 1 | 100 || 2 | 200 || 3 | 300 |+----+-----+3 rows in set (0.05 sec) 使用 EXPLAIN 来分析普通的查询语句 1234567mysql&gt; explain select * from a;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+1 row in set (0.04 sec) 通过上面的例子可以很清楚的知道 EXPLAIN 命令的使用方法，使用了 EXPLAIN 关键之后会生成一个分析结果的表格，该表格有id、select_type、table、partitions、type、possible_keys、key、key_len、ref、rows、filtered、Extra一共12列，而这12列中的内容代表的含义是我们学习的重点，也是我们进行优化的依据，这个结果集可能包含多行，其中每一行都是关于一个表的查询信息，可以针对于具体的表查询优化。 本来想每种情况后面紧跟一个例子的，但是发现这样会造成重点内容分散，不利于整体把握，所以还是先把各列可能的取值说清楚，然后在文末针对于上文的取值给出例子，如果看描述就能明白就可以省略例子的内容，否则可以对照着例子的写法理解一下, [eg：&lt;id-1&gt;]表示参考后面的例子&lt;id-1&gt;，想对照的话搜索即可。 EXPLAIN 各列的可能取值idid 列的取值通常是一组数字，表示select查询的序号，也有可能是一个NULL： 取值 含义 例子编号 id数字相同 优先级相同，从上往下执行 [eg：&lt;id-1&gt;] id数字不同 数字越大优先级越高，越先执行，比如包含子查询的语句，内部查询优先执行 [eg：&lt;id-2&gt;] id值为NULL 通常是使用了union，表示该行是一个结果集，不需要使用它来进行查询 [eg：&lt;id-2&gt;] select_typeselect_type 列表示查询的类型，主要是用于区分普通查询、联合查询、子查询等复杂的情况： 取值 含义 例子编号 SIMPLE 简单的查询语句，查询中不包括UNION操作和子查询 [eg：&lt;id-1&gt;] PRIMARY 在复杂查询中处于最外层的查询 [eg：&lt;id-2&gt;] UNION 查询语句中处于 UNION 关键字之后的查询 [eg：&lt;id-2&gt;] DEPENDENT UNION 查询语句中处于 UNION 关键字之后的查询，需要依赖于外部查询 [eg：`&lt;id-&gt;`]* UNION RESULT 表示 UNION 操作之后的结果，本身并不需要参与查询，通常该记录id字段为 NULL [eg：&lt;id-2&gt;] SUBQUERY 在子查询中的第一个查询 [eg：`&lt;id-&gt;`]* DEPENDENT SUBQUERY 在子查询中的第一个查询，需要依赖于外部查询 [eg：`&lt;id-&gt;`]* DERIVED 在 FROM 列表中包含的子查询 [eg：&lt;id-4&gt;] MATERIALIZED 物化子查询 [eg：&lt;id-5&gt;] UNCACHEABLE SUBQUERY 一个结果不能被缓存并且对于外部查询的每一行都需要重新评估自身的子查询 [eg：`&lt;id-&gt;`]* UNCACHEABLE UNION 查询语句中处于 UNION 关键字之后的子查询，并且其结果属于UNCACHEABLE SUBQUERY类型 [eg：`&lt;id-&gt;`]* tabletable 列表示查询所引用到的表名，如果查询中使用了别名，那么会显示别名，此外还有一些其他类型的引用： 取值 含义 例子编号 表名、别名 查询时引用了这个表 [eg：&lt;id-1&gt;] &lt;unionM,N&gt; 查询时引用了由id为M和N的两个查询的结果集构成的临时结果集 [eg：&lt;id-2&gt;] &lt;derivedN&gt; 查询时引用了id为N的查询形成的结果集 [eg：&lt;id-4&gt;] &lt;subqueryN&gt; 查询时引用了id为N的物化子查询形成的结果集 [eg：&lt;id-5&gt;] partitionspartitions 列表示查询结果集所涉及到的分区，值为 NULL 时表示该表并未分区： 取值 含义 例子编号 分区名 查询结果集引用到了这个分区 [eg：&lt;id-6&gt;] NULL 该表格并未分区，或者结果集中数据不再分区中 [eg：&lt;id-1&gt;] typetype 列表示访问类型，也就是找到所需数据所能使用的最好方式，取值类型很多，在下表中从上到下效果越来越差： 取值 含义 例子编号 NULL 查询经过优化执行时不用访问表数据，可能通过索引就搞定了，或者根本没有数据 [eg：&lt;id-8&gt;] system 在MyISAM类型的表中只有一行数据时出现，如果在 Innodb 类型表中只有一行数据通常显示 ALL [eg：&lt;id-9&gt;] const 表格使用了唯一索引或者主键，并且将其作为判定相等的筛选条件，得到一条记录 [eg：&lt;id-10&gt;] eq_ref 连接查询时，驱动表的每一条记录的条件列，与后面连接表的主键或唯一索引判定相等，后表采用的连接方式 [eg：&lt;id-11&gt;] ref 连接查询时，驱动表的每一条记录的条件列，与后面连接表的索引列判定相等，后表采用的连接方式，索引列数据不要求唯一，不连接表时就是查索引列等于一个具体的值 [eg：&lt;id-7&gt;] ref_or_null 与 ref 基本一致，另外包含查询索引列为 NULL 的记录 [eg：&lt;id-12&gt;] fulltext 包含全文索引的表的查询方式，全文索引的优先级要高于普通索引 [eg：`&lt;id-&gt;`]* index_merge 至少用到了两个索引，并且用到了索引合并优化 [eg：`&lt;id-&gt;`]* unique_subquery where条件in形式的子查询子查询返回唯一结果时，等价于将类型为 eq_ref 的查询作为子查询 [eg：`&lt;id-&gt;`]* index_subquery where条件in形式的子查询引用了非唯一索引，等价于将类型为 ref 的查询作为子查询 [eg：`&lt;id-&gt;`]* range 对于表的索引列使用范围判定的查询 [eg：&lt;id-13&gt;] index 除了查找索引树之外，与 ALL 选项基本一致，通常由于索引较小查询会快一点 [eg：&lt;id-14&gt;] ALL 完整浏览整个表格，查找符合条件的结果，属于最差的访问方式 [eg：&lt;id-1&gt;] possible_keyspossible_keys 列表示查询所需数据过程可能用到的索引名，具体是否使用还要依赖于查询过程中表的连接顺序，该值为 NULL 时表示无索引可用，此时需要考虑对表进行优化来改善查询结果情况了。 取值 含义 例子编号 索引名 查询时可能用到的索引名，是否使用取决于查询连接顺序 [eg：&lt;id-7&gt;] NULL 该查询没有可用索引，需要考虑优化 [eg：&lt;id-1&gt;] keykey 列表示查询所需数据过程确实用到的索引名，该值为 NULL 时表示无索引可用，此时也需要考虑对表进行优化。 取值 含义 例子编号 索引名 查询时确实用到的索引名 [eg：&lt;id-7&gt;] NULL 查询时没有可用索引，需考虑优化 [eg：&lt;id-1&gt;] key_lenkey_len 列表示查询所需数据过程用到的索引长度，该值为 NULL 时表示没有使用索引，由于存储格式的不同，对于可以为 NULL 的列储存索引所需空间要比不能为 NULL 列的大一个字节。 取值 含义 例子编号 索引长度 查询时确实用到的索引长度 [eg：&lt;id-7&gt;] NULL 没有使用到索引 [eg：&lt;id-1&gt;] refkey_len 列表示查询时使用常数或者某一列来和索引列比较，有时会显示 func，表示使用了一些函数的结果与索引比较，值为 NULL 时表示没用到索引比较 取值 含义 例子编号 引用列名 查询时与索引比较的列名，形式可能为&lt;subquery2&gt;.id，表示引用了子查询结果中的id列 [eg：&lt;id-5&gt;] const 查询时与索引比较的为常数 [eg：&lt;id-10&gt;] func 查询时与索引比较的一些函数结果 [eg：`&lt;id-&gt;`]* NULL 不是上述几种情况，可能没有使用索引比较 [eg：&lt;id-1&gt;] rowsrows 列表示查询符合条件的结果时所要检查的数据行数，在 InnoDB 类型的表中，这个值是一个估计值，可用来参考并不精确，值为 NULL 时表示表中无数据，或者无法找到匹配行，比如查找一条主键中不包含的数据。 取值 含义 例子编号 数字 查询时所要检查的数据行数 [eg：&lt;id-3&gt;] NULL 没有数据或者不需要检测 [eg：&lt;id-1&gt;] filteredfiltered 列表示通过筛选条件的记录数占可能参与检查的记录数，是一个估计值，该值与 rows 的乘积大概就是结果集中的记录数： 取值 含义 例子编号 数字 通过筛选条件的记录数占可能参与检查的记录数，最大为 100.00 [eg：&lt;id-3&gt;] ExtraExtra 列表示Mysql处理查询所使用的额外信息，类型很多，其中一些情况是需要进行优化的信号，对于SQL分析很有帮助： 取值 含义 例子编号 Child of &#39;tbl_name&#39; pushed join@1 当表被当做另一个表’tbl_name’的子表能被存放在 NDB 内核的时候，该值只出现在存储选项被开启的 NDB 集群上 - const row not found 当一个系统表没有数据可查的时候 - Deleting all rows 对于 DELETE 操作， MyISAM 引擎支持一个可以简单快速删除表数据的方法，如果使用了整个优化则显示此选项 - Distinct 查询时使用了 distinct 关键字，当查找到一个第一个匹配值后，相同匹配就不再搜索了 - const 当一个系统表没有数据可查的时候 - FirstMatch(tbl_name) 当 semi-join FirstMatch 访问简化策略被使用时候， 通常出现在 Where的 in 子句中，找到一个值后，后面相同值不再匹配出现 - Full scan on NULL key 当优化器不能使用索引查找访问方法时，将会显示该值，表示将子查询作为一种后备策略 - Impossible HAVING 当 HAVING 子句的条件总是不成立，无法匹配出任何数据 [eg：&lt;id-23&gt;] Impossible WHERE 当 WHERE 子句的条件总是不成立，无法匹配出任何数据 [eg：&lt;id-22&gt;] Impossible WHERE noticed after reading const tables 在读取const表和system表时，WHERE 子句的条件总是不成立 - LooseScan(m..n) 当 semi-join LooseScan 策略被使用的时候. m 和 n 是索引的编号 - No matching min/max row 在查询中包括系统函数，但是通过条件查询无法匹配出数据的时候， 比如SELECT MIN(...) FROM ... WHERE CONDITION - no matching row in const table 当连表查询时有一个空表或者没有匹配唯一索引的数据时，会给出此提示 [eg：&lt;id-18&gt;] No tables used 当查询中没有 FROM 子句或者只有 FROM DUAL子句时 [eg：&lt;id-20&gt;] Not exists 发生在左外连接的优化，当要求右侧表字段为空时，如果查找到一条不为空匹配，则停止查找匹配这项记录， 比如 SELECT * FROM t1 LEFT JOIN t2 ON t1.id=t2.id WHERE t2.id IS NULL - Plan isn&#39;t ready yet 执行命令 EXPLAIN FOR CONNECTION 时，优化器在命名连接中还没有完成为语句执行创建执行计划 - Range checked for each record(index map: N) 当没有好的默认索引可使用时，但当我们可以将以前表中的所有列都视为常量时，可能会使用某些索引就是这种情况 - Scanned N databases 表示在执行对INFORMATION_SCHEMA表的查询时，服务器执行了多少次目录扫描，数字可以是0,1或者任何整数 - Select tables optimized away 优化器发现只有一行，并且通过索引直接皆可以获得想要的数据，而不需要真正访问表数据，比如在索引列使用聚合函数 [eg：&lt;id-16&gt;] Skip_open_table 对于 INFORMATION_SCHEMA 表的查询，不需要打开表，只需要浏览目录就可以完成查询 - Open_frm_only 对于 INFORMATION_SCHEMA 表的查询，需要打开 .frm 文件完成查询 - Open_full_table 对于 INFORMATION_SCHEMA 表的查询，需要打开 .frm, .MYD, .MYI 文件完成查询 - Start temporary,End temporary 表示使用临时表用于 semi-join Duplicate Weedout 策略 [eg：&lt;id-15&gt;] unique row not found 当一个拥有 UNIQUE 索引或者 PRIMARY 索引的表没有查到满足条件数据时 - Using filesort 无法仅通过引用索引就完成排序，需要一个额外的阶段来进行外部排序，并且按排序结果取回记录 [eg：&lt;id-17&gt;] Using index 只通过索引排序就可以取得排序后的数据，无需做额外的搜索真实记录数据的工作 [eg：&lt;id-7&gt;] Using index condition 首先通过访问索引元组的方式来读取表格，除非必要时会通过索引索引信息延迟读取整个表格数据 - Using index for group-by 索引用于处理包含 GROUP BY 和 DISTINCT 的查询，由于重复项会被快速跳过。所以非常高效 - Using join buffer (Block Nested Loop) 连接访问之前表格数据被部分读入连接缓存区，然后使用缓存中的行与当前表进行连接，括号内容为使用算法 [eg：&lt;id-1&gt;] Using join buffer (Batched Key Access) 连接访问之前表格数据被部分读入连接缓存区，然后使用缓存中的行与当前表进行连接，括号内容为使用算法 - Using MRR 表格数据会通过 Multi-Range Read 优化策略来读取 - Using sort_union(...), Using union(...), Using intersect(...) 针对于 index_merge 选项，表明索引浏览被合并的特定算法 - Using temporary 需要创建一个临时表来存储结果，通常出现在包含了作用在不同列的上的 GROUP BY 子句和 ORDER BY子句 [eg：&lt;id-2&gt;] Using where 当查询使用了 WHERE 子句来过滤结果发送给客户端的时候 [eg：&lt;id-3&gt;] Using where with pushed condition 仅适用于 NDB 类型表，它意味着NDB集群正在使用 “条件存储”优化选项来提高接近于非索引列和常量之间直接比较的效率。 - Zero limit 当查询语句包含 LIMIT 0子句并不能查到任何记录的时候 [eg：&lt;id-19&gt;] EXPLAIN 各列的可能取值对应的例子建表操作(1为了展示id取值的不同先创建 a、b两个表，然后插入测试数据，由于是测试的开始，我们分别查看表结构和数据，之后为了减少篇幅，只给出命令，不再查询表结构和数据，接着我们开始测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051mysql&gt; create table a(id int, num int);Query OK, 0 rows affected (0.04 sec)mysql&gt; insert into a values(1, 100),(2,200),(3,300);Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; desc a;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.02 sec)mysql&gt; select * from a;+----+-----+| id | num |+----+-----+| 1 | 100 || 2 | 200 || 3 | 300 |+----+-----+3 rows in set (0.02 sec)mysql&gt; create table b(id int, num int);Query OK, 0 rows affected (0.03 sec)mysql&gt; insert into b values(1, 100),(2,200),(4,400);Query OK, 3 rows affected (0.00 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; desc b;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.03 sec)mysql&gt; select * from b;+----+-----+| id | num |+----+-----+| 1 | 100 || 2 | 200 || 4 | 400 |+----+-----+3 rows in set (0.03 sec) &lt;id-1&gt;12345678mysql&gt; explain select * from a, b;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL || 1 | SIMPLE | b | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+2 rows in set (0.05 sec) &lt;id-2&gt;123456789mysql&gt; explain select * from a union select * from b;+------+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+------+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+| 1 | PRIMARY | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL || 2 | UNION | b | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL || NULL | UNION RESULT | &lt;union1,2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+3 rows in set (0.05 sec) &lt;id-3&gt;12345678mysql&gt; explain select id from a where id = (select id from b where num = 100);+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | PRIMARY | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 33.33 | Using where || 2 | SUBQUERY | b | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 33.33 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+2 rows in set (0.06 sec) &lt;id-4&gt;123456789101112131415161718192021222324252627282930313233mysql&gt; select @@version;+-----------+| @@version |+-----------+| 5.6.33 |+-----------+1 row in set (0.05 sec)mysql&gt; explain select num from (select * from a) t where t.id &gt; 1;+----+-------------+------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+------+---------------+------+---------+------+------+-------------+| 1 | PRIMARY | &lt;derived2&gt; | ALL | NULL | NULL | NULL | NULL | 3 | Using where || 2 | DERIVED | a | ALL | NULL | NULL | NULL | NULL | 3 | NULL |+----+-------------+------------+------+---------------+------+---------+------+------+-------------+2 rows in set (0.06 sec)-- 很神奇的是我在5.7版本操作了半天也没出现，内部进行了优化，相同的语句操作如下：mysql&gt; select @@version;+------------+| @@version |+------------+| 5.7.21-log |+------------+1 row in set (0.05 sec)mysql&gt; explain select num from (select * from a) t where t.id &gt; 1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 33.33 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set (0.05 sec) 建表操作(2新建两个带索引的表格 c 和 d，其中表格c带有普通索引，表格d带有主键，这两个表格会参与后面用作展示的例子： 12345678910111213mysql&gt; create table c(id int, num int, key idindex(id));Query OK, 0 rows affected (0.21 sec)mysql&gt; insert into c values(1,103),(2,203),(6,603);Query OK, 3 rows affected (0.05 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; create table d(id int, num int, primary key(id));Query OK, 0 rows affected (0.14 sec)mysql&gt; insert into d values(1,104),(2,204),(6,504);Query OK, 3 rows affected (0.11 sec)Records: 3 Duplicates: 0 Warnings: 0 &lt;id-5&gt;123456789mysql&gt; explain select id from d where id in (select id from a);+----+--------------+-------------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------+-------------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+| 1 | SIMPLE | &lt;subquery2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | 100.00 | Using where || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | &lt;subquery2&gt;.id | 1 | 100.00 | Using index || 2 | MATERIALIZED | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL |+----+--------------+-------------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+3 rows in set (0.05 sec) 建表操作(3为了测试 partitions 字段的取值，创建表格 p，其实就是创建了一个带有分区的表，这个表格会参与后面用作展示的例子： 123456mysql&gt; create table p(id int, num int) partition by range(id)(partition p0 values less than(3), partition p1 values less than(6));Query OK, 0 rows affected (0.02 sec)mysql&gt; insert into p values(1, 111),(2,222),(4,444),(5,555);Query OK, 4 rows affected (0.01 sec)Records: 4 Duplicates: 0 Warnings: 0 &lt;id-6&gt;1234567mysql&gt; explain select * from p where id &lt; 5;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | p | p0,p1 | ALL | NULL | NULL | NULL | NULL | 4 | 33.33 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set (0.05 sec) &lt;id-7&gt;1234567mysql&gt; explain select id from c where id = 1;+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------------+| 1 | SIMPLE | c | NULL | ref | idindex | idindex | 5 | const | 1 | 100.00 | Using index |+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------------+1 row in set (0.06 sec) 建表操作(4为了测试 type 字段的取值，有时需要特定的数据引擎才可以，所以创建了以 MyISAM引擎类型的表格 m，然后进行一些测试： 12mysql&gt; create table m(id int, num int)engine=myisam;Query OK, 0 rows affected (0.02 sec) &lt;id-8&gt;1234567mysql&gt; explain select * from m;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+1 row in set (0.03 sec) &lt;id-9&gt;12345678910mysql&gt; insert into m values(1,1001);Query OK, 1 row affected (0.00 sec)mysql&gt; explain select * from m;+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | m | NULL | system | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+1 row in set (0.04 sec) &lt;id-10&gt;1234567mysql&gt; explain select id from d where id = 1;+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------------+| 1 | SIMPLE | d | NULL | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | Using index |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------------+1 row in set (0.04 sec) &lt;id-11&gt;12345678mysql&gt; explain select a.id from a, d where a.id = d.id;+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+-------------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using where || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | sqltest2.a.id | 1 | 100.00 | Using index |+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+-------------+2 rows in set (0.05 sec) &lt;id-12&gt;1234567mysql&gt; explain select id from c where id = 1 or id is null;+----+-------------+-------+------------+-------------+---------------+---------+---------+-------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------------+---------------+---------+---------+-------+------+----------+--------------------------+| 1 | SIMPLE | c | NULL | ref_or_null | idindex | idindex | 5 | const | 2 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------------+---------------+---------+---------+-------+------+----------+--------------------------+1 row in set (0.04 sec) &lt;id-13&gt;1234567mysql&gt; explain select id from c where id &gt; 1;+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | c | NULL | range | idindex | idindex | 5 | NULL | 2 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+--------------------------+1 row in set (0.04 sec) &lt;id-14&gt;1234567mysql&gt; explain select id from c;+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+| 1 | SIMPLE | c | NULL | index | NULL | idindex | 5 | NULL | 3 | 100.00 | Using index |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+1 row in set (0.03 sec) &lt;id-15&gt;123456789mysql&gt; explain select id from c where id in (select a.id from a, d where a.id = d.id);+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+------------------------------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using where; Start temporary || 1 | SIMPLE | c | NULL | ref | idindex | idindex | 5 | sqltest2.a.id | 1 | 100.00 | Using index || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | sqltest2.a.id | 1 | 100.00 | Using index; End temporary |+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+------------------------------+3 rows in set (0.04 sec) &lt;id-16&gt;1234567mysql&gt; explain select min(id) from c;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Select tables optimized away |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------------------+1 row in set (0.05 sec) &lt;id-17&gt;1234567mysql&gt; explain select id from c order by num;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| 1 | SIMPLE | c | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using filesort |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+1 row in set (0.04 sec) &lt;id-18&gt;1234567mysql&gt; explain select id from d where id = 100;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+1 row in set (0.04 sec) &lt;id-19&gt;1234567mysql&gt; explain select id from d limit 0;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Zero limit |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+1 row in set (0.04 sec) &lt;id-20&gt;1234567mysql&gt; explain select 1 from dual;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+1 row in set (0.04 sec) &lt;id-22&gt;1234567mysql&gt; explain select * from a where 1 = 2;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+1 row in set (0.03 sec) &lt;id-23&gt;1234567mysql&gt; explain select sum(num) from a group by id having 1 =2;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible HAVING |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------+1 row in set (0.04 sec) 总结 关于 EXPLAIN 命令的所有可能取值后面，还有部分例子是空的，完全是由于个人水平有限，等找到所说的取值情况再补充，也欢迎大家提供例子 另外 EXPLAIN 提供的信息中没有关于触发器、存储过程的信息或者评估用户自定义函数对查询的影响情况 所有的可能取值中 possible_keys、rows、filtered中的统计信息基本是估算的，并非精确值，只能用来做优化参考 Extra 列的信息对于尝试优化起到了至关重要的作用，当出现 Using filesort、Using temporary、Using join buffer的时候一般就要考虑采取优化方案了 首先了解这些可能出现的情况，之后我们留这里利用这些说明来进行查询优化了]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>查询</tag>
        <tag>Mysql</tag>
        <tag>EXPLAIN</tag>
        <tag>Extra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gcc编译生成可执行文件的过程中发生了什么]]></title>
    <url>%2Fblog%2F2019%2F04%2F16%2Fgcc%E7%BC%96%E8%AF%91%E7%94%9F%E6%88%90%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[前言一直好奇程序的编译过程到底做了哪些工作，后来学会在Ubuntu上使用gcc编译程序，知道了生成可执行文件需要分为预编译、编译、汇编和链接4个步骤，逐渐了解了其中的细节，但是过一段时间之后总是记不太清楚了，所以总结一下增强记忆，同时方便日后查找使用。 编译方式一步到位使用gcc命令可以一步将main.c源文件编译生成最终的可执行文件main_direct 1gcc main.c –o main_direct 分步执行gcc的编译流程通常认为包含以下四个步骤，实际上就是将上面的命令分成4步执行，这也是gcc命令实际的操作流程，生成的可执行文件main与上面单条命令生成的可执行文件main_direct是一模一样的 预处理，生成预编译文件（.i文件）：gcc –E main.c –o main.i 编译，生成汇编代码（.s文件）：gcc –S main.i –o main.s 汇编，生成目标文件（.o文件）：gcc –c main.s –o main.o 链接，生成可执行文件（executable文件）：gcc main.o –o main 编译流程这里的编译是指将源文件（.c）生成可执行文件（executable）的这个完整的过程，而不是上面提到的四个步骤中的第二步，为了弄清楚编译过程究竟都做了哪些工作，接下来我们可以分步骤来看一下gcc编译.c文件的过程，了解了每一步的内容，也就明白了整个编译流程，先给出源文件 mian.c 的源代码。1234567891011121314151617#include &lt;stdio.h&gt;#define A 100// calc sumint sum(int a, int b)&#123; return a + b;&#125;int main()&#123; int b = 1; int c = sum(A, b); printf("sum = %d\n", c); return 0;&#125; 预处理预处理又叫预编译，是完整编译过程的第一个阶段，在正式的编译阶段之前进行。预处理阶段将根据已放置在文件中的预处理指令来修改源文件的内容，对于C语言来说预处理的可执行程序叫做 cpp，全称为C Pre-Processor（C预处理器），是一个与 C 编译器独立的小程序，预编译器并不理解 C 语言语法，它仅是在程序源文件被编译之前，实现文本替换的功能。简单来说，预处理就是将源代码中的预处理指令根据语义预先处理，并且进行一下清理、标记工作，然后将这些代码输出到一个 .i 文件中等待进一步操作。 一般地，C/C++ 程序的源代码中包含以 # 开头的各种编译指令，被称为预处理指令，其不属于 C/C++ 语言的语法，但在一定意义上可以说预处理扩展了 C/C++。根据ANSI C 定义，预处理指令主要包括：文件包含、宏定义、条件编译和特殊控制等4大类。 预处理阶段主要做以下几个方面的工作： 文件包含：#include 是 C 程序设计中最常用的预处理指令，格式有尖括号 #include &lt;xxx.h&gt; 和双引号 #include &quot;xxx.h&quot; 之分，分别表示从系统目录下查找和优先在当前目录查找，例如常用的 #include &lt;stdio.h&gt; 指令，就表示使用 stdio.h 文件中的全部内容，替换该行指令。 添加行号和文件名标识： 比如在文件main.i中就有类似 # 2 &quot;main.c&quot; 2 的内容，以便于编译时编译器产生调试用的行号信息及用于编译时产生编译错误或警告时能够显示行号。 宏定义展开及处理： 预处理阶段会将使用 #define A 100 定义的常量符号进行等价替换，文中所有的宏定义符号A都会被替换成100，还会将一些内置的宏展开，比如用于显示文件全路径的__FILE__，另外还可以使用 #undef 删除已经存在的宏，比如 #undef A 就是删除之前定义的宏符号A。 条件编译处理: 如 #ifdef，#ifndef，#else，#elif，#endif等，这些条件编译指令的引入使得程序员可以通过定义不同的宏来决定编译程序对哪些代码进行处理，将那些不必要的代码过滤掉，防止文件重复包含等。 清理注释内容： // xxx 和 /*xxx*/ 所产生的的注释内容在预处理阶段都会被删除，因为这些注释对于编写程序的人来说是用来记录和梳理逻辑代码的，但是对编译程序来说几乎没有任何用处，所以会被删除，观察 main.i 文件也会发现之前的注释都被删掉了。 特殊控制处理: 保留编译器需要使用 #pragma 编译器指令，另外还有用于输出指定的错误信息，通常来调试程序的 #error 指令。 查看main.i文件 编译编译过程是整个程序构建的核心部分，也是最复杂的部分之一，其工作就是把预处理完生成的 .i 文件进行一系列的词法分析、语法分析、语义分析以及优化后产生相应的汇编代码文件，也就是 .s 文件，这个过程调用的处理程序一般是 cc 或者 ccl。汇编语言是非常有用的，因为它给不同高级语言的不同编译器提供了可选择的通用的输出语言，比如 C 和 Fortran 编译产生的输出文件都是汇编语言。 词法分析： 主要是使用基于有线状态机的Scanner分析出token，可以通过一个叫做 lex 的可执行程序来完成词法扫描，按照描述好的词法规则将预处理后的源代码分割成一个个记号，同时完成将标识符存放到符号表中，将数字、字符串常量存放到文字表等工作，以备后面的步骤使用。 语法分析： 对有词法分析产生的token采用上下文无关文法进行分析，从而产生语法树，此过程可以通过一个叫做 yacc 的可执行程序完成，它可以根据用户给定的语法规则对输入的记号序列进行解析，从而构建一棵语法树，如果在解析过程中出现了表达式不合法，比如括号不匹配，表达式中缺少操作符、操作数等情况，编译器就会报出语法分析阶段的错误。 语义分析： 此过程由语义分析器完成，编译器 cc 所能分析的语义都是静态语义，是指在编译期间可以确定的语义，通常包括声明和类型的匹配，类型的转换等。比如将一个浮点型的表达式赋值给一个整型的表达式时，语义分析程序会发现这个类型不匹配，编译器将会报错。而动态语义一般指在运行期出现的语义相关问题，比如将0作为除数是一个运行期语义错误。语义分析过程会将所有表达式标注类型，对于需要隐式转换的语法添加转换节点，同时对符号表里的符号类型做相应的更新。 代码优化： 此过程会通过源代码优化器会在源代码级别进行优化，针对于编译期间就可以确定的表达式（例如：100+1）给出确定的值，以达到优化的目的，此外还包括根据机器硬件执行指令的特点对指令进行一些调整使目标代码比较短，执行效率更高等操作。 查看main.s文件 汇编汇编过程是整个程序构建中的第三步，是将编译产生的汇编代码文件转变成可执行的机器指令。相对来说比较简单，每个汇编语句都有相对应的机器指令，只需根据汇编代码语法和机器指令的对照表翻译过来就可以了，最终生成目标文件，也就是 .o 文件，完成此工作的可执行程序通常是 as。目标文件中所存放的也就是与源程序等效的目标的机器语言代码，通常至少包含代码段和数据段两个段，并且还要包含未解决符号表，导出符号表和地址重定向表等3个表。汇编过程会将extern声明的变量置入未解决符号表，将static声明的全局变量不置入未解决符号表，也不置入导出符号表，无法被其他目标文件使用，然后将普通变量及函数置入导出符号表，供其他目标文件使用。 代码段： 包含主要是程序的指令。该段一般是可读和可执行的，但一般却不可写。 数据段： 主要存放程序中要用到的各种全局变量或静态的数据，一般数据段都是可读，可写，可执行的。 未解决符号表： 列出了在本目标文件里有引用但不存在定义的符号及其出现的地址。 导出符号表： 列出了本目标文件里具有定义，并且可以提供给其他目标文件使用的符号及其在出现的地址。 地址重定向表： 列出了本目标文件里所有对自身地址的引用记录。 查看main.o文件 链接链接过程是程序构建过程的最后一步，通常调用可执行程序 ld 来完成，可以简单的理解为将目标文件和库文件打包组装成可执行文件的过程，其主要内容就是把各个模块之间相互引用的部分都处理好，将一个文件中引用的符号同该符号在另外一个文件中的定义连接起来，使得各个模块之间能够正确的衔接，成为一个能够被操作系统装入执行的统一整体。 虽然汇编之后得到的文件已经是机器指令文件，但是依然无法立即执行，其中可能还有尚未解决的问题，比如源代码 main.c 中的 printf 这个函数名就无法解析，需要链接过程与对应的库文件对接，完成的重定位，将函数符号对应的地址替换成正确的地址。前面提到的库文件其实就是一组目标文件的包，它们是一些最常用的代码编译成目标文件后打成的包。比如 printf的头文件是 stdio.h，而它的实现代码是放在动态库 libc.so.6 中的，链接的时候就要引用到这个库文件。 从原理上讲，连接的的工作就是把一些指令对其他符号地址的引用加以修正，主要包括了地址和空间分配、符号决议和重定位等步骤，根据开发人员指定的链接库函数的方式不同，链接过程可分为静态链接和动态链接两种，链接静态的库，需要拷贝到一起，链接动态的库需要登记一下库的信息。 静态链接： 函数的代码将从其所在地静态链接库中被拷贝到最终的可执行程序中。这样该程序在被执行时，代码将被装入到该进程的虚拟地址空间中，静态链接库实际上是一个目标文件的集合，其中的每个文件含有库中的一个或者一组相关函数的代码，最终生成的可执行文件较大。 动态链接： 函数的代码被放到动态链接库或共享对象的某个目标文件中。链接处理时只是在最终的可执行程序中记录下共享对象的名字以及其它少量的登记信息。在这样该程序在被执行时，动态链接库的全部内容将被映射到运行时相应进程的虚地址空间，根据可执行程序中记录的信息找到相应的函数代码。这种连接方法能节约一定的内存，也可以减小生成的可执行文件体积。 ​查看main可执行文件 总结 gcc编译器的工作过程：源文件 --&gt; 预处理 --&gt; 编译 --&gt; 汇编 --&gt; 链接 --&gt; 可执行文件 gcc编译过程文件变化：main.c –&gt; main.i –&gt; mian.s –&gt; main.o –&gt; main 通过上面分阶段的解释编译过程，我们也明白了gcc其实只是一个后台程序的包装，它会根据阶段要求来调用 cpp、cc、as、ld 等命令 源代码整个编译过程产生的中间文件及最终结果可以通过传送门—— gcc编译项目 来获得，其中还有gcc和g++分别调用的对比，查看生成的文件可以发现，同样的源代码使用gcc和g++生成的文件是不一样的，总的来说使用g++编译生成的可执行文件要大一些。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>gcc</tag>
        <tag>预处理</tag>
        <tag>编译</tag>
        <tag>汇编</tag>
        <tag>链接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++版本更迭历程]]></title>
    <url>%2Fblog%2F2019%2F04%2F09%2FC-C-%E7%89%88%E6%9C%AC%E6%9B%B4%E8%BF%AD%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言使用 C/C++ 实现功能的时候经常需要上网搜索一些解决方案，但是当你把代码粘贴到自己项目中时偶尔会出现编译失败的问题，其中一个原因就是新加的代码所使用的特性在当前的编译环境中并不支持，就好像不久前我们还在使用VS2003写着C++98标准的代码（2015年），虽然对C++11的特性垂涎已久，但是无奈在项目中就是无法使用，只能是遥望着它发飞快地发展出了C++14和C++17。 涉及到C/C++版本和标准的最常见的地方就是编译选项了，比如常见的 -std=c++11 就是使用C++11的标准编译，关于 C/C++ 各个版本标准的差异我们可能无法全部记住，但是一些主要的版本更替，还是很有必要了解一下的。 C语言版本更迭 年份 C标准 通用名 别名 标准编译选项 GNU扩展选项 1972 Birth C - - - - 1978 K&amp;R C - - - - 1989-1990 X3.159-1989, ISO/IEC 9899:1990 C89 C90, ANSI C, ISO C -ansi, -std=c90, -std=iso9899:1990 -std=gnu90 1995 ISO/IEC 9899/AMD1:1995 AMD1 C94, C95 -std=iso9899:199409 - 1999 ISO/IEC 9899:1999 C99 - -std=c99, -std=iso9899:1999 -std=gnu99 2011 ISO/IEC 9899:2011 C11 - -std=c11, -std=iso9899:2011 -std=gnu11 2018 ISO/IEC 9899:2018 C18 - -std=c18, -std=iso9899:2018 -std=gnu18 C++版本更迭 年份 C++标准 通用名 别名 标准编译选项 GNU扩展选项 1978 C with Classes - - - - 1998 ISO/IEC 14882:1998 C++98 - -std=c++98 -std=gnu++98 2003 ISO/IEC 14882:2003 C++03 - -std=c++03 -std=gnu++03 2011 ISO/IEC 14882:2011 C++11 C++0x std=c++11, std=c++0x std=gnu++11, std=gnu++0x 2014 ISO/IEC 14882:2014 C++14 C++1y std=c++14, std=c++1y std=gnu++14, std=gnu++1y 2017 ISO/IEC 14882:2017 C++17 C++1z std=c++17, std=c++1z std=gnu++17, std=gnu++1z 2020 to be determined C++20 C++2a -std=c++2a std=gnu++2a 号外C/C++标准 看了C++的发展史才知道，原来从1978年Bjarne Stroustrup就开始了C++雏形的使用，直到20年后的1998年才确定了第一个C++标准 C++11之前被称为C++0x，据说C++0x是C++11的草案，所以有些编译器使用C++11的编译参数是：-std=c++0x，后来使用：-std=c++11，但是据说不完全相同 关于C++20，协程的加入应该是一大惊喜了，值得期待！官方还表示，C++20 应该会是一个像 C++11 那样的大版本 gcc/g++ gcc发展到今天已经不单单可以编译C语言了，还可以编译C++、Java、Object-C等多种其他语言 有一种说法是GCC的全名是GNU Compiler Collection(GUN 编译器集合)，而gcc是GCC中用于编译c语言的编译器 事实上，gcc看起来并不像是一个编译器，而像一个调度器，针对于不同的文件调用不同编程语言的编译器 对于后缀为*.c的文件，gcc把它当作是C语言程序源代码，而g++当作是C++程序源代码 对于后缀为*.cpp的文件，gcc和g++都会当作是C++程序源代码 使用g++编译文件时，g++会自动链接标准库STL，而gcc不会自动链接STL，所以再使用gcc编译C++程序是有时会报错 在用gcc编译C++文件时，为了能够使用STL，需要加参数 –lstdc++ ，但这并不代表 gcc –lstdc++ 和 g++等价 据说g++会调用gcc，对于C++代码，因为gcc命令不能自动和C++程序使用的库联接，所以通常用g++来完成链接 需要注意的是，虽说g++会调用gcc，对于*.c文件来说，编译出来的可执行文件也不一样，因为gcc会当成C语言程序编译，而g++调用的gcc会把它当做C++语言程序来编译，这或许就能解释为什么用g++就可以编译所有C/C++的程序，还要有gcc的存在（就我测试来看，同样的C语言代码，g++编译出来的程序体积要大一些）]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>历史</tag>
        <tag>标准</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql表连接：内连接、外连接、交叉连接、自然连接真的都不一样吗]]></title>
    <url>%2Fblog%2F2019%2F04%2F03%2FMysql%E8%A1%A8%E8%BF%9E%E6%8E%A5%EF%BC%9A%E5%86%85%E8%BF%9E%E6%8E%A5%E3%80%81%E5%A4%96%E8%BF%9E%E6%8E%A5%E3%80%81%E4%BA%A4%E5%8F%89%E8%BF%9E%E6%8E%A5%E3%80%81%E8%87%AA%E7%84%B6%E8%BF%9E%E6%8E%A5%E7%9C%9F%E7%9A%84%E9%83%BD%E4%B8%8D%E4%B8%80%E6%A0%B7%E5%90%97%2F</url>
    <content type="text"><![CDATA[前言提起这几种表连接方式就让人头大，想当初还因为这个面试被刷了，长得挺像，用法挺像，可就是有点不一样，其实的它们的差异不是固定的，要在一个具体的环境下才能进行对比，比如在Mysql环境下, JOIN, INNER JOIN, CROSS JOIN 三者在语法上是等价的，也就是作用相同，但是在标准的SQL下却又存在差异。 选一个自己熟悉的环境对比一下，那就是Mysql数据库的表连接了，测试的多了渐渐的发现了一些规律和神坑，貌似一切表连接都是以内连接为基础，然后再此基础上进行变换可以得到一种新的连接，接下来就采用这种对比的逻辑，看看这些连接类型都有什么区别和联系。 测试环境 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. 创建测试数据 新建第一个测试表格a，包含id和name两列 1create table a(id int, name varchar(64), primary key(id)); 插入测试数据 1234insert into a values(1, 'albert');insert into a values(2, 'bella');insert into a values(3, 'amy');insert into a values(4, 'forier'); 新建第二个测试表格b，包含id和age两列 1create table b(id int, age int, primary key(id)); 插入测试数据 1234insert into b values(1, 18);insert into b values(2, 19);insert into b values(3, 25);insert into b values(5, 70); 分别查看两表中的数据如下 123456789101112131415161718192021mysql&gt; select * from a;+----+--------+| id | name |+----+--------+| 1 | albert || 2 | bella || 3 | amy || 4 | forier |+----+--------+4 rows in set (0.04 sec)mysql&gt; select * from b;+----+-----+| id | age |+----+-----+| 1 | 18 || 2 | 19 || 3 | 25 || 5 | 70 |+----+-----+4 rows in set (0.05 sec) 对比测试这篇对比文章可能和以往你看到的不太一样，对比的基础是内连接，其他的连接基本可以看做是在内连接的基础上加了一些条件和扩展得到的，所以首先我们需要先来看一下内连接。 内连接内连接基础语法是a inner join b，不过其中的inner可以省略，也就是可以写成a join b，如果不添加条件就是a表中的每条记录分别与b表中的每条记录做匹配，形成笛卡尔积，查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445mysql&gt; select * from a inner join b;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 1 | 18 || 3 | amy | 1 | 18 || 4 | forier | 1 | 18 || 1 | albert | 2 | 19 || 2 | bella | 2 | 19 || 3 | amy | 2 | 19 || 4 | forier | 2 | 19 || 1 | albert | 3 | 25 || 2 | bella | 3 | 25 || 3 | amy | 3 | 25 || 4 | forier | 3 | 25 || 1 | albert | 5 | 70 || 2 | bella | 5 | 70 || 3 | amy | 5 | 70 || 4 | forier | 5 | 70 |+----+--------+----+-----+16 rows in set (0.03 sec)mysql&gt; select * from a join b;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 1 | 18 || 3 | amy | 1 | 18 || 4 | forier | 1 | 18 || 1 | albert | 2 | 19 || 2 | bella | 2 | 19 || 3 | amy | 2 | 19 || 4 | forier | 2 | 19 || 1 | albert | 3 | 25 || 2 | bella | 3 | 25 || 3 | amy | 3 | 25 || 4 | forier | 3 | 25 || 1 | albert | 5 | 70 || 2 | bella | 5 | 70 || 3 | amy | 5 | 70 || 4 | forier | 5 | 70 |+----+--------+----+-----+16 rows in set (0.04 sec) 需要注意的是内连接的连接条件是可选择，如果不加就是笛卡尔积，如果想加的话可以选择on子句或者using子句，比如需要得到a表与b表中id一致的数据记录就可以使用如下on子句的写法： 123456789mysql&gt; select * from a join b on a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec) 同时对于上述例子中这个on子句中是被连接的两表的同时存在的字段时，可以使用using子句简化，写成如下查询，需要注意下结果集的变化，记录的条数与on子句相同，但是共有的id列被优化掉了一个，这也是on和using子句的区别，使用时根据需要选择： 123456789mysql&gt; select * from a join b using(id);+----+--------+-----+| id | name | age |+----+--------+-----+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 |+----+--------+-----+3 rows in set (0.04 sec) 交叉连接交叉连接基础语法是a cross join b，在Mysql的语法环境中，内连接与交叉连接完全一致，这一点可以通过下面几条查询与内连接的查询做对比得知： 123456789101112131415161718192021222324252627282930313233343536373839404142mysql&gt; select * from a cross join b;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 1 | 18 || 3 | amy | 1 | 18 || 4 | forier | 1 | 18 || 1 | albert | 2 | 19 || 2 | bella | 2 | 19 || 3 | amy | 2 | 19 || 4 | forier | 2 | 19 || 1 | albert | 3 | 25 || 2 | bella | 3 | 25 || 3 | amy | 3 | 25 || 4 | forier | 3 | 25 || 1 | albert | 5 | 70 || 2 | bella | 5 | 70 || 3 | amy | 5 | 70 || 4 | forier | 5 | 70 |+----+--------+----+-----+16 rows in set (0.04 sec)mysql&gt; select * from a cross join b on a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec)mysql&gt; select * from a cross join b using(id);+----+--------+-----+| id | name | age |+----+--------+-----+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 |+----+--------+-----+3 rows in set (0.04 sec) 外连接在Mysql中外连接分为左外连接和右外连接，但不存在全外连接，这一点与Oracle有些不同，不过可以通过左外连接和右外连接合并出全外连接的结果集，需要注意的是外连接必须添加on子句或者using子句，否则会报语法错误，对于左、有外连接可以分别看一下： 左外连接左外连接基础语法是a left outer join b，其中的outer可以省略，与内连接相比就是在与内连接相同条件下，在内连接的结果集中添加a表在b表中找不到匹配的记录，换句话说就是，结果集中会包含a表中的所有记录，如果b表中有匹配的记录就出现在结果集，否则使用NULL代替，也就是把a表看成根本，不可缺失记录，查询结果如下: 123456789101112131415161718192021mysql&gt; select * from a left outer join b on a.id = b.id;+----+--------+------+------+| id | name | id | age |+----+--------+------+------+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 || 4 | forier | NULL | NULL |+----+--------+------+------+4 rows in set (0.04 sec)mysql&gt; select * from a left join b on a.id = b.id;+----+--------+------+------+| id | name | id | age |+----+--------+------+------+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 || 4 | forier | NULL | NULL |+----+--------+------+------+4 rows in set (0.03 sec) 这个左外连接查询同样可以使用using子句来化简，并且也会将共有的字段省略一个： 12345678910mysql&gt; select * from a left join b using(id);+----+--------+------+| id | name | age |+----+--------+------+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 || 4 | forier | NULL |+----+--------+------+4 rows in set (0.04 sec) 右外连接右外连接基础语法是a right outer join b，其中的outer可以省略，与内连接相比就是在与内连接相同条件下，在内连接的结果集中添加b表在a表中找不到匹配的记录，换句话说就是，结果集中会包含b表中的所有记录，如果a表中有匹配的记录就出现在结果集，否则使用NULL代替，也就是把b表看成根本，不可缺失记录，作用与左外连接恰好相反，查询结果如下: 1234567891011121314151617181920212223242526272829303132mysql&gt; select * from a right outer join b on a.id = b.id;+------+--------+----+-----+| id | name | id | age |+------+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 || NULL | NULL | 5 | 70 |+------+--------+----+-----+4 rows in set (0.03 sec)mysql&gt; select * from a right join b on a.id = b.id;+------+--------+----+-----+| id | name | id | age |+------+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 || NULL | NULL | 5 | 70 |+------+--------+----+-----+4 rows in set (0.04 sec)mysql&gt; select * from a right join b using(id);+----+-----+--------+| id | age | name |+----+-----+--------+| 1 | 18 | albert || 2 | 19 | bella || 3 | 25 | amy || 5 | 70 | NULL |+----+-----+--------+4 rows in set (0.04 sec) 自然连接自然连接从名字来看就是两个表很自然的就连接上了，这要求两个表需要有可以参照的数据，具体到表设计上就是要求两个表必须要有相同的列，需要注意的是自然连接不允许添加连接子句，否则会报语法错误。自然连接分为一般自然连接、左外连接和自然右外连接连接，还是以内连接为标准，看看自然连接有什么不同： 一般自然连接一般自然连接基础语法是a natural join b，它不能加连接条件，使用两个表共有的字段id来“自然”地链接，同时会省略共有的字段，其作用相同于内连接使用using子句来查询，通过下面的对比，你会发现他们的作用是一样的。 12345678910111213141516171819mysql&gt; select * from a natural join b;+----+--------+-----+| id | name | age |+----+--------+-----+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 |+----+--------+-----+3 rows in set (0.03 sec)mysql&gt; select * from a join b using(id);+----+--------+-----+| id | name | age |+----+--------+-----+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 |+----+--------+-----+3 rows in set (0.04 sec) 自然左外连接自然左外连接基础语法是a natural left outer join b，其中的outer可以省略，它也不能加连接条件，使用两个表共有的字段id来“自然”地链接，同时会省略共有的字段，其作用相同于内连接使用using子句同时包含a表中的所有记录，以a表作为根本，包含所有记录，并且显示b表中匹配记录，如没有与a表匹配的记录则以NULL代替，其实就是左外连接省略掉using子句： 1234567891011121314151617181920212223242526272829303132mysql&gt; select * from a natural left outer join b;+----+--------+------+| id | name | age |+----+--------+------+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 || 4 | forier | NULL |+----+--------+------+4 rows in set (0.04 sec)mysql&gt; select * from a natural left join b;+----+--------+------+| id | name | age |+----+--------+------+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 || 4 | forier | NULL |+----+--------+------+4 rows in set (0.03 sec)mysql&gt; select * from a left join b using(id);+----+--------+------+| id | name | age |+----+--------+------+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 || 4 | forier | NULL |+----+--------+------+4 rows in set (0.03 sec) 自然右外连接自然左外连接基础语法是a natural right outer join b，其中的outer可以省略，它也不能加连接条件，其作用与自然左外连接相反，其实就是右外连接省略掉using子句： 1234567891011121314151617181920212223242526272829303132mysql&gt; select * from a natural right outer join b;+----+-----+--------+| id | age | name |+----+-----+--------+| 1 | 18 | albert || 2 | 19 | bella || 3 | 25 | amy || 5 | 70 | NULL |+----+-----+--------+4 rows in set (0.04 sec)mysql&gt; select * from a natural right join b;+----+-----+--------+| id | age | name |+----+-----+--------+| 1 | 18 | albert || 2 | 19 | bella || 3 | 25 | amy || 5 | 70 | NULL |+----+-----+--------+4 rows in set (0.04 sec)mysql&gt; select * from a right join b using(id);+----+-----+--------+| id | age | name |+----+-----+--------+| 1 | 18 | albert || 2 | 19 | bella || 3 | 25 | amy || 5 | 70 | NULL |+----+-----+--------+4 rows in set (0.04 sec) STRAIGHT_JOINSTRAIGHT_JOIN的基础语法是a STRAIGHT_JOIN b，确实没有找到这种连接的中文说法，不过它与内连接几乎一样，只是它总是把左侧a表作为驱动表优先读入，它只能加on子句，无法使用using子句，在Sql优化的过程中常常使用，也就是拒绝了Mysql的语句优化，而使用自己指定的顺序来连接表格，不过使用时需慎重，你得比Mysql聪明才可以！ 123456789mysql&gt; select * from a straight_join b on a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec) 逗号分隔连接表在from之后用逗号分隔的两个表格像极了内连接，只不过用逗号分隔的表不能使用子句连接，只可以用where来做条件筛选，不过作用之后的结果是一致的，可以对比看一下： 12345678910111213141516171819mysql&gt; select * from a join b on a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec)mysql&gt; select * from a, b where a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec) 各种连接对比通过描述可能有些关系还是没理解太清楚，所以整理了下面的表格，对比的更清楚一点，其中[]中的内容在编写sql时可以省略： 连接类型 语法 不加条件 加ON子句 加USING子句 与内连接关系 内连接 a [INNER] JOIN b 两表中任意两条记录分别匹配，形成笛卡尔积 只按照条件匹配，所有列均显示 按照共有的列匹配，去除重复列 X 交叉连接 a [CROSS] JOIN b 两表中任意两条记录分别匹配，形成笛卡尔积 只按照条件匹配，所有列均显示 按照共有的列匹配，去除重复列 语法等价，完全相同 左外连接 a LEFT [OUTER] JOIN b 必须加条件，否则报语法错误 只按照条件匹配，并且包含a表中没有匹配上的记录 按照共有的列匹配，并且包含a表中没有匹配上的记录 额外包含a表中没有匹配上的记录 右外连接 a RIGHT [OUTER] JOIN b 必须加条件，否则报语法错误 只按照条件匹配，并且包含b表中没有匹配上的记录 按照共有的列匹配，并且包含b表中没有匹配上的记录 额外包含b表中没有匹配上的记录 一般自然连接 a NATURAL JOIN b 使用两表中共有的字段匹配 不能使用ON子句 不能使用USING子句 相当于内连接使用USING子句 自然左外连接 a NATURAL LEFT [OUTER] JOIN b 使用两表中共有的字段匹配，并且包含a表中没有匹配上的记录 不能使用ON子句 不能使用USING子句 相当于内连接使用USING子句，并且包含a表中没有匹配上的记录 自然右外连接 a NATURAL RIGHT [OUTER] JOIN b 使用两表中共有的字段匹配，并且包含b表中没有匹配上的记录 不能使用ON子句 不能使用USING子句 相当于内连接使用USING子句，并且包含b表中没有匹配上的记录 STRAIGHT_JOIN a STRAIGHT_JOIN b 两表中任意两条记录分别匹配，形成笛卡尔积 只按照条件匹配，所有列均显示 不能使用USING子句 在内连接基础上确定读表顺序 逗号分隔表 a, b 两表中任意两条记录分别匹配，形成笛卡尔积 不能使用ON子句 不能使用USING子句 不能使用连接子句，只能使用Where筛选 总结 总的来看外连接中的outer是最没有存在感的，凡是它出现的地方都可以省略 黑魔法：a inner join b与a cross join b是等价的，后来我偶然间拼写错误发现a across join b也是可以的，另外a love join b也行，开始还以为发现了bug，后来再理解应该是拼错的单词作了表a的别名，虚惊一场！ 通过上面的表格发现每种连接貌似都和内连接扯上了关系，那就以内连接为基础，通过扩展来记忆也是不错的 如果表格不够清晰，换成思维导图或许会更好一些]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>查询</tag>
        <tag>Mysql</tag>
        <tag>表连接</tag>
        <tag>内连接</tag>
        <tag>外连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IP地址常见分类：A类、B类、C类、D类、E类]]></title>
    <url>%2Fblog%2F2019%2F04%2F03%2FIP%E5%9C%B0%E5%9D%80%E5%B8%B8%E8%A7%81%E5%88%86%E7%B1%BB%EF%BC%9AA%E7%B1%BB%E3%80%81B%E7%B1%BB%E3%80%81C%E7%B1%BB%E3%80%81D%E7%B1%BB%E3%80%81E%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[前言虽然IPv6渐渐出现在了人们的视线之中，但是目前来看IPv4仍然占据着主导地位，在日常的编码过程中两者都会接触到，但实际上两者在使用范围、消息头结构等细节上有诸多不同，具体的那些细节对于应用层来说可能体会不到，所以我们先从两者的表示方式来看看，学会认出哪些是IPv4类型的地址，而哪些是IPv6类型的地址。 IPv4地址表示方法每个IPv4地址占用4个字节，长度为32位，由网络号和主机号部分组成，最常采用点分十进制表示法，格式为 ddd.ddd.ddd.ddd，其中 0 &lt;= ddd &lt;= 255，而每个 d 都是十进制数，可省略前导零，比如常见的192.168.1.1，理论上最多能表示的地址个数为$2^32$。 IPv6地址表示方法每个IPv6地址占用16个字节，长度为128位，占用空间是IPv4地址的4倍，但是这里要注意，IPv4所能表示的地址个数相比于IPv4来说可不是4倍的关系，IPv6理论上最多能表示的地址个数为$2^128$，是IPv4能力的$2^96$倍，换算成10进制也就是大约79228162514264337593543950336倍，在很长一段时间内不用再担心IP地址不够用的问题了。 而IPv6地址由于占用位数较多，所以采用更容易书写和理解的冒分十六进制表示法，格式为xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx，其中每个 x 表示一个十六进制的符号，也就是0-9A-F，比如一个普通IPv6地址23CD:0F01:0005:0789:ABED:EF01:2345:67D9，前导零也是可以省略的，例如前一个地址可以记作23CD:F01:5:789:ABED:EF01:2345:67D9。 在某些情况下，一个IPv6地址内可能包含很长的一段0，这时可以把连续的一段0压缩为::, 但要注意是，地址中::只能出现一次，这样才能保证地址解析的唯一性，比如地址FF20:A:0:0:0:0:0:1AC2，可以写成FF20:A::1AC2，而地址FF20:A:0:0:1:0:0:1AC2中间有两段0，为保证解析的唯一性，只能选择一段0来压缩，比如写成FF20:A::1:0:0:1AC2或者是FF20:A:0:0:1::1AC2。 为了实现IPv4地址和IPv6地址互相通信，在IPv6的环境下，IPv4地址会被扩展成IPv6地址，此时地址的格式常表示为：xxxx:xxxx:xxxx:xxxx:xxxx:FFFF:ddd.ddd.ddd.ddd，前面的96位采用IPv6冒分十六进制表示，而后面32位地址则使用IPv4的点分十进制表示，例如常用的IPv4地址192.168.1.1与表示成IPv6地址就是FFFF:192.168.1.1。 IPv4地址常见分类192.168.1.1这个IP地址随着路由器的普及逐渐为人们所熟知，有些人可能知道这是一个C类地址，究竟什么是C类地址？难道还有A类、B类地址？不是说IP地址是惟一表示一台主机的吗，为什么我家的路由器和邻居家的路由器地址都是192.168.1.1，关于这些问题就涉及到了IPv4地址的常见分类，其实IPv4地址一般被分为A、B、C、D、E五类，其中还包含一些保留地址和局域网地址，关于这些信息为了对比方便，我整理了下面这幅图，有关分类的疑惑可以看图了解一下： 书愤陆游早岁那知世事艰，中原北望气如山。楼船夜雪瓜洲渡，铁马秋风大散关。塞上长城空自许，镜中衰鬓已先斑。出师一表真名世，千载谁堪伯仲间。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>网络</tag>
        <tag>IP</tag>
        <tag>IPv4</tag>
        <tag>Ipv6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql开启、查看慢查询日志]]></title>
    <url>%2Fblog%2F2019%2F03%2F25%2FMysql%E5%BC%80%E5%90%AF%E3%80%81%E6%9F%A5%E7%9C%8B%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[前言想要优化现有的数据库结构或者查询语句，首先要找到需要的优化的地方，不然就会出现费了很大精力优化却不达目的的情况，这就和上学考试一样，想要取得好的成绩，先要分析自己差在哪里，重点学习才会有快速的提升。 关于查询Mysql的瓶颈，或者说查询Mysql出现操作缓慢的问题，我们可以使用Mysql自带的慢查询日志来分析，优化不是改正错误，那种错误在开发过程中叫做BUG，伴随着软件开发工程师的一生，而优化是指在逻辑正确的前提下，让程序运行的更快更稳定，一般来说就是优化比较耗时的操作，提升用户体验。比如一个信息系统，如果输入账号密码后需要10分钟才能登录成功，我想基本上也不会有人使用了。 慢查询日志就是一种特殊的记录，用于统计Mysql操作过程中一些耗时的语句，生成文件或者表格，为优化Mysql操作提供分析数据，从名字也很好理解，慢查询日志就是记录一些查询的比较慢的记录，帮助使用者定位具体的问题，接下来就简单地描述一下慢查询日志是如何开启和查看的。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 开启慢查询日志修改过Mysql配置的小伙伴应该知道，Mysql的一些环境配置可以通过命令行直接修改，也可以修改配置文件后重新启动Mysql服务来生效，我们这次选择修改配置文件的方法，首先找到配置文件my.ini（Mysql5.7Windows版本，Linux版本应该叫my.cnf吧），一般在安装目录所在的ProgramData目录下，比如我的是在 C:/ProgramData/MySQL/MySQL Server 5.7 ，如果找不到可以下载一个叫Everything的软件搜一下（顺便安利一下，真的挺好用）。 用记事本或者其他编辑软件打开，搜索 long_query_time找到配置区域，其中有一些其他的配置，与慢查询无关已经剔除：12345# General and Slow logging.log-output=FILEslow-query-log=1slow_query_log_file="0491NPORIURNUYO-slow.log"long_query_time=0.01 slow-query-log：慢查询日志的开关，1表示开启，0表示关闭。Mysql5.1之前貌似是默认关闭的，而我这个版本在安装完成后自动开启了，如果没有可以将其设置为1，重启服务后会自动开启 long_query_time：这个参数很关键，表示慢查询日志的阈值，花费时间超过这个值的sql语句才会被记录，单位是秒，这一点需要注意，网上有些文章说这个参数是毫秒，不知道是不是版本问题，使用时可以测试一下，经测试可以配置成小数 log-output：表示日志存储的方式。FILE 表示将日志存入文件，默认值是FILE，另外可以取值 TABLE ，表示将日志存入数据库表，当然也可以两者都选择，配成 FILE,TABLE 。需要注意的是将日志记录到数据库表中，需耗费更多的系统资源，建议优先记录到文件 log-slow_query_log_file：当配置将日志记录到文件中时，这个参数可以指定文件名，路径一般在配置文件的同级的Data目录下，比如我的是在C:/ProgramData/MySQL/MySQL Server 5.7/Data，当发现sql运行较慢时可以打开这个文件看一下 具体例子 首先我们将long_query_time设置为0.05，也就是记录查询时间超过50毫秒的sql操作，这是个很随意的值，具体的生产环境根据具体情况配置，重启Mysql服务使其生效 创建一张测试表格 slow_query_test 1create table slow_query_test(id int, num int, money int); 然后创建一个存储过程，用来给数据表填充数据，命名为fill_slow_query_test 123456789CREATE PROCEDURE `fill_slow_query_test`()BEGIN DECLARE i int default 1; WHILE i &lt;= 100000 do insert into slow_query_test values(i, i, i); set i = i + 1; END WHILE;END 调用存储过程插入测试数并查询，得到以下结果 12345678910mysql&gt; call fill_slow_query_test();Query OK, 1 row affected (24.45 sec)mysql&gt; select * from slow_query_test where id = 9990;+------+------+-------+| id | num | money |+------+------+-------+| 9990 | 9990 | 9990 |+------+------+-------+1 row in set (0.09 sec) 通过观察结果可以发现，两次操作的时间都超过了0.05s，所以都应该被记录到慢查询日志日志中，打开日志查看内容 12345678910111213C:\Program Files\MySQL\MySQL Server 5.7\bin\mysqld.exe, Version: 5.7.21-log (MySQL Community Server (GPL)). started with:TCP Port: 3306, Named Pipe: (null)Time Id Command Argument# Time: 2019-03-25T03:14:50.241968Z# User@Host: root[root] @ localhost [::1] Id: 2# Query_time: 24.447215 Lock_time: 0.000364 Rows_sent: 0 Rows_examined: 0SET timestamp=1553483690;call fill_slow_query_test();# Time: 2019-03-25T03:15:27.862107Z# User@Host: root[root] @ localhost [::1] Id: 2# Query_time: 0.050133 Lock_time: 0.000133 Rows_sent: 1 Rows_examined: 100000SET timestamp=1553483727;select * from slow_query_test where id = 9990; 日志中的内容与我们猜想的基本一致，但是还要多一些，首先前3行是Mysql服务启动的记录，接下来的是Mysql慢查询日志的正文，每组都通过3行注释分隔： Time：记录执行操作时的日期和时间，默认没有包含时区信息，是标准的UTC时间 User@Host 记录执行操作的主机和用户，以及Mysql连接id等信息 Query_time 记录了查询消耗的时间，以及其他的一些操作信息接下来未被注释的内容就是真正执行的操作，包含当时的时间戳和具体执行的语句 简单优化针对于上述的例子，我们可以运用上一篇文章《Mysql查询可通过给条件字段添加索引提高查询速度》 提到的方法，简单优化一下： 首先给id字段添加简单索引 123mysql&gt; ALTER TABLE slow_query_test ADD INDEX id_index(id);Query OK, 0 rows affected (0.14 sec)Records: 0 Duplicates: 0 Warnings: 0 然后使用相同的sql语句再次查询 1234567mysql&gt; select * from slow_query_test where id = 9990;+------+------+-------+| id | num | money |+------+------+-------+| 9990 | 9990 | 9990 |+------+------+-------+1 row in set (0.04 sec) 对比前后的查询我们发现，加了索引的表格查询耗时已经小于0.05s，所以该查询不会被记录到慢查询日志中了 慢查询日志格式化处理有时面对慢查询日志文件内众多的数据确实无从下手，这是可以考虑使用mysql自带的mysqldumpslow工具来分析慢查询日志，该工具一般与mysql可执行文件在同一目录，比如我的是在C:/Program Files/MySQL/ySQL Server 5.7/bin&gt;，直接运行发现会报错 12C:\Program Files\MySQL\MySQL Server 5.7\bin&gt;mysqldumpslow'mysqldumpslow' 不是内部或外部命令，也不是可运行的程序或批处理文件。 仔细观察会发现，这个工具不知何时已经从可执行文件变成了一个Perl脚本mysqldumpslow.pl（之前使用linux版本是可执行文件）了，不能直接运行，需要安装Perl运行环境，这里有官方的下载地址，如果下载太慢的话可以下载我的备份文件，版本是一样的。 安装完成之后查看帮助（注意文件路径）12345678910111213141516171819202122232425262728PS C:\Program Files\MySQL\MySQL Server 5.7\bin&gt; perl mysqldumpslow.pl --helpUsage: mysqldumpslow [ OPTS... ] [ LOGS... ]Parse and summarize the MySQL slow query log. Options are --verbose verbose --debug debug --help write this text to standard output -v verbose -d debug -s ORDER what to sort by (al, at, ar, c, l, r, t), 'at' is default al: average lock time ar: average rows sent at: average query time c: count l: lock time r: rows sent t: query time -r reverse the sort order (largest last instead of first) -t NUM just show the top n queries -a don't abstract all numbers to N and strings to 'S' -n NUM abstract numbers with at least n digits within names -g PATTERN grep: only consider stmts that include this string -h HOSTNAME hostname of db server for *-slow.log filename (can be wildcard), default is '*', i.e. match all -i NAME name of server instance (if using mysql.server startup script) -l don't subtract lock time from total time 看完使用方法，我们可以按照查询消耗的时间排序输出前两条日志（注意日志的路径，为方便可以拷贝到mysqldumpslow工具目录）: 12345678PS C:\Program Files\MySQL\MySQL Server 5.7\bin&gt; perl mysqldumpslow.pl -s t -v -t 2 0491NPORIURNUYO-slow.logReading mysql slow query log from 0491NPORIURNUYO-slow.logCount: 1 Time=24.45s (24s) Lock=0.00s (0s) Rows=0.0 (0), root[root]@localhost call fill_slow_query_test()Count: 1 Time=0.05s (0s) Lock=0.00s (0s) Rows=1.0 (1), root[root]@localhost select * from slow_query_test where id = N 可以发现还是很方便的，最耗时的操作已经排到了第一位，在实际的优化过程中，这或许就是我们需要拿来开刀的目标了。 总结 通过修改配置文件my.ini中的slow-query-log、long_query_time来调整慢查询日志的开关和具体阈值 通过mysqldumpshow工具可以格式化慢查询日志，方便定位问题和分析问题]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>慢查询</tag>
        <tag>日志</tag>
        <tag>分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql查询可通过给条件字段添加索引提高查询速度]]></title>
    <url>%2Fblog%2F2019%2F03%2F15%2FMysql%E6%9F%A5%E8%AF%A2%E5%8F%AF%E9%80%9A%E8%BF%87%E7%BB%99%E6%9D%A1%E4%BB%B6%E5%AD%97%E6%AE%B5%E6%B7%BB%E5%8A%A0%E7%B4%A2%E5%BC%95%E6%8F%90%E9%AB%98%E6%9F%A5%E8%AF%A2%E9%80%9F%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[前言当使用sql语句查询表数据时，会发现随着表中记录的增多，查询的速度也会也来越慢，特别是那种日志记录，少则几十万，多则上百万，甚至上千万数据，如果查询一次耗时太长，会严重影响业务逻辑，这时候可以考虑给经常作为条件的字段添加索引了，这样做会大大加快查询速度，这里所说的条件字段，就是指sql语句中放到where条件中用于筛选记录的字段，关于加索引提高查询速度的做法，我们可以做一下试验，对比一下看看是否真的有效。 测试环境 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. 测试过程 首先创建一个不带有索引的数据表 tb_without_index 1create table tb_without_index(id int, num int, money int); 然后创建一个存储过程，用来给无索引数据表填充数据，命名为fill_tb_without_index 123456789CREATE PROCEDURE `fill_tb_without_index`()BEGIN DECLARE i int default 1; WHILE i &lt;= 100000 do insert into tb_without_index values(i, i, i); set i = i + 1; END WHILE;END 接着创建一个带有索引用来做对比的数据表 tb_with_index 1create table tb_with_index(id int, num int, money int, key `id_index`(id)); 同样创建一个给带索引数据表填充数据的存储过程 fill_tb_with_index 123456789CREATE PROCEDURE `fill_tb_with_index`()BEGIN DECLARE i int default 1; WHILE i &lt;= 100000 do insert into tb_with_index values(i, i, i); set i = i + 1; END WHILE;END 分别调用存储过程来填充数据，每个表填充需要20多秒，还是挺费时间的 12345mysql&gt; call fill_tb_without_index();Query OK, 1 row affected (25.48 sec)mysql&gt; call fill_tb_with_index();Query OK, 1 row affected (25.64 sec) 查询对比 对于单条数据的查询对比 123456789101112131415mysql&gt; select * from tb_with_index where id = 67853;+-------+-------+-------+| id | num | money |+-------+-------+-------+| 67853 | 67853 | 67853 |+-------+-------+-------+1 row in set (0.05 sec)mysql&gt; select * from tb_without_index where id = 67853;+-------+-------+-------+| id | num | money |+-------+-------+-------+| 67853 | 67853 | 67853 |+-------+-------+-------+1 row in set (0.08 sec) 对于范围数据的查询对比 123456789101112131415mysql&gt; select count(id) from tb_without_index where id &gt; 87862;+-----------+| count(id) |+-----------+| 12138 |+-----------+1 row in set (0.09 sec)mysql&gt; select count(id) from tb_with_index where id &gt; 87862;+-----------+| count(id) |+-----------+| 12138 |+-----------+1 row in set (0.05 sec) 结果分析 通过上面两种情况的对比，我们可以发现虽然每组对比只差零点零几秒的时间，但是从耗时来看有索引的表格查询比没有索引的表格查询节省了大约40%的时间，由此可见，给待查字段添加上索引，确实可以加快查询速度。 既然加上索引的效率可以提升这么多，那么可不可以把所有字段都加上索引呢？答案是不可以，这一点可以从测试过程的第5步结果来分析，这一步中给表格 tb_without_index 添加10万条数据耗时25.48秒，给表格 tb_with_index 添加10万条数据耗时25.64秒，也就是给有索引的表添加数据时要多花0.16秒的时间，这不是偶然的，可以反复测试，每次的测试结果都是有索引表的数据插入过程更耗时一点。 通过上面的对比和分析，可以知道，虽然添加索引可以加快查找速度，但是会拖慢插入和更新的速度，因为在有索引的数据表上更新和插入需要多花费时间来维护索引，至于两者之间的平衡，就需要使用者自己把握了。 添加索引 像上面提到的那样，可以在建表的时候就定义好索引，查询表结构发现字段id所在行的Key列值为MUL，表示它的值是可以重复的索引，其他两个字段都没有 12345678910create table tb_with_index(id int, num int, money int, key `id_index`(id));mysql&gt; desc tb_with_index;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | MUL | NULL | || num | int(11) | YES | | NULL | || money | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+3 rows in set (0.05 sec) 在已有的表格上创建索引，比如可以在列num上创建一个索引，语法：CREATE INDEX index_name ON table_name(column_list) 12345678910111213mysql&gt; CREATE INDEX num_index ON tb_with_index(num);Query OK, 0 rows affected (0.23 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc tb_with_index;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | MUL | NULL | || num | int(11) | YES | MUL | NULL | || money | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+3 rows in set (0.05 sec) 修改表结构添加索引，比如可以给列num添加一个索引，语法：ALTER TABLE table_name ADD INDEX index_name(column_list) 12345678910111213mysql&gt; ALTER TABLE tb_with_index ADD INDEX money_index(money);Query OK, 0 rows affected (0.21 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc tb_with_index;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | MUL | NULL | || num | int(11) | YES | MUL | NULL | || money | int(11) | YES | MUL | NULL | |+-------+---------+------+-----+---------+-------+3 rows in set (0.06 sec) 查看索引可以查看一个表上的所有索引信息，语法为：show index from table_name，查询结果如下 123456789 mysql&gt; show index from tb_with_index; +---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+ ---------------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | +---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+ ---------------+ | tb_with_index | 1 | id_index | 1 | id | A | 98715 | NULL | NULL | YES | BTREE | | | | tb_with_index | 1 | num_index | 1 | num | A | 100035 | NULL | NULL | YES | BTREE | | | | tb_with_index | 1 | money_index | 1 | money | A | 100035 | NULL | NULL | YES | BTREE | | | +---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+ ---------------+3 rows in set (0.06 sec) 总结 给条件字段添加索引可以大大加快数据的查询速度，提高系统的性能。 不要考虑在所有的字段上添加索引，创建索引和维护索引都要耗费时间，这种时间随着数据量的增加而增加。 适合添加索引的字段：总是作为条件查询的字段、常用来做连接的字段、作为主键或者强调唯一的列上。 不适合加索引的字段：块数据类型的字段、取值很少的字段。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>索引</tag>
        <tag>index</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下查找包含指定内容的文件及其所在行数]]></title>
    <url>%2Fblog%2F2019%2F03%2F13%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9F%A5%E6%89%BE%E5%8C%85%E5%90%AB%E6%8C%87%E5%AE%9A%E5%86%85%E5%AE%B9%E7%9A%84%E6%96%87%E4%BB%B6%E5%8F%8A%E5%85%B6%E6%89%80%E5%9C%A8%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言在linux系统下搜索文件一般情况下一个命令就搞定了，之前搜索文件的时候一直使用find，今天排查问题时想查一个函数的调用者在哪个文件中，发现不会写了，搜了一下发现使用grep命令就可以实现，改变了我对grep命令的理解，原来使用grep命令的情况通常是作为结果的过滤函数，比如ps aux | grep gameserver，这次发现他居然还可以直接用来搜索，其实也是过滤的一种。 使用方法这里直接给出命令的写法，简单替换搜索内容就可以使用，也方便自己后续查找使用(例如查找包含stream的文件)：1grep -rn 'stream' . --include='*.cpp' 命令解析上述命令是一种比较常用的写法，就是在当前目录下（一定要注意那个.）查找包含stream的文件，并显示其所在的行，搜索的文件类型是.cpp，其实--include=后面的内容是遵循glob语法的，详细的就不展开了，简单来说就是支持通配符，而查找选项-rn中的r表示递归查找，其中的n表示显示行号，此外还可以使用选项-i表示忽略大小写，下面简单展示一下3个选项的功能： -r：只递归查找不显示行号 1234567891011[albert@localhost#18:17:41#/home/albert/test]$grep -r 'stream' . --include='*.cpp'./testPtr.cpp:#include &lt;iostream&gt;./crab-server/code/main/crab.cpp:#include &lt;iostream&gt;./testConstructor.cpp:#include &lt;iostream&gt;./smartptr/auto_ptr.cpp:#include &lt;iostream&gt;./io.cpp:#include &lt;fstream&gt;./io.cpp: ofstream logfile("./logs/players_distribution.txt", ios::out);./ptr.cpp:#include &lt;iostream&gt;./gdbtest/main.cpp:#include &lt;iostream&gt;./test_t.cpp:#include &lt;iostream&gt;./testshareptr.cpp:#include &lt;iostream&gt; -rn：递归查找并显示行号 1234567891011[albert@localhost#18:17:48#/home/albert/test]$grep -rn 'stream' . --include='*.cpp'./testPtr.cpp:4:#include &lt;iostream&gt;./crab-server/code/main/crab.cpp:3:#include &lt;iostream&gt;./testConstructor.cpp:1:#include &lt;iostream&gt;./smartptr/auto_ptr.cpp:4:#include &lt;iostream&gt;./io.cpp:1:#include &lt;fstream&gt;./io.cpp:9: ofstream logfile("./logs/players_distribution.txt", ios::out);./ptr.cpp:4:#include &lt;iostream&gt;./gdbtest/main.cpp:1:#include &lt;iostream&gt;./test_t.cpp:1:#include &lt;iostream&gt;./testshareptr.cpp:4:#include &lt;iostream&gt; -rni：递归查找显示行号并且忽略大小写 12345678910111213141516171819202122[albert@localhost#18:17:53#/home/albert/test]$grep -rni 'stream' . --include='*.cpp'./testPtr.cpp:4:#include &lt;iostream&gt;./crab-server/code/main/crab.cpp:3:#include &lt;iostream&gt;./testConstructor.cpp:1:#include &lt;iostream&gt;./epoll_cs_demo/testfd.cpp:5: int listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:8: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:11: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:14: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:21: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:25: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/client.cpp:18: int client_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/server.cpp:24: int listen_fd = socket(AF_INET, SOCK_STREAM, 0);./smartptr/auto_ptr.cpp:4:#include &lt;iostream&gt;./io.cpp:1:#include &lt;fstream&gt;./io.cpp:9: ofstream logfile("./logs/players_distribution.txt", ios::out);./network/zgetaddrinfo.cpp:37: hints.ai_socktype = SOCK_STREAM;/* Stream socket */./linux_version/client.cpp:15: int client_fd = socket(AF_INET, SOCK_STREAM, 0);./linux_version/server.cpp:15: int listen_fd = socket(AF_INET, SOCK_STREAM, 0);./ptr.cpp:4:#include &lt;iostream&gt;./gdbtest/main.cpp:1:#include &lt;iostream&gt;./test_t.cpp:1:#include &lt;iostream&gt;./testshareptr.cpp:4:#include &lt;iostream&gt; 总结 查找指定内容的简单命令：grep -rn &#39;stream&#39; . --include=&#39;*.cpp&#39; 这个grep有很多附加的参数，看了文档之后发现了一个点，原来用法：egrep即grep -E，fgrep即rep -F，但是 egrep 和 fgrep现在都不建议使用了，无论是man手册还是--help选项中都提到了这一点]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>find</tag>
        <tag>查找</tag>
        <tag>linux</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为目标打好基础的希尔排序]]></title>
    <url>%2Fblog%2F2019%2F03%2F09%2F%E4%B8%BA%E7%9B%AE%E6%A0%87%E6%89%93%E5%A5%BD%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言刚刚分析过的插入排序通常被叫做简单插入排序或者直接插入排序，而这篇文章刚好以插入排序为基础来说说希尔排序，还是先从名字开始，结果发现完全没有头绪，说实话第一次听说这个排序时还以为是个特别神奇的高端算法，结果了解一番之后发现其实是一个被改造的插入排序，“希尔”居然是发明者的名字，所以从名字来判断算法思想在这里行不通，甚至说快速排序起码说明了这种方法排序快，而希尔排序等于什么都没说。 希尔排序的基础是插入排序，整个排序也是在新元素不断插入到有序序列适当位置的过程中完成的，唯一的不同的就是通过不同的步长将整个序列划分为不同的小序列不断插入，直到步长为1时就退化成了最基本的直接插入排序，但是此时整个序列已经“基本”有序了，需要交换的元素对比一开始直接插入的方法明显减少，从而可以加快排序的速度，因为最后步长为1的一次插入排序与简单插入排序完全相同，所以前面的几趟排序完全可以看做是最后的排序目标“打基础”，让最后一次的排序序列尽可能有序，下面描述一下希尔排序的过程，前提是你已经了解简单插入排序的过程，可以参考文章抓扑克牌风格的插入排序熟悉一下。 希尔排序希尔排序的有一个关键的元素是步长，关于步长的选择有很多种方法，比如只选奇数，选择互为质数等等，其目的就是为了减少重复比较的次数，我们现在只为了解希尔排序的过程，所以先选择一种简单的步长选定方法，以元素个数的一半为基础，每次减少一半直到步长降为1，比如10个元素的步长选择分别为5,2,1，本质思想就是分别以步长5,2,1对整个待排序列进行简单的插入排序，最后就完成了整个序列的排序。 我们用物品重量排序作为例子吧，原来插入排序的例子是将新得到的扑克牌不断插入到有序序列中得到最终排序，这次可以直接先给出物品质量序列的初始排列，假设为99, 34, 54, 65, 11, 1, 5, 12, 89, 42，一共10件物品摆在面前，目标为将物品重量从小到大排序，首先选取步长5开始排序过程： 最开始的排序序列如下:99, 34, 54, 65, 11, 1, 5, 12, 89, 42 以步长为5将整个序列分为5组，分组情况如下：99, _, _, _, _, 1, _, _, _, __, 34, _, _, _, _, 5, _, _, __, _, 54, _, _, _, _, 12, _, __, _, _, 65, _, _, _, _, 89, __, _, _, _, 11, _, _, _, _, 42 将这五组子序列分别使用简单插入排序，得到以下序列：1, _, _, _, _, 99, _, _, _, __, 5, _, _, _, _, 34, _, _, __, _, 12, _, _, _, _, 54, _, __, _, _, 65, _, _, _, _, 89, __, _, _, _, 11, _, _, _, _, 42 这五个子序列组成完整的中间临时序列为：1, 5, 12, 65, 11, 99, 34, 54, 89, 42 然后以步长为2将整个序列划分，得到以下分组情况：1, _, 12, _, 11, _, 34, _, 89, __, 5, _, 65, _, 99, _, 54, _, 42 将这两组子序列使用简单插入排序，得到以下序列：1, _, 11, _, 12, _, 34, _, 89, __, 5, _, 42, _, 54, _, 65, _, 99 将子序列整体来看得到中间临时序列：1, 5, 11, 42, 12, 54, 34, 65, 89, 99 最后再将整个待排序列进行一次简单插入排序，便可得到最终排好的序列，实际上最后一次插入排序只有中间几个元素需要移动了：1, 5, 11, 12, 34, 42, 54, 65, 89, 99 代码实现12345678910111213141516171819202122232425262728293031323334353637/*功能： 交换两个变量参数： element1--被交换的第一个元素的地址 element2--被交换的第二个元素的地址返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void swap_data(int* element1, int* element2)&#123; int middle_value = *element1; *element1 = *element2; *element2 = middle_value;&#125;/*功能： 希尔排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 count--数组元素的个数返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void shell_sort(int array[], int count)&#123; int step = count / 2; while (step &gt; 0) &#123; for (int pos = step; pos &lt; count; ++pos) &#123; for (int insert_index = pos; insert_index &gt;= step; insert_index -= step) &#123; if (array[insert_index] &lt; array[insert_index - step]) swap_data(&amp;array[insert_index], &amp;array[insert_index - step]); &#125; &#125; step /= 2; &#125;&#125; 对比插入排序源代码，找找不同 1234567891011void insert_sort(int array[], int count)&#123; for (int pos = 1; pos &lt; count; ++pos) &#123; for (int insert_index = pos; insert_index &gt; 0; --insert_index) &#123; if (array[insert_index] &lt; array[insert_index - 1]) swap_data(&amp;array[insert_index], &amp;array[insert_index - 1]); &#125; &#125;&#125; 代码分析以上代码就是希尔排序的实现方式了，对比直接插入的源代码发现，如果将希尔排序的初始步长设置成1，那么整个希尔排序的代码就和简单插入排序完全一样了，这也符合我们之前分析的过程，其实希尔排序就是分多次，每次用不同的步长执行简单插入排序。 还有一点就是代码执行的过程与上面示例中的分组插入看起来有些不同，只是因为这个写起来更方便一些，分组只是为了人脑能更快的理解算法的思想，但是代码编写时还要考虑复杂性，将数据拆分成几组然后分别进行插入排序完全可以做到，但是实际上完全没有必要。 比如分成两组排序的那一步，直观上先排索引为0,2,4,6,8上的元素，依次做插入操作，然后排索引为1,3,5,7,9上的元素，在依次做插入操作，但是在实现的代码中就做了变通，反正都要做插入操作，并且步长都是2，所以可以直接对索引是0,1,2,3,4,5,6,7,8,9上的元素做插入排序，只要注意步长是2，就不会影响到其他组（实际上并不存在）的元素了，整个过程顺着代码，一步步执行就明白了。 运行测试希尔排序–源码 如果你想运行测试一下，可以复制或者下载源代码，在本机运行测试一下，当然也可以选择在线C++编译器，把源代码复制到网页中运行查看结果，建议不明白的可以在本地环境单步调试一下，这样有助于理解算法思路。]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
        <tag>希尔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下运行程序常用的nohup和&的区别]]></title>
    <url>%2Fblog%2F2019%2F02%2F25%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F%E5%B8%B8%E7%94%A8%E7%9A%84nohup%E5%92%8C-%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言复杂问题简单记，先了解一下概念，对于一般的小程序而言这两种启动方法应该用不上，如果程序瞬间的就结束了，是否挂起与是否后台也就没有了意义，所以标题中提到的方式常用来启动需要一直运行的程序，比如游戏服务器。 假如我们直接通过命令行./game_server运行一个简单的游戏服务器，那么会发现这个运行程序霸占了整个命令窗口，此时，我们无法再运行其他的程序，所有的输入都变成了game_server的输入，而命令终端此时也只能输出game_server程序的输出信息了。 接着再来了解两个信号，针对于霸占了命令终端的game_server我们可以采用以下方式将其终止掉，使用Ctrl+C组合键，实际上是给程序发送了SIGINT信号，可以以直接关掉命令终端，这个进程也会死掉，实际上是给程序发送了SIGHUP信号，而标题中的所说的两种方式就是针对于这两种信号的。 两种方式的区别 nohupnohup是no hang up的缩写，就是不挂断的意思，忽略SIGHUP信号，在关闭命令终端后程序依旧运行 &amp;&amp;是只后台运行，即忽略SIGINT信号，也就是按Ctrl+C不会终止程序，但是关闭命令行终端程序终止 总结所以要想程序忽略SIGINT和SIGHUP两种信号需要两种表示方法一同使用，总结如下 命令 忽略信号 按Ctrl+C结果 关闭终端 标准输入 输出 ./game 无 程序终止 程序终止 只能给game输入 终端输出 nohup ./game SIGHUP 程序终止 依旧运行 输入被忽略 输出到nohup.out文件 ./game &amp; SIGINT 依旧运行 程序终止 输入正常，终端可用 无输出 nohup ./game &amp; SIGINT、SIGHUP 依旧运行 依旧运行 输入正常，终端可用 输出到nohup.out文件]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nohup</tag>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下服务器程序的查看与gdb调试]]></title>
    <url>%2Fblog%2F2019%2F01%2F11%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9F%A5%E7%9C%8B%E4%B8%8Egdb%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[前言这一篇主要是记录下调试服务器程序常用的命令，内容很简单，但是长时间不用很容易记混，因为游戏服务器也不是天天宕机，所以当有一天突然挂掉需要调试的时候，如果记不清调试命令很容易耽误时间，有好几次我就把gdb gameserver core记成了gdb core gameserver，所以干脆把这些内容统计到一起，查询的时候也方便。 查询程序的运行情况 ps aux命令是常用来查询程序进程运行情况的，基本上不会漏掉，但是显示的无关程序太多，看着不方便所以常配合grep过滤 ps aux | grep gameserver可以显示指定过滤内容的程序，但是这种显示方式没有标题，对于不熟悉的人来说看不太明白，就像下面这样 123$ps aux | grep initroot 1 0.0 0.0 19232 976 ? Ss 2018 0:01 /sbin/init510 2042 0.0 0.0 105492 932 pts/2 S+ 10:04 0:00 grep init 所以这里给出ps aux默认的输出格式：USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND其中STAT: 该行程的状态，linux的进程常见状态：D 不可中断 uninterruptible sleep (usually IO)R 运行 runnable (on run queue)S 中断 sleepingT 停止 traced or stoppedZ 僵死 a defunct or zombie process注: 其它状态还包括W(无驻留页), &lt;(高优先级进程), N(低优先级进程), L(内存锁页). ps -eo pid,lstart,etime,command | grep gameserver有时需要查询特定进程的指定信息，比如运行时间，那么可以通过-o选项来指定，显示信息很明确 1215499 Thu Jan 10 23:22:53 2019 11:24:12 ./gameserver -d18097 Fri Jan 11 10:47:04 2019 00:01 grep gameserver 杀死指定进程 killall -10 gameserver: 按照进程名杀死进程，-10为自定义杀死信号 kill -10 gameserver_pid: 按照进程ID杀死进程，-10为自定义杀死信号 kill -9 gameserver_pid: -9为强制杀死进程的信号，无法被捕捉 kill -6 gameserver_pid: -6可以杀死进程并产生core文件 gdb调试通常要想使用gdb调试需要在编译程序时加上-g选项，之后才能用gdb进行调试：gcc -g main.c -o gameserver，如果想在程序崩溃时产生core文件，还需要设置系统命令ulimit -c unlimited才可以，调试程序又分为直接启动调试、调试core文件和附加到正在运行的进程调试，每种方式的参数略有不同： 直接启动调试，gdb gameserver这种方式相当于直接通过gdb启动了程序，并开启了调试模式，所以是拉取了新的进程 调试core文件，gdb gameserver core.xxx这种方式相当于展示程序崩溃前的堆栈情况，并进行调试，所以也算是拉取了新的进程 附加进程调试，gdb attach gameserver_pid/gdb gameserver gameserver_pid这种方式是将gdb调试工具附加到程序运行的当前进程上，并没有拉取新的进程，操作上也可以先敲gdb回车，然后再attach gameserver_pid，不过这种情况对于其他用户启动的程序，通常会提示“ptrace: 不允许的操作.”，所以需要使用sudo运行gdb gdb常用命令以下命令为调试linux程序常用的gdb命令，都是在调试服务器程序core文件过程中不断积累的，还有一些高级命令一般很少用到，掌握下面这些基本上就可以应付很多场景了，其中打印信息的命令print在打印map、vector等显示不友好，可以参考gdb调试脚本中的内容。 run/r：重新开始运行文件 break/b: 设置断点（I. b filename:linenum, II. b functioname, 条件断点: b position if condition） info/i: 查看信息（I. i b:查看断点信息，i locals: 查看当前帧局部变量值，i threads: 查看线程） delete/d: 删除断点（delete 3：删除通过info breakpoints查到的第3个断点，其实还可以删除别的） list/l: 查看原代码（list -n：显示第n行前后的源代码。list 函数名：查看具体函数） next/n: 逐过程调试（类似于VS的F10） step/s: 逐语句调试（类似于VS的F11） frame/f：切换函数的栈帧（可以调试输出指定函数内的情况） print/p：打印值及地址（用来显示变量值） thread/t: 切换线程（调试指定线程，用来处理多线程程序） continue/c：继续运行（常用调试完断点之后） backtrace/bt：显示堆栈（可以查看函数的调用的栈帧和层级关系） source: 加载脚本（可以在调试过程中使用脚本完成复杂逻辑调试）]]></content>
      <categories>
        <category>gdb</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>ps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抓扑克牌风格的插入排序]]></title>
    <url>%2Fblog%2F2018%2F12%2F04%2F%E6%8A%93%E6%89%91%E5%85%8B%E7%89%8C%E9%A3%8E%E6%A0%BC%E7%9A%84%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言上次聊到了快速排序，我们说到快排这个名字是非常抽象的，究竟什么是快排，从名字上我们无从得知，或许叫二分排序都比快速排序要形象的多，可是这又和归并排序重复了，所以我们还是不要在意快排的名字了，接下来看一下今天的插入排序，这里指的是简单的插入排序。 插入排序相比于快速排序要形象很多，整个排序过程就是在不断的插入操作中完成的，如果你打过扑克基本上很容易理解这种排序方法，排序的过程几乎与抓扑克牌的过程一模一样，假设三个人斗地主，每人一张牌依次抓取，其实一旦开始抓一张牌，那么牌堆里哪些牌是你的就已经确定了，只不过是隔两张之后的那张就是你的，所有这些归属于你的牌在牌堆里的顺序就是这些牌的初始顺序，而你抓牌摆牌的过程就是给这些牌从小到大（当然可以从大到小）排序的过程。 插入排序整个排序过程可以使用抓牌来模拟，抓第一张牌的时候无所谓顺序，放在手里就好，抓第二张牌的时候和第一张比较，按从小到大排好顺序，抓第三张牌的时候，和前面两张比较，“插入”适当的位置，后面的牌依次类推插入正确位置，最后手里的牌也就排好了顺序，还有一点需要注意，抓牌时可以真的将一张牌插入到另外两张牌之间的（实际上也是占用了原来牌的位置），但是在内存中，比如连续下标的一个数组中，要想在元素2和元素3中间插入一个数字是做不到的，如果确实要放到这两个数中间，那就需要将元素3往后移动，给需要插入的这个数字腾出一个地方，元素3后面如果也有其他元素呢？那就也需要向后移动，一直到后面没有需要移动的元素为止。 想象一下完整的抓牌过程，其中的关键点就在于拿到一张新牌（元素）后，和之前有序的手牌进行比较，找到合适的插入位置，依次移动手牌位置（为了仿照内存中移动，我们把牌向后移动，也就是从后向前比较找插入位置），为新来的牌腾出一个位置，把新抓到的牌放入空位，一直到完成最后一张牌的插入，我们也就同时完成了手牌的排序。其中的关键词有之前有序、移动、插入。 接下来可以举个例子操作一下，假设我开了天眼，可以看到牌堆里所有的牌，那么确定了抓牌顺序之后，我也就知道我会抓到哪些牌了，他们分别是6, 2, 7, 3, 8, 9，下面来模拟一下这个牌堆中的牌到了我的手里时候怎么就有序了，还有一个情况就是我用右手摸牌，左手拿牌，但是左手比较小，只能放的下6张牌，这时候可以看看实际的抓牌流程了。 起初情况是左手没有牌，右手抓了一张6:L=_, _, _, _, _, _，R=6 这时候没有什么犹豫的，直接放到左手第一个位置就好了：L=6, _, _, _, _, _ ，R=_ 然后又抓到一张2，移动左手的牌，拿2和左手有序的牌进行比较，这是左手就1张6，将其向后移动得到：L=_, 6, _, _, _, _ ，R=2 接下来需要把右手的2放到左手腾出的位置即可：L=2, 6, _, _, _, _ ，R=_ 紧接着又抓到一张7，发现放到后面就可以，不用移动元素了：L=2, 6, 7, _, _, _ ，R=_ 然后又抓到一张3，其实找插入位置还有另一种形式，就是放到最后，然后不断的换到合适的位置，用这张3来试一下：L=2, 6, 7, _, _, _ ，R=3 先放到左手最后：L=2, 6, 7, 3, _, _ ，R=_ 然后和前面比3大的换一下位置：L=2, 6, 3, 7, _, _ ，R=_ 再换一次找到了真正插入的位置：L=2, 3, 6, 7, _, _ ，R=_ 后面的8，9两张都不需要交换位置，直接放到最后就得到了最终的结果：L=2, 3, 6, 7, 8, 9 ，R=_ 代码实现1234567891011121314151617181920212223242526272829303132/*功能： 交换两个变量参数： element1--被交换的第一个元素的地址 element2--被交换的第二个元素的地址返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void swap_data(int* element1, int* element2)&#123; int middle_value = *element1; *element1 = *element2; *element2 = middle_value;&#125;/*功能： 插入排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 count--数组元素的个数返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void insert_sort(int array[], int count)&#123; for (int pos = 1; pos &lt; count; ++pos) &#123; for (int insert_index = pos; insert_index &gt; 0; --insert_index) &#123; if (array[insert_index] &lt; array[insert_index - 1]) swap_data(&amp;array[insert_index], &amp;array[insert_index - 1]); &#125; &#125;&#125; 代码分析以上代码就是模拟的抓牌过程，新加入的牌放到手牌最后，然后不断的和前面的手牌交换位置，“插入”到有序的手牌序列中，最后得到整体有序，配合前面具体的例子，可以把具体的那些数字带入到这段代码中，头脑中或者在纸上“运行”一下，你就会了解插入排序的原理了。 运行测试快速排序–源码 如果你想运行测试一下，可以复制或者下载源代码，在本机运行测试一下，当然也可以选择在线C++编译器，把源代码复制到网页中运行查看结果，建议不明白的可以在本地环境单步调试一下，这样有助于理解算法思路。]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
        <tag>插入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中使用select into语句给变量赋值没有匹配记录时的结果]]></title>
    <url>%2Fblog%2F2018%2F11%2F17%2FMysql%E4%B8%AD%E4%BD%BF%E7%94%A8select-into%E8%AF%AD%E5%8F%A5%E7%BB%99%E5%8F%98%E9%87%8F%E8%B5%8B%E5%80%BC%E6%B2%A1%E6%9C%89%E5%8C%B9%E9%85%8D%E8%AE%B0%E5%BD%95%E6%97%B6%E7%9A%84%E7%BB%93%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[前言对select into语句感兴趣是因为看了项目中的一个存储过程引起的，在程序运行之前看了存储过程的逻辑，本以为没有数据时会报错，结果程序却正常运行，这说明我对select into语句理解的问题，同时也暴露了一个知识盲点，所以写了个小例子测试一下，并把测试的过程记录方便日后查找。 创建测试表格为了更清楚的表明问题，我们创建的表格尽可能的简单，同时为了测试空值的情况，数据列我们不设置默认值，表格命名为’intotest’，创建语句如下： 12345CREATE TABLE `intotest` ( `id` int(4) NOT NULL AUTO_INCREMENT, `number` int(4), PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=binary ROW_FORMAT=DYNAMIC; 插入测试数据表格建立完成可以使用可视化工具或者insert语句插入测试数据，插入测试数据后查询结果如下： 123456789mysql&gt; select * from intotest;+----+--------+| id | number |+----+--------+| 1 | 1 || 2 | 2 || 3 | NULL |+----+--------+3 rows in set (0.00 sec) 建立一个存储过程我们建立一个用于测试的存储过程，主要的逻辑就是看看当select into语句找不到匹配记录时，被赋值的变量会怎么样，建立存储过程的代码如下： 1234567CREATE PROCEDURE `select_into_value2`()BEGIN DECLARE _value INT DEFAULT 0; SELECT number FROM intotest WHERE id=1 INTO _value; SELECT _value;END 这个存储过程运行正常，配合刚才我们插入表格的记录可以知道，运行后的结果为1: 123456789mysql&gt; call select_into_value();+--------+| _value |+--------+| 1 |+--------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) 测试过程 当查询结果中不存在符合条件的记录时会怎样，修改存储过程定义，然后查看运行结果： 1234567CREATE PROCEDURE `select_into_value2`()BEGIN DECLARE _value INT DEFAULT 0; SELECT number FROM intotest WHERE id=5 INTO _value; SELECT _value;END 123456789mysql&gt; call select_into_value();+--------+| _value |+--------+| 0 |+--------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) 结果为0，也就是说当查不到匹配结果时，不会执行select into的赋值效果。 当匹配到查询结果但是查询出来的数值为null会怎样，修改存储过程定义，然后查看运行结果： 1234567CREATE PROCEDURE `select_into_value2`()BEGIN DECLARE _value INT DEFAULT 0; SELECT number FROM intotest WHERE id=3 INTO _value; SELECT _value;END 123456789mysql&gt; call select_into_value();+--------+| _value |+--------+| NULL |+--------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) 结果为NULL，也就是说当查到匹配结果时，不管结果时什么都会赋值到指定的变量中（类型不匹配的sql错误除外）。 当连续查询赋值中间出现不匹配会怎样，修改存储过程定义，然后查看运行结果： 12345678CREATE PROCEDURE `select_into_value2`()BEGIN DECLARE _value INT DEFAULT 0; SELECT number FROM intotest WHERE id=2 INTO _value; SELECT number FROM intotest WHERE id=5 INTO _value; SELECT _value;END 123456789mysql&gt; call select_into_value();+--------+| _value |+--------+| 2 |+--------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) 想必明白了前两种情况，这第三种也应该明白了，两条语句顺序执行，找到匹配的就赋值，找不到就放弃操作，结果就保留了上一次成功赋值的结果。 总结 关于select into语句赋值的规则就一句话，找到了符合条件的记录就赋值，找不到就算了。 在找到记录的前提下，如果类型不匹配会导致赋值失败并报错，比如查询到字符串赋值给整型变量。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>查询赋值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua中关于table对象引用传递的注意事项]]></title>
    <url>%2Fblog%2F2018%2F09%2F18%2FLua%E4%B8%AD%E5%85%B3%E4%BA%8Etable%E5%AF%B9%E8%B1%A1%E5%BC%95%E7%94%A8%E4%BC%A0%E9%80%92%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[前言最近写了挺长一段时间的Lua，发现Lua这个语言真的是很随意，产生这种感觉的根本原因应该是它把“函数” 作为了“第一类值”，也就是说函数也可以作为变量的“值”，这使得Lua可以随处定义函数，进而改变逻辑的走向，整个流程任你摆布。 虽说把一个函数来回设置方便了许多，但是同样带来了一些不容易发现的问题，如果搞不清定义域和引用关系，常常会导致程序错误，比如最近用Lua写按钮的触发事件时，使用公有函数创建了对应的闭包，一开始感觉table的引用有问题，写了很多中转的代码，最后发现直接就可以使用，浪费了不少时间，最后仔细分析就是闭包最根本的形式，只不过被业务逻辑给干扰了视线，接下来我们一起看看，table和闭包究竟会发生什么关系！ 代码测试 table作为函数参数时的操作 123456789101112131415print("\nexample 1:");data_table = &#123;a = 1, b = 2, 3, 4, 5, 6&#125;;function filter(data_tb) for k,v in pairs(data_tb) do if v % 2 == 0 then data_tb[k] = nil; end endend-- 过滤掉偶数filter(data_table);for k,v in pairs(data_table) do print(k,v)end 1234example 1:1 33 5a 1 以上为去掉table中的偶数的代码，直接操作参数data_tb就可以对传入的data_table进行改变，这样的逻辑一般不会出错，接着我们看下接下来需求，直接将表中数据清空。 1234567891011print("\nexample 2:");data_table = &#123;a = 1, b = 2, 3, 4, 5, 6&#125;;function destroy(data_tb) data_tb = &#123;&#125;;end-- 销毁整个表destroy(data_table);for k,v in pairs(data_table) do print(k,v)end 1234567example 2:1 32 43 54 6b 2a 1 看到这次的输出可能有些人就感到奇怪了，怎么上个例子改变元素可以，而这里直接给变量data_tb赋值，变成空表怎么不行了？这是因为data_tb是对变量data_table的整体引用，所以可以通过data_tb来改变变量data_table内部的值，但是当执行data_tb = {};代码时表示data_tb不再引用data_table，而去引用{}了，也就是data_tb和data_table脱离了关系，这一点可以类比C++代码： 1234567891011121314151617181920#include &lt;iostream&gt;using namespace std;void change_string(char* pStr)&#123; pStr[0] = '5'; pStr[1] = '0'; pStr = "only test\n";&#125;int main()&#123; char szContent[32] = "help"; change_string(szContent); cout &lt;&lt; szContent &lt;&lt; endl; return 0;&#125; 分析一下这段代码的输出结果，如果你能知道结果为50lp，那说明你的C++水平已经超过了入门级别，理解了这段代码有助于清楚的理解前两段Lua代码。 看一个标准闭包实现的计数器 12345678910111213print("\nexample 3:");function counter() local count = 0; return function() count = count + 1; return count; endendfunc = counter();print(func());print(func());print(func()); 1234example 3:123 这段代码的不同之处就在于变量count，这是一个标准的计数器，也是一个标准的闭包，也就是说Lua支持这样的语法，闭包中可以在定义之后一直引用外部的变量，并且在返回函数的整个使用生命周期内都可以引用这个变量，加入外部修改了这个变量，闭包中引用的值也会改变，换句话来说就是闭包这种引用是引用的变量，而不是仅仅保存了一个值。 lua中常见的table引用 12345print("\nexample 4:");local t1 = &#123;i = 1&#125;;local t2 = t1;t1.i = 666;print(t2.i) 12example 4:666 这个例子类似于前面“过滤掉偶数”的代码，首先定义了表t1，然后定义了变量t2引用了变量t1，实际上这里t2不是定义了变量t1本身，而是引用了t1的值，也就是引用的是{i=1}，这里通过t1.i = 666也可以影响到变量t2，其实这个例子看不出引用的究竟是变量t1还是t1的值，可以接着看下面的例子。 12345print("\nexample 5:");local t1 = &#123;i = 1&#125;;local t2 = t1;t1 = &#123;i = 11&#125;;print(t2.i) 12example 5:1 通过这个例子就很清楚了，前面的部分和上个例子一致，但是后面直接给变量t1赋值时并没有改变t2的值，由此可以看出t1和t2已经“分离”了。 table引用和闭包结合的例子 12345678910111213print("\nexample 6:");local tb = &#123;i= 1&#125;;function outer() return function() local t = tb; print(t.i); endendlocal show = outer();tb = &#123;i = 6&#125;;show(); 12example 6:6 这个例子应该会有猜错结果的人，我自己就是在类似的代码中搞糊涂的，一种想法是函数outer定义的时候变量t的值已经定义了，还有一种就是认为在返回函数show的时候变量t的值会定义，但是这两种想法都是错误的，实际上是调用函数show的时候才给t赋值，这时变量t引用的是拥有最新值的tb，所以t.i的值是6，如果你猜对了这个例子的结果，接下来看看下面的代码。 12345678910111213print("\nexample 7:");local tb = &#123;i= 1&#125;;function outer() local t = tb; return function() print(t.i); endendlocal show = outer();tb = &#123;i = 7&#125;;show(); 12example 7:1 如果清楚了上个例子的运行过程，就应该很容易知道这个例子的结果，其中变量t的值是在调用函数outer时确定的，所以后面的赋值tb = {i = 7};对变量t的值没有影响。 总结 lua中操作变量注意值和引用，其实很多语言都有这种区分。 注意闭包可以访问外部变量的特性，程序中使用起来非常方便。 实际使用过程中往往还夹杂着业务逻辑，要学会挖掘本质问题，这样往往可以看到真正的运行逻辑。 测试源码示例传送门：lua中table引用]]></content>
      <categories>
        <category>Lua</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Lua</tag>
        <tag>table</tag>
        <tag>引用传递</tag>
        <tag>闭包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unique_ptr浅析]]></title>
    <url>%2Fblog%2F2018%2F09%2F12%2Funique-ptr%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言unique_ptr这个指针是C++11标准时被引入标准库的，有一种说法称它是boost::scoped_ptr的一个分身，并且它在C++11的时候“转正”了，但是scoped_ptr还被留在boost库中，看来没有转正的机会了，不过unique_ptr与scoped_ptr确实很像，unique_ptr只比scoped_ptr多了一个移动语义，可以通过std::move()函数来转移内部对象的所有权。 其实在我看来，unique_ptr与auto_ptr是最像的，他设计之初就是为了替代auto_ptr，其实两者基本上没有区别，如果把auto_ptr限制一下，使其不能通过拷贝构造和赋值获得所有权，但是可以通过std::move()函数获得所有权，那基本上就变成了unique_pr，这一点通过下面的函数分析也可以看出，两者的函数基本一致。 unique_pr作为一个模板类，可以直接用它来定义一个智能指针的对象，例如std::unique_pr&lt;Test&gt; pa(new Test);，查看unique_pr的代码时发现，它主要有get、release、reset、operator*、operator-&gt;、operator=、swap、operator bool、get_deleter几个函数，相比于auto_ptr常用函数来说，只多了swap、operator bool、get_deleter这三个函数，基本上没什么变化，不过get_deleter这个函数值的详细解释一下，下面通过一些例子来了解一下unique_pr的具体用法。 使用环境 VS2015 + Windows7（应该是C++11标准） 头文件#include &lt;memory&gt; 命名空间using namespace std; 测试过程首先我们先编写一个测试类，用来测试智能指针各个函数的作用，以及可能出现的问题，测试类的代码如下：123456789101112131415161718class Example&#123;public: Example(int param = 0) &#123; number = param; cout &lt;&lt; "Example: " &lt;&lt; number &lt;&lt; endl; &#125; ~Example() &#123; cout &lt;&lt; "~Example: " &lt;&lt; number &lt;&lt; endl; &#125; void test_print() &#123; cout &lt;&lt; "in test print: number = " &lt;&lt; number &lt;&lt; endl; &#125; void set_number(int num) &#123; number = num; &#125;private: int number;&#125;; 测试函数get、release、reset、operator*、operator-&gt;、swap、operator bool这些函数在解释auto_ptr的时候基本都提到过，swap、operator bool作为两个新的函数在解释shared_ptr的时候也演示过，所以此处就不花过多的篇幅举例了，这里写到一个测试函数中，体会一下用法就好： 123456789101112131415161718192021222324252627282930void test1()&#123; unique_ptr&lt;Example&gt; ptr1(new Example(1)); // Example: 1(输出内容) if (ptr1.get()) // 调用get函数，判断内部指针的有效性 &#123; ptr1.get()-&gt;test_print(); // in test print: number = 1(输出内容) ptr1-&gt;set_number(2); // 调用了operator-&gt; (*ptr1).test_print(); // in test print: number = 2(输出内容) &#125; if (ptr1) // 调用operator bool 检测内部对象的有效性 cout &lt;&lt; "ptr1 is valid\n"; // ptr1 is valid(输出内容) Example *p = ptr1.release(); // 调用release函数，取出内部对象 if (!ptr1) // 调用operator bool 检测内部对象的有效性 cout &lt;&lt; "ptr1 is invalid\n"; // ptr1 is invalid(输出内容) ptr1.reset(p); // 调用reset函数，重新设置内部对象 if (ptr1) // 调用operator bool 检测内部对象的有效性 cout &lt;&lt; "ptr1 is valid\n"; // ptr1 is valid(输出内容) ptr1-&gt;test_print(); // in test print: number = 2(输出内容) unique_ptr&lt;Example&gt; ptr2(new Example(20)); // Example: 20(输出内容) ptr1.swap(ptr2); // 调用swap函数，重新设置内部对象 ptr1-&gt;test_print(); // in test print: number = 20(输出内容) ptr2-&gt;test_print(); // in test print: number = 2(输出内容) ptr1.reset(); // ~Example: 20(输出内容)// 重置内部对象被销毁&#125; // ~Example: 2(输出内容) // 出作用域被析构 测试函数operator=operator=这个函数是unique_ptr与auto_ptr最大的区别，因为在auto_ptr中，这个操作函数往往是导致问题出现的罪魁祸首，赋值之后所有权转移，原智能指针对象无效，这样往往会导致程序崩溃，所以在unique_ptr中operator=被禁止使用了，取而代之的是具有移动语义的std::move()函数，如果unique_ptr的对象直接赋值的话，会在编译期间就提示错误： 12345678910void test2()&#123; //unique_ptr&lt;Example&gt; ptr2 = new Example(2);// 编译错误，不支持原始指针到智能指针的隐式转换 unique_ptr&lt;Example&gt; ptr2(new Example(2)); // Example: 2(输出内容) //unique_ptr&lt;Example&gt; ptr3 = ptr2; // 编译错误，...: 尝试引用已删除的函数 //unique_ptr&lt;Example&gt; ptr4(ptr2); // 编译错误，...: 尝试引用已删除的函数 unique_ptr&lt;Example&gt; ptr5(std::move(ptr2)); // 正常编译，使用move移动语义，符合预期效果 ptr5-&gt;test_print(); // in test print: number = 2(输出内容)&#125; // ~Example: 2(输出内容) // 出作用域被析构 测试unique_ptr作为参数和返回值unique_ptr是可以作为参数和返回值的，不过因为operator=不允许使用，所以在作为参数的时候需要使用函数std::move()，但是作为返回值却不需要，这里留个疑问，最后分析一下： 1234567891011121314151617181920212223242526void test3_inner1(unique_ptr&lt;Example&gt; ptr3_1)&#123; ptr3_1-&gt;test_print(); // in test print: number = 3（输出内容）&#125; // ~Example: 3(输出内容) // 出作用域被析构unique_ptr&lt;Example&gt; test3_inner2()&#123; unique_ptr&lt;Example&gt; ptr3_2(new Example(32));// Example:32（输出内容） ptr3_2-&gt;test_print(); // in test print: number = 32（输出内容） return ptr3_2;&#125;void test3()&#123; unique_ptr&lt;Example&gt; ptr3(new Example(3)); // Example:3（输出内容） ptr3-&gt;test_print(); // in test print: number = 3（输出内容） //test3_inner1(ptr3); // 直接作为参数传递会报编译错误,不存在拷贝构造 test3_inner1(std::move(ptr3)); // 但是可以使用std::move的移动语义来实现 if (!ptr3) cout &lt;&lt; "ptr3 is invalid\n"; // ptr1 is valid(输出内容),移动之后ptr3无效 ptr3 = test3_inner2(); // 由于不允许调用构造或者赋值，此处使用了移动语义move ptr3-&gt;test_print(); // in test print: number = 32（输出内容）&#125; // ~Example: 32（输出内容）,出定义域ptr3释放内部对象 测试unique_ptr类型的指针或者引用作为参数这一点没有什么问题，因为不会发生所有权的转移和引用计数的增加，所有的智能指针，包括auto_ptr在内在这种用法的情况下都不会发生问题： 123456789101112131415161718void test4_inner1(unique_ptr&lt;Example&gt;* ptr4_1)&#123; (*ptr4_1)-&gt;test_print(); // in test print: number = 4（输出内容） &#125; // 指针传递没有析构void test4_inner2(unique_ptr&lt;Example&gt;&amp; ptr4_2)&#123; ptr4_2-&gt;test_print(); // in test print: number = 4（输出内容）&#125; // 引用传递没有析构void test4()&#123; unique_ptr&lt;Example&gt; ptr4(new Example(4)); // Example:4（输出内容） ptr4-&gt;test_print(); // in test print: number = 4（输出内容） test4_inner1(&amp;ptr4); // 取地址作为参数 test4_inner2(ptr4); // 引用作为参数&#125; // ~Example: 4（输出内容）,出定义域ptr4释放内部对象 测试unique_ptr作为容器元素前面分析auto_ptr的时候已经说过，auto_ptr在作为容器元素时，是不具有跨平台性质的，因为在有的平台表现很正常，有的环境下直接编译报错，原因就是使用auto_ptr很容易出错，不是说一定会出错，而是可能出问题，所以个别平台直接在编译期报错，防止后续的错误。而unique_ptr作为容器元素时，表现很统一，没有任何问题，但是我感觉这里就有点牵强，后续再说，注意v[6] = unique_ptr&lt;Example&gt;(new Example(56));这一句，是不是感觉很神奇，居然不报编译错误，我感觉和作为返回值时是相同的处理。 12345678910111213141516171819202122232425262728void test5()&#123; vector&lt;unique_ptr&lt;Example&gt;&gt; v(7); for (int i = 0; i &lt; 6; i++) &#123; v[i] = unique_ptr&lt;Example&gt;(new Example(50 + i)); // 依次输出Example:70,...Example:75 &#125; // 直接赋值，迷之成功，不是不能operator=吗,这里实际上调用的还是std::move类似的移动语义？ v[6] = unique_ptr&lt;Example&gt;(new Example(56));// Example:56（输出内容） // 直接将unique_ptr对象push_back v.push_back(unique_ptr&lt;Example&gt;(new Example(57))); // Example:57（输出内容） // 利用移动语义push_back v.push_back(std::move(unique_ptr&lt;Example&gt;(new Example(58)))); // Example:58（输出内容） // 利用make_unique创建unique_ptr,C++14才支持 v.push_back(make_unique&lt;Example&gt;(59)); // Example:59（输出内容） // 循环调用 for (int i = 0; i &lt; 10; i++) &#123; v[i]-&gt;test_print(); &#125;// 依次输出in test print: number = 50....in test print: number = 59&#125;// 依次输出~Example: 50,~Example: 51...~Example: 59 测试函数get_deleter这个函数还是第一次提到，作用就是获得unique_ptr对象的“删除器”，如果不手动指定就会获得默认的删除器，否则就返回你指定的，举个例子一看就明白了，代码如下： 123456789101112131415161718192021222324252627// a custom deleterclass custom_deleter &#123; int flag;public: custom_deleter(int val) : flag(val) &#123;&#125; template &lt;class T&gt; void operator()(T* p) &#123; std::cout &lt;&lt; "use custom deleter, flag=" &lt;&lt; flag ; delete p; &#125;&#125;;void test6()&#123; custom_deleter dlter(666); unique_ptr&lt;Example, custom_deleter&gt; ptr6(new Example(6), dlter); // Example:6（输出内容） ptr6-&gt;test_print(); // in test print: number = 6（输出内容） unique_ptr&lt;Example, custom_deleter&gt; ptr7(new Example(7), ptr6.get_deleter()); // 调用get_deleter // 重置智能指针，内部对象使用自定义删除器删除 ptr6.reset(); // 输出：use custom deleter, flag = 666~Example: 6 ptr7-&gt;test_print(); // in test print: number = 7（输出内容）&#125; // 输出：use custom deleter, flag = 666~Example: 7 现象分析上面的几个例子都很简单，基本上看一遍就知道怎么用了，但是有一点让人很迷惑，就是operator=的使用，最开始已经说过了，unique_ptr中的operator=已经被禁止使用了，但是例子中有两处很有争议，就是unique_ptr作为函数返回值和直接把unique_ptr赋值给vector元素，一开始我也不是太清楚，后来找资料时发现了一些线索，和大家分享一下: 当函数返回一个对象时，理论上会产生临时变量，那必然是会导致新对象的构造和旧对象的析构，这对效率是有影响的。C++编译针对这种情况允许进行优化，哪怕是构造函数有副作用，这叫做返回值优化（RVO)，返回有名字的对象叫做具名返回值优化(NRVO)，就那RVO来说吧，本来是在返回时要生成临时对象的，现在构造返回对象时直接在接受返回对象的空间中构造了。假设不进行返回值优化，那么上面返回unique_ptr会不会有问题呢？也不会。因为标准允许编译器这么做：1.如果支持move构造，那么调用move构造。2.如果不支持move，那就调用copy构造。3.如果不支持copy，那就报错吧。 很显然，unique_ptr本身是支持move构造的，所以unique_ptr对象可以被函数返回，另外我推测将unique_ptr直接赋值给vector元素也利用了相似的操作，这里不太确定，希望了解的小伙伴能告知一下其中的原因。 说到这里，我们对unique_ptr也有了整体的认识，说unique_ptr是auto_ptr的替代品，可是unique_ptr真的优秀了吗？我看未必，它并非不会再犯错，只是犯错的成本大了一些，如果使用std::move()转移了所有权之后，再直接使用原来的智能指针对象，同样会使得程序崩溃。 其实auto_ptr和unique_ptr给我的感觉就是就好比租房子，租房时有些人喜欢看一下房东的房产证，有的人则无所谓，来个人说是房东他就敢跟人签合同，房屋所有权是通过房产证来转移的，使用auto_ptr就好像两个人可以私下交易，把钱和房产证直接交换，房产证的转移很随便，使用unique_ptr就好比在转移房产的时候需要放鞭炮、然后在全世界广播一下，比较麻烦，并且有可能被租房的人看到，但是本质是一样的，都是拿钱来转移房的所有权，关键还是看租房的人，如果租房先看房产证，即使是房产证的转移很随便（也就是使用auto_ptr），也不会出问题，如果租房根本不看房产证，即使房产证交易通知了世界上所有人（即使用unique_ptr），也会租到没证的房子（程序崩溃）。 所以说unique_ptr并没有消除错误，仅仅是提高了犯错的成本。 总结 对比auto_ptr和unique_ptr后发现，unique_ptr几乎只是将auto_ptr的operator=改为std::move()函数。 现在标准库中只剩下了shared_ptr、weak_ptr和unique_ptr三个智能指针，weak_ptr是为了解决shared_ptr的循环引用问题而存在的，有其特定的使用情况，所以只剩下了shared_ptr和unique_ptr的选择，选择的标准就是看是否需要对原对象共享所有权，如果需要使用shared_ptr，如果不需要是独占所有权的使用unique_ptr。 unique_ptr并没有从根本上消除可能错误，仅仅是提高了犯错的成本，并且给出移动所有权的提示，但是在容器vector元素赋值时依然很隐晦，可能造成auto_ptr相同的错误。 测试源码示例传送门：unique_ptr用法]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>unique_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[weak_ptr浅析]]></title>
    <url>%2Fblog%2F2018%2F09%2F01%2Fweak-ptr%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言weak_ptr这个指针天生一副“小弟”的模样，也是在C++11的时候引入的标准库，它的出现完全是为了弥补它老大shared_ptr天生有缺陷的问题，其实相比于上一代的智能指针auto_ptr来说，新进老大shared_ptr可以说近乎完美，但是通过引用计数实现的它，虽然解决了指针独占的问题，但也引来了引用成环的问题，这种问题靠它自己是没办法解决的，所以在C++11的时候将shared_ptr和weak_ptr一起引入了标准库，用来解决循环引用的问题。 weak_ptr本身也是一个模板类，但是不能直接用它来定义一个智能指针的对象，只能配合shared_ptr来使用，可以将shared_ptr的对象赋值给weak_ptr，并且这样并不会改变引用计数的值。查看weak_ptr的代码时发现，它主要有lock、swap、reset、expired、operator=、use_count几个函数，与shared_ptr相比多了lock、expired函数，但是却少了get函数，甚至连operator* 和 operator-&gt;都没有，可用的函数数量少的可怜，下面通过一些例子来了解一下weak_ptr的具体用法。 使用环境 VS2015 + Windows7（应该是C++11标准） 头文件#include &lt;memory&gt; 命名空间using namespace std; 测试过程 weak_ptr解决shared_ptr循环引用的问题定义两个类，每个类中又包含一个指向对方类型的智能指针作为成员变量，然后创建对象，设置完成后查看引用计数后退出，看一下测试结果： 123456789101112131415161718192021222324252627282930313233343536373839class CB;class CA&#123;public: CA() &#123; cout &lt;&lt; "CA() called! " &lt;&lt; endl; &#125; ~CA() &#123; cout &lt;&lt; "~CA() called! " &lt;&lt; endl; &#125; void set_ptr(shared_ptr&lt;CB&gt;&amp; ptr) &#123; m_ptr_b = ptr; &#125; void b_use_count() &#123; cout &lt;&lt; "b use count : " &lt;&lt; m_ptr_b.use_count() &lt;&lt; endl; &#125; void show() &#123; cout &lt;&lt; "this is class CA!" &lt;&lt; endl; &#125;private: shared_ptr&lt;CB&gt; m_ptr_b;&#125;;class CB&#123;public: CB() &#123; cout &lt;&lt; "CB() called! " &lt;&lt; endl; &#125; ~CB() &#123; cout &lt;&lt; "~CB() called! " &lt;&lt; endl; &#125; void set_ptr(shared_ptr&lt;CA&gt;&amp; ptr) &#123; m_ptr_a = ptr; &#125; void a_use_count() &#123; cout &lt;&lt; "a use count : " &lt;&lt; m_ptr_a.use_count() &lt;&lt; endl; &#125; void show() &#123; cout &lt;&lt; "this is class CB!" &lt;&lt; endl; &#125;private: shared_ptr&lt;CA&gt; m_ptr_a;&#125;;void test_refer_to_each_other()&#123; shared_ptr&lt;CA&gt; ptr_a(new CA()); shared_ptr&lt;CB&gt; ptr_b(new CB()); cout &lt;&lt; "a use count : " &lt;&lt; ptr_a.use_count() &lt;&lt; endl; cout &lt;&lt; "b use count : " &lt;&lt; ptr_b.use_count() &lt;&lt; endl; ptr_a-&gt;set_ptr(ptr_b); ptr_b-&gt;set_ptr(ptr_a); cout &lt;&lt; "a use count : " &lt;&lt; ptr_a.use_count() &lt;&lt; endl; cout &lt;&lt; "b use count : " &lt;&lt; ptr_b.use_count() &lt;&lt; endl;&#125; 测试结果如下： 123456CA() called!CB() called!a use count : 1b use count : 1a use count : 2b use count : 2 通过结果可以看到，最后CA和CB的对象并没有被析构，其中的引用效果如下图所示，起初定义完ptr_a和ptr_b时，只有①③两条引用，然后调用函数set_ptr后又增加了②④两条引用，当test_refer_to_each_other这个函数返回时，对象ptr_a和ptr_b被销毁，也就是①③两条引用会被断开，但是②④两条引用依然存在，每一个的引用计数都不为0，结果就导致其指向的内部对象无法析构，造成内存泄漏。 解决这种状况的办法就是将两个类中的一个成员变量改为weak_ptr对象，因为weak_ptr不会增加引用计数，使得引用形不成环，最后就可以正常的释放内部的对象，不会造成内存泄漏，比如将CB中的成员变量改为weak_ptr对象，代码如下：1234567891011class CB&#123;public: CB() &#123; cout &lt;&lt; "CB() called! " &lt;&lt; endl; &#125; ~CB() &#123; cout &lt;&lt; "~CB() called! " &lt;&lt; endl; &#125; void set_ptr(shared_ptr&lt;CA&gt;&amp; ptr) &#123; m_ptr_a = ptr; &#125; void a_use_count() &#123; cout &lt;&lt; "a use count : " &lt;&lt; m_ptr_a.use_count() &lt;&lt; endl; &#125; void show() &#123; cout &lt;&lt; "this is class CB!" &lt;&lt; endl; &#125;private: weak_ptr&lt;CA&gt; m_ptr_a;&#125;;测试结果如下：12345678CA() called!CB() called!a use count : 1b use count : 1a use count : 1b use count : 2~CA() called!~CB() called!通过这次结果可以看到，CA和CB的对象都被正常的析构了，引用关系如下图所示，流程与上一例子相似，但是不同的是④这条引用是通过weak_ptr建立的，并不会增加引用计数，也就是说CA的对象只有一个引用计数，而CB的对象只有2个引用计数，当test_refer_to_each_other这个函数返回时，对象ptr_a和ptr_b被销毁，也就是①③两条引用会被断开，此时CA对象的引用计数会减为0，对象被销毁，其内部的m_ptr_b成员变量也会被析构，导致CB对象的引用计数会减为0，对象被销毁，进而解决了引用成环的问题。 测试weak_ptr对引用计数的影响其实weak_ptr本身设计的很简单，就是为了辅助shared_ptr的，它本身不能直接定义指向原始指针的对象，只能指向shared_ptr对象，同时也不能将weak_ptr对象直接赋值给shared_ptr类型的变量，最重要的一点是赋值给它不会增加引用计数： 1234567891011121314151617181920212223void test1()&#123; // 编译错误 // error C2665: “std::weak_ptr&lt;CA&gt;::weak_ptr”: 3 个重载中没有一个可以转换所有参数类型 // weak_ptr&lt;CA&gt; ptr_1(new CA()); shared_ptr&lt;CA&gt; ptr_1(new CA()); cout &lt;&lt; "ptr_1 use count : " &lt;&lt; ptr_1.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 1 shared_ptr&lt;CA&gt; ptr_2 = ptr_1; cout &lt;&lt; "ptr_1 use count : " &lt;&lt; ptr_1.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 2 cout &lt;&lt; "ptr_2 use count : " &lt;&lt; ptr_2.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 2 weak_ptr&lt;CA&gt; wk_ptr = ptr_1; cout &lt;&lt; "ptr_1 use count : " &lt;&lt; ptr_1.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 2 cout &lt;&lt; "ptr_2 use count : " &lt;&lt; ptr_2.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 2 // 编译错误 // error C2440 : “初始化”: 无法从“std::weak_ptr&lt;CA&gt;”转换为“std::shared_ptr&lt;CA&gt;” // shared_ptr&lt;CA&gt; ptr_3 = wk_ptr;&#125; 测试weak_ptr常用函数的用法weak_ptr中只有函数lock和expired两个函数比较重要，因为它本身不会增加引用计数，所以它指向的对象可能在它用的时候已经被释放了，所以在用之前需要使用expired函数来检测是否过期，然后使用lock函数来获取其对应的shared_ptr对象，然后进行后续操作： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void test2()&#123; shared_ptr&lt;CA&gt; ptr_a(new CA()); // 输出：CA() called! shared_ptr&lt;CB&gt; ptr_b(new CB()); // 输出：CB() called! cout &lt;&lt; "ptr_a use count : " &lt;&lt; ptr_a.use_count() &lt;&lt; endl; // 输出：ptr_a use count : 1 cout &lt;&lt; "ptr_b use count : " &lt;&lt; ptr_b.use_count() &lt;&lt; endl; // 输出：ptr_b use count : 1 weak_ptr&lt;CA&gt; wk_ptr_a = ptr_a; weak_ptr&lt;CB&gt; wk_ptr_b = ptr_b; if (!wk_ptr_a.expired()) &#123; wk_ptr_a.lock()-&gt;show(); // 输出：this is class CA! &#125; if (!wk_ptr_b.expired()) &#123; wk_ptr_b.lock()-&gt;show(); // 输出：this is class CB! &#125; // 编译错误 // 编译必须作用于相同的指针类型之间 // wk_ptr_a.swap(wk_ptr_b); // 调用交换函数 wk_ptr_b.reset(); // 将wk_ptr_b的指向清空 if (wk_ptr_b.expired()) &#123; cout &lt;&lt; "wk_ptr_b is invalid" &lt;&lt; endl; // 输出：wk_ptr_b is invalid 说明改指针已经无效 &#125; wk_ptr_b = ptr_b; if (!wk_ptr_b.expired()) &#123; wk_ptr_b.lock()-&gt;show(); // 输出：this is class CB! 调用赋值操作后，wk_ptr_b恢复有效 &#125; // 编译错误 // 编译必须作用于相同的指针类型之间 // wk_ptr_b = wk_ptr_a; // 最后输出的引用计数还是1，说明之前使用weak_ptr类型赋值，不会影响引用计数 cout &lt;&lt; "ptr_a use count : " &lt;&lt; ptr_a.use_count() &lt;&lt; endl; // 输出：ptr_a use count : 1 cout &lt;&lt; "ptr_b use count : " &lt;&lt; ptr_b.use_count() &lt;&lt; endl; // 输出：ptr_b use count : 1&#125; 现象分析引用计数的出现，解决了对象独占的问题，但是也带来了循环引用的困扰，使用weak_ptr可以打破这种循环，当你理不清引用关系的时候，不妨采用文中画图的方式来理一理头绪，或许就会有眼前一亮的感觉。 总结 weak_ptr虽然是一个模板类，但是不能用来直接定义指向原始指针的对象。 weak_ptr接受shared_ptr类型的变量赋值，但是反过来是行不通的，需要使用lock函数。 weak_ptr设计之初就是为了服务于shared_ptr的，所以不增加引用计数就是它的核心功能。 由于不知道什么之后weak_ptr所指向的对象就会被析构掉，所以使用之前请先使用expired函数检测一下。 测试源码示例传送门：weak_ptr用法]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>weak_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shared_ptr浅析]]></title>
    <url>%2Fblog%2F2018%2F08%2F15%2Fshared-ptr%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言这个指针近乎完美，原来出现在boost库中，C++11时引入了标准库，解决了auto_ptr对内部对象独占的机制，转而采用引用计数的方式，每增加一次赋值，则引用计数加1，每析构一个智能指针对象，则引用计数减1，当引用计数为1时销毁智能指针对象的同时，也析构内部对象。这种采用引用计数方式避免了对象所有权转移，所以作为函数返回值，函数参数，容器的元素都不会有问题，但是因为引用计数的加入，相应的会带来对引用计数维护的开销。 与auto_ptr一样，shared_ptr本身也是一个模板类，那么一般情况下直接用它来定义一个智能指针的对象，例如std::shared_ptr&lt;Test&gt; pa(new Test);需要注意的是pa虽然叫智能指针，但是它是一个对象，在它的内部保存着一个原始的对象的指针。查看shared_ptr的代码时发现，它主要有get、swap、reset、unique、use_count、operator bool、operator*、operator-&gt;、operator=几个函数，与auto_ptr相比少了release函数，但是多了swap、unique、use_count、operator bool四个函数,下面通过一些例子来了解一下shared_ptr的具体用法。 使用环境 VS2015 + Windows7（应该是C++11标准） 头文件#include &lt;memory&gt; 命名空间using namespace std; 测试过程首先我们先编写一些测试类，用来测试智能指针各个函数的作用，以及可能出现的问题，测试类的代码如下：1234567891011121314151617181920class Example&#123;public: Example(int param = 0) &#123; number = param; cout &lt;&lt; "Example: " &lt;&lt; number &lt;&lt; endl; &#125; ~Example() &#123; cout &lt;&lt; "~Example: " &lt;&lt; number &lt;&lt; endl; &#125; void test_print() &#123; cout &lt;&lt; "in test print: number = " &lt;&lt; number &lt;&lt; endl; &#125; void set_number(int num) &#123; number = num; &#125; int get_number() &#123; return number; &#125;private: int number;&#125;; 测试函数get reset operator* operator-&gt;这几个函数与auto_ptr智能指针的用法一样，可以参考auto_ptr用法，get函数可以获得智能指针包装的原始指针，可以用来判断被包装对象的有效性，也可以用来访问被包装对象，operator*可以直接对智能指针包装的原始指针解引用，获得被包装的对象，operator-&gt;用来取得原始对象的指针，引用成员时与get函数作用相同，reset函数用于重新设置内部对象，若参数为空，则表示取消对内部对象的引用，此时若引用计数大于1则进行减1操作，否则直接析构内部对象。需要注意的是普通的对象指针是无法隐式转换成shared_ptr的，需要利用构造函数实现，示例代码如下： 123456789101112131415161718void test1()&#123; //error C2440: “初始化”: 无法从“Example *”转换为“std::shared_ptr&lt;Example&gt;” //shared_ptr&lt;Example&gt; ptr1 = new Example(1); shared_ptr&lt;Example&gt; ptr1(new Example(1)); // Example: 1（输出内容） if (ptr1.get()) // 调用函数get，获取原始指针，判断有效性 &#123; cout &lt;&lt; "ptr1 is valid" &lt;&lt; endl; // 原始指针有效 &#125; ptr1-&gt;test_print(); // in test print: number = 1（输出内容），调用operator-&gt; ptr1.reset(); // ~Example: 1（输出内容）,调用函数reset，设置为空，释放原内部对象 ptr1.reset(new Example(2)); // Example: 2（输出内容）,重新申请对象并设置 (*ptr1).test_print(); // in test print: number = 1（输出内容），调用operator*&#125; // ~Example: 1（输出内容）,出定义域，释放内部对象 测试函数operator bool用法operator bool函数其实就是用来判断内部对象是否有效的，若内部对象不为空则返回true，否则返回false，大概的实现就是return this-&gt;get() != nullptr;，测试代码如下： 1234567891011void test2()&#123; shared_ptr&lt;Example&gt; ptr2(new Example(2)); // Example: 2（输出内容） if (ptr2) // 调用operator bool cout &lt;&lt; "ptr2 is valid" &lt;&lt; endl; // ptr2 is valid（输出内容），说明ptr2是有效的 ptr2.reset(); // ~Example: 2（输出内容），设置内部对象为空 if (ptr2) // 调用operator bool cout &lt;&lt; "ptr2 is valid" &lt;&lt; endl; // 没有输出，说明ptr2已经无效&#125; 测试函数swap用法从这个名字就可以看出，这个函数用于交换，那么是用来交换什么的呢？实际上是用来交换内部对象的，看下面的例子一试便知，代码运行过后，通过打印可以发现智能指针对象ptr3和ptr4的内部对象进行了交换： 12345678910111213void test3()&#123; shared_ptr&lt;Example&gt; ptr3(new Example(3)); // Example: 3（输出内容） shared_ptr&lt;Example&gt; ptr4(new Example(4)); // Example: 4（输出内容） ptr3-&gt;test_print(); // in test print: number = 3（输出内容） ptr4-&gt;test_print(); // in test print: number = 4（输出内容） ptr3.swap(ptr4); // 调用函数swap ptr3-&gt;test_print(); // in test print: number = 4（输出内容） ptr4-&gt;test_print(); // in test print: number = 3（输出内容）&#125; // ~Example: 3（输出内容）,出定义域，释放内部对象 // ~Example: 4（输出内容）,出定义域，释放内部对象 测试函数unique use_count operator=用法为什么把这几个函数放到一起来说，因为他们是息息相关的，首先函数operator=是用来处理赋值操作的，而赋值操作就会影响引用计数的变化，也就是赋值操作后，use_count函数查询到的引用计数会发生变化，而当use_count返回引用计数是1时，用来表明是否独自引用内部对象的函数unique也会返回true，换句话说unique函数的实现基本就是return this-&gt;use_count() == 1，测试代码如下： 12345678910111213141516171819202122void test4()&#123; shared_ptr&lt;Example&gt; ptr4(new Example(4)); // Example: 4（输出内容） if (ptr4.unique()) &#123; cout &lt;&lt; "ptr4 is unique" &lt;&lt; endl; // ptr4 is unique（输出内容） cout &lt;&lt; "ptr4 use count : " &lt;&lt; ptr4.use_count() &lt;&lt; endl;// ptr4 use count : 1（输出内容） &#125; shared_ptr&lt;Example&gt; ptr5 = ptr4; if (ptr4) cout &lt;&lt; "ptr4 is valid" &lt;&lt; endl;// ptr4 is valid(输出内容）说明赋值之后两个智能指针对象都有效 if (ptr5) cout &lt;&lt; "ptr5 is valid" &lt;&lt; endl;// ptr5 is valid(输出内容）说明赋值之后两个智能指针对象都有效 if (ptr4.unique()) cout &lt;&lt; "ptr4 is unique" &lt;&lt; endl; // 没有输出，说明ptr4不是唯一管理内部对象的智能指针了 cout &lt;&lt; "ptr4 use count : " &lt;&lt; ptr4.use_count() &lt;&lt; endl; // ptr4 use count : 2（输出内容） cout &lt;&lt; "ptr5 use count : " &lt;&lt; ptr5.use_count() &lt;&lt; endl; // ptr4 use count : 2（输出内容）&#125; // ~Example: 4（输出内容）,出定义域，释放内部对象 测试用同一个对象指针生成两个shared_ptr对象与auto_ptr一样，我测试的结果是崩溃，官方标准网站上说是结果未定义，基本上就是说不靠谱，别这样干，仔细想想也能理解，虽说shared_ptr是通过引用计数方式实现，但也不是无所不能，比如这种情况，两个对象都是通过构造生成的，对内部对象的指针p都是“唯一”引用的，也就是两个对象的内部引用计数都是1，当第一个智能指针对象销毁时，会析构内部对象，当第二个智能指针对象销毁时，同样会析构内部对象，这样就造成了崩溃，测试如下： 12345678910void test5()&#123; Example *p = new Example(5); // Example: 5（输出内容） shared_ptr&lt;Example&gt; ptr5(p); shared_ptr&lt;Example&gt; ptr6(p); cout &lt;&lt; "ptr4 use count : " &lt;&lt; ptr5.use_count() &lt;&lt; endl;// ptr4 use count : 1（输出内容） cout &lt;&lt; "ptr5 use count : " &lt;&lt; ptr6.use_count() &lt;&lt; endl;// ptr5 use count : 1（输出内容）&#125; // ~Example: 3（输出内容）,出定义域，ptr5释放内部对象 // ~Example : -572662307（输出内容）,出定义域，ptr6释放内部对象，程序崩溃 测试shared_ptr作为函数参数和返回值因为shared_ptr内部是引用计数，而不是独占所有权，所以在赋值的时候只改变引用计数，不会发生所有权转移，所以这两种用法基本没有问题，发生在auto_ptr上的崩溃惨剧也不会在这里上演，测试代码如下： 12345678910111213141516171819202122232425void test6_inner1(shared_ptr&lt;Example&gt; ptr6_1)&#123; ptr6_1-&gt;test_print(); // in test print: number = 6（输出内容） cout &lt;&lt; "ptr6_1 use count : " &lt;&lt; ptr6_1.use_count() &lt;&lt; endl;// ptr6 use count : 2（输出内容）&#125;shared_ptr&lt;Example&gt; test6_inner2()&#123; shared_ptr&lt;Example&gt; ptr6_2(new Example(62)); // Example:62（输出内容） ptr6_2-&gt;test_print(); // in test print: number = 62（输出内容） cout &lt;&lt; "ptr6_2 use count : " &lt;&lt; ptr6_2.use_count() &lt;&lt; endl;// ptr6_2 use count : 1（输出内容） return ptr6_2;&#125;void test6()&#123; shared_ptr&lt;Example&gt; ptr6(new Example(6)); // Example:6（输出内容） ptr6-&gt;test_print(); // in test print: number = 6（输出内容） cout &lt;&lt; "ptr6 use count : " &lt;&lt; ptr6.use_count() &lt;&lt; endl;// ptr6 use count : 1（输出内容） test6_inner1(ptr6); cout &lt;&lt; "ptr6 use count : " &lt;&lt; ptr6.use_count() &lt;&lt; endl;// ptr6 use count : 1（输出内容） ptr6 = test6_inner2(); // ~Example: 6（输出内容）,ptr6接管新的对象，原来对象被析构 cout &lt;&lt; "ptr6 use count : " &lt;&lt; ptr6.use_count() &lt;&lt; endl;// ptr6 use count : 1（输出内容）&#125; // ~Example: 62（输出内容）,出定义域，ptr6释放内部对象 测试shared_ptr作为容器元素在这里也不存在auto_ptr作为容器元素时的争议，同样是引用计数的机制发挥了作用，使得他满足的容器的要求——其元素对象的拷贝与原对象相同或者等价，所以这里也不会出现问题，同时那些针对于容器的算法在shared_ptr上也可以大显身手，比如下面这个排序的例子： 12345678910111213141516171819202122232425262728// 一般会写成只读引用类型，这里为了说明问题才这样定义bool comp(shared_ptr&lt;Example&gt; a, shared_ptr&lt;Example&gt; b)&#123; return a-&gt;get_number() &gt; b-&gt;get_number();&#125;void test7()&#123; vector&lt;shared_ptr&lt;Example&gt;&gt; v(10); for (int i = 0; i &lt; 10; i++) &#123; v[i] = shared_ptr&lt;Example&gt;(new Example(70+i)); &#125;// 依次输出Example:70,Example:71,Example:72...Example:79 // 循环调用 for (int i = 0; i &lt; 10; i++) &#123; v[i]-&gt;test_print(); &#125;// 依次输出in test print: number = 70....in test print: number = 79 sort(v.begin(), v.end(), comp); // 这可以正常运行，但是使用auto_ptr会死的很难看 // 循环调用 for (int i = 0; i &lt; 10; i++) &#123; v[i]-&gt;test_print(); &#125;// 依次输出in test print: number = 79....in test print: number = 70&#125;// 依次输出~Example: 79,~Example: 78...~Example: 70 测试使用指针或者引用作为参数虽然shared_ptr作为参数、返回值、容器元素貌似没有丝毫问题了，但是有时还是使用shared_ptr对象的指针或者引用比较好，因为这样可以减少对对象的拷贝，毕竟对象的拷贝是需要消耗时间的，用更好的方式为什么不用呢，参考下面的用法，没有任何问题： 12345678910111213141516171819202122void test8_inner1(shared_ptr&lt;Example&gt;* ptr8_1)&#123; (*ptr8_1)-&gt;test_print(); // in test print: number = 8（输出内容） cout &lt;&lt; "ptr8_1 use count : " &lt;&lt; (*ptr8_1).use_count() &lt;&lt; endl;// ptr8_1 use count : 1（输出内容）&#125;void test8_inner2(shared_ptr&lt;Example&gt;&amp; ptr8_2)&#123; ptr8_2-&gt;test_print(); // in test print: number = 8（输出内容） cout &lt;&lt; "ptr8_2 use count : " &lt;&lt; ptr8_2.use_count() &lt;&lt; endl;// ptr8_2 use count : 1（输出内容）&#125;void test8()&#123; shared_ptr&lt;Example&gt; ptr8(new Example(8)); // Example:8（输出内容） ptr8-&gt;test_print(); // in test print: number = 8（输出内容） cout &lt;&lt; "ptr8 use count : " &lt;&lt; ptr8.use_count() &lt;&lt; endl;// ptr8 use count : 1（输出内容) test8_inner1(&amp;ptr8); cout &lt;&lt; "ptr8 use count : " &lt;&lt; ptr8.use_count() &lt;&lt; endl;// ptr8 use count : 1（输出内容) test8_inner2(ptr8); cout &lt;&lt; "ptr8 use count : " &lt;&lt; ptr8.use_count() &lt;&lt; endl;// ptr8 use count : 1（输出内容)&#125; // ~Example: 8（输出内容）,出定义域，ptr8释放内部对像 现象分析shared_ptr与auto_ptr相比要优秀的多，这得益于其内部引用计数的实现，正是这种非独占所有权的方式，使其摆脱了auto_ptr的种种限制，并将其踢出了C++标准（auto_ptr在C++17中被移除），但是shared_ptr也不是完美无缺的，引用计数不能解决所的问题，并且可能会带来一些问题，比如“循环引用问题”，这个得靠后面我们即将说到的weak_ptr来解决，所以说没有什么结构是完美的，选择合适的就是最好的，综合前面多个测试的例子，可以得到一些经验。 总结 shared_ptr作为目前最优秀的指针，取代auto_ptr是必然的，所以能使用shared_ptr的地方还是尽量使用shared_ptr。 不要使用同一个原始对象的指针生成多个shared_ptr对象，这样使用会导致未定义的行为，比如test5这个函数就导致了崩溃和错误的输出。 shared_ptr不是万能的，如果不加思考的把原始指针都替换成shared_ptr，虽然大部分能防止内存泄露，但是还会造成其他的问题，比如循环引用，这种情况需要使用weak_ptr来解决问题，如果不解决就会造成另一种形式的内存泄漏。 不要使用get返回的指针来初始化一个shared_ptr对象，这种的做法的本质与第2点一样，会造成未定义的行为。 尽量不要保存get函数返回的指针，因为你不知道什么时候这个指针对应的对象就被析构掉了，所以请“随用随取”。 测试源码示例传送门：shared_ptr用法]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>shared_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[auto_ptr浅析]]></title>
    <url>%2Fblog%2F2018%2F08%2F08%2Fauto-ptr%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言之前简单的列举了一下各种智能指针的特点，其中提到了这个历经沧桑的指针，C++98中引入，C++11中弃用，C++17中被移除，弃用的原因主要是使用不当容易造成内存崩溃，不能够作为函数的返回值和函数的参数，也不能在容器中保存auto_ptr。其实说这个指针“不能够作为函数的返回值和函数的参数，也不能在容器中保存”，这个结论过于武断了，经过一系列的测试后发现，原来真正的结论不应该说“不能”，准确来说是“不建议”。 auto_ptr本身是一个模板类，那么一般情况下直接用它来定义一个智能指针的对象，例如std::auto_ptr&lt;Test&gt; pa(new Test);需要注意的是pa虽然叫智能指针，但是它是一个对象，在它的内部保存着一个原始的对象的指针，其原理就是 RAII(Resource Acquisition Is Initialization) ，在智能指针构造的时候获取资源，在析构的时候释放资源，并进行相关指针操作的重载，使其使用起来就像普通的指针一样方便。 查看auto_ptr的代码时发现，它主要有get、release、reset、operator*、operator-&gt;、operator=几个函数，下面通过一些例子来了解一下auto_ptr的具体用法。 使用环境 VS2015 + Windows7（应该是C++11标准） 头文件#include &lt;memory&gt; 命名空间using namespace std; 测试过程首先我们先编写一些测试类，用来测试智能指针各个函数的作用，以及可能出现的问题，测试类的代码如下：123456789101112131415161718class Example&#123;public: Example(int param = 0) &#123; number = param; cout &lt;&lt; "Example: " &lt;&lt; number &lt;&lt; endl; &#125; ~Example() &#123; cout &lt;&lt; "~Example: " &lt;&lt; number &lt;&lt; endl; &#125; void test_print() &#123; cout &lt;&lt; "in test print: number = " &lt;&lt; number &lt;&lt; endl; &#125; void set_number(int num) &#123; number = num; &#125;private: int number;&#125;; 测试函数get、operator*、operator-&gt;get函数可以获得智能指针包装的原始指针，可以用来判断被包装对象的有效性，也可以用来访问被包装对象，operator*可以直接对智能指针包装的原始指针解引用，获得被包装的对象，operator-&gt;用来取得原始对象的指针，引用成员时与get函数作用相同，示例代码如下： 1234567891011void test1()&#123; auto_ptr&lt;Example&gt; ptr1(new Example(6)); // Example: 6(输出内容) if (ptr1.get()) // 判断内部指针的有效性 &#123; // 以下为访问成员的3种方法 ptr1.get()-&gt;test_print(); // in test print: number = 6(输出内容) ptr1-&gt;set_number(8); (*ptr1).test_print(); // in test print: number = 8(输出内容) &#125;&#125; // ~Example: 8(输出内容) // 出作用域被析构 测试函数release错误用法release函数是很容易让人误解的函数，一般看到release会想起释放、回收的含义，函数的作用通常就是回收掉申请的资源，但是这里就要注意了，auto_ptr对象的release函数只有释放的意思，指的是释放指针的所有权，说简单点就是auto_ptr的对象与原始的指针脱离关系，但是并不回收原始指针申请的内存，如果不主动释放就会造成内存泄露，就像下面这样： 12345678910111213void test2()&#123; //auto_ptr&lt;Example&gt; ptr2 = new Example(6); // 编译错误，不支持不同指针到智能指针的隐式转换 auto_ptr&lt;Example&gt; ptr2(new Example(6)); // Example: 6(输出内容) if (ptr2.get()) // 判断内部指针的有效性 &#123; ptr2.release(); // 调用release之后会释放内存所有权，但是不会析构，造成内存泄漏 if (!ptr2.get()) cout &lt;&lt; "ptr2 is invalid" &lt;&lt; endl; // ptr2 is invalid(输出内容) ptr2.release(); // 多写一遍没有任何作用 &#125;&#125; 测试函数release正确用法知道了relsease函数的错误用法，那么正确用法也就应该清楚了，需要自己调用delete，话说如果自己调用了delete那还用智能指针干什么，下面展示正常的用法： 123456789101112void test3()&#123; auto_ptr&lt;Example&gt; ptr3(new Example(3)); // Example: 3(输出内容) if (ptr3.get()) // 判断内部指针的有效性 &#123; Example *p = ptr3.release(); // release函数调用之后会释放内存的所有权，并且返回原始指针 if (!ptr3.get()) cout &lt;&lt; "ptr3 is invalid" &lt;&lt; endl; // ptr3 is invalid(输出内容) delete p; // ~Example: 3(输出内容) // 主动析构Example对象 &#125;&#125; 测试函数reset用法reset函数取其字面含义，就是重新设置的意思，也就是给一个指着对象设置一个新的内存对象让其管理，如果设置之前智能指针的已经管理了一个对象，那么在设置之后原来的对象会被析构掉，具体看测试结果： 12345678void test4()&#123; auto_ptr&lt;Example&gt; ptr4(new Example(4)); // Example: 4(输出内容) cout &lt;&lt; "after declare ptr4" &lt;&lt; endl; // after declare ptr4 ptr4.reset(new Example(5)); // Example: 5 // ~Example: 4 cout &lt;&lt; "after function reset" &lt;&lt; endl; // after function reset&#125; 测试函数operator=用法operator=也就是赋值运算符，是智能指针auto_ptr最具争议的一个方法，或者说一种特性，它的种种限制完全来自于这个赋值操作，作为面向的对象中的一部分，如果把一个对象赋值给另一个对象，那么两个对象就是完全一样的，但是这一点却在auto_ptr上打破了，智能指针auto_ptr的赋值，只是移交了所有权，将内部对象的控制所有权从等号的右侧转移到左侧，等号右侧的智能指针丧失对原有内部对象的控制，如果右侧的对象不检测内部对象的有效性，就会造成程序崩溃，测试如下： 1234567891011121314void test5()&#123; auto_ptr&lt;Example&gt; ptr5(new Example(5)); // Example: 5(输出内容) auto_ptr&lt;Example&gt; ptr6 = ptr5; // 没有输出 if (ptr5.get()) cout &lt;&lt; "ptr5 is valid" &lt;&lt; endl; // 没有输出，说明ptr5已经无效，如果再调用就会崩溃 if (ptr6.get()) cout &lt;&lt; "ptr6 is valid" &lt;&lt; endl; // ptr6 is valid(输出内容) ptr6-&gt;test_print(); // in test print: number = 5(输出内容) //ptr5-&gt;test_print(); // 直接崩溃 &#125; 测试auto_ptr类型返回一些文章中指出，auto_ptr不能作为函数的返回值，但是在我的测试环境下，可以正常执行，并且结果正确，但是还是不建议这样做，原因就是operator=，后面统一总结，先看下这个正常的例子： 1234567891011auto_ptr&lt;Example&gt; test6_inner()&#123; auto_ptr&lt;Example&gt; ptr6(new Example(6)); // Example: 6(输出内容) return ptr6;&#125;void test6()&#123; auto_ptr&lt;Example&gt; ptr6 = test6_inner(); // 测试auto_ptr类型返回值 ptr6-&gt;test_print(); // in test print: number = 6(输出内容)&#125; // ~Example: 6(输出内容) // 主动析构Example对 测试auto_ptr作为参数这是常常容易出错的情况，原因还是operator=的操作引起的，因为auto_ptr的赋值会转移控制权，所以你把auto_ptr的对象作为参数传递给一个函数的时候，后面再使用这个对象就会直接崩溃： 1234567891011void test7_inner(auto_ptr&lt;Example&gt; ptr7)&#123; ptr7-&gt;test_print(); // in test print: number = 6(输出内容)&#125; // ~Example: 7(输出内容) // 主动析构Example对象void test7()&#123; auto_ptr&lt;Example&gt; ptr7(new Example(7)); // Example: 7(输出内容) test7_inner(ptr7); // 传递参数 //ptr7-&gt;test_print(); // 直接崩溃&#125; 两个auto_ptr管理一个指针这种错误稍微出现的明显一点，因为智能指针的对象在析构时会回收内部对象的内存，如果两个智能指针同时管理一个内部对象，那么两个auto_ptr对象析构时都会试图释放内部对象的资源，造成崩溃问题： 1234567void test8()&#123; Example *p = new Example(8); // Example: 7(输出内容) auto_ptr&lt;Example&gt; ptr8(p); auto_ptr&lt;Example&gt; ptr9(p);&#125; //~Example: 8(输出内容) // 主动析构Example对象 //~Example: -572662307(输出内容) // 第二次析构崩溃 测试auto_ptr作为容器元素这是一个被广泛讨论的问题，可能你已经猜到了，一般说auto_ptr不能作为容器的元素也是因为operator=操作，但是我在Windows平台上成功运行了下面的代码，并且输出了正常的对象构造信息和析构信息，但是在Linux平台根本就编译不过去，出现大段的编译错误，其中重要的一句就是.../bits/stl_construct.h:73: 错误：对‘std::auto_ptr&lt;Example&gt;::auto_ptr(const std::auto_ptr&lt;Example&gt;&amp;)’的调用没有匹配的函数，其实可以说是operator=的锅，也可以说是拷贝构造函数的锅，但最根本的问题还是赋值时控制权转移导致的，测试代码如下： 123456789void test9()&#123; vector&lt;auto_ptr&lt;Example&gt;&gt; v(10); int i = 0; for (; i &lt; 10; i++) &#123; v[i] = auto_ptr&lt;Example&gt;(new Example(i));// windows下正常构造、析构，linux下无法通过编译 &#125;&#125; 测试auto_ptr的引用作为参数传递这个例子比较正常，就是将auto_ptr的对象进行引用传递，这种方式不会造成控制权转移，所以不会出现问题： 1234567891011void test10_inner(auto_ptr&lt;Example&gt;&amp; ptr10)&#123; ptr10-&gt;test_print(); // in test print: number = 6(输出内容)&#125; // 这里没有析构void test10()&#123; auto_ptr&lt;Example&gt; ptr10(new Example(10)); // Example: 10(输出内容) test10_inner(ptr10); // 传递引用参数 ptr10-&gt;test_print(); // in test print: number = 10(输出内容)&#125; //~Example: 10(输出内容) // 主动析构Example对象 测试auto_ptr的指针作为参数传递这个例子本质上同上个例子一样，就是将auto_ptr的对象的地址传递，这种指针的方式不会造成控制权转移，所以也不会出现问题： 1234567891011void test11_inner(auto_ptr&lt;Example&gt;* ptr11)&#123; (*ptr11)-&gt;test_print(); // in test print: number = 11(输出内容)&#125; // 这里没有析构void test11()&#123; auto_ptr&lt;Example&gt; ptr11(new Example(11)); // Example:11(输出内容) test11_inner(&amp;ptr11); // 传递地址参数 ptr11-&gt;test_print(); // in test print: number = 11(输出内容)&#125; // ~Example: 11(输出内容) // 主动析构Example对象 现象分析上述这些例子比较简单，主要是说明auto_ptr的用法，其中比较有争议的也就是6,7,9三个例子，也就是我们前文所说的“不建议”将auto_ptr作为函数返回值、函数参数、容器内的元素，这三个例子中只有作为函数参数的那个例子崩溃了，但是如果我们调用完函数test7_inner之后，不在使用智能指针ptr7也就不会崩溃了，那么是不是说只要我们注意到可能发生的问题，就可以使用auto_ptr在这些情况呢，目前来看是这样的。 但是为什么在Windows上成功运行的test9在Linux上却编译不过呢？简单点说就是为了怕你犯错，而对你采取管制措施，实际上你可以把auto_ptr作为容器的元素，但是因为这样太容易出错了，所以压根就不允许你这样做。 那么Linux是怎样在编译时期就提示auto_ptr这种错误，而Windows又是怎样绕过这种错误的呢？其实从应用的方便性和安全角度出发，容器应该要求其元素对象的拷贝与原对象相同或者等价，但是很明显auto_ptr做不到这一点，因为它的赋值是实质上是控制权的转移，而不是等价的复制，所以拷贝之后原对象必然被改变，linux版本的auto_ptr就是利用了这一点，使其违反C++的静态类型安全规则，这个版本的auto_ptr只实现构造函数auto_ptr(auto_ptr&amp; other)和赋值函数auto_ptr&amp; operator=(auto_ptr&amp; other)，因为参数都是非const，在构造或者赋值的时候原对象可能会发生变化，所以与容器对元素要求的不符合，这样在编译阶段就会检查出错误，也就是我们上面test9函数中提示的错误.../bits/stl_construct.h:73: 错误：对‘std::auto_ptr&lt;Example&gt;::auto_ptr(const std::auto_ptr&lt;Example&gt;&amp;)’的调用没有匹配的函数，这样就避免了把auto_ptr作为容器的元素。 关于Windows平台上正常运行test9函数的疑惑，实际上可以从两个方面来考虑，一种方式就是放宽容器对元素的要求，也就是说允许容器中的元素赋值之后，原对象被改变；另一种方式就是auto_ptr只提供构造函数auto_ptr(const auto_ptr&amp; other)和赋值函数auto_ptr&amp; operator=(const auto_ptr&amp; other)，这样就就可以通过容器的检测了，但是还有一个问题需要解决，那就是auto_ptr肯定要改变原对象，const类型就没法改变了，其实还有一种神奇的操作叫强制类型转换，使用const_cast就可以改变const对象，这样就达到了使用auto_ptr作为容器元素的目的，具体细节参考: auto_ptr到底能不能作为容器的元素? 前面提到把auto_ptr作为容器元素时很容易出错，这是为什么各个版本的auto_ptr实现的差异会这么大的原因，出错的根本原因就是auto_ptr构造和赋值时控制权的转移，试想一下，对一个容器进行排序，然后提供一个排序函数，然后排序时把容器中的元素传入比较函数，结果容器中元素的内部对象全都被清空了，这显然不是我们想要的，但是如果你不使用类似操作，那么把auto_ptr作为容器元素也没有什么不可。 总结 既然auto_ptr在C++17中已经被移除，那么我们也应该顺应潮流，尽量不使用auto_ptr了。 虽然不建议使用auto_ptr了，但是他的用法和注意事项我们还是应该了解，毕竟存在了这么多年，还有很多老代码中在用着。 由于各平台差异很大，目前auto_ptr作为容器元素不可移植，无论你使用的STL平台是否允许auto_ptr容器，你都不应该这样做。 通过分析发现auto_ptr能不能作为容器的元素并非绝对的，不仅与STL的实现有关，而且与STL容器的需求和安全性以及容器的语义有关。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>auto_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能指针分类及简单特性]]></title>
    <url>%2Fblog%2F2018%2F08%2F06%2F%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%E5%88%86%E7%B1%BB%E5%8F%8A%E7%AE%80%E5%8D%95%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[前言智能指针的种类繁多，我听说过的就有这些：auto_ptr、shared_ptr、weak_ptr、unique_ptr、scoped_ptr、scoped_array、shared_array、intrusive_ptr，这些智能指针看起来种类繁多，但实际上常用的就只有两三种，他们是shared_ptr、weak_ptr和unique_ptr，先简单了解一下这几个指针，后续再列出具体的例子和选择标准。 分类及特性 auto_ptr 这个指针历经沧桑，C++98中引入，C++11中弃用，C++17中被移除，弃用的原因主要是使用不当容易造成内存崩溃，不能够作为函数的返回值和函数的参数，也不能在容器中保存auto_ptr。 shared_ptr 据说是最好用的智能指针，使用引用计数实现，每使用它一次，内部的引用计数加1，每析构一次，内部的引用计数减1，减为0时，自动删除所指向的堆内存。shared_ptr内部的引用计数是线程安全的，但是对象的读取需要加锁。 weak_ptr 没有什么存在感，基本只在解除 shared_ptr循环引用时使用，weak_ptr没有共享资源，它的构造不会引起指针引用计数的增加，使用weak_ptr的成员函数use_count()可以观测资源的引用计数，使用成员函数lock()从被观测的shared_ptr获得一个可用的shared_ptr对象。 unique_ptr 一种比auto_ptr更加优秀的指针，可以唯一的拥有一个对象，auto_ptr通过等号赋值改变所有权后，再次引用原对象会造成内存崩溃，但是unique_ptr可以用过std::move改变所有权，并且引用原对象会在编译时期就指出错误，同时在容器算法中也可以使用，另有一种说法是说unique_ptr是scoped_ptr在标准库中的一个分身。 scoped_ptr 存在于boost库而非标准库中，要把资源限制在作用域里的，并且永远不能被复制，是一种轻量级的智能指针，和const auto_ptr很像，但是可以被reset，并可以更加清楚地表明意图。 scoped_array 跟scoped_ptr一样，也是独享所有权的，用于管理动态数组，不支持复制，并且初始化的时候需要使用动态数组，没有重载operator*，需要使用get()函数。 shared_array 跟 shared_ptr 一样，内部使用了引用计数，可以复制，通过参数来传递等，需要使用动态数组来初始化。 intrusive_ptr 这是一种侵入式的智能指针，内部不含有引用计数，要求被存储的对象自己实现引用计数功能，不然编译不过，还要提供intrusive_ptr_add_ref和intrusive_ptr_release函数接口供intrusive_ptr调用。 总结 智能指针的种类很多，但是只要掌握shared_ptr、weak_ptr、unique_ptr这三种指针的用法，就可以处理绝大多数问题。 智能指针的选择就根据特性来选，但是auto_ptr尽量不要用了，虽然历史悠久，但是毕竟由于各种诟病被抛弃了。 以上只给出了分类和简单特性，后续有时间会依次给出示例，指出用法和需要注意的点。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>shared_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简述TCP三次握手和四次挥手流程]]></title>
    <url>%2Fblog%2F2018%2F07%2F11%2F%E7%AE%80%E8%BF%B0TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言关于TCP的连接过程，很多从事程序开发的小伙伴应该都听过三次握手，可这三次握手的细节还是有很多人不太清楚的，特别是有些参数记不清楚，我也经常弄错，所以我根据自己的理解画了两张图，将TCP连接和断开的流程简单记录一下，以方便后续查找复习之用。 三次握手 初始状态：客户端A和服务器B均处于CLOSED状态，然后服务器B创建socket，调用监听接口使得服务器处于LISTEN状态，等待客户端连接。（后续内容用A，B简称代替） A首先向B发起连接，这时TCP头部中的SYN标识位值为1，然后选定一个初始序号seq=x（一般是随机的），消息发送后，A进入SYN_SENT状态，SYN=1的报文段不能携带数据，但要消耗一个序号。 B收到A的连接请求后，同意建立连接，向A发送确认数据，这时TCP头部中的SYN和ACK标识位值均为1，确认序号为ack=x+1，然后选定自己的初始序号seq=y（一般是随机的），确认消息发送后，B进入SYN_RCVD状态，与连接消息一样，这条消息也不能携带数据，同时消耗一个序号。 A收到B的确认消息后，需要给B回复确认数据，这时TCP头部中的ACK标识位值为1，确认序号是ack=y+1，自己的序号在连接请求的序号上加1，也就是seq=x+1，此时A进入ESTABLISHED状态，当B收到A的确认回复后，B也进入ESTABLISHED状态，至此TCP成功建立连接，A和B之间就可以通过这个连接互相发送数据了。 四次挥手 初始状态：客户端A和服务器B之间已经建立了TCP连接，并且数据发送完成，打算断开连接，此时客户端A和服务器B是等价的，双方都可以发送断开请求，下面以客户端A主动发起断开请求为例。（后续内容用A，B简称代替） A首先向B发送断开连接消息，这时TCP头部中的FIN标识位值为1，序号是seq=m，m为A前面正常发送数据最后一个字节序号加1得到的，消息发送后A进入FNI_WAIT_1状态，FIN=1的报文段不能携带数据，但要消耗一个序号。 B收到A的断开连接请求需要发出确认消息，这时TCP头部中的ACK标识位值为1，确认号为ack=m+1，而自己的序号为seq=n,n为B前面正常发送数据最后一个字节序号加1得到的，然后B进入CLOSE_WAIT状态，此时就关闭了A到B的连接，A无法再给B发数据，但是B仍然可以给A发数据（此处存疑），同时B端通知上方应用层，处理完成后被动关闭连接。然后A收到B的确认信息后，就进入了FIN_WAIT_2状态。 B端应用层处理完数据后，通知关闭连接，B向A发送关闭连接的消息，这时TCP头部中的FIN和ACK标识位值均为1，确认号ack=m+1，自己的序号为seq=k，（B发出确认消息后有发送了一段数据，此处存疑），消息发送后B进入LACK_ACK状态。 A收到B的断开连接的消息后，需要发送确认消息，这是这时TCP头部中的ACK标识位值为1，确认号ack=k+1，序号为m+1（因为A向B发送断开连接的消息时消耗了一个消息号），然后A进入TIME_WAIT状态，若等待时间经过2MSL后，没有收到B的重传请求，则表明B收到了自己的确认，A进入CLOSED状态，B收到A的确认消息后则直接进入CLOSED状态。至此TCP成功断开连接。 总结 关于三次握手，参考了很多资料说服务器是被动打开连接，对此有些不解，希望知道的朋友给出提示和建议。 关于四次挥手，在我的叙述中有两处存疑，就是B收到的A的主动断开请求后，进入CLOSE_WAIT状态，是否还能发送数据到A，参考了一些资料说A不能发数据给B，但是B能发数据给A，并且A也可以接收，但是无论我在Windows环境测试还是Linux环境下测试这种状态A都无法收到B的数据，不知道我是不是理解错了，希望明白原理的小伙伴能解答一下。 在四次挥手的最后阶段，有一个等待时间2MSL，这个不是一个时间单位，而是一个表明时间段的名词，这段等待时间就是为了在B没收到确认消息时，接收B的重传请求的，如果不等待这一段时间直接进入CLOSED状态，那么B未收到A的确认消息就会发送重传请求，而此时A已经关闭，就不会再给B重传了，其中MSL的是Maximum Segment Lifetime英文的缩写，可简单译为“报文最大生存时间”，也就是说如果B没有收到确认信息，那么在2MSL这段时间内很大概率就会发送重传请求，并且被A收到，RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。 在连接和断开的过程都有提到ACK和ack，这一点要注意区分，大写的ACK代表TCP头部中6个标识位之一，是表明这是个确认报文，而小写的ack拜师确认序号，表明对方发来的数据到ack这个序号前的都已经收到了。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Socket</tag>
        <tag>网络连接</tag>
        <tag>网络断开</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[结构体sockaddr、sockaddr_in、sockaddr_in6之间的区别和联系]]></title>
    <url>%2Fblog%2F2018%2F07%2F10%2F%E7%BB%93%E6%9E%84%E4%BD%93sockaddr%E3%80%81sockaddr-in%E3%80%81sockaddr-in6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[前言最近在学习网络相关的知识，虽然之前代码写了不少，但是长时间不写难免会忘记，简单地复习了一下IO多路复用的方式，对比了解了一下epoll模式和select模式的异同，不过写代码的时候发现，这个socket连接中有几个结构还是挺让人头大的，用着用着突然就强转成其他的类型了，加上年前改了半天IPv6的连接，这几个结构体更加混乱，所以今天角色放到一起，从源码的角度看一下sockaddr、sockaddr_in、sockaddr_in6这三个结构体之间的联系，以及为什么有些情况可以直接强转。 代码分析 看一下这三个结构的定义，先说明一下版本，操作系统为CentOS，头文件版本应该挺古老了，在’/usr/include/netinet/in.h’ 中发现版权信息：Copyright (C) 1991, 1992, 1994-2001, 2004, 2006, 2007, 2008, 2009, 2010，看着很古老，但之后的版本应该没有改动很大吧，反正不太清楚，我们就分析当前这一个版本吧。 1234567891011121314151617181920212223242526272829303132333435/* /usr/include/bits/socket.h *//* Structure describing a generic socket address. */struct sockaddr&#123; __SOCKADDR_COMMON (sa_); /* Common data: address family and length. */ char sa_data[14]; /* Address data. */&#125;;/* /usr/include/netinet/in.h *//* Structure describing an Internet socket address. */struct sockaddr_in&#123; __SOCKADDR_COMMON (sin_); in_port_t sin_port; /* Port number. */ struct in_addr sin_addr; /* Internet address. */ /* Pad to size of `struct sockaddr'. */ unsigned char sin_zero[sizeof (struct sockaddr) - __SOCKADDR_COMMON_SIZE - sizeof (in_port_t) - sizeof (struct in_addr)];&#125;;/* /usr/include/netinet/in.h */#ifndef __USE_KERNEL_IPV6_DEFS/* Ditto, for IPv6. */struct sockaddr_in6&#123; __SOCKADDR_COMMON (sin6_); in_port_t sin6_port; /* Transport layer port # */ uint32_t sin6_flowinfo; /* IPv6 flow information */ struct in6_addr sin6_addr; /* IPv6 address */ uint32_t sin6_scope_id; /* IPv6 scope-id */&#125;;#endif /* !__USE_KERNEL_IPV6_DEFS */ 看到3个结构的定义想到了什么？只是看着有点像吧，真正的区别我们往下看，其中3个结构里都包含了 __SOCKADDR_COMMON 这个宏，我们先把它的定义找到，最后在’usr/inlcue/bits/sockaddr.h’中找到如下代码， 1234567891011/* POSIX.1g specifies this type name for the `sa_family' member. */typedef unsigned short int sa_family_t;/* This macro is used to declare the initial common members of the data types used for socket addresses, `struct sockaddr', `struct sockaddr_in', `struct sockaddr_un', etc. */#define __SOCKADDR_COMMON(sa_prefix) \ sa_family_t sa_prefix##family#define __SOCKADDR_COMMON_SIZE (sizeof (unsigned short int)) 由此我们知道，这三个结构的第一个字段都是一个unsigned short int 类型，只不过用宏来定义了三个不同的名字，至此第一个结构就清楚了，在一般环境下（short一般为2个字节），整个结构占用16个字节，变量sa_family占用2个字节，变量sa_data 保留14个字节用于保存IP地址信息。 接着我们发现第二个结构中还有in_port_t和struct in_addr两个类型没有定义，继续找下去吧，在文件‘/usr/include/netinet/in.h’发现以下定义 123456789/* Type to represent a port. */typedef uint16_t in_port_t;/* Internet address. */typedef uint32_t in_addr_t;struct in_addr&#123; in_addr_t s_addr;&#125;; 这么看来sockaddr_in这个结构也不复杂，除了一开始的2个字节表示sin_family，然后是2个字节的变量sin_port表示端口，接着是4个字节的变量sin_addr表示IP地址，最后是8个字节变量sin_zero填充尾部，用来与结构sockaddr对齐 现在我们该分析结构sockaddr_in6了，这里边只有一个未知的结构in6_addr，经过寻找发现其定义也在’/usr/include/netinet/in.h’中 12345678910111213141516171819#ifndef __USE_KERNEL_IPV6_DEFS/* IPv6 address */struct in6_addr&#123; union &#123; uint8_t __u6_addr8[16];#if defined __USE_MISC || defined __USE_GNU uint16_t __u6_addr16[8]; uint32_t __u6_addr32[4];#endif &#125; __in6_u;#define s6_addr __in6_u.__u6_addr8#if defined __USE_MISC || defined __USE_GNU# define s6_addr16 __in6_u.__u6_addr16# define s6_addr32 __in6_u.__u6_addr32#endif&#125;;#endif /* !__USE_KERNEL_IPV6_DEFS */ 这个结构看起来有点乱，但是如果抛开其中的预编译选项，其实就是8个字节，用来表示IPV6版本的IP地址，一共128位，只不过划分字节的段数有些不同，每段字节多一点那么段数就少一点，反义亦然。 那接下来我们整理一下，为了看的清楚，部分结构使用伪代码，不能通过编译，主要是方便对比，整理如下 12345678910111213141516171819202122232425/* Structure describing a generic socket address. */struct sockaddr&#123; uint16 sa_family; /* Common data: address family and length. */ char sa_data[14]; /* Address data. */&#125;;/* Structure describing an Internet socket address. */struct sockaddr_in&#123; uint16 sin_family; /* Address family AF_INET */ uint16 sin_port; /* Port number. */ uint32 sin_addr.s_addr; /* Internet address. */ unsigned char sin_zero[8]; /* Pad to size of `struct sockaddr'. */&#125;;/* Ditto, for IPv6. */struct sockaddr_in6&#123; uint16 sin6_family; /* Address family AF_INET6 */ uint16 sin6_port; /* Transport layer port # */ uint32 sin6_flowinfo; /* IPv6 flow information */ uint8 sin6_addr[16]; /* IPv6 address */ uint32 sin6_scope_id; /* IPv6 scope-id */&#125;; 这么来看是不是就清晰多了，由此我们发现结构 sockaddr 和 sockaddr_in 字节数完全相同，都是16个字节，所以可以直接强转，但是结构 sockaddr_in6 有28个字节，为什么在使用的时候也是直接将地址强制转化成(sockaddr*)类型呢？ 强转的可能性其实sockaddr 和 sockaddr_in 之间的转化很容易理解，因为他们开头一样，内存大小也一样，但是sockaddr和sockaddr_in6之间的转换就有点让人搞不懂了，其实你有可能被结构所占的内存迷惑了，这几个结构在作为参数时基本上都是以指针的形式传入的，我们拿函数bind()为例，这个函数一共接收三个参数，第一个为监听的文件描述符，第二个参数是sockaddr*类型，第三个参数是传入指针原结构的内存大小，所以有了后两个信息，无所谓原结构怎么变化，因为他们的头都是一样的，也就是uint16 sa_family，那么我们也能根据这个头做处理，原本我没有看过bind()函数的源代码，但是可以猜一下: 1234567891011121314151617int bind(int socket_fd, sockaddr* p_addr, int add_size)&#123; if (p_addr-&gt;sa_family == AF_INET) &#123; sockaddr_in* p_addr_in = (sockaddr_in*)p_addr; //... &#125; else if (p_addr-&gt;sa_family == AF_INET6) &#123; sockaddr_in6* p_addr_in = (sockaddr_in6*)p_addr; //... &#125; else &#123; //... &#125;&#125; 由以上代码完全可以实现IPv4和IPv6的版本区分，所以不需要纠结内存大小的不同 总结 通过等价替换的方式我们可以更好的了解sockaddr、sockaddr_in、sockaddr_in6之间的异同。 网路接口函数针对于IPv4和IPv6虽然有不同的结构，但是接口基本相同，主要是为了用户（开发者）使用方便吧。 有时间可以看一下bind()、accept()等函数，看看其中对于结构的使用到底是怎样的。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Socket</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS2015调试dump文件时提示未找到xxx.exe或xxx.dll]]></title>
    <url>%2Fblog%2F2018%2F06%2F23%2FVS%E8%B0%83%E8%AF%95dump%E6%96%87%E4%BB%B6%E6%97%B6%E6%8F%90%E7%A4%BA%E6%9C%AA%E6%89%BE%E5%88%B0Xxxx-exe%E6%88%96xxx-dlll%2F</url>
    <content type="text"><![CDATA[前言游戏开发的过程中，经常会出现客户端宕机的问题，这时候一个小小的dump文件可以记录当时的内存及堆栈情况，对于解决崩溃的问题有巨大的帮助，之前用VS2008的时候调试过dump文件，但是最近客户端升级为VS2015以后，调试dump文件时经常会出现未找到xxx.exe或xxx.dll的情况，之前一直好使的方法现在却行不通了，于是决定找找解决的办法。 问题原因起初尝试过新建dump文件所显示的路径，复制exe或dll到指定路径下，复制dump文件到exe所在路径下都提示找不到，甚至是手动指定dll或者exe文件都无法打开，这就很奇怪了，原来只要把dump文件放在exe所在目录就可以啊，怎么这次不行了呢？终于，经过多次试验之后发现，原来在VS2015上调试dump文件，要求dump文件的版本与产生dump文件的exe或者dll必须一致，也就是说你要调试一个dump文件，就必须找到找到对应版本dll和exe，否则就会提示无法找到xxx.exe或xxx.dll，下面我们来试验一下。 产生dump文件产生dump文件的方法网上很容易找到，如果想测试的话可以自己找一找，也可以使用下面的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include "stdafx.h"#include "Windows.h"#include "DbgHelp.h"int GenerateMiniDump(PEXCEPTION_POINTERS pExceptionPointers)&#123; // 定义函数指针 typedef BOOL(WINAPI * MiniDumpWriteDumpT)( HANDLE, DWORD, HANDLE, MINIDUMP_TYPE, PMINIDUMP_EXCEPTION_INFORMATION, PMINIDUMP_USER_STREAM_INFORMATION, PMINIDUMP_CALLBACK_INFORMATION ); // 从 "DbgHelp.dll" 库中获取 "MiniDumpWriteDump" 函数 MiniDumpWriteDumpT pfnMiniDumpWriteDump = NULL; HMODULE hDbgHelp = LoadLibrary(_T("DbgHelp.dll")); if (NULL == hDbgHelp) &#123; return EXCEPTION_CONTINUE_EXECUTION; &#125; pfnMiniDumpWriteDump = (MiniDumpWriteDumpT)GetProcAddress(hDbgHelp, "MiniDumpWriteDump"); if (NULL == pfnMiniDumpWriteDump) &#123; FreeLibrary(hDbgHelp); return EXCEPTION_CONTINUE_EXECUTION; &#125; // 创建 dmp 文件件 TCHAR szFileName[MAX_PATH] = &#123; 0 &#125;; TCHAR* szVersion = _T("dump_file_v1.0"); SYSTEMTIME stLocalTime; GetLocalTime(&amp;stLocalTime); wsprintf(szFileName, L"%s-%04d%02d%02d-%02d%02d%02d.dmp", szVersion, stLocalTime.wYear, stLocalTime.wMonth, stLocalTime.wDay, stLocalTime.wHour, stLocalTime.wMinute, stLocalTime.wSecond); HANDLE hDumpFile = CreateFile(szFileName, GENERIC_READ | GENERIC_WRITE, FILE_SHARE_WRITE | FILE_SHARE_READ, 0, CREATE_ALWAYS, 0, 0); if (INVALID_HANDLE_VALUE == hDumpFile) &#123; FreeLibrary(hDbgHelp); return EXCEPTION_CONTINUE_EXECUTION; &#125; // 写入 dmp 文件 MINIDUMP_EXCEPTION_INFORMATION expParam; expParam.ThreadId = GetCurrentThreadId(); expParam.ExceptionPointers = pExceptionPointers; expParam.ClientPointers = FALSE; pfnMiniDumpWriteDump(GetCurrentProcess(), GetCurrentProcessId(), hDumpFile, MiniDumpWithDataSegs, (pExceptionPointers ? &amp;expParam : NULL), NULL, NULL); // 释放文件 CloseHandle(hDumpFile); FreeLibrary(hDbgHelp); return EXCEPTION_EXECUTE_HANDLER;&#125;LONG WINAPI ExceptionFilter(LPEXCEPTION_POINTERS lpExceptionInfo)&#123; // 这里做一些异常的过滤或提示 if (IsDebuggerPresent()) &#123; return EXCEPTION_CONTINUE_SEARCH; &#125; return GenerateMiniDump(lpExceptionInfo);&#125;void create_dump()&#123; // 给空指针赋值，使程序崩溃产生 Dump 文件 int *ptr = NULL; *ptr = 101;&#125;int main()&#123; // 加入崩溃dump文件功能 SetUnhandledExceptionFilter(ExceptionFilter); create_dump();&#125; 将上述代码编译成exe文件，然后点击运行就会在exe所在目录产生一个dump文件，例如我产生的dump文件为dump_file_v1.0-20180623-123940.dmp 调试dump文件双击打开刚刚生成的dump文件，会出现如下界面： 点击右侧 “使用 仅限本机 进行调试” 按钮，就会显示出程序崩溃时的堆栈信息和内存情况以及崩溃位置的代码，如下图： 以上是正常的调试情况，接下来不需要改变代码，重新编译一下程序，得到新版本的exe文件，然后双击刚刚的dump文件dump_file_v1.0-20180623-123940.dmp，点击右侧 “使用 仅限本机 进行调试” 按钮，情况就会发生变化，显示结果如下图： 点击 “中断” 按钮，就会出现标题所说的未找到vsDump.exe。在小型转储中未找到 vsDump.exe。 您需要加载二进制文件才能查找当前堆栈帧的源代码。 看到了吧，只要是dump文件不是这个exe产生的，不管源代码是不是一样，结果都会提示找不到exe，至此我们就找到了“VS2015调试dump文件时提示未找到xxx.exe或xxx.dll”的原因。 总结 VS2015调试dump文件时需要保证dump文件和exe、dll版本一致 遇到奇怪的问题可以手动模拟一下，往往可以重现，然后找到具体的原因]]></content>
      <categories>
        <category>VS</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>VS</tag>
        <tag>dump文件调试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作指向类成员的指针需要了解的两个操作符->*和.*]]></title>
    <url>%2Fblog%2F2018%2F05%2F12%2F%E6%93%8D%E4%BD%9C%E6%8C%87%E5%90%91%E7%B1%BB%E6%88%90%E5%91%98%E7%9A%84%E6%8C%87%E9%92%88%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%93%8D%E4%BD%9C%E7%AC%A6-%E5%92%8C%2F</url>
    <content type="text"><![CDATA[前言关于 -&gt;* 这种写法在很早就在项目代码里见过了，并且还写过，不过当时并没有正确的理解这样写的含义，一直到最近发现这样写很奇怪，于是根据自己的理解，开始改代码，发现无论怎么改都无法通过编译，仔细搜索后才发现这是一种固定的写法，也就是说 -&gt;* 是一个操作符，无法拆分，同时还有一个 .* 也是相同的作用，只不过是用于对象上，而 -&gt;* 是用于对象的指针上。 那么这两个操作符究竟有什么作用呢？实际上它们主要用于操作指向类成员的指针，可能你会说指向类成员的指针直接定义就好了，为什么这么麻烦，还要是用这两个操作符呢？接下来我们举几个例子就明白了。 指向类数据成员的指针12345678910111213141516#include &lt;iostream&gt;class C&#123;public: int m;&#125;;int main()&#123; int C::* p = &amp;C::m; // pointer to data member m of class C C c = &#123;7&#125;; std::cout &lt;&lt; c.*p &lt;&lt; '\n'; // prints 7 C* cp = &amp;c; cp-&gt;m = 10; std::cout &lt;&lt; cp-&gt;*p &lt;&lt; '\n'; // prints 10&#125; 看到上述代码中的p指针有什么不同了吧，这是一个指向类成员变量的指针，如果我们不这样定义p也想操作c对象的成员变量m要怎么办呢？我们可以这样写： 123456789101112131415#include &lt;iostream&gt;class C&#123;public: int m;&#125;;int main()&#123; C c = &#123;7&#125;; int *p = &amp;c.m; std::cout &lt;&lt; *p &lt;&lt; '\n'; // prints 7 *p = 10; std::cout &lt;&lt; *p &lt;&lt; '\n'; // prints 10&#125; 这样代码中的变量p就变成了一个简单的指向整型数据的指针，我们也可以通过它访问c对象的m变量，并且给它赋值，但是你有没有发现区别，前一种指针p只依赖于类C的定义，可以在类C创建对象之前就给指针p定义赋值，但是后一种数据指针p就只能在类C创建对象之后才能给它赋值，还有一点，前一种指针p可以根据调用它的对象不同而访问不同类C对象的值，而后一种指针p就只能访问它所指向的那个对象的m值，如果要访问其他对象，需要重新给p赋值。 注意指向类成员指针的定义和赋值方法，是int C::* p = &amp;C::m;，取变量m的地址还有两种写法，&amp;(C::m) 或者 &amp;m这两种写法只能写在类C的成员函数中，所表示的也就是一个简单的指向整型变量的指针，即int*，与 &amp;C::m的含义是大不相同的。 而操作符-&gt;* 和.*在代码中起什么作用呢，我们只看这一句std::cout &lt;&lt; cp-&gt;*p &lt;&lt; &#39;\n&#39;;，其中表达式cp-&gt;*p用到了操作符-&gt;*，根据我的理解这个操作符的作用就是将后面的指针解引用，然后再被前面的对象调用，首先我们看cp是一个指向c对象的指针，如果想访问m变量，可以直接使用cp-&gt;m，假设现在不想这么写，我们有一个指向类C中m变量的指针p，那么直接写成cp-&gt;p肯定是不行的，因为p并不是类C的成员，它只是一个指向类C成员的指针，所以需要将其解引用，转换成真正的成员才能被cp指针引用到，那么*cp其实就是类C中的m，组合到一起就是cp-&gt; *p，这只是理解，其实-&gt;*是一个不可分割的操作符，需要紧挨着写成cp-&gt;*p才能编译通过。 另外关于指向类成员指针，在操作对象是父类对象和子类对象时有什么不同呢?答案是：指向可访问的非虚拟基类的数据成员的指针可以隐式地转换为指向派生类的同一数据成员的指针，反过来结果就是未定义的了，可以参考代码： 1234567891011121314151617#include &lt;iostream&gt;class Base&#123;public: int m;&#125;;class Derived : public Base &#123;&#125;;int main()&#123; int Base::* bp = &amp;Base::m; int Derived::* dp = bp; Derived d; d.m = 1; std::cout &lt;&lt; d.*dp &lt;&lt; '\n'; // prints 1 std::cout &lt;&lt; d.*bp &lt;&lt; '\n'; // prints 1&#125; 指向类成员函数的指针其实前面的例子我在工作中还真没遇到过，但是指向类数据成员的指针确实经常用，熟悉函数指针的工程师都知道，类似于void (*func)(); 就是定义了指向一个无返回值无参数函数的指针，调用时只要写成(*func)();就行，但是如果定义指向类成员函数的指针可就麻烦一点了，接下来看一个例子： 1234567891011121314class C&#123;public: void f(int n) &#123; std::cout &lt;&lt; n &lt;&lt; '\n'; &#125;&#125;;int main()&#123; void (C::* p)(int) = &amp;C::f; // pointer to member function f of class C C c; (c.*p)(1); // prints 1 C* cp = &amp;c; (cp-&gt;*p)(2); // prints 2&#125; 这个例子中的函数指针p是有作用域的，也就是只能指向类C中的无返回值并且有一个整型参数的函数，代码中赋值为&amp;C::f，这个形式与数据成员指针的赋值一样，其实函数f就是类C的一个成员而已。 那么它是怎么通过p指针调用到函数f的呢？我们看一句代码(cp-&gt;*p)(2);其实-&gt;*在这里还是起到了解引用并访问的作用，如果要访问f函数，只要cp-&gt;f(2)即可，但是这里没有f只有一个指向f的指针p，所以将f替换成*p编程cp-&gt;*p(2);但是这样无法通过编译，它无法区分那一部分是函数体，那一部分是参数，所以加个括号指明一下变成(cp-&gt;*p)(2);就可以正常访问f函数了。 实际上对面向对象编程了解的深入一点就会知道，调用对象的成员函数，实际上就是把对象的指针this作为函数第一个参数传进去，比如cp-&gt;f(2)，假如函数f的函数指针是func，那么cp-&gt;f(2)就是调用func(cp, 2)，这样在函数f中就可以调用对象的成员变量或者其他的成员函数了，但是如果你的成员函数中没有访问成员内容，那么这个this指针传什么都可以，也就是说func(cp, 2)和func(0, 2)、func(0x1234567890, 2)都是等价的，在这个例子中就是这样，所有你可以这样来写一段代码：(((C*)0)-&gt;*p)(2)，也是可以打印出数字2的。 另外关于指向类成员函数指针，在操作对象是父类对象和子类对象时与成员变量的规则一致：指向可访问的非虚拟基类的成员函数的指针可以隐式地转换为指向派生类的同一成员函数的指针，反过来也是未定义，可以参考代码： 12345678910111213141516#include &lt;iostream&gt;class Base&#123;public: void f(int n) &#123; std::cout &lt;&lt; n &lt;&lt; '\n'; &#125;&#125;;class Derived : public Base &#123;&#125;;int main()&#123; void (Base::* bp)(int) = &amp;Base::f; void (Derived::* dp)(int) = bp; Derived d; (d.*dp)(1); (d.*bp)(2);&#125; 具体使用前面提到过指向类数据成员的指针我之前真的没用到过，但是指向成员函数的指针，我却用了不少，一般都是放在函数数组中使用，比如有这样一个场景，游戏npc根据状态执行对应的状态函数，这些状态函数是成员函数，为此我们需要将npc所有的状态函数添加到一个函数数组中，假设有idle、run、walk、jump四种状态，下面是实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;iostream&gt;class CNpc&#123; typedef void (CNpc::*StateFunction)();public: int state; // 0,1,2,3 对应 idle、run、walk、jump StateFunction state_function_array[4];public: CNpc() &#123; state = 0; state_function_array [0] = &amp;CNpc::idle; state_function_array [1] = &amp;CNpc::run; state_function_array [2] = &amp;CNpc::walk; state_function_array [3] = &amp;CNpc::jump; &#125; void change_state(int new_state) &#123; state = new_state; &#125; void process_state() &#123; if (state_function_array[state]) &#123; (this-&gt;*state_function_array[state])(); // 调用函数指针的地方 &#125; &#125;private: void idle() &#123; std::cout &lt;&lt; "state = idle" &lt;&lt; std::endl; &#125; void run() &#123; std::cout &lt;&lt; "state = run" &lt;&lt; std::endl; &#125; void walk() &#123; std::cout &lt;&lt; "state = walk" &lt;&lt; std::endl; &#125; void jump() &#123; std::cout &lt;&lt; "state = jump" &lt;&lt; std::endl; &#125;&#125;;int main()&#123; CNpc npc; npc.process_state(); npc.process_state(); npc.change_state(1); npc.process_state(); npc.change_state(3); npc.process_state(); npc.process_state(); npc.process_state(); npc.change_state(2); npc.process_state(); npc.change_state(0); npc.process_state(); npc.process_state(); return 0;&#125; 运行结果 123456789state = idlestate = idlestate = runstate = jumpstate = jumpstate = jumpstate = walkstate = idlestate = idle 总结 牢记-&gt;*和.*也是一种操作符，使用的时候不要拆开 理解操作符中的*符号的解引用的作用 如果有理解不正确的地方欢迎大家批评指正]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>类成员访问</tag>
        <tag>成员指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bat批处理替换字符串中匹配的子串]]></title>
    <url>%2Fblog%2F2018%2F04%2F12%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%E6%9B%BF%E6%8D%A2%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E5%8C%B9%E9%85%8D%E7%9A%84%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[前言关于字符串的操作通常是编程生涯中不可避免的，在各种竞赛中、工作中常常能使用到，许多语言中都有专门负责处理字符串的模块或者类，对于字符串的替换一般也有专门的函数，比如Lua中的string.gsub()、Python中的replece()等，那么批处理在进行字符串操作的时候，有没有好用的替换函数呢？ 前两天在使用批处理更新资源文件的时候发现，批处理中也有专门处理字符串替换的方法，并且这是我见到的最有意思的字符串替换方式，就是利用A:B=C的方式来替换字符串，具体含义就是在字符串变量A中查找所有的子串B并且替换成子串C，看起来很有意思吧？下面举一个具体的示例看一下。 代码示例12345678910111213141516171819202122@echo offSET INPUT_PARAM=%1rem 替换输入变量中的world为Chinaecho source string is %INPUT_PARAM%echo === China replace world ===echo replace result is %INPUT_PARAM:world=China%echo.rem 将路径中的反斜杠替换成斜杠SET IMAGE_PATH=C:\NVIDIA\AndroidWorks\001echo source string is %IMAGE_PATH%echo === \ replace / ===echo replace result is %IMAGE_PATH:\=/%echo.echo ABCD:A=apause 代码中举了两个例子，将变量中的world为China、将路径中的反斜杠替换成斜杠都成功地替换了子串的内容，但是我们发现这个的作用对象只能是变量，对于最后一句echo ABCD:A=a并没有发生替换，下面可以看一下运行结果。 运行结果1234567891011E:\batTool&gt;Replace.bat &quot;Hello wolrd, All over the world!&quot;source string is &quot;Hello wolrd, All over the world!&quot;=== China replace world ===replace result is &quot;Hello wolrd, All over the China!&quot;source string is C:\NVIDIA\AndroidWorks\001=== \ replace / ===replace result is C:/NVIDIA/AndroidWorks/001ABCD:A=a请按任意键继续. . . 总结 bat处理字符串替换的方式比较有意思，需要知道A:B=C形式的替换方法 字符串替换只能是针对变量，对于文本貌似不起作用。]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[略显神秘的快速排序]]></title>
    <url>%2Fblog%2F2018%2F04%2F08%2F%E7%95%A5%E6%98%BE%E7%A5%9E%E7%A7%98%E7%9A%84%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言继续我的填坑旅程，上次说到《排序算法系列之（二）——冒泡排序名字最为形象的一个》2017-09-16 10:42:07，又过了半年多，终于再一次鼓起勇气决定聊一聊快速排序的思路，不过与冒泡排序不同的是，这个快速排序的名字似乎和算法的思路没有什么关系，这个名字太抽象了，起这个名字可能当初仅仅是因为它比别的排序快一点。咳咳！ 抽象的名字不利于我们对于算法思路的理解，或许这就是我为什么当初认为快速排序是最难理解的排序算法，也可能是当初还没接触过堆排序、希尔排序等这些另类的排序吧！毕竟工作5年之后再来看这个快速排序，思路也是很清晰的，忽然发现它当初那份神秘的气息消散了许多。 快速排序我们今天同样略过各种复杂度，直奔主题——快速排序，既然它的名字不是说算法思路，那就是说性质了，通俗点说就是在一般情况下它比选择排序、冒泡排序要快，先不用关心它为什么快，我们先来模拟一个最简单的快速排序。 快速排序的核心思想是分治、递归，将原本问题的规模不断缩小，递归求解，这类算法往往代码很简单，但是理解起来难度大一点，说一下总体思路，我们先来举个例子。假设将N个数从小到大排序，首先是在等待排序的数组N中随便选一个数M，为了简单我们选择第一个，然后遍历待排数组，把比M小的数放到M的左边，把比M大的数放到M的右边，一次遍历结束M左边有m1个数，右边有m2个数(m1+m2+1=N)，然后就形成了两个待排数组N1和N2，对于每个待排数组重复上述操作，直到待排数组缩小到一个数字，则待排数据排序完毕，整个数组变为有序。 因为这个排序比较抽象，所以前面的橘子、苹果的例子很难解释清楚，但是我们可以用标了号的橙子来理解，是不是感觉橙子伟大了一点，为什么橙子可以，因为早上刚刚吃过橙子，嗯！就是这么任性！假设桌上摆着一排橙子，他们的重量分别是6, 2, 7, 3, 8, 9，什么？你问我重量单位是什么，那就是斤吧，谁叫这些橙子变异了呢，大的大，小的小，好了，能帮助我们理解算法就好了，自从有了转基因，今后多重的橙子都可能遇到。 事先解释一下，我们这些橙子在桌上排成了一排，并且每一个橙子都放在了盘子里，盘子不移动，我们只移动盘子里的橙子，空盘子用*表示，手里的橙子用M表示，为了省点力气，我们尽可能的少移动橙子。 起初桌子上盘子里的橙子情况是这样的:6, 2, 7, 3, 8, 9，M=* 用手拿起第一个盘子里的橙子后：*, 2, 7, 3, 8, 9 ，M=6 从后往前找到第一个比M小的橙子放到前面，9、8、3，发现3是第一个符合条件的，把它拿到前面的盘子，变成了这样：3, 2, 7, *, 8, 9 ，M=6 然后第一个不算从前往后找到第一个比M大的橙子放到后面，2、7，发现7是第一个符合条件的，把它放在后面的空盘子：3, 2, *, 7, 8, 9 ，M=6 到此为止，我们已经把所有位置都遍历一遍了，这就是所谓的一趟排序，如果中间还有位置没有比较，重复步骤3和步骤4，直到所有的位置的橙子都被遍历到，把M=6放到最后的空盘子中就变成了：3, 2, 6, 7, 8, 9 ，M=* 执行到这个步骤，原来的这些橙子就被分成了两部分，比M=6小的放到了它的前面，比M=6大的放到了它的后面，现在就变成了两个规模较小的数组排序，我们以前面的待排数组N1为例，重复步骤2，先取出第一个橙子，拿在手里：*, 2 ，M=3 重复步骤3，从后往前找到第一个比M小的橙子放到前面，发现2这个橙子，然后把它放到前面的空盘子，现在的情况如下：2, * ，M=3 本来应该重复步骤4，但是此时发现所有的位置已经遍历过了，所以步骤4省略，直接步骤5，把M=3放在空盘子中：2, 3 ，M=* 此时被M=3分割的就过只有一部分，并且不大于一个橙子，所以左半部分排序结果，总体来看顺序为：2, 3, 6, 7, 8, 9 ，M=* 接着就要对步骤5后面的右半部分排序了，也就算是对7, 8, 9，虽然现在数据少我们一眼就能看出这结果数是有序的，但是如果在程序代码中，还是会对这部分橙子重复步骤3、4、5来达到有序，这里就不再逐步解释了，最后的结果就是：2, 3, 6, 7, 8, 9 ，M=* 代码实现1234567891011121314151617181920212223242526272829/*功能： 快速排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 low --待排序区间的起始索引 high --待排序区间的结束索引返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void quick_sort(int array[], int low, int high)&#123; if (low &gt;= high) return; int front = low, back = high, key = array[low]; // 选取第一个元素作为中轴 while (front &lt; back) &#123; while (front &lt; back &amp;&amp; array[back] &gt;= key) --back; array[front] = array[back]; // 从后面找到第一个比中轴小的交换 while (front &lt; back &amp;&amp; array[front] &lt;= key) ++front; array[back] = array[front]; // 从前面找到第一个比中轴大的交换 &#125; array[front] = key; quick_sort(array, low, front - 1); // 递归快排前半段 quick_sort(array, low, front + 1); // 递归快排后半段&#125; 代码分析上述代码与橙子排序的示例思路完全一致，key = array[low]是步骤2，选取第一个元素作为中轴；最外层的while循环是反复重复步骤3和步骤4，保证遍历所有位置的橙子；内部的第一个while循环是步骤3，从后面找到第一个比中轴小的；内部的第二个while循环是步骤4，从前面找到第一个比中轴大的；array[front] = key;就是步骤5，把手里的橙子放回到空盘子中；接下来的两个函数调用都是调用自己，也就是递归调用，分别处理小于M的一段和大于M的一段，怎么样？代码是不是好理解多了？如果觉得我理解的有问题或者代码有错，也欢迎大家批评指正。 运行测试快速排序–源码 如果你想运行测试一下，可以复制或者下载源代码，在本机运行测试一下，当然也可以选择在线编辑器，这是我新发现的在线编译器，样子还挺好看的，把源代码复制到网页中运行查看结果。]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神秘莫测的时间复杂度]]></title>
    <url>%2Fblog%2F2018%2F03%2F29%2F%E7%A5%9E%E7%A7%98%E8%8E%AB%E6%B5%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[前言说到时间复杂度，作为程序员或多或少都会有所接触，特别是算法工程师肯定是天天和这个概念打交道，其实前几篇总结排序的文章我一直没有提时间复杂度，因为网上太多的文章讲这个概念了，所以我只总结了一下我对几种排序算法的理解，以及简单的实现代码，而当我今天准备总结一下快速排序的时候，我发现各个关于快速排序的文章都有讲到时间复杂度，有的甚至直接就给出 O(N*log(N))，这个对数是以2为底的，还是以10为底的都没有说清楚，这简直让人感到莫名其妙，所以我决定还是简单说一下我对时间复杂度这个概念的理解。 其实时间复杂的作用很简单，就是用来表示算法运行时间与数据规模的关系，其中的数据规模常常用字母N或者n表示，将算法的运行时间表示为n的函数表达式应该为 t=f(n) ，而时间复杂度就是表达式中幂最高的那一项，然后去掉常数系数，比如当算法运行时间表达式为 t = f(n) = 3 * n^2 + n 时，那么这个算法的时间复杂度就是O(n^2)，字母O用来表示时间复杂度，也就是该算法与数据规模成平方阶的关系，为什么要去掉一次项n，因为在数据规模扩大时，它相对于平方阶来说基本可以忽略不计。 时间复杂度计算方法虽然很多编程工作者都接触过时间复杂度，但是真正要想把时间复杂度算明白可不是件容易的事，特别是遇到时间复杂度中包含对数项的，那就更让人糊涂了，很多人搞不明白为什么会有对数次的运算，实际上二分法常常是导致时间复杂度中出现对数项的一种算法，要算出时间复杂度，本质上就是算出执行完算法算所需要的操作次数，这种操作通常是比较、赋值、交换等等，而时间复杂度与数据规模相关，通常把一个算法执行完所需的操作次数，表示成和数据规模n相关的函数，这就是我们所说的时间复杂度。时间复杂度是用来衡量算法所需时间资源的，所以只保留最高次幂的项就可以，不用纠结于是O(n^2+n)还是O(n^2)，在大规模的数据面前，这两种时间复杂度几乎相等，总之一句话，看一个算法的时间复杂度，就是数这个算法执行完所需要的操作次数，并且把这个次数表示成与n相关的简单函数。 时间复杂度示例接下来我们举几个简单的例子，来理解一下什么是时间复杂度，并且了解一下“计算时间复杂度就是数算法执行操作的次数”这句话的意思，接下来我们一起数一下： 常规方法计算区间[1,n]中所有整数的和 12345int sum = 0, n = 10000;for (int i = 1; i &lt;= n; i++)&#123; sum += i;&#125; 上述代码就是一个很简单的计算方法，具体操作就是遍历区间[1,n]内的所有整，然后依次相加，执行次数很好数吧？就是n次，因为每遍历一个数，就会执行一次累加操作，一共有n个数字，所以执行完这段代码需要进行n次累加运算，随着数据规模的扩大，也就是数字n的扩大，计算次数也在扩大，但是还是n次，所以可以用n来表示这段代码的时间复杂度，也就是O(n)。 利用等差数列公式计算区间[1,n]中所有整数的和 12int n = 10000;int sum = (1 + n) * n / 2; 等差数列的求和公式相信很多人都是知道的，那么这种计算方法对于遍历来说快了太多，因为它是直接计算出来的，也就是1次就能计算出结果，计算次数不会随着数据规模的扩大而扩大，那么这个算法的时间复杂度就是O(1)，也许有的人会纠结这里有加法、乘法、除法，不应该是3次运算吗？实际上就是3次运算，但是常数级的时间复杂度都会用O(1)来表示，它是一个衡量的指标，不需要精确到具体的次数，即使这个常数次数是1000也写成O(1)即可，所以通过这点可以看出，时间复杂度是O(1)的算法未必就比时间复杂度是O(n)的算法计算次数少，比如这个例子中，当n小于3时，第一种算法反而计算的次数少，但是时间复杂度通常是用来衡量数据规模很大的时候，算法所需时间的情况，所以通常情况下O(1)的算法在时间上还是优于O(n)的算法。 利用冒泡排序对所给数组进行排序 12345678910111213void bubble_sort(int array[], int n)&#123; for (int bubble_count = 0; bubble_count &lt; n - 1; ++bubble_count) &#123; for (int bubble_pos = 0; bubble_pos &lt; n - 1 - bubble_count; ++bubble_pos) &#123; if (array[bubble_pos] &gt; array[bubble_pos + 1]) &#123; swap_data(&amp;array[bubble_pos], &amp;array[bubble_pos + 1]); // 交换数据 &#125; &#125; &#125;&#125; 冒泡排序算是排序算法中规则比较简单的了，那么它的时间复杂度怎样来计算呢，或者说怎样来数它的执行次数呢，本例的执行操作次数指的是比较和交换，随着数据规模的扩大，也就只有这些操作次数是跟着变的，那么我们来数一数执行这些操作的次数，首先这是个双重循环，外层循环会遍历n-1次，随着外层循环增多内层循环次数会逐渐减少，听起来很麻烦的，外层和内层都在变化，怎么计算呢？其实可以回归本质，我们看看一共执行了多少次比较运算就可以，第一遍冒泡，内层循环执行了n-1次比较，第二遍冒泡，内层循环执行了n-2次比较，依次类推最后肯定是执行了1次比较，一共比较了多少次是不是就很好计算了，这些次数就是一个等差数列，求和就是这个算法的执行次数，f(n) = (1 + n - 1) * (n - 1) / 2 = n * (n - 1) / 2 = n^2 / 2 - n /2，根据时间复杂度定义，取最高次幂的项去掉系数就得到冒泡排序的时间复杂度O(n^2)。 计算一个十进制数的所有位上（个位、十位、百位…）上1的个数，例如12341这个数中包含2个1 123456789101112int count_one(int n)&#123; int count = 0; while(n &gt; 0) &#123; if ((n % 10) == 1) ++count; n /= 10; &#125; return count;&#125; 这也是一个很简单的算法，我们只要取出每一位上的数字，看看是不是1就可以，如果是1的话统计的变量count就加1就可以，那么这个算法的操作次数与数据规模n有什么关系呢，实际上这次数我们很清楚，就是n一共有几位数，就需要执行几次操作，当数字是34时，我们需要执行两次操作，当数字是24353的时候我们需要执行5次操作，那么怎么把这个次数表示成n的函数呢，仔细想想者原来就是对数，在本例中f(n) = (int)lg(n) + 1，也就是对n取10的对数，然后取整后加1，那么这个算法的时间复杂度就出来了，就是去掉常数项和修饰符变为O(lg(n))。 总结写这篇总结的初衷就是网上太多的算法直接给出了时间复杂度，而缺乏必要的说明，其实时间复杂度的计算并不算太复杂，只要你回归定义的本质，仔细的算算究竟需要多少次操作就可以得到大概的时间复杂度，并且一个算法的时间复杂度也不是固定的，比如快速排序，一般大家都喜欢说它是N乘以对数级的，但是当它的最坏情况发生时，它会退化成平方级的。]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>概念</tag>
        <tag>O(n)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bat批处理遍历指定目录下文件并更新]]></title>
    <url>%2Fblog%2F2018%2F03%2F21%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%E9%81%8D%E5%8E%86%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%87%E4%BB%B6%E5%B9%B6%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[前言先来看这样一个需求，假设有A，B两个目录，其中A目录是资源目录，B目录是工作目录，其中资源目录不定期更新，资源文件都在A目录下，并且没有子目录层级关系，但是资源要被使用时需要更新到B工作目录，B目录根据工程需要建立了一个包含众多子目录的层级体系，这样当A目录中的一个资源文件更新后，需要手动复制A目录中更新的资源文件，然后在B目录中找到对应的位置，然后进行粘贴替换，这样的操作如果很久一次、或者每次只有1、2个文件还好，如果资源文件大范围更新，那么要一个个找到更新文件在B目录中的位置然后替换就成了一件令人苦恼的事情，所以根据这个需求，才有了下面的探索过程。 思路的转变一开始想把A目录作为出发点，毕竟A目录中包含了修改后的资源文件，但是A目录更新后怎样才能准确的修改对应的B目录呢？我想到了配表，每次新增资源后，都会修改配置表，将A目录中的各个文件资源与B目录中的位置建立对应关系，这样A目录下的资源更新后就可以根据配置文件统一更新B目录了。 但这样的做法就是，需要经常维护配置文件，特别是增加或者删除资源的时候，然后我就想到了现在的这个做法，从B目录出发，注意本文主要解决的是资源文件的更新，而不是新增，更新就说明是原有的文件，只是内容发生了变化，比如一些UI文件，这些文件经常会做布局格式的调整，控件的增加和删除等等，调整结束后需要更新到工作目录。 实现过程实现的过程并没有想象的那么顺利，期间遇到了诸多问题和一些新的概念，比如for循环的语法，for循环中的变量定义，if条件的语法，字符串变量的替换，文件目录的处理，延迟环境变量扩展等等，这些问题每一项都可以作为一个单独的知识点，后续我会抽时间慢慢总结到一起，总之最后终于可以用了，前后大约花了1个半小时的时间，想想也是醉了，下面是一个具体的示例及对应的实现代码。 A资源目录对应实际的”E:/dirZ”，结构如下： 12345678910root:[E:/dirZ]+--aaa.txt+--bbb.txt+--ccc.txt+--ddd.txt+--eee.txt+--extra.c+--extra.h+--fff.txt+--ggg.txt B工作目录对应实际的”E:/dirA”，结构如下： 123456789101112131415root:[E:/dirA]+--aaa.txt+--bbb.txt+--dirB| +--ccc.txt| +--extra.c| +--extra.h+--dirC| +--ddd.txt| +--dirD| | +--eee.txt+--dirE| +--dirF| | +--fff.txt| | +--ggg.txt 现在需要把E:/dirZ目录中的txt文件，按照E:/dirA目录的层级结构，更新到对应位置，并且不更新ggg.txt文件，以下是实现的代码: 1234567891011121314151617181920212223242526272829303132333435@echo offrem 启用延迟环境变量扩展setlocal enabledelayedexpansionrem 定义不需要更新的文件SET EXCEPT_FILE=ggg.txtrem 定义工作目录和资源目录SET WORK_PATH=E:\dirA\SET RESO_PATH=E:\dirZ\rem 简单输出查看一下echo WORK_PATH is %WORK_PATH%echo RESO_PATH is %RESO_PATH%echo ------------------------rem for循环递归遍历WORK_PATH目录中的.txt文件，文件的全路径放在变量f中for /R %WORK_PATH% %%f in (*.txt) do ( rem 使用TARGET_FILE变量记录绝对文件名，注意延迟变量的使用 SET TARGET_FILE=%%f echo !TARGET_FILE! rem 去掉路径，只保留文件名及扩展名 SET "FILE_PATH_NO_EXT=%%~nxf" rem 利用资源路径和文件名，拼接出资源的绝对全路径 SET SOURCE_FILE=%RESO_PATH%!FILE_PATH_NO_EXT! echo !SOURCE_FILE! rem 条件判断是否是不需要更新的文件 if NOT !FILE_PATH_NO_EXT!==%EXCEPT_FILE% ( copy !SOURCE_FILE! !TARGET_FILE! ))pause 运行结果 123456789101112131415161718192021222324WORK_PATH is E:\dirA\RESO_PATH is E:\dirZ\------------------------E:\dirA\aaa.txtE:\dirZ\aaa.txt已复制 1 个文件。E:\dirA\bbb.txtE:\dirZ\bbb.txt已复制 1 个文件。E:\dirA\dirB\ccc.txtE:\dirZ\ccc.txt已复制 1 个文件。E:\dirA\dirC\ddd.txtE:\dirZ\ddd.txt已复制 1 个文件。E:\dirA\dirC\dirD\eee.txtE:\dirZ\eee.txt已复制 1 个文件。E:\dirA\dirE\dirF\fff.txtE:\dirZ\fff.txt已复制 1 个文件。E:\dirA\dirE\dirF\ggg.txtE:\dirZ\ggg.txt请按任意键继续. . . 总结到此为止我们就解决了这个资源更新的实际问题，每次资源更新后只要运行这个批处理文件就可以更新工作目录中对应的资源文件了，在这个例子中关于目录的截取，一开始走了很多弯路，其实有很多现成的方式，所以需要在此记录一下，方便以后查找使用，具体查看示例代码： 12345678910111213141516171819202122232425ECHO offSETlOCAL enabledelayedexpansion SET FIND_DIR=E:\dirA\dirC\dirDfor /R %FIND_DIR% %%f in (*.txt) do ( SET FULL_PATH=%%f ECHO 完整的路径: !FULL_PATH! SET FILE_DIR=%%~dpf ECHO 所在的目录: !FILE_DIR! SET FILE_NAME=%%~nf ECHO 无后缀文件: !FILE_NAME! SET FILE_EXT=%%~xf ECHO 文件名后缀: !FILE_EXT! SET "FILE_NAME_NOT_PATH=%%~nxf" ECHO 无路径文件: !FILE_NAME_NOT_PATH! SET "FULL_PATH_NOT_EXT=%%~dpnf" ECHO 无后缀全名: !FULL_PATH_NOT_EXT!)pause 运行结果： 123456完整的路径: E:\dirA\dirC\dirD\eee.txt所在的目录: E:\dirA\dirC\dirD\无后缀文件: eee文件名后缀: .txt无路径文件: eee.txt无后缀全名: E:\dirA\dirC\dirD\eee]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于游戏中仓库类的设计]]></title>
    <url>%2Fblog%2F2018%2F02%2F25%2F%E5%85%B3%E4%BA%8E%E6%B8%B8%E6%88%8F%E4%B8%AD%E4%BB%93%E5%BA%93%E7%B1%BB%E7%9A%84%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[前言这个游戏中的仓库类设计开始于春节前，和大家一样，我也是期盼着放假而无心工作，所以在放假前一天虽然蹦出了思维的火花，我却没有使用文字记录下来，但是大致的思路我已经记录到脑子中了，这一次的突然感悟，与上次突然明白什么是选择排序，什么是冒泡排序很类似，都是一瞬间突然明白，是一个从量变到质变的过程，接下来简单记录下我关于仓库的理解。 初觉不妥游戏中的仓库是用来存放道具的，这是我在接触这套游戏代码时形成的稳固的印象，结果就是代码中充斥着道具属性的判断，因为是很古老的代码，一开始我并没有产生疑问，同时也是修修补补的解决了许多BUG，可是渐渐的问题暴露了出来，设计上仓库里存储的是道具的索引，通过索引可以找到唯一的一个道具，这个思想根深蒂固，导致在写代码时自然而然的就在仓库的类里直接判断了道具属性，仔细想想这是不正确的。 起初感觉有问题时，大概是工作两年后，第一次重构道具系统的时候，当时在写放入道具和取出道具的时候总感觉怪怪的，但是又说不出问题出在哪里，其实就是在放入和取出的逻辑中，操作了道具的属性，修改了道具的坐标。也就是在仓库类的代码中设置了道具的属性，但是他们两个类不是依赖关系，硬生生的产生了依赖关系。 新的任务道具系统的第一次重构，我并没有找到为什么代码怪怪的，也就没有修改，但是新的任务在工作4年之后给了我一个新的机会，再写一遍道具系统，这时候那段奇怪的代码给我的感觉更强烈了，绝对有问题，也就是那么一瞬，我似乎明白了，仓库这个类被我们强加了太多的东西，谁说仓库中就一定要放道具了，我们在游戏中也没有直接把道具的对象保存在仓库中，而是把道具的索引存在了仓库中，也就是仓库中存储了道具的身份证，同理如果我们把人的身份中存在仓库中，那么仓库就是管理人的，如果我们把车牌号存储在仓库中，那么仓库就是管理车辆的。 因为起初游戏中的仓库只保存了道具的索引，所以我们想当然的认为仓库中只能保存道具，所以把一大堆的道具操作代码写到了仓库类中，是时候把代码提出来了，仓库就是仓库，它只根据坐标存储对应数据的ID，而这个ID对应的数据，应该在仓库以外的类中操作，这个ID可能对应道具、可能对应人、也可能对应车辆，干干净净的仓库管理了一组数据的ID，至于对ID对应数据的操作，一概不应该放在仓库类中进行。 重构的结果仓库类简简单单，保存着道具ID，只提供按位置放入ID，按位置取出ID，能够给出仓库的使用情况，能够初始化仓库的状态，仓库类有以上这些操作足以，仓库本身并不应该知道自己存的是道具还是车辆，真正要修改道具的属性，或者要查找指定属性的道具，放到道具管理类中来编写逻辑即可。]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>游戏</tag>
        <tag>架构设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim、Xshell、远程终端莫名卡死的原因]]></title>
    <url>%2Fblog%2F2018%2F02%2F03%2FVim%E3%80%81Xshell%E3%80%81%E8%BF%9C%E7%A8%8B%E7%BB%88%E7%AB%AF%E8%8E%AB%E5%90%8D%E5%8D%A1%E6%AD%BB%E7%9A%84%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[前言最近通过Xshell连接远程服务器，然后使用Vim修改文件时会莫名其妙的卡死，一开始我还没注意，因为近期的网络一直不太好，经常出现按下一个字母半天才反应过来的情况，所以我没有太在意，直接关闭终端重新打开就好。直到有一天我开着两个终端的时候，Vim又卡着不动了，而另一个终端还以流畅的处理我敲击的命令，我就断定这肯定不是网络原因了。 原因既然是Vim卡住了那就查查Vim本身有什么BUG吧，结果上网搜了一圈发现原来是远程终端的问题，根本就不关Vim的事，它只是躺着中枪了而已(*^▽^*)，实际上就是不小心按下了快捷键 Ctrl+S 导致的，为什么常常是Vim卡住呢？那是因为很多人习惯了在 Windows上 的保存快捷键，写写文档总是习惯性按下快捷键 Ctrl+S 保存一下，来避免程序突然崩溃导致文档丢失，这就解释了为什么出问题的总是Vim，因为使用Vim编辑文本有时会习惯性的按下 Ctrl+S 保存，而在执行Shell命令是很小的概率会按 Ctrl+S，所以大多数人卡住往往是在使用Vim的时候。 可是快捷键 Ctrl+S 为什么会导致终端卡死呢？实际上这个快捷键的含义是“阻断向终端输出内容”，很多人说这个快捷键的作用是暂停终端，我个人感觉这种说法并不准确，实际是上终端并没有暂停，按下 Ctrl+S快捷键后，你依然可以像终端发送命令，终端也会正常执行，只是不会将反馈内容和结果显示在终端上而已，这个特性可以用来暂停显示快速滚动输出的内容，比如在编译大型项目的时候。 解决办法解除这种状态的方法很简单，按下快捷键 Ctrl+Q 就可以“恢复向终端输出内容”，只是很多时候我们并不知道，以为是终端卡死了然后错杀了程序！ 附注关于这个问题，Vim文档中“SECTION 32 - VIM ON UNIX”一节也给出了回答，有兴趣的小伙伴可以自己看一下： 32.1. I am running Vim in a xterm. When I press the CTRL-S key, Vim freezes. What should I do now? vimdoc.sourceforge.net]]></content>
      <categories>
        <category>Vim</category>
      </categories>
      <tags>
        <tag>Vim</tag>
        <tag>Xshell</tag>
        <tag>终端卡死</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim中简单格式化代码]]></title>
    <url>%2Fblog%2F2018%2F02%2F02%2FVim%E4%B8%AD%E7%AE%80%E5%8D%95%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[前言写这个总结的起因是我在把Windows上VS中的代码粘贴在Linux服务器的Vim中时，代码格式惨不忍睹，我就搞不明白为什么它每一行都要向后缩进，搞得我的代码最后像倒立的楼梯似的，就像这样： 123456789int main(int argc, char *argv[])&#123; int i=0,j=0; if (i != 0) &#123;i++; &#125; else &#123;i--; &#125; for(i=0;i&lt;5;i++)j++; printf("i=%d,j=%d\n",i,j); return 0;&#125; 往常我一般就是切换到插入模式，然后使用删除键删除掉前面多余的空格，可是这一次我决定不再忍受了，我要找到快速格式化的方法，还别说，方法其实很简单，各种格式化方法的核心就是符号=。 何谓“简单”其实一开始我想标题的时候并没有加上“简单”二字，直到我发现了一个求知者看似“无理”的要求，他要求在Vim中把上面格式的代码格式化成下面这样： 123456789101112131415161718int main(int argc, char *argv[])&#123; int i=0,j=0; if (i != 0) &#123; i++; &#125; else &#123; i--; &#125; for(i=0;i&lt;5;i++) j++; printf("i=%d,j=%d\n",i,j); return 0;&#125; 如果你也是这样想的，很抱歉，你可以关掉这个页面了，本文提供的方法无法满足你的要求，这就是我的标题中为什么加上了“简单”二字，而Vim中的简单格式化只能是格式化成下面这样，以行为单位，保证每行的缩进都是正确的： 123456789int main(int argc, char *argv[])&#123; int i=0,j=0; if (i != 0) &#123;i++; &#125; else &#123;i--; &#125; for(i=0;i&lt;5;i++)j++; printf("i=%d,j=%d\n",i,j); return 0;&#125; 如果你确实要把大括号的换行也显示正确，那么只能通过安装插件、编写脚本、或者把源代码中对应的位置敲如回车，变成下面这样格式的代码，然后再使用本文后面叙述的方法来格式化就可以了。 1234567891011121314151617int main(int argc, char *argv[])&#123; int i=0,j=0; if (i != 0) &#123; i++; &#125; else &#123; i--; &#125; for(i=0;i&lt;5;i++) j++; printf("i=%d,j=%d\n",i,j); return 0;&#125; 神奇的=其实格式化的核心内容就是这个 =，其中绝大部分的方法都是 = 的变种，只是让人不容易发觉，甚至有些方法例如 gg=G 包装的让人都无法注意到真正起作用的就是那个 =，格式化的前提是处于命令模式，也就是按完 ESC 时的模式，而格式化时 = 真正起作用的只有两种情况： 先按=，再选区域 先说应用最广泛的全文格式化的方法gg=G，就是这种情况的变种，分析一下命令的含义，先是gg表示回到文档最开始，= 表示要格式化，G 表示到文档末尾，也就是说 gg=G 的含义就是： 跳到文档开头-&gt;开始格式化-&gt;一直格式化到文档末尾 既然明白了原理，假如此时光标就在文档开始处，那么使用命令 =G 也是可以格式化全文的，同理命令 G=gg也可以达到格式化全文的效果，而命令 =100j 就是从文档当前位置向下格式化100行。 先选区域，再按= 这种方式我反正用不习惯，不过也说一下，就是先按 v (可视化编辑)或 shift+v (可视化编辑行模式)或 ctrl+v (可视化编辑块模式)，然后利用方向键 h,j,k,l 选择区域，最后按 = 完成格式化，简单操作例如 vjjj= 就是从当前位置向下格式化3行代码。 直接输入== 不是说两种情况吗，为什么会有第3条呢？其实在命令模式下输入 == ，也就是连着输入两个等号，就是格式化当前行的方法，我感觉它和上两种情况一样，可能是又不知道归入哪一种情况比较好，所以就单列出来咯。 总结 本文中所提到的格式化代码只是很简单的格式化，以行为单位保证缩进正常，无法处理大括号换行等情况。 如果要挑起“大括号换行”的战争，麻烦装一个格式化插件吧，Vim只和Emacs打架，不想参与“大括号换行”战争。 如果要部分格式化，首先保证要格式化的代码之前的内容是格式化好的，否则格式化无效，请选择全文格式化吧！]]></content>
      <categories>
        <category>Vim</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>Vim</tag>
        <tag>代码格式化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql查询时case when语句的使用]]></title>
    <url>%2Fblog%2F2018%2F02%2F01%2FMysql%E6%9F%A5%E8%AF%A2%E6%97%B6case-when%E8%AF%AD%E5%8F%A5%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言前几天在技术论坛论坛上发现一个求助帖，大体意思就是要把一个表中的数据按条件分成两类，每一类排序方式不同，然后整体作为查询的结果集，乍一看这问题不是很难，很多人给出的答案是分别查询排序后再 union合并到一起，但是后来楼主明确指出不想使用 union 操作，这时有一位高人巧用 case when 语句解决了问题，其实这是我第一次接触 case when 语句，于是查询了一下具体用法，在此做个小结，方便日后查询使用。 创建示例表格数据库表格结构很简单，马上要期末了，就以学习成绩为数据来建立一张数据表，表中包含唯一ID、学号、姓名、性别、分数等列，其中性别这一列用整数代表，0表示男，1表示女，建立表格的sql语句如下： 123456789CREATE TABLE `grade` ( `id` int(4) NOT NULL AUTO_INCREMENT, `number` int(4) NOT NULL DEFAULT '0', `name` varbinary(32) NOT NULL DEFAULT '', `sex` int(4) NOT NULL DEFAULT '0', `score` int(4) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `find_index` (`number`,`name`)) ENGINE=InnoDB DEFAULT CHARSET=binary ROW_FORMAT=DYNAMIC; 插入测试数据表格建立完成可以使用可视化工具或者insert语句插入测试数据，插入测试数据后查询结果如下： 123456789101112131415161718mysql&gt; select number,name,sex,score from grade;+----------+----------+-----+-------+| number | name | sex | score |+----------+----------+-----+-------+| 20180001 | xiaoming | 0 | 68 || 20180002 | xiaohong | 1 | 98 || 20180003 | xiaobing | 0 | 78 || 20180004 | xiaoli | 0 | 88 || 20180005 | zhangsan | 0 | 32 || 20180006 | zhaosi | 0 | 58 || 20180007 | marry | 1 | 78 || 20180008 | tom | 0 | 100 || 20180009 | feifei | 1 | 90 || 20180010 | lili | 1 | 92 || 20180011 | xiaozhao | 0 | 52 || 20180012 | xiaowang | 0 | 62 |+----------+----------+-----+-------+12 rows in set (0.00 sec) 获取平均成绩班主任们坐在一起做喜欢做的事就是比一下自己的学生和别人班的差距，谁让他们每个人带的学生都是一届不如一届呢！（你们是我带过的学生中最差的一届！！！）说到比成绩一般都是比较并均分，sql语句可能会写成下面这样： 1234567mysql&gt; select avg(score) as 平均分 from grade;+-----------+| 平均分 |+-----------+| 74.6667 |+-----------+1 row in set (0.02 sec) 是的，很简单就能获得班级的平均分，如果要分组呢？比如分别查一下男生和女生的平均分，因为我们知道表中的sex表示性别，所以直接按照sex分组就可以实现，可以将语句简单写成这样： 12345678mysql&gt; select sex as 性别, avg(score) as 平均分 from grade group by sex;+--------+-----------+| 性别 | 平均分 |+--------+-----------+| 0 | 67.2500 || 1 | 89.5000 |+--------+-----------+2 rows in set (0.00 sec) 是不是很简单？可是性别显示成0和1确实不利于阅读，但是表中又没有保存0、1与男、女的对应关系，应该怎么办呢？这就要用到我们今天所要用到的case when语句了，语法上共有两种写法，看着具体例子体会一下吧。 case when 语句的使用 第一种用法：case后面跟列名，when后面跟对应值 12345CASE case_value WHEN when_value THEN statement_list [WHEN when_value THEN statement_list] ... [ELSE statement_list]END 这种用法正好解决我们刚刚提出的问题，当sex值为0时当前列显示“男”，否则显示“女”，sql写法如下： 123456789mysql&gt; select (case sex when 0 then '男' else '女' end) as 性别, avg(score) as 平均分 -&gt; from grade group by sex;+--------+-----------+| 性别 | 平均分 |+--------+-----------+| 男 | 67.2500 || 女 | 89.5000 |+--------+-----------+2 rows in set (0.00 sec) 第二种用法：case后面空白，when后面跟着判断条件 12345CASE WHEN search_condition THEN statement_list [WHEN search_condition THEN statement_list] ... [ELSE statement_list]END 针对于这种写法，我们考虑这样一种需求，学生成绩是有评分的，大于等于90分的学生是A，小于90分大于等于60分的学生是B， 其余的学生是C，现在要查询评分为A、B、C的学生成绩的平均分分别是多少，因为成绩评分并不是单独的一列，所以不能简单的 使用 group by 来分组实现了，但是可以利用 case when 语句实现，写起来也很简单，看看下面的sql语句就知道了！ 12345678910mysql&gt; select (case when score &gt;= 90 then 'A' when score &lt; 60 then 'C' else 'B' end) as 等级, -&gt; avg(score) as 平均分 from grade group by 等级;+--------+-----------+| 等级 | 平均分 |+--------+-----------+| A | 95.0000 || B | 74.8000 || C | 47.3333 |+--------+-----------+3 rows in set (0.00 sec) 总结 case when 语句共有两种写法，使用时要区别两种用法的差异。 使用 case when 语句可以实现修改数值的对应关系，还可以按照复杂的条件进行分组。 关于 case when 语句的详细用法，有兴趣的同学可以参考一下官方文档：13.6.5.1 CASE Syntax]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>分组查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql验证联合索引的最左原则]]></title>
    <url>%2Fblog%2F2018%2F01%2F29%2FMysql%E9%AA%8C%E8%AF%81%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95%E7%9A%84%E6%9C%80%E5%B7%A6%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[前言如果你接触过数据库，应该听说过在数据库表的某些列上建立索引能够加快查找速度，如果研究更深入一点的人，可能还听说过联合索引，那么索引为什么能够加快查找速度呢？联合索引究竟又是什么呢？下面说说我的简单理解。 索引试想一下，把1~10000这1万个数字打乱顺序存储在数组中，如果要找到5000这个数字在哪，那就得从数组第0个元素开始，依次遍历找到5000这个数字所在的位置，运气好了1次就能找到，运气不好需要查询1万个数，可是如果把这1万个数作为map的key，每个数存在数组中的位置作为value，存储在map结构中很快就能找到，通常情况下要比直接遍历快的多。 其实这里的map充当的是一个索引的作用，我们知道map存储数据时使用树形结构，会根据要查找的值和当前节点比较，来确定继续查找左分支还是右分支，而数据库中的索引充当的也是这样的作用，mysql中的索引是BTree结构（多路搜索树），就是利用建立索引的列中的所有值建立了一棵树，通过有序的树形查找一般要比全局搜索快多了吧！ 联合索引简单了解了一下索引的含义，那么什么是联合索引呢？其实mysql数据库中的索引不止可以建立在一个列上，它可以将一个索引同时建立在说多个列上，也就是我们所说的联合索引，联合索引的作用特别大，有时会超过单列索引，至于什么时候建立单列索引，什么时候建立联合索引同样是个很复杂的问题，在此不做描述。有兴趣的读者可以自行搜索一下。 最左原则当你在多个列上建立一个索引时，怎样的查找才能利用索引加快速度呢？说到这我们先建立一个带有索引的表格，具体的分析一下什么叫做索引的最左原则。 123456789CREATE TABLE IF NOT EXISTS `test_index`( `id` int(4) NOT NULL AUTO_INCREMENT, `a` int(4) NOT NULL DEFAULT '0', `b` int(4) NOT NULL DEFAULT '0', `c` int(4) NOT NULL DEFAULT '0', `data` int(4) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `union_index` (`a`,`b`,`c`))ENGINE=InnoDB ROW_FORMAT=DYNAMIC DEFAULT CHARSET=binary; 分析上述建表语句，创建了一个名为test_index 的数据库表格，然后在a、b、c三列上建立了联合索引，索引名字为union_index，而最左原则指的就是当你建立了这样一个索引的时候，等于建立了(a)、 (a,b)、 (a,b,c)三个索引，通过条件 (a), (a,b), (a,b,c) 这三种条件查询的时候都可以利用索引加快速度，所以在建立索引的时候要把最常用的条件列放到联合索引的最左边，接下来我们来验证一下，工具就是mysql自带的explain命令。 测试版本 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 13Server version: 5.7.21-log MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. 验证过程 首先以列a作为条件查询数据，我们看到 type: ref 表示引用查找， key_len: 4 表示索引长度为4，也就说明利用了索引来进行查找 123456789101112131415mysql&gt; explain select data from test_index where a = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.01 sec) 然后以列b作为条件查询数据，可以看到type: ALL表示全表查找， key_len: NULL 表示没有索引，也就说明如果只使用b作为查询条件，不能利用索引来加快查找速度 123456789101112131415mysql&gt; explain select data from test_index where b = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 接着以列c作为条件查询数据，可以看到type: ALL表示全表查找， key_len: NULL 表示没有索引，情况与用b作为条件一样，只使用c作为查询条件也不能利用索引来加快查找速度 123456789101112131415mysql&gt; explain select data from test_index where c = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 现在来测一下使用a、b作为条件的情况，我们看到 type: ref 表示引用查找， key_len: 8 表示索引长度为8，也就是说我们利用上了a、b联合索引来进行查找 123456789101112131415mysql&gt; explain select data from test_index where a = 1 and b = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 紧接着来测一下使用a、c作为条件的情况，我们看到 type: ref 表示引用查找， key_len: 4 表示索引长度为4，这就奇怪了，按照最左原则来说，a、c上是不会建立索引的，为什么会有索引长度呢？其实与a、b上的索引一比较我们就能发现，a、c上的索引长度只有4，而且单独的c上是没有索引的，所以4字节长度的索引只能是a上的，也就是说这种情况我们只使用了a列上的索引来进行查找 123456789101112131415mysql&gt; explain select data from test_index where a = 1 and c = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 为了进一步验证上面的想法，这一次测一下使用b、c作为条件的情况，我们看到 type: ALL 表示全表查找， key_len: NULL 表示没有索引可以使用，按照最左原则来说，b列上没有索引，c列上也没有索引，同时b、c的上也不存在联合索引，所以使用b、c作为查询条件时无法利用联合索引 123456789101112131415mysql&gt; explain select data from test_index where b = 1 and c = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 1.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 测试完两个条件的情况，接下来测试一下使用a、b、c作为条件的情况，我们看到 type: ref 表示引用查找， key_len: 12 表示索引长度为12，这完全符合联合索引的最左原则，同时使用3个条件查询可以利用联合索引 123456789101112131415mysql&gt; explain select data from test_index where a = 1 and b = 1 and c = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 12 ref: const,const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 测试进行到现在，我测试了所有的情况吗？不是的！还可以颠倒顺序啊，我原来一直以为联合索引是有顺序的，结果测试后才发现，利用索引的条件符合“交换律”，也就是下面这种情况也能利用a、b上的联合索引，索引长度为8 123456789101112131415mysql&gt; explain select data from test_index where b = 1 and a = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 再来试试这种情况，按照最左原则，c上没有建立索引，a上有索引，c、a没有建立联合索引，所以只能使用a上的索引进行查找，结果索引长度只有4，验证了我们的想法，联合查询条件使用索引时满足“交换律” 123456789101112131415mysql&gt; explain select data from test_index where c = 1 and a = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 接下来几种交换顺序的情况(c,b)、(a,c,b)、(c,b,a)等，大家可以自己进行验证，到此为止，mysql联合索引的最左原则也就验证结束了！ 总结 联合索引的最左原则就是建立索引KEY union_index (a,b,c)时，等于建立了(a)、(a,b)、(a,b,c)三个索引，从形式上看就是索引向左侧聚集，所以叫做最左原则，因此最常用的条件应该放到联合索引的组左侧。 利用联合索引加速查询时，联合查询条件符合“交换律”，也就是where a = 1 and b = 1 等价于 where b = 1 and a = 1，这两种写法都能利用索引KEY union_index (a,b,c)。 遇到这种不确定的问题还是需要实际测试一下，简单的调整一下索引顺序可能会极大的提升效率哦！]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>实用工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python统计指定号码的历史中奖情况]]></title>
    <url>%2Fblog%2F2018%2F01%2F11%2FPython%E7%BB%9F%E8%AE%A1%E6%8C%87%E5%AE%9A%E5%8F%B7%E7%A0%81%E7%9A%84%E5%8E%86%E5%8F%B2%E4%B8%AD%E5%A5%96%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[前言生活在寸土寸金的帝都，买房变成了一件可遇而不可求的事情，作为一个朝九晚九挣工资的人来说，买房或许只是出现在梦里，但是梦想总是要有的，万一实现了呢？其实真正赚钱的方式都明明白白地写在了刑法里，可是咱可是本分人，不能做哪些违法的事情，想要赚钱买房，还有一种比较随意的方式——买彩票，如今这帝都的房价，不是你中一张500万彩票就能买得起的，如果要买的起，那就需要中两张！ 经历中两张500万的彩票，想想就知道这种概率有多大了，跟你被天上掉下的陨石砸中脑袋差不多，是不是感觉没有希望了，不过不要灰心嘛，万一你要被砸中了呢？ 对于预测彩票号码这件事情，很抱歉，我不是神仙，没有方法可以办到，不过有件事你需要知道一下：那就是双色球自从问世以来，十几年间从未出现过一次号码完全相同的情况，也就是说你买个和历史号码一样的，基本中不了大奖了，但是也不一定，万一就有两颗陨石同时砸中你呢! 对于买彩票这件事，有的人喜欢买一个号，坚持多年从不动摇，一心做着发财梦；而有的人却是很随意，每天机选，靠天吃饭；还有一部分大神就比较高端了，每天窝在彩票投注站里，写写画画，仿佛可以窥探天机一样，每期少则几十，多则几百的砸着自己的血汗钱，我劝你们还是醒醒吧。 而我呢，就属于半个第一种人，坚持着一个号，做着发财梦，偶然间看到彩票投注站就买一张，遇不到就算了，典型的佛系彩票购买者，这样买了一段时间，忽然有个想法，我买的这个号到底在历史上中没中过大奖呢？于是作为程序猿的我决定写个程序查一下不就好了，所以才有了下面这段代码。 代码 引入库函数，其实需要的函数特别简单，就是要处理csv格式的双色球历史开奖数据。 1import csv 定义奖项，也就是中奖号码情况与奖项的对应关系，注意中一个篮球是6等奖哟！ 1234567891011121314151617# list award classifyaward_classify = &#123; (6,1): 1, (6,0): 2, (5,1): 3, (5,0): 4, (4,1): 4, (4,0): 5, (3,1): 5, (3,0): 0, (2,1): 6, (2,0): 0, (1,1): 6, (1,0): 0, (0,1): 6, (0,0): 0&#125; 定义比对函数，这个函数要能判断出我的号码跟一个历史号码相比，中了几个红球和蓝球。 123456789101112131415161718# count red balls and blue ballsdef count_red_blue_balls(my_number, history_number): red_count, blue_count = 0, 0 my_index, history_index = 0, 0 while my_index &lt; 6 and history_index &lt; 6: if my_number[my_index] == history_number[history_index]: my_index += 1 history_index += 1 red_count += 1 elif my_number[my_index] &lt; history_number[history_index]: my_index += 1 else: history_index += 1 if my_number[6] == history_number[6]: blue_count = 1 return red_count, blue_count 查询历史中奖情况，使用我选择的号码和历史开奖情况逐一比对，得到每个奖项中奖次数。 123456789# count award situation of my numberdef count_award_situation(my_number): local_award_statistics = [0,0,0,0,0,0,0] with open('lottery_history_data.csv', 'r') as file: data_content = csv.reader(file) for row_data in data_content: local_award_statistics[award_classify[count_red_blue_balls(my_number, list(map(int, row_data[1:8])))]] += 1 return local_award_statistics 展示查询结果，将统计结果以友好的方式呈现。 1234567# show award statictics for a numberdef show_award_result(my_number, award_statistics): print("my number is %s\n" % my_number) print("history award record list:") for index in range(0,7): print("award %d: %4d times" % (index, award_statistics[index])) 启动函数，读取自己定义的号码，然后进行统计 123456789# main functionif __name__ == '__main__': #my_number = [5,6,10,11,25,30,11] #my_number = [5,8,10,15,26,30,6] print("请输入6个红球和1个蓝球号码，空格分隔：") my_number = list(map(int, input().split())) award_statistics = count_award_situation(my_number) show_award_result(my_number, award_statistics) 运行结果 总结 这段代码只是一时好奇的产物，所以说好奇心带来了生产力。 统计代码只是做简单使用，所以一些特殊情况并未判断，比如输入字母或者未排序的数字。 看到我的号码连个4等奖都没有中过，不知道是该高兴还是难过，是不是这个大奖在等着我啊！ 说到这也该结束了，不好意思上周六(20180113)买的双色球又中了6等奖，明天(20180121)要去领奖喽！ 源码代码传送门(附双色球历史数据)：一触即达]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python查找文件中包含中文的行]]></title>
    <url>%2Fblog%2F2018%2F01%2F06%2FPython%E6%9F%A5%E6%89%BE%E6%96%87%E4%BB%B6%E4%B8%AD%E5%8C%85%E5%90%AB%E4%B8%AD%E6%96%87%E7%9A%84%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[前言近几天在做多语言版本的时候再次发现，区分各种语言真的是一件比较困难的事情，上一次做中文提取工具的就花了不少时间，这次决定用python试一试，结果写起来发现真是方便不少，自己整理了一下方便以后查找使用。 代码123456789101112131415161718192021222324#!/usr/bin/env python3# -*- coding: utf-8 -*-# find the line of containing chinese in files__author__ = 'AlbertS'import redef start_find_chinese(): find_count = 0; with open('ko_untranslated.txt', 'wb') as outfile: with open('source_ko.txt', 'rb') as infile: while True: content = infile.readline() if re.match(r'(.*[\u4E00-\u9FA5]+)|([\u4E00-\u9FA5]+.*)', content.decode('utf-8')): outfile.write(content) find_count += 1; if not content: return find_count# start to findif __name__ == '__main__': count = start_find_chinese() print("find complete! count =", count) 文件 输入：source_ko.txt 3 캐릭터 Lv.50 달성8 캐릭터 Lv.80 달성10 캐릭터 Lv.90 달성……2840 飞黄腾达4841 同归于尽8848 캐릭터 Lv.50 달 输出：ko_untranslated.txt 2840 飞黄腾达4841 同归于尽 总结 其实这段小小的代码中包含了两个常用的功能，那就是读写文件和正则表达式。 这也是两个重要的知识点，其中with操作可能防止资源泄漏，操作起来更加方便。 正则表达式可是一个文字处理的利器，代码中的正则可能还不太完善，后续我会继续补充更新。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2018%2F01%2F05%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy Picture Watermark1?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9hbGJlcnRnaXRodWJob21lLmdpdGh1Yi5pby9ibG9nL2Fib3V0,size_18,color_FFFFFF,t_70#pic_center) Fans aticlehttps://blog.csdn.net/albertsh/article/details/100594143 https://blog.csdn.net/albertsh/article/details/100540338 !-- https://blog.csdn.net/albertsh/article/details/52788106 https://blog.csdn.net/albertsh/article/details/52797519 --https://blog.csdn.net/albertsh/article/details/82286999 https://blog.csdn.net/albertsh/article/details/90736859 Blog Record博客记录QQ：347070901微信公众号：写代码的苏东坡===============================我的Github：AlbertGithubHome===============================刚刚起步 2013-10-05 08:39 写了第一篇转载博文 2015-11-19 11:20 收到第一条博客评论 2016-12-27 20:18 翻译第一篇英文资料 2017-03-27 09:55 访问第一次突破三万 2017-05-21 09:25 积分第一回到达一仟 2017-05-21 09:25 等级第一次满足四级 2017-09-14 21:00 版式第一次不让更改 2017-11-14 11:59 排名第一次有了数字 2018-07-16 19:39 访问第一次达二十万2018-12-08 10:41 访问第一回破三十万2019-06-22 18:30 访问第一次破四十万2019-09-24 17:04 博客嗖一下到了六级2020-01-02 15:55 访问第一回达五十万2020-02-17 20:17 粉丝第一回超过千人2020-06-20 12:09 评论第一次跨越千次2020-06-25 16:31 等级飞一般来到七级2020-07-31 11:05 点赞第一次达到千人2020-08-09 20:48 积分第一回突破一万2021-01-27 23:46 访问嗡一下到了百万2021-08-27 10:21 粉丝刷一下到破一万 imgroot More info: Deployment]]></content>
  </entry>
</search>
