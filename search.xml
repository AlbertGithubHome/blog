<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[因为glibc版本不兼容不得已我重装了Dell R620的系统]]></title>
    <url>%2Fblog%2F2024%2F07%2F20%2F%E5%9B%A0%E4%B8%BAglibc%E7%89%88%E6%9C%AC%E4%B8%8D%E5%85%BC%E5%AE%B9%E4%B8%8D%E5%BE%97%E5%B7%B2%E6%88%91%E9%87%8D%E8%A3%85%E4%BA%86Dell-R620%E7%9A%84%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[前言我总以为这件事发生很久了，但刚刚去翻了聊天记录居然是上周发生的事情，趁着还没忘干净赶紧记录一下，对于服务器安装系统来说这还是第一次。 好久很久以前（大概是1个月前），我找运维装了一台服务器Dell PowerEdge R620，当时说好是Ubuntu20.04版本，但是装完后我也没注意，期间发生了很多事情，终于到上周的时候开始在这台机器上搭建Jenkins打包环境，整个过程还是比较顺利的，但是发现打包机上编译的程序版本无法在本地环境下运行，这时候仔细看才发现，原来运维安装的系统是 Ubuntu22.04，也不知道是因为粗心还是大意，不能运行的原因就是打包机上的glibc版本过高，本地环境无法支持，但这却引起了我的疑惑。 glibc版本和常用命令先附一个Linux发行版本和默认glibc版本对应关系的图表 Ubuntu Debian CentOS Glibc 22.04 - - 2.34 20.04 11 - 2.31 - 10 8 2.28 18.04 - - 2.27 - 9 - 2.24 16.04 - - 2.23 14.04 8 - 2.19 13.04 - 7 2.17 12.04 - - 2.15 - 7 - 2.13 - - 6 2.12 查询glibc版本的命令为 1ldd --version 查询动态库和可执行文件中的符号版本 123objdump -T my-program-or-so | grep GLIBC_strings /lib/x86_64-linux-gnu/libc.so.6 | grep GLIBC 查询一个目录下所有动态库中使用符号的glibc版本 1ls | grep '.so' | xargs objdump -T | grep GLIBC_ &gt; /tmp/symbol.log glibc版本差异的疑问目前我维护了两个项目，对于编译环境和运行环境不一样的问题，两个项目中都存在，但是A项目没问题，但是B项目有问题，具体的情况如下： A项目C++11编写，打包机Ubuntu16.04，glibc版本2.23，运行环境有CentOS7和Ubuntu16.04，相同的系统大概率没问题，而CentOS7的glibc是2.17版本，也没有问题，这让我很奇怪 打包机编译环境12345678910111213141516171819root@jenkins1604:~# cat /etc/os-releaseNAME="Ubuntu"VERSION="16.04.2 LTS (Xenial Xerus)"ID=ubuntuID_LIKE=debianPRETTY_NAME="Ubuntu 16.04.2 LTS"VERSION_ID="16.04"HOME_URL="http://www.ubuntu.com/"SUPPORT_URL="http://help.ubuntu.com/"BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"VERSION_CODENAME=xenialUBUNTU_CODENAME=xenialroot@jenkins1604:~# ldd --versionldd (Ubuntu GLIBC 2.23-0ubuntu5) 2.23Copyright (C) 2016 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.Written by Roland McGrath and Ulrich Drepper.root@jenkins1604:~# 运行环境11234567891011121314151617181920$ cat /etc/os-releaseNAME="Ubuntu"VERSION="16.04.6 LTS (Xenial Xerus)"ID=ubuntuID_LIKE=debianPRETTY_NAME="Ubuntu 16.04.6 LTS"VERSION_ID="16.04"HOME_URL="http://www.ubuntu.com/"SUPPORT_URL="http://help.ubuntu.com/"BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"VERSION_CODENAME=xenialUBUNTU_CODENAME=xenial# shz @ 20200116GF in /mnt/e/linux on git:7bd9be2 x [9:47:34]$ ldd --versionldd (Ubuntu GLIBC 2.23-0ubuntu11) 2.23Copyright (C) 2016 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.Written by Roland McGrath and Ulrich Drepper. 运行环境2123456789101112131415161718192021222324[root@hk-dev Debug]# cat /etc/os-releaseNAME="CentOS Linux"VERSION="7 (Core)"ID="centos"ID_LIKE="rhel fedora"VERSION_ID="7"PRETTY_NAME="CentOS Linux 7 (Core)"ANSI_COLOR="0;31"CPE_NAME="cpe:/o:centos:centos:7"HOME_URL="https://www.centos.org/"BUG_REPORT_URL="https://bugs.centos.org/"CENTOS_MANTISBT_PROJECT="CentOS-7"CENTOS_MANTISBT_PROJECT_VERSION="7"REDHAT_SUPPORT_PRODUCT="centos"REDHAT_SUPPORT_PRODUCT_VERSION="7"You have mail in /var/spool/mail/root[root@hk-dev Debug]# ldd --versionldd (GNU libc) 2.17Copyright (C) 2012 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.Written by Roland McGrath and Ulrich Drepper. B项目C++20编写，原打包机Ubuntu20.04，glibc版本2.31，运行环境有Ubuntu20.04，相同的系统没出问题，后来因打包机替换系统升到Ubuntu22.04后，glibc版本为2.34，再到Ubuntu20.04系统上运行就报错了 ./server: /lib/x86_64-linux-gnu/libc.so.6: version GLIBC_2.34&#39; not found (required by ./server) 看起来是glibc版本问题导致的 原打包机编译环境12345678910111213141516171819root@ubuntu2004:~# cat /etc/os-releaseNAME="Ubuntu"VERSION="20.04.2 LTS (Focal Fossa)"ID=ubuntuID_LIKE=debianPRETTY_NAME="Ubuntu 20.04.2 LTS"VERSION_ID="20.04"HOME_URL="https://www.ubuntu.com/"SUPPORT_URL="https://help.ubuntu.com/"BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"VERSION_CODENAME=focalUBUNTU_CODENAME=focalroot@ubuntu2004:~# ldd --versionldd (Ubuntu GLIBC 2.31-0ubuntu9.16) 2.31Copyright (C) 2020 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.Written by Roland McGrath and Ulrich Drepper. 新打包编译环境环境 Ubuntu22.04 glibc 2.34 运行环境123456789101112131415161718192021$ cat /etc/os-releaseNAME="Ubuntu"VERSION="20.04.2 LTS (Focal Fossa)"ID=ubuntuID_LIKE=debianPRETTY_NAME="Ubuntu 20.04.2 LTS"VERSION_ID="20.04"HOME_URL="https://www.ubuntu.com/"SUPPORT_URL="https://help.ubuntu.com/"BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"VERSION_CODENAME=focalUBUNTU_CODENAME=focal# shzhen @ 20200116GF in /mnt/c/Users/Administrator [20:42:28]$ ldd --versionldd (Ubuntu GLIBC 2.31-0ubuntu9.2) 2.31Copyright (C) 2020 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.Written by Roland McGrath and Ulrich Drepper. 问题疑惑同样是高版本glibc的机器上编译，在低版本的glibc上运行，为什么A项目没问题，但是B项目出了问题呢，而且A项目的glibc版本2.23–&gt;2.17相差的还多，B项目的glibc版本2.34–&gt;2.31相差的还少，出问题的却是B项目，分析原因glibc应该是向下兼容的，高编译低运行本就不可行，属于正常现象，如果想实现这样运行，就要保证调用的符号在两个版本的库中没有变化，所以我查了A项目和B项目的符号表，发现： A项目中符号最高的版本是GLIBC_2.17，所以即使“高编译低运行”也是满足条件的 B项目中符号最高的版本是GLIBC_2.34，所以不能在glibc.2.31的Ubuntu20.04上运行 但B项目中使用高版本符号的就有几个，剩下的都低于2.29版本，理论上只要去掉这几个符号的引用就可以在Ubuntu20.04上运行，但是为了避免后续的麻烦，我还是把打包编译的机器重新做了系统Ubuntu20.04 123456789101112131415160000000000000000 DF *UND* 0000000000000000 (GLIBC_2.33) stat0000000000000000 w DF *UND* 0000000000000000 (GLIBC_2.34) pthread_once0000000000000000 w DF *UND* 0000000000000000 (GLIBC_2.34) __pthread_key_create0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) sem_trywait0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) dladdr0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) sem_wait0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) sem_init0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) sem_timedwait0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) sem_destroy0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) dlclose0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) dlsym0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) dlerror0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) dlopen0000000000000000 w DF *UND* 0000000000000000 (GLIBC_2.34) pthread_once0000000000000000 w DF *UND* 0000000000000000 (GLIBC_2.34) __pthread_key_create0000000000000000 DF *UND* 0000000000000000 (GLIBC_2.34) dladdr 将服务器系统改为Ubuntu20.04对于重装系统我内心是拒绝的，前两天刚安装了一个PC电脑的windows系统，再往前推都好几年没装过了，不想来回折腾，再者说PC电脑安装Windows很熟悉，即使有问题网上搜一下也解决了，但是服务器安装系统之前确实没做过，并且还有一个我不愿意安装服务器系统的原因，就是这些服务器的硬盘配置的是Raid5磁盘阵列，因为不熟悉我真怕给它做坏了。 所以对于改为Ubuntu20.04系统这件事，我首先想到的就是将系统降级，但是尝试了一圈没有成功之后，我不得不硬着头皮重装系统了，当然在重装之前还查了Raid5相关信息，发现硬盘配置有专门的Raid卡控制，与操作系统没有关系，只要安装的时候不要把系统安装到原来的数据盘就不会影响原来的数据信息，单纯对做系统来说，硬盘可以当成普通的硬盘来看待，安装时选对盘符就行了。 开始安装安装教程可以参考《Dell服务器安装Ubuntu系统》、《dell r620安装Ubuntu全过程》 镜像地址参考 官方镜像下载、清华镜像，注意要选择服务器版本的，不需要带桌面那一套东西，体积会小一点，性能会好一点 首先是做系统启动盘，按照教程选择了UltraISO，之前用这个比较多，本来一直点下一步就行了，但是无论选择官方镜像还是清华的镜像，在制作的时候都会提示一个可能无法引导系统的警告，但最后是成功的，我就用它试了试 把U盘插到服务器上，开机按F11进入System BOIS设置，把U盘调到第一启动项，重启后进入U盘拷贝的进度条，但总是无法正确安装，还多次进入了 Emergency mode，重启关机都不好使，最后不得不强行关机了 然后我换成了使用Rufus来烧制启动U盘，这里涉及到几种选项，比如选BOIS还是UEFI，文件系统选GPT还是MBR，提供一个详细的步骤 运行 Rufus：打开 Rufus 应用程序（无需安装）。 选择设备：在 “Device” 下拉菜单中选择你的 U 盘。 选择引导类型：点击 “Boot selection” 旁边的 “Select” 按钮，选择你下载的 Ubuntu ISO 文件。 分区方案： 如果 Dell R620 使用 UEFI 固件模式，选择 “GPT” 作为分区方案。 如果你希望使用传统的 BIOS 模式，选择 “MBR” 作为分区方案。 目标系统： 如果选择 “GPT” 分区方案，目标系统应自动设置为 “UEFI (non CSM)”。 如果选择 “MBR” 分区方案，目标系统应自动设置为 “BIOS or UEFI”. 文件系统：选择 “FAT32”。 卷标：可以给你的 U 盘设置一个名称，比如 “Ubuntu_Install”。 启动创建：点击 “Start” 按钮，Rufus 会开始创建启动盘。过程大约需要几分钟。 BOIS 和 UEFI的区别BIOS（Basic Input/Output System）和UEFI（Unified Extensible Firmware Interface）是两种计算机固件接口，用于在操作系统启动之前初始化硬件和加载操作系统。它们在功能、性能和设计上有一些显著的区别。 BIOS，Basic Input/Output System 传统固件接口：BIOS是较早的固件接口，起源于20世纪80年代。 16位模式：运行在16位处理器模式下，具有1MB的地址空间限制。 MBR分区表：使用主引导记录（MBR）分区表，最多支持4个主分区，每个分区最大支持2TB。 启动速度较慢：由于初始化硬件的方式较为低效，启动速度较慢。 用户界面：通常是文本模式的界面，操作较为复杂。 扩展性差：扩展性和灵活性较差，难以适应现代硬件和软件的需求。 UEFI，Unified Extensible Firmware Interface 现代固件接口：UEFI是BIOS的继任者，设计初衷是替代BIOS，提供更丰富的功能和更好的性能。 32位或64位模式：运行在32位或64位处理器模式下，具有更大的地址空间。 GPT分区表：使用GUID分区表（GPT），支持更多的分区（最多128个），且每个分区可以超过2TB。 启动速度较快：初始化硬件的方式更高效，启动速度更快。 用户界面：支持图形用户界面（GUI），操作更加直观和友好。 扩展性强：具有更好的扩展性和灵活性，支持网络启动、远程诊断和修复等现代功能。 具体区别 启动过程： BIOS：在启动过程中，BIOS执行一系列的POST（Power-On Self Test）检查，然后查找并加载MBR中的引导代码。 UEFI：在启动过程中，UEFI固件会查找并执行EFI系统分区（ESP）中的引导管理程序，可以直接加载操作系统。 分区表支持： BIOS：仅支持MBR分区表，存在4个主分区的限制。 UEFI：支持GPT分区表，可以有更多的分区，且每个分区大小不受2TB限制。 硬件支持： BIOS：对现代硬件的支持有限，尤其在大容量存储设备和新型设备的支持上存在不足。 UEFI：设计时考虑了现代硬件的需求，支持更大容量的存储设备和多种新型设备。 安全性： BIOS：安全性较低，缺乏现代安全特性。 UEFI：支持Secure Boot等安全特性，防止恶意软件和未授权操作系统的启动。 BIOS和UEFI是两种不同的固件接口，UEFI是BIOS的继任者，提供了更丰富的功能、更好的性能和更强的扩展性。随着计算机技术的发展，UEFI已经成为现代计算机系统中广泛采用的固件接口，逐渐取代了传统的BIOS。 之前我不太关心这些，这次我听了ChatGPT的建议选择了UEFI、GPT、FAT32的选项，这也为我后面启动埋下了一个坑 再次安装这次无比顺利，将U盘设置为启动盘 选择语言 选择键盘 选择服务器类型Ubuntu Server 手动配置IP和网关 跳过代理设置 选择安装的盘符，注意别选错了，我这里是小心再小心，这是我最担心的一步了 输入主机名、用户名和密码 跳过升级Ubuntu Pro 安装OpenSSH Server 不勾选其他Feature安装 然后就等着安装完、更新完重启就行了 然后等它安装完我就等不及了，没等更新我就重启了，因为从早上捣鼓到下午了，然后电脑就打不开了，一直卡在开机界面，启动项也重新调整了，就是不起作用 解决开机问题每次开机屏幕就卡在下面这些信息上： No boot device avaiableCurrent boot node is set to BIOSPlease ensure aompatible bootable dedia is availableUse the system setup pargram to chaneg the boot mode as needed strlke F1 to retry boot, F2 for system setup, F11 for BIOS boot manager 一开始我总是把注意力放在最后一句，每个选项都尝试了也不行，找到两篇相关问题的解答，但是没解决问题 戴尔服务器安装Ubuntu系统时跳过strlke F1 to retry boot, F2 for system setup,F11 for BIOS boot manager解决方法 windowsServer2008开机出现strike f1 to retry boot，f2 for system setup ，f11 for BIOS boot manager 于是仔细看到这一句 “Use the system setup pargram to chaneg the boot mode as needed” 按F2 进入 System BOIS，将Boot Mode改为 UEFI 模式，成功进入系统，因为之前是BIOS，我制作的引导盘采用的是UEFI模式，后续买云服务器用的也是 Ubuntu 20.04 64位 UEFI版，保持统一吧 总结 保证开发环境、打包环境、运行环境一致，可以避免很多坑，这或许就是docker火起来的意义 使用系统自带的glibc版本，千万不要自己升级或降级，会引起连锁反应，这是很多前人给出的忠告 glibc的特点，兼容低版本的内容，所以理论上“低版本编译高版本运行”没问题，反过来一般不行，但存在特例 BIOS（Basic Input/Output System）和UEFI（Unified Extensible Firmware Interface）是两种计算机固件接口 UEFI是BIOS的继任者，提供了更丰富的功能、更好的性能和更强的扩展性，使用GUID分区表（GPT），支持更多的分区 Use the system setup pargram to chaneg the boot mode as needed ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 是你一直坚持走熟悉舒服的老路，过一眼望到头的人生，那看不见新的风景，遇不到新的经历也就怨不得别人了~ 2024-7-19 19:56:16]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>Dell</tag>
        <tag>R620</tag>
        <tag>glibc</tag>
        <tag>BIOS</tag>
        <tag>UEFI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[忙忙碌碌的混沌之中差点扑了个空而错过年中这条线]]></title>
    <url>%2Fblog%2F2024%2F06%2F30%2F%E5%BF%99%E5%BF%99%E7%A2%8C%E7%A2%8C%E7%9A%84%E6%B7%B7%E6%B2%8C%E4%B9%8B%E4%B8%AD%E5%B7%AE%E7%82%B9%E6%89%91%E4%BA%86%E4%B8%AA%E7%A9%BA%E8%80%8C%E9%94%99%E8%BF%87%E5%B9%B4%E4%B8%AD%E8%BF%99%E6%9D%A1%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[前言突然意识到过完这个周末已经7月份了，他预示着我的2024年已经过半了，过年回家仿佛还是昨天的事情，怎么转眼间已经到了年中了。心里还是不愿承认这件事，翻开自己2024年的打卡清单，列表显示已经到了26周，从另一方面证明了年中这条线已经到来，再看看打卡清单上这惨不忍睹的“成果”，真的有点难以接受。 2024打卡清单是2023年终总结的产物，为的是将时间周期细化，每周记录自己的变化，分别从总结、刷题、跑步、读书来简单记录自己完成了哪些，如今看看这清单，已经有一个月没总结了、一个月没刷题了、2个半月没有读书了，跑步属于硬蹭，每周忙忙碌碌的奔跑我暂且将其归为跑步，否则这一两个月的打卡就是0蛋了。 一个规律行进的人突然出现了一两个月的空白一定是遇到了什么问题，也不算是找借口，我也来回忆一下这两个月究竟发生了什么，借口也罢、困难也罢，这些暂时都过去了。 初见端倪面对周围环境的不景气，同学们即将被毕业的传闻在2023年底传了出来，在真真假假的传闻过后，同组一位同学不得不离开去寻找新的出路，剩下的伙伴报团取暖，也在努力探索着的新的途径，忙忙碌碌工作到年底，随着新年的喜气洋洋的气息，毕业的寒风好像被冲淡了很多，但是新年回来后的工作热情并没有持续多久，就有更多悲观的信息陆续流传出来。 话说无风不起浪，这种环境下大家还是宁可信其有，不可信其无，也有不少人早早的开始做了准备，平时我也是会和大家聊聊做好总结，寻找新机会的话题，但接下来这次的毕业信息雷厉风行，4月中旬组内另一小伙伴在3天内毕业离开了，从此也拉开了我这最近两个月的混沌经历。 混沌初始随着并肩战斗的小伙伴被迫依次离开，其实剩下的人都如履薄冰，好在这股风已经刮了很久，大家在心里上没有很意外，无非是对未来出路的迷茫，虽然不知道自己什么时候离开，但是基本默认自己离开只是时间早晚的事。 而这段时间我在做什么呢？简单来说就是总结加准备吧，因为项目组是有游戏在线上运行的，不管后续项目组的人要怎样处理，必然涉及到项目的整理和交接，所以提早做准备免得最后被毕业时手忙脚乱吧。 当然这段时间还没到真正混乱时刻，我还在接触新的知识，比如Go的语法、新的框架、帧同步，k8s，AI应用等等，就这样在边总结、边准备的情况下，我的时间来到了五一劳动节，平稳的度过了这个节日，而混沌的经历也从此开始。 力不从心自劳动节过完从河北回京，到端午结束从天津回京，这一个月的时间，真正的体会了一次力不从心的感觉，老人生病，带着挂号、看病、预约检查、手术排期、住院，最多一天跑3家医院。而我也迎来了毕业通知，准备办理离职、项目交接、指定方案、反复沟通、新工作的寻找，办卡。小孩子看病、挂号、运动会的参加、儿童节参与、跟老师的互动和沟通。租住的出租屋要换上下水管道，要有人盯着。老家的麦收，农忙，播种，全家人多地的来回奔波。其中很多事情我并没有参与多少，这都是一家人一起挺过来的，但是真的心累，累的不行，每天歇下来时只想睡觉，好在事情一件一件地解决了。 这也是为什么最近这段时间的生活打卡清单上只有“跑步”这一项了，每天都是风风火火的奔跑忙碌。 心力交瘁因为之前意识到要做项目交接，所以上文提到我已经做了准备，对于数据的交接基本没什么问题，但是数据量巨大，每次同步得10个小时左右，只要有一点问题那便是再花费个10小时，另外工作流程的移交才是交接工作的难点。 时间来到了6月初，已经完成了数据交接，并在月初按预定计划申请了交接的各类账号和权限，其中各种求爷爷告奶奶的心酸只有自己才懂，最后一天因数据同步验证问题忙到了很晚，没有拿到离职证明，不得不第二周星期一办完最后手续，本来以为在入职新公司前这段时间可以好好休息一下了，不曾想去拿离职证明那天却是我接下来一个月最轻松的一天。 6月份这将近一个月对我的洗礼不仅是知识层面的，更多的是精神上的，作为游戏服务端的主程，对比之前的执行大兵，免不了要处理团队内外沟通的事情，相比之前闷头开发时的纯粹真的是累了很多，但是这一个月的洗礼让我认识到，跟人打交道真的必跟计算机打交道累的太多了。 很多流程只能按照规章制度办，但是如果你的权限足够高，或者你有一个可以说得上话的领导，那么这些规章制度就可以靠边站，流程也可以特批特办，倒不是说去做违规的事，比如半夜12点让流程线上的人帮你审批多少有点不近人情，并且大半夜的你也找不到人，但是在紧急情况下，你如果可以联系到对方的领导，还是可以紧急特批特办的，毕竟是合规的事情，也不需要担什么风险，只是一个大头兵对于半夜卡流程这件事一般还是推不动的，只能向上反映。 这一个月说了太多的好话，请了太多的人帮忙，走了太多的审批流，特批了太多的紧急状况，吃了太多的外卖，过去几年我都没吃这么多外卖，回望上周的工作总结，我觉得自己就是个超人，这工作强度已经赶上前年出版本，以及去年上线测试时的强度了，这种活真的挺难干的，一方面是专业技能的掌握，而另一方面也考验着人际关系。 拾遗补缺最近这一个月的工作环境错综复杂，工作地点就换了好几次，有过忍饥挨饿，也有过通宵处理，有时连最基本的网络都保证不了，原本一个月都用不了3个G流量的我，这个月竟然破天荒的使用了20多个G，虽然有很多需要靠人际关系才能解决的问题，但是也有很多专业性的问题，即使不是第一次遇到，也有了新的认知和理解，所以简单罗列下，作为后续深入理解的源头吧。 首先是网络Vlan的搭建，重温了子网的划分与设计，网络拓扑查看，无线网与有线网的连通，3层路由器与二层交换机的数据转发，IP与MAC地址的绑定，DHCP的配置与地址池的设定，CentOS和Ubuntu静态IP的配置，WiFi6对老旧网卡的不兼容性。 Dell R620服务器的基本硬件构成，iDrac接口的IP配置和用户信息配置，通过iDrac对机器的功率、温度、硬盘的监控，服务器硬盘的保护，服务器周围环境注意的问题，温度的控制，UPS电源的选择等等。 Raid的构成和分类，独立硬盘冗余阵列 Raid0、Raid1、Raid2、Raid3、Raid4、Raid5、Raid6、Raid10、Raid50、Raid100等，根据情况不同，选择不同的配置，但是2、4、6好像很少用，而比较中庸的Raid5是常常出现的选择，可以容忍一块硬盘损坏，出现问题后，将损坏的硬盘取下，换上Dell原装硬盘可以自动重建。 gitlab备份和还原，解决git库迁移是lfs check出错的问题，svn服务器备份，svn在linux环境下保存密码，Linux查看硬件配置dmidecode，查看硬盘信息，第一次完成硬盘挂载，虚拟机向物理机同步数据，rysnc-daemon模式的配置。 docker环境下搭建jenkins，gitlab，svn等，docker虽然可以挂在硬盘保证数据不丢失，但是如果出现镜像内访问外部服务实现起来会比较麻烦，如果访问镜像内原本没有的程序也比较麻烦，举个例子，用docker搭建gitlab，只对外提供访问，可以很方便的实现，也能快速升级，但是docker搭建jenkins就很麻烦了，单独的jenkins镜像缺少某些特定工具，并且作为打包环境需要经常访问其他的服务，放在容器中我感觉不太方便。 在一天之内接触了4种苹果类型的设备，这是之前都没碰过的，iMac、Mac Studio、Mac Mini、黑苹果，了解了Darwin名字的来源，Mac Os系统的变迁，我觉的“访达”这个对Finder的翻译非常棒，苹果打包所需的Unity和XCode的安装，jenkins-lts的安装，MacOs的开机启动程序，jenkins版本的替换，Java11的安装，Grooxy语法等。 服务器的迁移，注意应用程序目录的数据迁移，还有用户目录下各个隐藏文件的迁移，比如.ssh、.subversion、.gitconfig 等等，这决定你是否可以用新机器完全替代旧机器。 Microsoft Intune的认证、Mac系统公司门户的认证，VPN远程访问，办公网、研发网，解决WOA无法登录问题，网络验证授权等。 总结 2024年已经过半，突然间才发现今年已经过了26周，而最近的打卡清单缺惨不忍睹 辛苦奔忙的一个月让自己感觉力不从心，而最近过渡的一个月仿佛让自己又回到了备战上线时的强度 程序员的工作本来只费身体和脑子，最近接触的人有点多，感觉心也跟着累了 人际关系的积累不是一朝一夕能实现的，而技术知识的学习却可以速成，多总结吧 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 但凡是由人操作的流程，都可以有一些例外，当你被告知仅能等待流程审批时，说明你还不够硬，起码没有硬到可以帮你特批的地步~ 2024-6-30 22:45:56]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>年中</tag>
        <tag>2024</tag>
        <tag>离职</tag>
        <tag>新起点</tag>
        <tag>人际关系</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有关服务器安全的反思]]></title>
    <url>%2Fblog%2F2024%2F05%2F28%2F%E6%9C%89%E5%85%B3%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E5%85%A8%E7%9A%84%E5%8F%8D%E6%80%9D%2F</url>
    <content type="text"><![CDATA[前言人都说学的越多，不懂的东西也就越多，很多人都会有这个感受，面对信息爆炸的互联网时代，有种“学不完，根本学不完”沧桑无力感，最近有关服务器安全的了解又扩展了我的知识面，填补了部分认知盲区，究竟什么是服务器安全？ MySQL数据库的安全这是前段时间的疑问，《linux环境下如果掌控了系统root账户就能对上面安装的MySQL数据为所欲为了吗》，当时出现这个疑问是因为安装完MySQL数据库，提示我有这样一样配置文件 /etc/mysql/debian.cnf，打开文件内容展示如下： 1234567891011# Automatically generated for Debian scripts. DO NOT TOUCH![client]host = localhostuser = debian-sys-maintpassword = xg9vyvjNly3vUSsmsocket = /var/run/mysqld/mysqld.sock[mysql_upgrade]host = localhostuser = debian-sys-maintpassword = xg9vyvjNly3vUSsmsocket = /var/run/mysqld/mysqld.sock 明明白白的写着用户名和密码，当时感觉不可思议，这数据库不是轻松就被破解了吗？事实确实如此，因为你是root用户，即便没有这个配置文件，你也可以通过修改MySQL配置重启服务来跳过密码，所以如果你拿到root密码，那么你就是这服务器世界的主宰，你就可以为所欲为。 物理服务器假设你没有服务器root用户密码，或者忘记了服务器root密码要怎么办呢？你可以在开机的时候按e修改grub的kernel行到single模式下，就可以轻易获得root权限，并且能够改root密码，有没有颠覆你的认知。什么？任何人只要重启下服务器就把我的root密码改了？这是真的吗？这确实是真的。 下面我摘录了一些网友的回复，挺逗的，但仔细想想说的很有道理 能接触到物理机的，都别说什么安全 这个事吧，放在机房的玩意有个不成文的规矩：能物理摸到的设备都允许你重置密码。 90年代的一句老话：能物理摸到设备的人，都可以看作是这台设备的主人。 Linux：我就是一个软件，你搞物理袭击，讲武德吗？ 退一万步说，就算在机器旁边也不能重设密码，那把硬盘拆了拿回去不也一样？ 物理安全是信息安全的基础。摸到你机器了，还谈什么安全。 日常运维或者使用当中需要面临的更大的挑战是丢失了，改错了密码。从我的经验来看，丢失密码的机会比丢失硬件的机会大至少100倍 比如海底电缆可以被剪断，怎么保证安全性？ 为什么要这样设计呢？仔细想想也能明白个大概，就是能触碰到物理机的人一定默认拥有所有权，他去改密码一定是到了不得不改的情况，如果你非说改密码的人是偷偷进去的，那是保安的责任，Linux系统不背这个锅！ 机器设计有一个规矩就是机器的物理持有者对机器拥有事实上的超管理员权限。比如说路由器背后的reset。比如说主板cmos清除针。因为事实上，管理员忘记密码往往比密码受到黑客攻击更常见，造成的损失也更大。至于提数据加密的，那是另一个话题，我们这里说的是操作权。黑客无法窃取你用bitlocker加密的数据，但这不妨碍他把你数据删光，所以从这个角度上，bitlocker不能保证你的数据安全，它保证的只是不泄密。 所以看到这些是不是就释然了，系统保证的安全是建立在物理机安全的基础之上的，连机器都在人家手里，还谈什么安全呢？ 总结 在Linux系统上如果登录了root用户就可以为所欲为进入任何一个MySQL数据库 如果摸到了物理服务器默认你就是它的主人，你可以通过很便捷的方式来修改它的密码 物理安全是信息安全基础，Linux就是一个软件，你搞物理偷袭它可扛不住 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 凡是人总有取舍，你取了你认为重要的东西，舍弃了我，这只是你的选择而已，若是我因为没有被选择，就心生怨恨，那这世界岂不是有太多不可原谅之处，毕竟谁也没有责任要以我为先，以我为重，无论我如何希望也不能强求。 2024-5-27 21:16:17]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>root</tag>
        <tag>安全</tag>
        <tag>密码</tag>
        <tag>物理机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面对AI你到底能做什么？怎样能让其为我所用]]></title>
    <url>%2Fblog%2F2024%2F05%2F07%2F%E9%9D%A2%E5%AF%B9AI%E4%BD%A0%E5%88%B0%E5%BA%95%E8%83%BD%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F%E6%80%8E%E6%A0%B7%E8%83%BD%E8%AE%A9%E5%85%B6%E4%B8%BA%E6%88%91%E6%89%80%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言面对AI铺天盖地的新闻，大有替代万事万物的之势，但如果要问AI能做什么？你是否会在心里打个问号？它好像只能用来聊天，又好像什么都能做，ChatGPT现在是我每天都要使用的工具，每天只是对话问问题，特定领域的问题确实比搜索引擎更精准一些，但是对AI的使用也就仅限于此，难道AI只有这么大点能力吗？ 仔细想想不是的，起码还有Midjourney这个生成图的应用，还有Sora 这个文本转视频的工具，但是在这些工具出现之前我怎么就没想到呢？我们是不是可以抛弃这种被动使用的局面，自动动手做一些AI方面的应用呢？答案是肯定的，之前没做出来是因为我们没有这样想过，一旦开了这个头，你会发现AI真的可以做很多东西！ 思路引导究竟怎么开始呢？先得找到需求，然后实现需求。作为一个程序员我们就从程序编码的例子出手，我们都会算两个数的加法，一开始数据比较小，需求计算的总量也比较小，所以口算、笔算都能满足需求，随着应用场景的增加，数据量也越来越大，那么人工计算就有些力不从心了。 这时作为以懒著称的程序员转了转脑筋，干脆实现一个加法计算器吧，需要计算时我把两个数字输入进去，直接就能得到结果，大大加快了计算速度 1234567891011121314#include&lt;iostream&gt;using namespace std;int add(int a, int b) &#123; return a + b;&#125;int main() &#123; int num1, num2; cout &lt;&lt; "请输入两个整数：" &lt;&lt; endl; cin &gt;&gt; num1 &gt;&gt; num2; cout &lt;&lt; "两数之和为：" &lt;&lt; add(num1, num2) &lt;&lt; endl; return 0;&#125; 我们使用已经掌握的代码知识，解决了现有的需求，提高了工作效率，那么AI相关的应用要怎么样实现呢？ 为AI找活干在创建AI应用之前，我们先来想想AI的特点，或者局限一点，想想ChatGPT的特点：语言理解与知识问答、逻辑推理与数学问题解答、编程代码的理解与编写、智能教育产品、儿童英语教育等等，我们抓住其中一点，来完成我们的工作，比如语言理解。 想想在学校时是否经常需要总结文章大意？看材料时是否经常需要概括全文思想？这些事不正是AI所擅长的吗，如果我们偷懒会把要总结的内容告诉ChatGPT，让它帮我们总结一下，第二天又有这个题目，再次告诉ChatGPT，让它帮我们总结一下，有没有发现？我们又陷入了经常做重复工作的情景，怎么抽象出一个需求呢？ 需求就是：实现一个应用，接受输入一段文字，20个字概括文字内容的大意，要求总结文字通俗易懂、言简意赅 这仿佛变成了我们上面的add函数，只是总结的过程是调用AI来完成的，我们只需要输入一段文字就能得到总结性输出，去除了中间多余的人工步骤 具体实现面对这个AI需求，我们可以利用ChatGPT的API接口从头搭建一个接受输入和返回输出的应用，类似实现一个 add 函数，也可以利用一些开放平台的能力，比如 讯飞开放平台，通过低代码的形式来快速搭建AI应用。 登录讯飞星火SparkDesk产品首页，在左上角，点击进入“助手中心” 在星火助手中心页面，点击右上方“助手创作中心” 在助手创作中心，点击“立即创建”，进入新建助手页面，开始创建助手的创建之旅 接下来的内容就自己探索吧，根据需求在左侧填写内容，右侧可以进行调试，觉得运行还不错的话可以点击创建按钮，也可以上架后给更多人使用至此，一个利用AI特点制作的应用就完成了，虽然很简单，但是开启了一个怎样使用AI的时代，话说我之前真不知道怎么用AI，做平台的这些人脑子真是太灵光了 总结 AI不是万能的，要学会利用AI，合理掌控AI为我所用 利用AI庞大知识库的特点，进一步封装成特定的应用，可以更好的满足需求 完事开头难，有时不是你不会，只是你不知，一旦开了头便打开了一扇窗 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 面对未知的状况，每个人心里都会有些小九九，根据自己的状况做出合适的选择，一旦选了便不需再回头，水有舟可渡，山有路可行~ 2024-5-6 21:41:21]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>AI</tag>
        <tag>Thinking</tag>
        <tag>卖铲子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[帧同步入门项目LockstepDemo的初步学习]]></title>
    <url>%2Fblog%2F2024%2F04%2F25%2F%E5%B8%A7%E5%90%8C%E6%AD%A5%E5%85%A5%E9%97%A8%E9%A1%B9%E7%9B%AELockstepDemo%E7%9A%84%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[前言上一篇文章介绍了 LockstepDemo 这个项目可以作为帧同步入门读物，解决了跨域限制的问题以后，这个开源项目就可以运行起来啦，虽然我没有使用js写过实际的项目，但看的多了自然也能看懂大部分的js代码了，作为一个帧同步领域的小白，我开始了阅读这个项目代码的旅程，看过之后确实解开了我之前的迷惑，所以简单记录一下学习心得。 基础的帧同步模式，每个客户端必须回报给服务器收到帧数，服务器再次发送确认包才执行帧数据，否则所有人等待，也就是一卡全卡。 基于现在手游的流行程度和弱网环境，手游一般都采取乐观锁模式。即收到服务器推帧后，客户端立即执行，不等待其他人。这样卡顿的人自己卡，不影响其他人的游戏体验。同时卡顿的人在收到数据后，自行加速补帧来追赶上正确的游戏速度。 连帧同步都出SDK了插播一条消息，今天在搜索帧同步资料的时候，无意间发现了游戏巨头——腾讯居然发布了帧同步SDK-LockStep，简直“丧心病狂”，真的是盘子大了什么都做啊，不过也挺好，我可以从中学到不少问题的处理方案，我摘抄官方文档部分内容，感兴趣的可以去看看 帧同步（LockStep）服务为手游开发提供一套快速、可靠的帧同步游戏开发框架。基于GCloud云服务进行快速部署，同时支持TCP、UDP、RUDP三种通道。帧同步开发框架提供一致性数学库与一致性检测工具，并且针对弱网设计具有高可靠、低延时的特性。 帧同步与更为传统的状态同步均为游戏常见同步方案，主要区别在于：状态同步主要逻辑计算放在服务器端，将计算结果下发给客户端；而帧同步服务器仅仅起到收集客户端输入并广播的作用。基于帧同步相对状态同步流量消耗更低、开发效率更高、打击感更好等优点，《王者荣耀》选择了帧同步方案。 《拳皇命运》项目从完全不熟悉帧同步技术的情况下，接入SDK仅用了两个月时间，将原有状态同步游戏，改造成为帧同步游戏；项目仅需编写游戏逻辑，无需关心同步、网络品质、录像回放等核心功能。 专业的事情交给专业的人来做，如果之前没有积累，将这套SDK直接拿过来用也是不错的，我看到支持C++和C#两种语言，其他的语言就得自己去沟通了 LockstepDemo言归正传，开始阅读LockstepDemo这个项目的源码，服务端app.js使用node运行，前端main.js+main.css+index.html，在浏览器中运行，其实主要的逻辑代码就在main.js中，后端就只有app.js一个文件，总共166行，前端还引用了jquery.min.js和socket.io.js两个库文件，但包含主要逻辑代码的main.js文件仅有328行。 在开始阅读自定义逻辑代码之前先来看看引用的这两个库： socket.io.js负责网络的建立、管理，消息的发送等等，看了逻辑中的调用真的是挺方便的 var io = require(&quot;socket.io&quot;)(server）: 创建基于 Node.js 的 WebSocket 服务器，并将其绑定到了一个 HTTP 服务器实例 server 上 io.on(&#39;connection&#39;, function (socket) {...]): 使用Socket.IO 的服务器实例的 on 方法来监听客户端与服务器建立连接的事件 socket.on(&#39;join&#39;, function(account) {...}): 使用与客户端建立的连接的对象 socket 的 on 方法来监听客户端发送的 ‘join’ 事件 socket.emit(&quot;open&quot;, {...}): 使用了 Socket.IO 库的 emit 方法来向当前客户端发送一个名为 “open” 的自定义事件，并附带自定义对象作为数据 socket.broadcast.emit(&#39;system&#39;, ...): 用于向除当前连接的客户端之外的所有客户端发送消息 io.sockets.emit(&#39;start&#39;, {...}): 用于向所有连接的客户端发送消息 jquery.min.js即使没有做过前端，jQuery这个库的大名也应该听过，它是一个流行的 JavaScript 库，简化了在网页开发中的 JavaScript 编程。它提供了一系列功能强大且易于使用的 API，使得诸如 DOM 操作、事件处理、动画效果、AJAX 请求等任务变得更加简单和高效，主要特点如下： 简化 DOM 操作：提供了简洁而强大的 DOM 操作方法，使得选择元素、修改元素属性、添加/删除元素等操作变得更加便捷 事件处理：提供了简单易用的事件处理方法，可以方便地为元素绑定事件、移除事件、触发事件等，大大简化了事件处理的代码编写 动画效果：提供了丰富的动画效果和特效，可以通过简单的方法实现页面元素的平滑过渡、淡入淡出、滑动等效果，为用户提供更流畅的交互体验 AJAX 请求：提供了简洁的 AJAX 方法，可以方便地进行异步数据加载和交互，从而实现更灵活和动态的网页内容加载和更新 跨浏览器兼容性：封装了复杂的跨浏览器兼容性处理，使得开发者可以更加轻松地编写跨浏览器兼容的代码 简单来说它就是一个封装了常用操作的库，稍后会在main.js发现它的使用方法，简单摘录如下： 文档加载完成事件：$(function () {...}) 用于在文档加载完成后执行的函数，这是 jQuery 的快捷方式，等同于 $(document).ready(function() {...})，表示在 DOM 树构建完成后执行指定的函数 元素选择：$(&#39;body&#39;)、$(&#39;#start_btn&#39;)、$(&#39;#reconnect_btn&#39;) 等通过 jQuery 选择器选择了 HTML 元素。这些选择器能够基于元素的标签名、ID、类名等来选择元素，返回 jQuery 对象，以便进行后续的操作 事件处理：$(&#39;body&#39;).keydown(function(e) {...}) 注册了键盘事件处理函数，当键盘按键按下时执行相应的操作 动画效果：$(&quot;#tips&quot;).animate({...}) 使用了 jQuery 的动画效果，在提示框显示时执行动画效果，让提示框从屏幕中间上方滑动到屏幕中间 样式操作：$(&quot;#tips&quot;).show()、$(&quot;#tips&quot;).fadeOut() 等使用了 jQuery 提供的方法来控制元素的显示和隐藏 在阅读app.js和main.js之前还是得说明一下，今天只介绍核心逻辑，像断线重连、消息提示、显示网络延迟等功能，都是在核心逻辑上的补充和优化，可以先忽略不看，并且这次看代码发现，整个运动的表现和实现逻辑是符合牛顿第一定律的，真的挺有意思： 一切物体总保持匀速直线运动状态或静止状态，直到有外力迫使它改变这种状态为止 这只是这个项目的特点，并不是所有的游戏都是这样的，有些游戏的实现就是和这惯性定律相违背的，比如很多游戏必须一直拖动摇杆才会移动，否则就会停住静止，它们所表达出来的就是，“力是维持物体运动的关键”，好了，扯得有点远了，一起看看代码实现吧 服务器app.js1234567891011121314var server = require('http').Server()var io = require("socket.io")(server, &#123; allowEIO3: true, cors: &#123; origin: ["http://127.0.0.1:8080", "null"], methods: ["GET", "POST"], credentials: true // 设置为true以允许使用凭据 &#125;&#125;);// 监听3000端口server.listen(3000, function()&#123; console.log("服务器启动成功，监听端口3000")&#125;) 定义服务器实例，启动并监听3000端口，这里已经做了跨域允许访问的配置 1234567891011121314var g_onlines = &#123;&#125; // 所有在线玩家var g_commands = new Array() // 指令数组var g_commands_histroy = new Array() // 历史指令，用于断线重连var g_joinCount = 0 // 已准备的人数var g_maxJoinCount = 2 // 最大人数var g_stepTime = 0 // 当前step时间戳var g_stepInterval = 100 // 每个step的间隔ms// 游戏状态枚举var STATUS = &#123; WAIT:1, START:2&#125;var g_gameStatus = STATUS.WAIT 一些游戏全局变量和游戏状态枚举的定义，注释写的很详细，记一遍看下面的逻辑时能想起来就行 12345678910111213141516171819202122// frame定时器var stepUpdateCounter = 0function update(dt) &#123; var now = Date.now() if(g_gameStatus == STATUS.START) &#123; stepUpdateCounter += dt if(stepUpdateCounter &gt;= g_stepInterval) &#123; g_stepTime++ stepUpdate() stepUpdateCounter -= g_stepInterval &#125; &#125;&#125;// 启动定时器var lastUpdate = Date.now()setInterval(function() &#123; var now = Date.now() var dt = now - lastUpdate lastUpdate = now update(dt)&#125;) 开始进入主要逻辑，lastUpdate是上次走过的时间，在游戏开始g_gameStatus == STATUS.START后不断将 dt 积累到变量 stepUpdateCounter，超过一帧的间隔后执行一帧逻辑 stepUpdate()，setInterval 是内部函数，第二个参数表示调用的时间间隔，默认为0，可以可以认为是游戏中常用的 tick() 函数 12345678910111213141516171819202122232425// step定时器function stepUpdate() &#123; // 过滤同帧多次指令 var message = &#123;&#125; for(var key in g_onlines) &#123; message[key] = &#123;step:g_stepTime, id:key&#125; &#125; for(var i = 0; i &lt; g_commands.length; ++i) &#123; var command = g_commands[i] command.step = g_stepTime message[command.id] = command &#125; g_commands = new Array() // 发送指令 var commands = new Array() for(var key in message) &#123; commands.push(message[key]) &#125; g_commands_histroy.push(commands) for(var key in g_onlines) &#123; g_onlines[key].socket.emit('message', new Array(commands)) &#125;&#125; 这是转发客户端操作的核心函数，首先是遍历所有玩家 g_onlines，执行 message[key] = {step:g_stepTime, id:key} 为每个玩家构建一个空指令，然后遍历当前收到的所有命令 g_commands，将命令的帧值设置为当前帧，并且过掉一帧中的多个指令，保证一帧只能朝一个方向运动，将收集到的所有指令 commands 通过 socket.emit() 函数发送给所有玩家 g_onlines 1234567891011121314io.on('connection', function (socket) &#123; socket.emit("open", &#123;id:socket.id, stepInterval:g_stepInterval&#125;) // 获取用户账户 function getAccount(socketId) &#123; for(var key in g_onlines) &#123; if(socketId == g_onlines[key].socket.id) &#123; return key &#125; &#125; &#125; //...&#125;) 这是服务器上所有事件监听的基础，io 监听新玩家连接事件，建立连接后向客户端发送一个名为 “open” 的自定义事件，并附带了一个包含 id 和 stepInterval 属性的对象作为数据，id的值是 socket.id，stepInterval表示每帧的时间间隔，getAccount() 函数的作用通过socket.id获取账号名 12345678910111213141516171819202122232425262728293031323334353637socket.on('join', function(account) &#123; // 顶号/断线重连 if(g_onlines[account]) &#123; g_onlines[account].socket.emit('system', "被顶号了") g_onlines[account].socket.disconnect() if(g_gameStatus == STATUS.START) &#123; g_onlines[account] = &#123;socket: socket, online: true&#125; socket.emit('join', &#123;result:true, message:"正在断线重连..."&#125;) console.log(account, "重连游戏") socket.broadcast.emit('system', account + "重新连接！") socket.emit('start', &#123;player:Object.keys(g_onlines)&#125;) socket.emit('message', g_commands_histroy) return &#125; &#125; // 房间已满 if(g_joinCount == g_maxJoinCount) &#123; console.log("房间已满", account, "加入失败") socket.emit('system', "房间已满") socket.disconnect() return &#125; // 加入游戏 if(g_joinCount &lt; g_maxJoinCount) &#123; console.log(account, "加入游戏") socket.emit('join', &#123;result:true, message:"匹配中..."&#125;) g_onlines[account] = &#123;socket: socket, online: true&#125; g_joinCount++ &#125; // 开始游戏 if(g_joinCount == g_maxJoinCount) &#123; g_commands = new Array() g_commands_histroy = new Array() g_gameStatus = STATUS.START io.sockets.emit('start', &#123;player:Object.keys(g_onlines)&#125;) &#125;&#125;) 这是监听socket收到 &#39;join&#39; 事件的处理函数，实现逻辑有些技巧，通过查询账号是否已经登录过服务器，来判定是否为重连，如果为重连则一次发送 &#39;join&#39;、&#39;start&#39;、&#39;message&#39;事件和数据，其中 &#39;message&#39; 事件中的数据是从游戏开始以来的所有指令 如果不是重连就要判断匹配人数，超过到 g_maxJoinCount 不允许进入，达到g_maxJoinCount游戏开始，通知客户开始游戏，否则通知客户端正在匹配中 12345678910111213socket.on('timeSync', function(time) &#123; socket.emit('timeSync', &#123;client:time, server:Date.now()&#125;)&#125;)socket.on('message', function(json) &#123; if(g_gameStatus == STATUS.START) &#123; // TODO:过滤高延迟的包 (json.step) json.id = getAccount(socket.id) if(json.id) &#123; g_commands.push(json) &#125; &#125;&#125;) socket.on(&#39;timeSync&#39;, function(time) {...}) 收到后立即返回，用于客户端计算延迟，socket.on(&#39;message&#39;, function(json) {...})，收到客户端指令后将其放入全局缓存指令，等待每帧处理 1234567891011121314151617181920212223socket.on('disconnect', function () &#123; var account = getAccount(socket.id) if(account) &#123; g_onlines[account].online = false console.log(account, "离开游戏") var isGameOver = true for(var key in g_onlines) &#123; if(g_onlines[key].online) &#123; isGameOver = false &#125; &#125; if(isGameOver) &#123; io.sockets.emit('system', "游戏结束") g_joinCount = 0 g_stepTime = 0 g_gameStatus = STATUS.WAIT g_onlines = &#123;&#125; console.log("游戏结束") &#125; else &#123; io.sockets.emit('system', account + "离开了游戏！") &#125; &#125;&#125;) socket.on(&#39;disconnect&#39;, function () {...}) 断开连接是否要结束游戏的处理逻辑，如果在玩家断开后还有其他玩家在线，则游戏继续，等待玩家重连回来，否则有些结束 完整的游戏代码就这么多，还是比较清晰的，记住服务器上监听和发送的各种事件，比如 &#39;join&#39;、&#39;start&#39;、&#39;message&#39;、&#39;disconnect&#39; 等，一会再客户端代码分析的时候也会出现，对照着分析逻辑就串起来了 客户端main.js客户端的代码行数相对多一些，我只把重要的部分列举出来： 1234567891011121314151617181920212223242526272829303132333435363738394041// 游戏对象var GameObject = function(id) &#123; this.id = id this.x = 0 this.y = 0 this.direction = DIRECTION.STOP this.speed = 100 this.move = function (dt) &#123; dt = dt / 1000 var x = this.x var y = this.y switch(this.direction) &#123; case DIRECTION.UP: &#123; y -= this.speed * dt break &#125; case DIRECTION.DOWN: &#123; y += this.speed * dt break &#125; case DIRECTION.LEFT: &#123; x -= this.speed * dt break &#125; case DIRECTION.RIGHT: &#123; x += this.speed * dt break &#125; &#125; if(x &lt;= (WIDTH - BOX_SIZE) &amp;&amp; x &gt;= 0) &#123; this.x = x &#125; if(y &lt;= (HEIGHT - BOX_SIZE) &amp;&amp; y &gt;= 0) &#123; this.y = y &#125; &#125;&#125; 这段代码定义了一个函数，该函数可以用来创建游戏对象（Game Objects）的实例，在 JavaScript 中，函数也可以用来定义对象的构造函数。在这个例子中，函数 GameObject 就是一个构造函数，用于创建具有特定属性和方法的游戏对象。当你使用 new GameObject(id) 来调用这个函数时，它会返回一个新的对象实例，该实例拥有指定的 id、x、y、direction、speed 属性以及 move() 方法，尽管这段代码中的 GameObject 是一个函数，但它被设计用来创建具有特定属性和行为的对象。 12345678910111213141516171819202122232425262728293031$(function () &#123; // 画布 var context = document.getElementById("canvas").getContext("2d") // 每个step的间隔ms，服务器返回 var stepInterval = 0 // 当前step时间戳 var stepTime = 0 // 输入方向 var inputDirection = null // 游戏状态 var gameStatus = STATUS.WAIT // 接受指令 var recvCommands = new Array() // 所有游戏对象 var gameObjects = &#123;&#125; // 是否连接socket var isConnected = false // 当前执行中的指令 var runningCommands = null // 当前用户 var currentAccount = null // 是否正在加速运行延迟到达的包 var isFastRunning = false // 初始化UI显示 $("#content").hide() $("#login").show() $("#tips").hide() //...&#125;) 这是利用jQuery库的写法，等同于 $(document).ready(function() {...})，表示在 DOM 树构建完成后执行指定的函数，也就是每次加载完页面都会执行这个函数，这个函数里包含了客户端绝大部分逻辑，函数的开始定义了一些变量，用于记录游戏数据，具体含义参考注释即可，之后便初始化了界面显示 123456789101112131415161718// 连接socketsocket = io.connect('http://127.0.0.1:3000')// socket连接成功socket.on('open', function(json) &#123; isConnected = true stepInterval = json.stepInterval id = json.id console.log("Socket连接成功：", id) // 断线重连自动登录 if(localStorage.account) &#123; setTimeout(function () &#123; $("#account").val(localStorage.account) localStorage.account = "" $('#start_btn').click() &#125;, 0) &#125;&#125;) 使用 socket = io.connect(&#39;http://127.0.0.1:3000&#39;) 语句连接服务器，再接收到 &#39;open&#39; 事件之后，使用服务器同步数据给客户端变量赋值，函数末尾是根据本地存储情况决定是否需要重连，这部分逻辑可以先不看 12345678910111213141516171819202122// 收到游戏开始事件socket.on('start',function(json) &#123; // 初始化GameObject for(var i = 0; i &lt; json.player.length; ++i) &#123; var id = json.player[i] gameObjects[id] = new GameObject(id) &#125; gameStatus = STATUS.START stepTime = 0 showTips("游戏开始")&#125;)// 收到指令socket.on('message',function(json)&#123; // 储存收到的指令 for(var i = 0; i &lt; json.length; ++i) &#123; var command = json[i] recvCommands.push(command) stepTime = command[command.length - 1].step console.log("**** recv " + stepTime + " ****") &#125;&#125;) 收到游戏开始事件&#39;start&#39;之后，根据服务器下发的数据创建游戏对象 gameObjects[id] = new GameObject(id)， 收到&#39;message&#39;缓存服务器发送的指令到 recvCommands 中，后续在tick函数中处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 发送指令function sendCommand() &#123; if(isFastRunning) &#123; console.log("正在加速执行延迟包，无法发送指令！") return &#125; var direction = inputDirection socket.emit("message", &#123; direction: direction, step:stepTime, &#125;)&#125;// 键盘事件$('body').keydown(function(e) &#123; if(gameStatus != STATUS.START) return switch(e.keyCode) &#123; case 38: &#123; inputDirection = DIRECTION.UP break &#125; case 40: &#123; inputDirection = DIRECTION.DOWN break &#125; case 37: &#123; inputDirection = DIRECTION.LEFT break &#125; case 39: &#123; inputDirection = DIRECTION.RIGHT break &#125; case 13: &#123; inputDirection = DIRECTION.STOP break &#125; &#125; sendCommand()&#125;) 监听键盘按键，键盘上下左右键移动方块，回车键停止方块，每次将按键指令发送给服务器，重点看下这段实现，这就涉及到我们前边提到的牛顿第一定律，也就是每次按键时向服务器发送一次，可以改变物体的运动方向或者停止，之后便不再向服务器发消息了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// frame定时器var stepUpdateCounter = 0function update(dt) &#123; if(gameStatus == STATUS.START) &#123; // TODO: 逻辑/UI分离 stepUpdateCounter += dt if(stepUpdateCounter &gt;= stepInterval) &#123; stepUpdate() stepUpdateCounter -= stepInterval &#125; // 积攒的包过多时要加速运行 var scale = Math.ceil(recvCommands.length / 3) if(scale &gt; 10) scale = 10 isFastRunning = (scale &gt; 1) // 执行指令 if(recvCommands.length &gt; 0) &#123; var ms = dt * scale if(runningCommands == null) &#123; runningCommands = recvCommands[0] runningCommands.ms = stepInterval &#125; if(runningCommands.ms &lt; ms) &#123; ms = runningCommands.ms &#125; for (var i = 0; i &lt; runningCommands.length; i++) &#123; var command = runningCommands[i] if(runningCommands.ms == stepInterval) console.log(command) var obj = gameObjects[command.id] if(command.direction) &#123; obj.direction = command.direction &#125; obj.move(ms) &#125; runningCommands.ms = runningCommands.ms - ms if(runningCommands.ms == 0) &#123; recvCommands.shift() runningCommands = null &#125; &#125; // 绘制 context.clearRect(0, 0, WIDTH, HEIGHT) for(var key in gameObjects) &#123; var obj = gameObjects[key] context.fillStyle = "#000000" context.fillRect(obj.x, obj.y, BOX_SIZE, BOX_SIZE) context.font = "15px Courier New"; context.fillStyle = "#FFFFFF"; context.fillText(key, obj.x, obj.y + BOX_SIZE, BOX_SIZE); &#125; &#125;&#125;// 启动定时器var lastUpdate = Date.now()setInterval(function() &#123; var now = Date.now() var dt = now - lastUpdate lastUpdate = now update(dt) if(isConnected == true) &#123; socket.emit("timeSync", now) &#125;&#125;) 这段代码是客户端的核心逻辑，需要多看几遍，看懂了这一段，帧同步的思想也就基本算掌握了，调用 setInterval 的逻辑比较好理解，之前在服务器代码中也存在，就是启动tick函数，不断积累 dt，用于做客户端物体移动的表现 重点在 setInterval 我们化繁为简，不用看函数末尾的绘制部分，这一段就是根据GameObject的坐标绘制图形，因为这个项目没有实现UI表现和逻辑分离，所以函数开始更新变量 stepUpdateCounter的逻辑也没有用，简化完成后函数逻辑就剩下这些： 12345678910111213141516171819202122232425262728293031function update(dt) &#123; // 积攒的包过多时要加速运行 var scale = Math.ceil(recvCommands.length / 3) if(scale &gt; 10) scale = 10 isFastRunning = (scale &gt; 1) // 执行指令 if(recvCommands.length &gt; 0) &#123; var ms = dt * scale if(runningCommands == null) &#123; runningCommands = recvCommands[0] runningCommands.ms = stepInterval &#125; if(runningCommands.ms &lt; ms) &#123; ms = runningCommands.ms &#125; for (var i = 0; i &lt; runningCommands.length; i++) &#123; var command = runningCommands[i] if(runningCommands.ms == stepInterval) console.log(command) var obj = gameObjects[command.id] if(command.direction) &#123; obj.direction = command.direction &#125; obj.move(ms) &#125; runningCommands.ms = runningCommands.ms - ms if(runningCommands.ms == 0) &#123; recvCommands.shift() runningCommands = null &#125; &#125;&#125; 注意这个参数 dt 很微妙，虽然传入的值是tick实际的时间间隔，但分析完代码你会发现这个dt传入任意值，因为真正调用obj.move函数进行移动出入的参数是 ms，当 ms 等于 dt 时，是正常播放，当 ms &gt; dt 时，是加速播放，当 ms &lt; dt 时是减速播放，这里的代码只存在加速和正常两种情况 看到没有，你可以通过 scale 变量人为的改变时间的快慢，是不是很神奇，所以在帧同步中绝对顺序是靠帧数来决定的，而物理时间只是一个数字，想快就快，想慢就慢 思考下为什么会有这个判断 if(runningCommands.ms &lt; ms) { ms = runningCommands.ms }, 它的含义是无论你怎么加速，每个指令执行时间不能超过一帧的时间间隔，不然就和正常播放的逻辑数据不一致了 1234if(command.direction) &#123; obj.direction = command.direction&#125;obj.move(ms) 这几句比较有意思，翻译过来就是如果命令我改变方向，那么我就改变方向后移动，否则我按照原来的方向移动或者保持静止，再想想是不是惯性定律？ 12345runningCommands.ms = runningCommands.ms - msif(runningCommands.ms == 0) &#123; recvCommands.shift() runningCommands = null&#125; 最后这几句处理的是一个tick跑不完一帧时间间隔的情况，逐个tick改变物体坐标，其实就是一个指令运行一帧的分段表现 好了，写到这里基本上也讲完了，有什么疑问欢迎交流哈，我要睡觉去了 总结 严格的帧同步，服务器必须等待所有客户端上报帧数才会下发当前帧命令，会造成一卡全卡 乐观锁模式，服务器不等，会定时推帧，卡顿的人在收到数据后，自行加速补帧来追赶上正确的游戏速度 一切物体总保持匀速直线运动状态或静止状态，直到有外力迫使它改变这种状态为止 在帧同步中绝对顺序是靠帧数来决定的，而物理时间只是一个数字，想快就快，想慢就慢 对于UDP丢包问题，上行采用重发3次，下行采用根据网络情况在2次到9次范围内调整 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 纵有千古，横有八荒，沧海一粟，还妄图超脱三界吗？ 2024-4-24 20:47:44]]></content>
      <categories>
        <category>LockStep</category>
      </categories>
      <tags>
        <tag>Lockstep</tag>
        <tag>帧同步</tag>
        <tag>js</tag>
        <tag>demo</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[帧同步入门之日遭遇CORS（跨域资源共享）问题的暴击]]></title>
    <url>%2Fblog%2F2024%2F04%2F24%2F%E5%B8%A7%E5%90%8C%E6%AD%A5%E5%85%A5%E9%97%A8%E4%B9%8B%E6%97%A5%E9%81%AD%E9%81%87CORS%EF%BC%88%E8%B7%A8%E5%9F%9F%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB%EF%BC%89%E9%97%AE%E9%A2%98%E7%9A%84%E6%9A%B4%E5%87%BB%2F</url>
    <content type="text"><![CDATA[前言工作以来的每个游戏项目都是用的状态同步，所以一直想找时间看一下帧同步的实现细节，网上搜到了一些开源项目，有cocos的、有Unity的，有原生C#的，大多数项目作为入门资料来看都比较重，虽然定点数计算、UDP加速这些已经成为了帧同步策略的标配，但是对于一个初学者来说，这些内容的引入无疑增加了学习成本，所以我还是想找一款帧同步纯逻辑实现的代码，后来发现 LockstepDemo 这个项目还不错，那就从它开始学吧。 可以学习的帧同步项目 ahuangege/lockstep ：cocos creator客户端demo dudu502/LittleBee ：Unity客户端，相当绚丽的演示demo Zealous-w/LockStep ：C++实现，内容较少 JiepengTan/Lockstep-Tutorial ：Unity客户端，较为复杂，太重度 minigame-lockstep-demo ：小游戏帧同步服务配套示例，js实现 ookcode/LockstepDemo ：js+html实现，网页作为前端，浏览器就可运行，个人认为很适合初学者 LockstepDemo这个项目比较轻量，如何运行Readme中写的很清楚： 服务端 123$ cd server$ npm install$ node app.js 客户端（也可使用任意方法启动一个web服务） 12$ cd client$ python3 -m http.server 8080 启动完成后访问：http://127.0.0.1:8080/ 大概的原理利用node运行app.js监听3000作为服务器，然后启动监听8080端口的web服务，浏览器访问web页面作为客户端，来展示CS架构下的帧同步 打开两个浏览器都访问这个页面，登录后就匹配成功了，操作简单，演示方便，真是入门必备读物，但是到这里不出意外的情况下，要出意外了 我打开界面输入用户名之后提示我“连接服务器失败！”，真是醉了 查找问题幸好我还有一段网站开发的经历，所以熟练的按下F12打开了浏览器的调试页面，看到了如下报错 Access to XMLHttpRequest at ‘http://127.0.0.1:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=OyAd7iL‘ from origin ‘http://127.0.0.1:8080‘ has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource.GET http://127.0.0.1:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=OyAd7iL net::ERR_FAILED 400 (Bad Request) 这个报错什么意思呢？就是说从’http://127.0.0.1:8080‘ 发起的原始请求在访问’http://127.0.0.1:3000‘ 时，其地址并不在访问资源允许的地址范围内，这个问题有个很常见的名字——跨域问题 好在前段时间做小游戏解决过跨域问题（其实第一次遇到这个问题在小游戏之前，先积累了经验才在小游戏中解决了这个问题），基本上知道朝哪个方向修改，所以一切还在可控范围之内，不过为了记录的全面一点，还是单独解释下跨域问题 CORS跨域问题跨域问题的英文简写是 “CORS”，全称是 “Cross-Origin Resource Sharing”，是一种用于在不同源之间共享资源的机制。”源”是由协议（如 http 或 https）、主机名和端口号组成的组合，如果两个 URL 的这三个部分都相同，那么它们属于相同的源。 在 Web 开发中，浏览器会实施同源策略，这是一种安全机制，用于防止在一个网页的上下文中加载的资源（例如 JavaScript、CSS、图像等）去访问另一个不同源的资源。换句话说，网页只能访问与其自身相同源的资源，不能直接访问其他源的资源。 CORS 提供了一种机制，允许服务器声明哪些源的请求是被允许的，以及允许在响应中附加一些特定的头信息来告诉浏览器是否允许跨源请求。如果服务器允许跨源请求，浏览器将允许网页访问该资源，并将响应头中包含的额外信息传递给网页的 JavaScript。 跨域限制是浏览器行为，不是服务器行为（看你个人理解，服务器只是配合，可以什么都不做），通过代理服务器，或者其他工具发送请求就能轻松绕过。 因为浏览器使用门槛非常低，为了防止别有用心的人攻击普通用户，所以引入同源策略。服务器间没有跨域这种说法，使用和破解难度较大，就交给用户自己防备了。 浏览器的同源策略，对于不同源的站点之间的相互请求会做限制，具体限制了以下行为： Cookie、LocalStorage 和 IndexDB 无法读取 DOM 和 JS 对象无法获取 Ajax请求发送不出去 跨域资源共享（CORS）的目的之一就是在允许跨源资源共享的同时保证安全性。如果不限制跨域，可能会引发以下安全问题： 跨站脚本攻击（XSS）： 恶意网站可以在用户访问时向另一个网站发送请求，并执行在受害者浏览器中执行恶意脚本，从而窃取用户的敏感信息、篡改页面内容或执行其他恶意操作。 跨站请求伪造（CSRF）： 恶意网站可以伪造请求，利用用户在其他网站上的登录状态，发送未经用户授权的请求，例如修改用户信息、发起转账等。 信息泄露： 恶意网站可以发送跨域请求，并读取其他网站的敏感信息，例如 Cookie、本地存储、IndexedDB 中的数据，从而获取用户的个人信息或其他敏感数据。 点击劫持： 恶意网站可以通过嵌入透明的 iframe 将另一个网站的页面覆盖在自己的页面上，并诱使用户在不知情的情况下点击覆盖的页面上的按钮或链接，执行未经用户授权的操作。 缓存投毒： 恶意网站可以通过发送恶意请求并利用缓存投毒技术来污染浏览器的缓存，从而在用户之后访问相同资源时执行恶意代码。 数据冒充和篡改： 恶意网站可以发送伪造的请求，并篡改或冒充其他网站的数据，例如修改在线购物网站的购物车内容。 说到这里可能还迷迷糊糊的，举个具体点的例子吧，如果觉得不对尽可以来喷，我改就是了： 假如你访问了一个恶意网站，如果没有同源策略，恶意网站可以向一个你已经登录的敏感信息上网站发送http请求，因为带着你的cookie信息，可以直接获取敏感数据，但有同源策略，这种行为就被浏览器限制住了，在服务器端不需要做任何设置就能实现。 但是如果A网站和B网站是兄弟网站，B网站的资源允许A网站访问，那么就可以在B网站的服务器上将A网站域名配置成可以跨域访问的origin，这样就可以了 解决跨域问题说了这么多，下面要开始解决问题了，从前面解释来看，这个问题应该在服务器上处理，也就是在 http://127.0.0.1:3000 上允许 http://127.0.0.1:8080 就行了 找到 LockstepDemo/server/app.js 文件，其中服务器部分是这样定义的 12var server = require('http').Server()var io = require('socket.io')(server) 改为如下写法，允许所有跨域请求，注意 origin 字段的值： 12345678var server = require('http').Server()var io = require("socket.io")(server, &#123; allowEIO3: true, cors: &#123; origin: "*", methods: ["GET", "POST"] &#125;&#125;); 依旧报错 Access to XMLHttpRequest at ‘http://127.0.0.1:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=OyAeQnU‘ from origin ‘http://127.0.0.1:8080‘ has been blocked by CORS policy: The value of the ‘Access-Control-Allow-Origin’ header in the response must not be the wildcard ‘*‘ when the request’s credentials mode is ‘include’. The credentials mode of requests initiated by the XMLHttpRequest is controlled by the withCredentials attribute.GET http://127.0.0.1:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=OyAeQnU net::ERR_FAILED 200 (OK) 根据CORS规范，当请求中包含凭据时，响应中的Access-Control-Allow-Origin头不能设置为通配符*，而必须指定允许请求的具体来源。 换句话说，如果你在客户端发起了带有凭据的请求（例如cookies或HTTP认证信息），服务器在响应中必须指定确切的来源，而不能使用通配符。 要解决这个问题，你需要在服务器端设置Access-Control-Allow-Origin头，将其设置为允许请求的确切来源。如果你的请求是来自特定的域（例如http://127.0.0.1:8080） ，那么你的服务器应该将Access-Control-Allow-Origin头设置为该域，修改如下： 12345678var server = require('http').Server()var io = require("socket.io")(server, &#123; allowEIO3: true, cors: &#123; origin: "http://127.0.0.1:8080", methods: ["GET", "POST"] &#125;&#125;); 还是报错 Access to XMLHttpRequest at ‘http://127.0.0.1:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=OyAgWOx‘ from origin ‘http://127.0.0.1:8080‘ has been blocked by CORS policy: The value of the ‘Access-Control-Allow-Credentials’ header in the response is ‘’ which must be ‘true’ when the request’s credentials mode is ‘include’. The credentials mode of requests initiated by the XMLHttpRequest is controlled by the withCredentials attribute.socket.io.js:1456GET http://127.0.0.1:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=OyAgWOx net::ERR_FAILED 200 (OK) 这个错误表明在响应的Access-Control-Allow-Credentials头中设置了空字符串，而应该设置为true。这是因为当请求中包含凭据（例如使用了withCredentials属性），并且服务器允许使用凭据时，响应的Access-Control-Allow-Credentials头必须设置为true，修改如下： 123456789var server = require('http').Server()var io = require("socket.io")(server, &#123; allowEIO3: true, cors: &#123; origin: "http://127.0.0.1:8080", methods: ["GET", "POST"], credentials: true // 设置为true以允许使用凭据 &#125;&#125;); 这次再刷新页面，控制台打印信息“Socket连接成功： 0LLd8Rm9mO3zbOkZAAAD” 这个问题算解决了，这里指定了 http://127.0.0.1:8080 可以跨域访问，如果不是通过web服务，而是使用浏览器直接打开index.html文件，地址为 file:///E:/WorkSpace/opensource/LockstepDemo/client/index.html，还会出现以下错误： Access to XMLHttpRequest at ‘http://127.0.0.1:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=OyAg_nn‘ from origin ‘null’ has been blocked by CORS policy: The ‘Access-Control-Allow-Origin’ header has a value ‘http://127.0.0.1:8080‘ that is not equal to the supplied origin. 相信你也知道怎么改了，把 “null” 加到允许列表就行了 123456789var server = require('http').Server()var io = require("socket.io")(server, &#123; allowEIO3: true, cors: &#123; origin: ["http://127.0.0.1:8080", "null"],, methods: ["GET", "POST"], credentials: true // 设置为true以允许使用凭据 &#125;&#125;); 演示界面如下，还挺不错的，今天就先写到这，以后抽时间再分析代码 总结 ookcode/LockstepDemo 项目可作为帧同步入门教程 跨域限制是浏览器行为，不是服务器行为，服务器只是配合，可以什么都不做，想实现跨域可以让服务器参与配置 Access-Control-Allow-Origin 和 Access-Control-Allow-Origin 都是跨域问题处理中常见字段，但不同语言中字段赋值形式不同 浏览器的同源策略是保护小白使用者的，在服务器间访问资源并不用遵守同源策略 这次尝试有一个问题没解决，就是怎样允许所有的跨域行为，origin配置成 *不行，说是客户端带有认证信息，但是我没找到在哪 客户端请求连接很简单 socket = io.connect(&#39;http://127.0.0.1:3000&#39;)，并未发现认证信息，有知道的大佬欢迎指出 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 不同的维度品不同的人生，你认为你还在，我看你已经无了~ 2024-4-23 21:33:52]]></content>
      <categories>
        <category>LockStep</category>
      </categories>
      <tags>
        <tag>Lockstep</tag>
        <tag>CORS</tag>
        <tag>跨域</tag>
        <tag>帧同步</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s小白的学习初体验]]></title>
    <url>%2Fblog%2F2024%2F03%2F29%2Fk8s%E5%B0%8F%E7%99%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[前言有些时候的巧合让人匪夷所思，前两周刚刚尝试了一遍Docker操作，紧接着就收到好朋友说要学习k8s容器部署的建议，最近两周抽空看了一些关于k8s的知识，相关概念真的是太多了，概念本身是枯燥的，但是当概念运行起来就有意思多了，这时候取出费曼学习大法，尝试着用自己的话来描述这些概念，看看自己是不是真的学会了。本文k8s相关概念均来自个人理解，后续随着学习深入可能会有改正，勿喷。 学习资料找到一些官方文档和一些写的比较通俗的教程，再有疑问直接去官网查吧 学习 Kubernetes 基础知识 什么是 Rancher？ Kubernetes（K8S）简介 K8S学习教程 学到了什么我就尝试用直白的语言来描述一下，不一定准确，听听看 ks8是Kubernetes的简称，Rancher是一个管理k8s的工具，可以操作k8s定义 Deployment 在 Node 上创建一系列包含若干 Container 的 Pod，也可以操作k8定义 Service 在指定 Pod 上建立统一对外的服务，Deployment、Service、Node、Pod 这些在k8s中被称为资源，为了便于管理可以划分到不同的 Namespace 中。 单单就这两句话包含了7、8个名词概念，要是把他们的定义列出来，保准得晕晕乎乎的，并且有些名词在不同的语境下有不同的含义，比如Docker是一个工具、是一个运行时、是一个平台等等，所以有些概念不结合具体的情景还真是说不清楚，我们还是来想想为什么出现这些东西吧。 当项目规模还小的时候，我们把程序放到服务器上运行起来就行了，简单方便，后来随着规模的扩大，部署的机器越来越多，发现机器资源分配不好控制，可能机器的大部分资源被某个应用占用，导致其他应用无法正常运行，无法做到应用隔离。 后来引入了虚拟机，虽然解决了资源控制问题，但在单个物理机上运行多个虚拟机，每个虚拟机都是完整独立的系统，性能损耗大，还有一个之前遗留的问题，就是运行环境的差异引发的问题常常使人焦头烂额，所以容器部署应运而生。 将运行程序和所依赖的环境打包成镜像，放到容器运行时上运行，可以保证所有程序的运行环境都一样，容器相比虚拟机性能损耗小，可实现资源隔离，CPU和内存可按需分配，所有容器共享主机的系统，最有名的要数Docker了 从这个角度来看docker或者容器运行时很像JVM，当年Java号称一次编写处处运行，靠的就是Java虚拟机，所有的Java源码会被编译成字节码，在不同平台上的JVM中被翻译中对应平台的指令，而Docker的身份和地位很像这个JVM，容器运行时可以在运行之前打包好的镜像，并且在不同的系统平台上模拟出目标平台的环境，来保证“一次打包处处运行” 现在知道了，我把应用程序和它依赖的运行环境打包成镜像，就可以在任何安装了Docker的机器上运行，不管这台机器是什么系统，这样就可以快速扩展程序规模了，但是新的问题还是会出现的 因为程序员太懒了，让他部署几个服务器还没啥问题，要是成百上千台服务器，每次扩展或更新都需要逐个拉取镜像、版本回滚、逐个启动，简直痛不欲生，在半夜被薅起来更新维护时还容易出错，所以这时候 Kubernetes 就诞生了，因其K和s中间有8个字母简称为k8s，它可以让你管理让你轻松管理成千上万台机器的集群，这些机器上的容器都归它管，一个命令就可以搞定版本升级、版本回滚等操作，还可以不停机的灰度更新，确保高可用、高性能、高扩展。 我的天，k8s都这么牛了，那 Rancher 又是啥？难道他比 k8s 还厉害，人类的懒惰是无极限的，k8s 虽然一个命令可以完成很多需求，但有些人就是一个命令也不想输啊，哈哈！ 尽管 k8s 是一个强大的容器编排平台，可以同时管理集群中数以万计的机器，但人们为了方便还是开发了Rancher吗，它提供了用户友好的图形用户界面、RBAC（基于角色的访问控制）、日志和监控、多集群管理等，同时它还提供了一个集成的应用商店，其中包含了许多常用的容器化应用程序和工具，如数据库、监控、日志、CI/CD 等，通过 Rancher 的界面轻松部署这些应用程序，并与 Kubernetes 集群集成。 有些人用k8s为了方便部署和管理容器，而有些人只想更方便，选择了 Rancher 来管理看 k8s，之后会不会有人开发的新东西来管理 Rancher 呢，这有什么不可能的呢？ 概念白话文前面提到了很多概念，也讲了一些有关这些东西的来历，接下来让我用自己的理解来说说这些名词到底指什么。 镜像：最早听说这个词是系统镜像，将后缀为 .iso 的系统镜像文件刻录成光盘，或者烧制成U盘启动盘来安装系统，容器化中的镜像含义和系统镜像类似，就是用过用程序以及用于运行应用程序所需的所有依赖项打个包，称为镜像，它可以放到容器运行时中运行，相同的镜像可以重现相同的环境，就像相同的ISO文件会安装出相同的系统一样 Docker：往大点说这是指一个容器化平台，当然也可以是一个软件，或者是一项技术规范，用它可以开发、打包、部署和运行应用程序。通过使用 Docker，开发人员可以将应用程序及其依赖项（如库、运行时环境等）打包成一个轻量级的容器，然后在任何支持 Docker 的环境中运行这个容器，而不用担心环境的差异性。简单点说，它是一个工具，可以打包一个镜像，也可以运行一个镜像 Dockerfile：用来说明怎么打包镜像的，使用特定的描述性语言，告诉Docker怎么打包出一个镜像 容器化：将引用程序和运行环境打包成镜像，并批量运行到其他容器运行时上的过程 镜像和容器的关系：镜像是 Docker 中用于打包应用程序及其依赖项的静态文件，是只读的，一旦构建完成就不能修改，容器是镜像的一个运行时实例，它可以被启动、停止、暂停、删除等操作。可以简单类比下 class 和 object 的关系 k8s：为了管理和编排大量的容器而开发出来的工具，可以批量自动化的完成一些容器的操作，由他又引出了大量新的概念 k3s：一个轻量级的 Kubernetes 发行版，体积小、资源消耗低，采用了更少的组件和依赖项，并且对 Kubernetes 的安装和配置进行了简化，使得部署和管理更加容易 Pod：k8s调度、管理的最小单位，它包含一个或多个紧密关联的容器 Node：是集群中的计算节点，有master主节点和worker工作节点之分，负责提供计算资源、网络和存储服务，可以是真实的物理机，也可以是运行在物理机上的虚拟机。 Deployment: 是 k8s 中用于声明式管理 Pod 和 ReplicaSet 的控制器，可以认为是一个描述怎样使用 Pod 资源的规则，应用一个Deployment就是按照它的描述来组织和使用Pod Service：是 k8s 中用于提供对一组 Pod 的网络访问的抽象，可以将一组 Pod 绑定统一对外提供服务 Namespace：k8s 中用于将资源分类的一个逻辑概念，类似于编程语言中的命名空间 Docker Compose：与k8s同类，也是用于管理容器化应用程序的工具，但规模较小适用于小型项目和开发环境 Rancher：一个开源的容器管理平台，拥有统一的可视化界面，可以同时管理多个k8s，括本地、云端或混合部署的集群 Rancher Desktop：是一款用于本地开发的工具，基于K3s构建，旨在提供轻量级的开发环境，使开发人员能够轻松地在自己的计算机上进行容器化应用的开发和测试 Workloads：工作负载，有哪些类型的活，包括Deployment（部署无状态Pod），StatefulSet（部署有状态Pod），DaemonSet（节点监控和日志收集），Job（一次性任务），CronJob（定时重复任务） 组件：每个角色要干指定活所以需要配备的一些功能模块，比如每个 Node 都有Kubelet组件，会与k8s控制平面进行通信，接收指令来管理此Node 上的 Pod 控制平面：运行在主节点上的集群的核心组件之一，负责管理集群的各种操作、监控集群状态，并确保集群的稳定运行，通常包含 kube-apiserver、etcd、kube-controller-manager、kube-scheduler、cloud-controller-manager 等 一些疑问 容器化是不是只能用Docker? 不是的，Docker只是实现容器化的一种可选的途径，只要实现了容器化标准的工具都可以实现容器化，作为容器运行时，比如 k8s 为了提高效率，减少不必要的开销，已经将默认的容器运行时从 Docker 改成了 containerd，其他类似于Docker的运行时还有 Podman、rkt（Rocket）、LXC（Linux Containers、CRI-O等 k8s 是唯一的容器编排工具吗？ 不是的，就像前文提到的 Docker Compose 也能管理容器的使用，只是适用规模较小，除此之外类似的工具还有 Docker Swarm、Apache Mesos、Nomad、OpenShift等 总结 Docker 生成和运行镜像，k8s 管理这些被Pod包装并运行在Node上的容器，Rancher 管理 k8s 集群 部署无状态Pod使用 Deployment，部署有状态Pod使用 StatefulSet，StatefulSet 会固定每个 Pod 的名字 k8s不会为你处理数据的存储，我们可以为运行数据库的Pod挂载一个磁盘来确保数据的安全，云存储、本地磁盘、NFS都是可以选择的对象 kubectl get all 可用于查看常用资源，包括Pod、Service、Deployment、ReplicaSet等 也可以单独查询某项信息 kubectl get namespace、kubectl get pod、kubectl get node、kubectl get deployment ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 惟愿吾儿愚且鲁，无灾无难到公卿，可能不是东坡的心里话吧，但一定是受到刺激以后才有的感想~ 2024-3-28 20:34:05]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>Kubernetes</tag>
        <tag>docker</tag>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将python编写的网站制作成docker镜像并上传到Github Packages上]]></title>
    <url>%2Fblog%2F2024%2F03%2F09%2F%E5%B0%86python%E7%BC%96%E5%86%99%E7%9A%84%E7%BD%91%E7%AB%99%E5%88%B6%E4%BD%9C%E6%88%90docker%E9%95%9C%E5%83%8F%E5%B9%B6%E4%B8%8A%E4%BC%A0%E5%88%B0Github-Packages%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[@[toc] 前言还记得上一篇《借助ChatGPT使用Python搭建一个工具网站》总结中我利用ChatGPT写了一个网站，最终它运行良好，就在昨天我看到了Github Packages，不久前刚刚使用了GitHub Actions，我发现Github在被微软收购后，并没有变的更“闭源”，之前广大网友还在调侃，最大的闭源软件公司收购了最大的开源平台，看来一切还在向好的方向发展，简单介绍下前面提到的这两个都是什么东西。 GitHub Actions 是一个Github原生的持续集成和部署的工作流组件。通俗来说就是Github免费给你提供虚拟主机，由你编写工作流脚本来进行源码的检出，编译，测试，和发布。类似的我们可以想象成Github给每个仓库都免费绑定了一个Jenkins服务，编写pipeline脚本即可进行源码的集成和发布。 GitHub Packages 是一个和每一个代码仓库关联的软件包仓库。通俗来说就是代码仓库中存放的是源码，软件包仓库中存放的是编译输出后的可以被各个语言生态的依赖管理工具直接依赖的lib，类似的我们熟知的有maven中央仓库和nmp仓库。 今天我们只看GitHub Packages这部分，并且将范围缩小到制作一个Docker镜像并上传到GitHub Packages，用来制作的项目就是前文中我们在ChatGPT辅助下编写的网站，本文只写流程，对于其中的概念不会过多解释，大家可以借助网络来解决概念性问题，但是我在完成目标过程中遇到的坑会列举出来，希望能帮助有相同经历的人，同时自己记录一下防止忘记。 Docker既然要制作docker镜像，那先简单了解下docker是什么 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 版本开始，则进一步演进为使用 runC 和 containerd Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(Continuous Delivery/Deployment) 系统进行自动部署。而且使用 Dockerfile 使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。 以上我只是摘抄了一部分，想要看更详细的解释请跳转至 《Docker 技术入门与实战》，总的来说就是能解决环境一致性问题，并且可以方便部署的，把你的软件及运行环境打个包，在目标机器上直接运行，最大程度的去除环境差异，被打成的这个包就是docker镜像，我们可以把这个镜像丢到“任何”一台机器上去执行，不用再操心安装繁琐的运行环境。 但是无论是制作docker镜像还是运行docker镜像都需要先安装docker环境，相当于它来做软件和机器之间的代理，是不是听起来像虚拟机，他俩常常被拿来比较，而docker更加轻量级，目前应用广泛，我们来列举下安装步骤，事先说明操作系统是Ubuntu20.04，项目是minimalist-tool-web，未特殊说明时则是在项目根目录下执行命令。 安装docker 更新apt包列表 1sudo apt update 安装依赖包，以允许apt通过HTTPS使用存储库 1sudo apt install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common 添加Docker的官方GPG密钥 1curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 添加Docker的APT存储库 1sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" 再次更新apt包列表 1sudo apt update 安装Docker 1sudo apt install -y docker-ce 验证Docker是否安装成功： 1sudo docker --version 注意事项 添加GPG密钥的时候有可能报错 curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to download.docker.com:443，相同命令多尝试几次就能看见 OK 我在 sudo apt update 的时候报错 Failed to fetch https://dl.google.com/linux/chrome/deb/dists/stable/InRelease The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY E88979FB9B30ACF2，通过执行 wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - 修复好的，不过这可能是个例，你们不一定遇到 创建Dockerfile在项目根目录下创建一个名为 Dockerfile 的文件，并编写 Docker 镜像的构建指令，以下是一个示例的 Dockerfile 文件，它是在初始命令模板下精简而成，目的是缩小镜像大小，调整命令后镜像从1.1G缩小到65M 1234567891011121314151617181920# 使用 Alpine Linux 作为基础镜像FROM python:3.8-alpine# 设置工作目录WORKDIR /app# 将当前目录下的所有文件复制到工作目录COPY . .# 安装项目依赖RUN pip install --no-cache-dir -r requirements.txt# 清理不必要的缓存RUN rm -rf /var/cache/apk/*# 暴露应用端口EXPOSE 9206# 运行应用CMD ["python3", "main.py"] 注意事项 WORKDIR /app 是指镜像中的目录，在当前操作系统中可能并不存在 COPY . . 很奇怪的写法，但是两个.含义是不同的，第一个只当前目录，第二个指镜像中的当前目录，也就是 /app 构建 Docker 镜像1$ sudo docker build -t minimalist-tool-web . 其中 -t minimalist-tool-web 指定了镜像的名称为 minimalist-tool-web，构建完成后，可以查看当前镜像 1234$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEminimalist-web-tool latest f11497961379 5 minutes ago 65.6MBhello-world latest d2c94e258dcb 10 months ago 13.3kB 运行 Docker 镜像1docker run -d -p 9206:9205 minimalist-tool-web 其中 -d参数表示在后台运行容器，-p 9206:9205 将容器内部的 9205 端口映射到宿主机的 9206 端口，这样就可以通过 http://localhost:9206 访问应用程序，至此，我的项目就已经制作成了 Docker 镜像，并且可以在 Docker 容器中运行了。 发布到Github Packages这部分确实花了我不少时间，我先把正确的流程描述完，然后再说一下我遇到的问题。 到Developer Settings (classic) 申请一个token，记得至少勾选 read:packages 和 write:packages 权限 登录docker，使用上一步申请的token 1sudo docker login ghcr.io --username your-github-username --password-stdin your-personal-access-token 给镜像添加标签 1sudo docker tag your-image-id ghcr.io/your-github-username/your-image-name:1.0.0 将docker镜像推送到Github Packages上 1sudo docker push ghcr.io/your-github-username/your-image-name:1.0.0 坑坑到位申请token的坑许多教程上申请token截图还是只有一个页面，但是现在（2024-3-8）的Github上申请的token的页面已经分成了两类，具体导航路径在『Github头像』-&gt;『Settings』-&gt;『Developer Settings』-&gt;『Personal access tokens』，到此处申请token页面分成了两个，分别是 Fine-grained tokens和 Tokens (classic)，最近几次其他的访问权限分配我都是用的前面这种Fine-grained tokens，虽然标记着 Beta，但这是官方推荐的形式。 但这一次我却遇到了 unauthorized: unauthenticated: User cannot be authenticated with the token provided. 这个问题，后来改成Tokens (classic)这种类型的token才解决，可以看到的是第一个Fine-grained tokens中我没有找到read:packages 和 write:packages 权限，不知道是不是和这个有关，我还在尝试。 那么在申请 Tokens (classic) 时要记得勾选read:packages 和 write:packages 权限，最好把 delete:packages 和 repo 也勾选上，如果想通过token更新Github Actions，最好也要勾选上 workflow Fine-grained tokens：这种token形如 github_pat_xxx，位数较长 Tokens (classic)：这种token形如 ghp_xxx，位数较短 docker登录的坑很多帖子可能是时间太久远了，里面提到的地址都是 docker.pkg.github.com，我这里倒是都显示登录成功，但是每次推送都会提示 no basic auth credentials，就像下面这样 12345678910111213141516171819$ cat ~/TOKEN.txt | docker login https://docker.pkg.github.com -u AlbertGithubHome --password-stdinWARNING! Your password will be stored unencrypted in /home/shz/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded$ sudo docker push docker.pkg.github.com/albertgithubhome/minimalist-tool-web/tools-web:1.0.0The push refers to repository [docker.pkg.github.com/albertgithubhome/minimalist-tool-web/tools-web]c0f047bfac29: Preparing87174263225b: Preparing5462d39b06ad: Preparing2c604764da2f: Preparinga469c5a79f90: Preparing7402639a4746: Waiting42672d5bf49e: Waiting678cac8b069e: Waitingd4fc045c9e3a: Waitingno basic auth credentials 后来在 Stack Overflow 上找到一个问题，里面用的是 ghcr.io 修改之后发现才可以 给镜像添加标签的坑这个需要注意的点是整个标签的字母必须都是小写的，而我的用户名是大写，所以我按照老版本的教程创建了类似 docker tag f11497961379 docker.pkg.github.com/albertgithubhome/minimalist-tool-web/tools-web:1.0.0 的标签，但是我的用户名是大写的，每次推送失败告诉我未验证时，我无法确定是哪类的问题: token类型有问题 权限有问题，登录没有用sudo 登录地址有问题，https://docker.pkg.github.com地址打不开 我的用户名有大写字母，与推送的镜像名中的用户名不匹配 反正费了九牛二虎之力吧，把这些原因一一排除掉，总算能推送上去了 还有一点，之前老版本的教程标签格式要求是 docker.pkg.github.com/your-github-username/your-repository/your-image-name:1.0.0，在新版的教程中改为了 ghcr.io/your-github-username/your-image-name:1.0.0，不要求添加库的名称了 docker推送的坑这一步的坑都是前面几步积累得来的，因为这一步的失败根本不知道前边哪一步出错了，更离谱的是在登录这一步，无论用户名是什么，token是什么都显示登录成功，导致后面查找原因浪费了不少时间 推送时直接填写前面的标签名即可 123456789101112$ sudo docker push ghcr.io/albertgithubhome/minimalist-tool-web:1.0.0The push refers to repository [ghcr.io/albertgithubhome/minimalist-tool-web]c0f047bfac29: Pushed87174263225b: Pushed5462d39b06ad: Pushed2c604764da2f: Pusheda469c5a79f90: Pushed7402639a4746: Pushed42672d5bf49e: Pushed678cac8b069e: Pushedd4fc045c9e3a: Pushed1.0.0: digest: sha256:f749fd2eba021248c97c8a44374772165372354b9d48f4f6b845bbce46d02402 size: 2201 这些参数可以在 ghcr.io 查看，页面上有一个示例 123echo $PAT | docker login ghcr.io --username phanatic --password-stdindocker tag app ghcr.io/phanatic/app:1.0.0docker push ghcr.io/phanatic/app:1.0.0 在Github Packages上查看打开你账号下的Packages页签就能看到了，点击进去可以将这个 package 关联到指定项目，这样在项目的首页右侧的packages区域就能看到这个package，在package的介绍页面会教你怎么使用这个镜像 1docker pull ghcr.io/albertgithubhome/minimalist-tool-web:1.0.0 总结 GitHub Actions 是一个Github原生的持续集成和部署的工作流组件，相当免费绑定了一个Jenkins服务 GitHub Packages 是和每一个代码仓库关联的软件包仓库，类似我们熟知的maven中央仓库和nmp仓库 运行docker命令 docker run -d -p 9206:9205 minimalist-tool-web，其中minimalist-tool-web是镜像名 停止docker命令 sudo docker stop af98cf2f93bf，其中af98cf2f93bf是容器ID 推送docker命令 sudo docker push ghcr.io/albertgithubhome/minimalist-tool-web:1.0.0 拉取docker命令 sudo docker pull ghcr.io/albertgithubhome/minimalist-tool-web:1.0.0 最终Fine-grained tokens应该会替换到Tokens (classic)，但现在感觉还不能完全代替 接下来会尝试用.yml编写一个GitHub Action控制项目更新时自动创建镜像和发布 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 从秦灭六国、焚书坑儒开始，从罢黜百家、独尊儒术开始，洗脑便已经登峰造极了~ 2024-3-8 20:59:13]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
        <tag>python</tag>
        <tag>GithubPackages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[借助ChatGPT使用Python搭建一个工具网站]]></title>
    <url>%2Fblog%2F2024%2F03%2F02%2F%E5%80%9F%E5%8A%A9ChatGPT%E4%BD%BF%E7%94%A8Python%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[前言不知不觉ChatGPT已经风靡一年多了，现在基本每天工作时都会用到，相比于传统的搜索引擎它究竟强在哪呢？我觉得以往的搜索引擎是一个机器，你给它关键信息它能返回匹配关键词的内容数据，而ChatGPT更像是一个机器“人”，它同样可以返回给你匹配关键信息的数据，同时可以按照你的指令进行加工，更优秀的是它还记得你跟它说过的每一句话，就像拥有一个可以存储交流记录的大脑一样，你可以赋予它一些性格和规则，让它来帮你做事情，交流的多了它就能变成一个性格鲜明的“生物”，并且可以无条件接受你的命令，所以最近突发奇想让它帮我搭一个网站怎么样，说干就干起来了。 网站搭建过程怀着期望的心情我给了它一个比较简单的任务 完成的还不错，按照他的代码和步骤确实得到了一个可以转换文字编码的单页面网站，但是flask有点陌生，我想了解更多，于是它给了我解答 但是这个网站在运行过程中控制台输出了一些警告，我让它帮我解决一下 虽然没解决，但我知道了警告的含义，于是我想改善一下页面工具使用体验 目前这个网站只能在本机访问，在其他机器访问时被拒绝，我又找它想办法 此时我发现了一个错误，不知什么时候它不能转base64了，通知它修正一下 现在是在控制台启动的，把它变成一个真正的后台网站 继续优化，将页面拆分，各司其职 页面分离后报了错误找它解决更改页面跳转样式 即使你说第一版最好它也不会发脾气 让它帮你记录当前项目以备后续扩展 就这样一步一步的，我又加了新的需求，网站出来后效果还不错，以下是展示页 这是主页面，点击卡片会进行跳转 ) 功能都可以正常使用，基本实现了我想要做一个极简工具网站的需求 总结 ChatGPT 是真的强，可以按照你的指令一步步思考和总结，再也不是搜索引擎的匹配规则了 强大的工具要学着使用，每一波新生代的工具都会带来一些变革，别再只把它当成搜索引擎了 这周参加了一堂课，讲解ChatGPT的一些实际应用，其实已经遍地开花了，很多项目组都已经玩出花了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 想要逃却逃不掉，只能转身面对~ 2024-3-1 21:49:59]]></content>
      <categories>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>工具</tag>
        <tag>ChatGPT</tag>
        <tag>web</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在golang语言中简单使用protobuf时遭遇go_package困难重重]]></title>
    <url>%2Fblog%2F2024%2F02%2F18%2F%E5%9C%A8golang%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8protobuf%E6%97%B6%E9%81%AD%E9%81%87go-package%E5%9B%B0%E9%9A%BE%E9%87%8D%E9%87%8D%2F</url>
    <content type="text"><![CDATA[前言Protobuf，全称Protocol Buffers，是一种由Google开发的用于序列化结构化数据的开源数据交换格式，Golang作为一种后端语言，免不了使用protobuf这种数据交换格式，本来打算写个小例子展示一下怎样在go语言中使用protobuf，结果遇到的问题到不少，特别是这个go_package的要求，也是之前在项目开发过程中遇到的，这次就查找一下具体原因以及怎么使用吧。 编写示例为了能对这个示例有一个整体的轮廓，我先把编写好的目录结构展示一下，后面再逐步实现 123456789$ tree lib_protobuflib_protobuf├── go.mod├── go.sum├── lib_protobuf.go├── msgproto│ └── examplemsg.pb.go└── srcproto └── examplemsg.proto 简单解释下，examplemsg.proto proto源文件，examplemsg.pb.go 是proto导出文件，lib_protobuf.go 是proto协议使用和测试文件 示例初始化执行下面的命令，创建示例基础目录文件结构 12345678910111213$ mkdir lib_protobuf$ cd lib_protobuf$ mkdir srcproto$ touch srcproto/examplremsg.proto$ touch lib_protobuf.go$ go mod init lib_protobufgo: creating new go.mod: module lib_protobufgo: to add module requirements and sums: go mod tidy$ go mod tidy$ lsgo.mod lib_protobuf.go srcproto 编写协议文件内容先编写proto文件，文件内容如下： 1234567syntax = "proto3";message Person &#123; string name = 1; int32 age = 2; repeated string hobbies = 3;&#125; 这个协议文件只定义了一个名为 Person 的结构，其中包括 name 、age、hobbies 三个字段 导出协议文件这是本文的重点，至于怎么用proto文件定义结构，各个语言都是相通的，但是在导出时遇到 go_package 文件却是 golang 语言特有的，所以本文重点在这部分多花些篇幅，解释一下遇到的各种问题 protoc 和 protoc-gen-go导出时需要是使用工具 protoc，而平时你可能听说过或者在.pb.go文件中看到过 protoc-gen-go 这个工具的名字，其实 protoc-gen-go 是一个用于生成 Go 代码的插件，它是 Protocol Buffers 编译器 protoc 的一部分。当您使用 protoc 编译 .proto 文件时，可以通过指定 --go_out 标志来指定要使用的 protoc-gen-go 插件，从而生成对应的 Go 代码文件。这个插件会根据 .proto 文件中定义的消息和服务等内容，生成与之对应的 Go 结构体、接口和方法等代码。因此，protoc-gen-go 和 protoc 是紧密相关的，它们共同用于将 Protocol Buffers 文件编译为 Go 语言中的数据结构和服务定义。 执行导出命令使用protoc将proto文件导出golang可用的文件最重要的两个参数如下： --proto_path：proto源文件所在文件夹--go_out：proto文件导出的目标文件夹 执行命令如下： 1234567891011$ cd lib_protobuf$ protoc --proto_path=srcproto --go_out=. examplemsg.protoprotoc-gen-go: unable to determine Go import path for "examplemsg.proto"Please specify either: • a "go_package" option in the .proto source file, or • a "M" argument on the command line.See https://developers.google.com/protocol-buffers/docs/reference/go-generated#package for more information.--go_out: protoc-gen-go: Plugin failed with status code 1. 不出意外的果然出意外了，告诉我不能确定 examplemsg.proto 文件的导入路径，换句话说就是现在这样不能导出一个可以被正常导入的包，有两种解决办法，第一种就是在.proto文件中添加 go_package 选项，第二种是在执行导出的命令行中添加M参数，详细的解释参考链接 https://developers.google.com/protocol-buffers/docs/reference/go-generated#package 官方推荐是用第一种方式，也就是直接在.proto文件中添加 go_package 选项，假如我们这样写： 1option go_package = "msgproto"; 然后直接导出会遇到新的错误，提示”msgproto”并不是文件”examplemsg.proto”的一个有效的导入路径 12345678$ protoc --proto_path=srcproto --go_out=. examplemsg.protoprotoc-gen-go: invalid Go import path "msgproto" for "examplemsg.proto"The import path must contain at least one period ('.') or forward slash ('/') character.See https://developers.google.com/protocol-buffers/docs/reference/go-generated#package for more information.--go_out: protoc-gen-go: Plugin failed with status code 1. 要求必须至少包含一个 . 或者 /，那么把选项改成下面这样就可以了 1option go_package = "./msgproto"; 有时为了在当前目录下生成，又要指定包名，可以写成这样： 1option go_package = "./;msgproto"; 使用分号将包路径和包名分开，但这种方法官方并不推荐，因为按照golang的包管理习惯，文件夹下的所有文件包名应该与文件名相同，通过包的路径能够推测是包名才对，否则写成 option go_package = &quot;./msgprotoA;msgprotoB&quot;; 就很奇怪了 关于go_package至少包含一个 . 或者 /的要求，据说v1.4.0以上的版本就要求必须加/了，最近查了一些资料，解释如下： 在 Protocol Buffers（Protobuf）中，go_package 选项用于指定生成的 Go 代码的包名和导入路径。要求导入路径（import path）必须包含至少一个点（.）或正斜杠（/）的原因与 Go 语言的模块系统和包导入机制有关 这个要求是因为在导出时，protoc 需要能够将生成的代码文件放置在文件系统的适当位置，并且需要能够以合适的包路径来导入这些生成的代码。在大多数情况下，包路径应该具有与文件系统路径相对应的结构，因此至少需要包含一个句点（.）或正斜杠（/）字符来指示层次结构。 如果导入路径不包含这些字符，那么生成的代码文件将无法以合适的方式组织和导入，可能会导致编译器无法正确识别代码的位置和包结构。因此，为了确保生成的代码能够被正确导入和使用，protoc 对导入路径设置了这个最低要求。 但是我觉得这个解释不能让人信服，我把go_package的值写成 .msgproto、msgproto. 或者 ..msgproto 都是能正常导出的，并没有看到这种限制的好处，其中深层次的原因还得在今后的使用过程中慢慢体会了 编写协议使用文件在文件 lib_protobuf.go 中编写如下内容，执行 go mod tidy 和 go run . 查看输出结果 12345678910111213141516171819202122232425262728293031323334package mainimport ( "fmt" "log" msg "lib_protobuf/msgproto" "google.golang.org/protobuf/proto")func main() &#123; p := &amp;msg.Person&#123; Name: "Alice", Age: 30, Hobbies: []string&#123;"reading", "running"&#125;, &#125; // 序列化 data, err := proto.Marshal(p) if err != nil &#123; log.Fatalf("Failed to encode person: %v", err) &#125; // 反序列化 newP := &amp;msg.Person&#123;&#125; err = proto.Unmarshal(data, newP) if err != nil &#123; log.Fatalf("Failed to decode person: %v", err) &#125; fmt.Println("Original Person:", p) fmt.Println("Decoded Person:", newP)&#125; 123$ go run .Original Person: name:"Alice" age:30 hobbies:"reading" hobbies:"running"Decoded Person: name:"Alice" age:30 hobbies:"reading" hobbies:"running" 体会有关 go_package 值官方给出的示例是这种 example.com/project/protos/fizz，这就跟go.mod文件中的module定义一样，比如 module github.com/tealeg/xlsx 或 module github.com/peterbourgon/diskv/v3，提供的是一种导出之后的引用写法 但是 go_package 和 module 还确实有一些不同，因为它在导出时真的会根据 go_package 生成相应的路径，所以针对这些去情况我觉得一种比较好的写法是从项目根目录开始写，导出的目录也是根目录，这样在引用时直接写 go_package 就可以了，但是具体情况还需灵活应对 1option go_package = "lib_protobuf/msgproto"; 12$ lib_protobuf$ protoc --proto_path=srcproto --go_out=.. examplemsg.proto 12345678import ( "fmt" "log" msg "lib_protobuf/msgproto" "google.golang.org/protobuf/proto") 总结 Protobuf是一种由Google开发的用于序列化结构化数据的开源数据交换格式 使用protoc 工具可以将 .proto 文件导出成golang可用的数据交换文件 protoc-gen-go 是一个用于生成 Go 代码的插件，它是 Protocol Buffers 编译器 protoc 的一部分 .proto文件中的go_package选项的值至少需要包含一个 . 或者 / go_package 的值官方给出的示例是这种类似module名的写法 example.com/project/protos/fizz ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 仰天大笑出门去，我辈岂是蓬蒿人。不好意思，我们绝大多数都是蓬蒿人，都是陪跑的NPC，只是在穷尽一生的努力想做一颗稍微不那么矮的蓬蒿而已~ 2024-2-18 16:44:18]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>开发语言</tag>
        <tag>protobuf</tag>
        <tag>golang</tag>
        <tag>protoc</tag>
        <tag>go_package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单聊聊go语言中引用模块的版本控制以及invalid: should be v0 or v1, not v2问题的解决]]></title>
    <url>%2Fblog%2F2024%2F02%2F09%2F%E7%AE%80%E5%8D%95%E8%81%8A%E8%81%8Ago%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%BC%95%E7%94%A8%E6%A8%A1%E5%9D%97%E7%9A%84%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E4%BB%A5%E5%8F%8Ainvalid-should-be-v0-or-v1-not-v2%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[前言如果你接触go语言比较早，一定有过当年所有go源码全部放入 GOPATH 下的混乱经历，不过发展到今天，go的包管理使用 go.mod 和 go.work 已经能得心应手，满足绝大多数的开发不成问题，其实在 go.mod 引入之后，go的包管理就有了明显的改善，再也不像原来那样好似闹着玩一样了，关于go.mod使用和管理我也是一知半解，想要教程可去官网或者查找各种详解资料，只是最近有点疑惑，这个go.mod以及go.sum要不要自己手动管理，带着这个疑问我做了一些测试可以得出，go.mod 的最初不需要手动管理，如果后来需要调整引入包的版本，可以手动调整。 以下是来自ChatGPT的关于 go.mod 和 go.sum 作用的回答，还是比较清晰的，如果还不太明白可以看看后面的例子感受一下 go.mod 和 go.sum 文件是 Go Modules 的重要组成部分，用于管理项目的依赖关系。共同为 Go 项目提供了依赖管理的基础，确保了项目的稳定性和可维护性： go.mod： 定义了项目的模块信息，包括模块名称、版本和依赖项等。 确定了项目的模块根目录。 记录了项目所依赖的其他模块及其版本信息。 提供了一种清晰的方式来管理项目的依赖关系，使得项目的构建和维护更加可靠和可控。 go.sum： 记录了每个依赖模块的哈希值，以保证项目的构建是可复现的。 确保了项目依赖项的完整性，防止恶意篡改或不完整的依赖项被用于构建项目。 当使用 go mod verify 命令时，go.sum 文件会被用来验证依赖项的完整性。 &gt; 通常情况下，go.mod 文件是由 go 工具自动生成和管理的，不需要手动修改。当你执行一些与模块相关的操作时（例如使用 go get 安装新的依赖、使用 go mod tidy 整理依赖、或者更新依赖版本等），go 工具会自动更新 go.mod 文件以反映这些变化，但是，在某些特定情况下，你可能需要手动编辑 go.mod 文件，例如： 手动添加或删除依赖项：你可能希望手动添加新的依赖项，或者删除不再需要的依赖项。在这种情况下，你需要手动编辑 go.mod 文件，并执行 go mod tidy 命令以确保依赖关系的一致性。 手动指定依赖版本：有时你可能希望显式地指定某个依赖项的特定版本。在这种情况下，你需要编辑 go.mod 文件，并指定对应的版本号。 总的来说，虽然 go.mod 文件通常是由 go 工具自动维护的，但在某些情况下手动编辑是必要的。在编辑 go.mod 文件时，确保遵循 Go 模块的规范，并注意保持文件的格式正确和依赖关系的一致性。 具体示例假设有一个需求，要读取 .xlsx 格式表格文件的所有页签名，解析Excel文件这么基础的工作肯定有人写好了，我们直接引入module不需要重新再写一遍了，我们新建一个包含go.mod的工程来使用它 12345678$ mkdir lib_xlsx$ cd lib_xlsx$ go mod init lib_xlsxgo: creating new go.mod: module lib_xlsx$ touch lib_xlsx.go$ lsgo.mod lib_xlsx.go go.mod的初始文件内容为 123module lib_xlsxgo 1.19 打开 lib_xlsx.go 文件编写如下代码： 1234567891011121314151617181920package mainimport ( "fmt" "log" "github.com/tealeg/xlsx")func main() &#123; xlFile, err := xlsx.OpenFile("example.xlsx") if err != nil &#123; log.Fatalf("Error opening file: %s", err) &#125; for _, sheet := range xlFile.Sheets &#123; fmt.Println("Sheet Name:", sheet.Name) &#125;&#125; 代码是不是很简单，此时不出意外IDE会提醒你 could not import github.com/tealeg/xlsx 的错误，因为我们还没下载这个包，只需要在命令行执行 go get github.com/tealeg/xlsx 既可下载，同时会自动更新 go.mod 和 go.sum 文件 12$ go get github.com/tealeg/xlsxgo: added github.com/tealeg/xlsx v1.0.5 自动更新后的 go.mod 文件内容，引用了 github.com/tealeg/xlsx 库的 v1.0.5 版本 12345module lib_xlsxgo 1.19require github.com/tealeg/xlsx v1.0.5 自动更新后的 go.sum 文件内容，记录了各个依赖库、间接依赖库以及项目go.mod文件的hash值 123456789github.com/kr/pretty v0.1.0 h1:L/CwN0zerZDmRFUapSPitk6f+Q3+0za1rQkzVuMiMFI=github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=github.com/kr/text v0.1.0 h1:45sCR5RtlFHMR4UwH9sdQ5TC8v0qDQCHnXt+kaKSTVE=github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=github.com/tealeg/xlsx v1.0.5 h1:+f8oFmvY8Gw1iUXzPk+kz+4GpbDZPK1FhPiQRd+ypgE=github.com/tealeg/xlsx v1.0.5/go.mod h1:btRS8dz54TDnvKNosuAqxrM1QgN1udgk9O34bDCnORM=gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15 h1:YR8cESwS4TdDjEe65xsg0ogRM/Nc3DYOhEAlW+xobZo=gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0= 此时程序已经写完，依赖关系也已经下载好了，直接运行就可以 1234$ go run .Sheet Name: Sheet1Sheet Name: Sheet2Sheet Name: Sheet3 因为 go.mod 和 go.sum 文件共同为这个项目提供了依赖管理，别人在下载你这份源码时，先执行 go mod tidy 根据依赖关系下载正确的版本库，之后就可以直接编译运行了 手动升级依赖库到目前为止，我们并没有手动编辑过 go.mod 文件，只是在使用 go get github.com/tealeg/xlsx 自动更新了这个文件，假设这个例子的需求要扩展，引用的 github.com/tealeg/xlsx 功能是在v1.0.6 版本新加的，那么我么只需要将 go.mod 文件中的 v1.0.5 替换成 v1.0.6，然后执行 go mod tidy 即可下载正确的依赖库。 但如果要引用的依赖库版本不是 v0 或者 v1 版本的就不能这么处理了，具体做法看下面的问题。 should be v0 or v1, not v2当你手动把 go.mod 文件中的 v1.0.5 替换成 v2.x.x 执行 go mod tidy 时就会报一个错误 errors parsing go.mod: /xxx/lib_xlsx/go.mod:5: require github.com/tealeg/xlsx: version “v2.0.0” invalid: should be v0 or v1, not v2 问题的原因及细节可以参考这篇文档《[Go] 到底 go get 的版號怎麼運作的?》中关于“Semantic import version”的描述 复杂的原因简单说：Go有一个规范，就是在启用了Go Module的仓库上如果想使用 v2 及以上版本，需要在原引用库上添加版本标识简写，比如这个例子中，我们需要将 lib_xlsx.go 文件中引用包的代码改成下面这样 123456import ( "fmt" "log" "github.com/tealeg/xlsx/v3") 执行 go mod tidy 命令 123$ go mod tidygo: finding module for package github.com/tealeg/xlsx/v3go: found github.com/tealeg/xlsx/v3 in github.com/tealeg/xlsx/v3 v3.3.5 如果遇到下面这个问题，确认修改无误的话多尝试几次 $ go mod tidygo: finding module for package github.com/tealeg/xlsx/v3lib_xlsx imports github.com/tealeg/xlsx/v3: module github.com/tealeg/xlsx/v3: Get “https://proxy.golang.org/github.com/tealeg/xlsx/v3/@v/list“: EOF 执行过后 go.mod 和 go.sum 两个文件改变后的内容如下： 123456789101112131415161718module lib_xlsxgo 1.19require github.com/tealeg/xlsx/v3 v3.3.5require ( github.com/frankban/quicktest v1.14.6 // indirect github.com/google/btree v1.0.0 // indirect github.com/google/go-cmp v0.5.9 // indirect github.com/kr/pretty v0.3.1 // indirect github.com/kr/text v0.2.0 // indirect github.com/peterbourgon/diskv/v3 v3.0.1 // indirect github.com/rogpeppe/fastuuid v1.2.0 // indirect github.com/rogpeppe/go-internal v1.9.0 // indirect github.com/shabbyrobe/xmlwriter v0.0.0-20200208144257-9fca06d00ffa // indirect golang.org/x/text v0.3.8 // indirect) 1234567891011121314151617181920212223242526272829github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=github.com/google/btree v1.0.0 h1:0udJVsspx3VBr5FwtLhQQtuAsVc79tTq0ocGIPAU6qo=github.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=github.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=github.com/niemeyer/pretty v0.0.0-20200227124842-a10e7caefd8e h1:fD57ERR4JtEqsWbfPhv4DMiApHyliiK5xCTNVSPiaAs=github.com/peterbourgon/diskv/v3 v3.0.1 h1:x06SQA46+PKIUftmEujdwSEpIx8kR+M9eLYsUxeYveU=github.com/peterbourgon/diskv/v3 v3.0.1/go.mod h1:kJ5Ny7vLdARGU3WUuy6uzO6T0nb/2gWcT1JiBvRmb5o=github.com/pkg/diff v0.0.0-20210226163009-20ebb0f2a09e/go.mod h1:pJLUxLENpZxwdsKMEsNbx1VGcRFpLqf3715MtcvvzbA=github.com/pkg/profile v1.5.0 h1:042Buzk+NhDI+DeSAA62RwJL8VAuZUMQZUjCsRz1Mug=github.com/rogpeppe/fastuuid v1.2.0 h1:Ppwyp6VYCF1nvBTXL3trRso7mXMlRrw9ooo375wvi2s=github.com/rogpeppe/fastuuid v1.2.0/go.mod h1:jVj6XXZzXRy/MSR5jhDC/2q6DgLz+nrA6LYCDYWNEvQ=github.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=github.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=github.com/shabbyrobe/xmlwriter v0.0.0-20200208144257-9fca06d00ffa h1:2cO3RojjYl3hVTbEvJVqrMaFmORhL6O06qdW42toftk=github.com/shabbyrobe/xmlwriter v0.0.0-20200208144257-9fca06d00ffa/go.mod h1:Yjr3bdWaVWyME1kha7X0jsz3k2DgXNa1Pj3XGyUAbx8=github.com/tealeg/xlsx/v3 v3.3.5 h1:dzmns01jRf0SveBe7VqkcO2LCLOcypcDI6H66PiZycQ=github.com/tealeg/xlsx/v3 v3.3.5/go.mod h1:KV4FTFtvGy0TBlOivJLZu/YNZk6e0Qtk7eOSglWksuA=golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=golang.org/x/text v0.3.8 h1:nAL+RVCQ9uMn3vJZbV+MRnydTJFPf8qqY42YiA6MrqY=golang.org/x/text v0.3.8/go.mod h1:E6s5w1FMmriuDzIBO73fBruAKo1PCIq6d2Q6DHfQ8WQ=golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=gopkg.in/check.v1 v1.0.0-20200902074654-038fdea0a05b h1:QRR6H1YWRnHb4Y/HeNFCTJLFVxaq6wH4YuVdsUOr75U= 将 github.com/tealeg/xlsx 从 v1.0.5 升级到 v3.3.5 真是加了不少间接依赖库啊 至此 go.mod 文件的自动更新和手动维护我们就都尝试过了 总结 go.mod 和 go.sum 文件是 Go Modules 的重要组成部分，用于管理项目的依赖关系 go.mod 文件是由 go 工具自动生成和管理的，不需要手动修改，常用命令 go get xxx 和 go mod tidy 如果想显式地指定某个依赖项的特定版本才需要手动编辑 go.mod 文件 go.sum 文件记录了每个依赖模块的哈希值，以保证项目的构建是可复现的 如果在启用了Go Module的仓库想使用 v2 及以上版本，需要在原引用库路径末尾添加版本标识简写，比如v2、v3等 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 穷则独善其身，达则兼济天下，如今在这纷繁的世间，独善其身已实属不易，天下交给你们，我自顺其自然吧~ 2024-2-8 15:59:19]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>mod</tag>
        <tag>import</tag>
        <tag>xlsx</tag>
        <tag>依赖库版本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用nslookup命令查询域名系统的信息]]></title>
    <url>%2Fblog%2F2024%2F01%2F29%2F%E4%BD%BF%E7%94%A8nslookup%E5%91%BD%E4%BB%A4%E6%9F%A5%E8%AF%A2%E5%9F%9F%E5%90%8D%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言这个命令出现在与运维同学沟通过程中，在此之前要是问我一个域名对应的服务器是IP地址是什么，我肯定优先想到的是 ping 这个命令，这个命令确实可以在回复的信息中看到IP地址，但是不全面，有时一个域名对应不止一个IP地址，这时就要用到 nslookup 命令了，这也是在沟通中学到的新知识点。 常规使用学这个命令就是拿来用的，最常见的用法就是 nslookup 域名 来查询域名对应的IP，并且这个用法在Windows(cmd)、Mac、Linux下都是通用的，算是一个不错的跨平台命令，对比一下 ping 看看有什么区别： 123456789101112C:\Users\Administrator&gt;ping a1.easemob.com正在 Ping a1-v2.easemob.com.x.easeslb.com [47.93.162.59] 具有 32 字节的数据:来自 47.93.162.59 的回复: 字节=32 时间=7ms TTL=88来自 47.93.162.59 的回复: 字节=32 时间=7ms TTL=88来自 47.93.162.59 的回复: 字节=32 时间=6ms TTL=88来自 47.93.162.59 的回复: 字节=32 时间=6ms TTL=8847.93.162.59 的 Ping 统计信息: 数据包: 已发送 = 4，已接收 = 4，丢失 = 0 (0% 丢失)，往返行程的估计时间(以毫秒为单位): 最短 = 6ms，最长 = 7ms，平均 = 6ms 12345678910C:\Users\Administrator&gt;nslookup a1.easemob.com服务器: UnKnownAddress: 10.20.30.40名称: a1-v2.easemob.com.x.easeslb.comAddresses: 2408:4000:200::c5 2408:4000:200::17 47.93.162.59 39.97.9.52Aliases: a1.easemob.com 以上都是在cmd下执行的，如果在PowerShell中有一个名叫 Resolve-DnsName 的替代命令，执行结果如下： 123456789101112131415161718192021222324252627282930PS C:\Users\Administrator&gt; Resolve-DnsName a1.easemob.comName Type TTL Section NameHost---- ---- --- ------- --------a1.easemob.com CNAME 0 Answer a1-v2.easemob.com.x.easeslb.comName : a1-v2.easemob.com.x.easeslb.comQueryType : AAAATTL : 0IP6Address : 2408:4000:200::17Name : a1-v2.easemob.com.x.easeslb.comQueryType : AAAATTL : 0Section : AnswerIP6Address : 2408:4000:200::c5QueryType : ATTL : 0Section : AnswerIP4Address : 47.93.162.59Name : a1-v2.easemob.com.x.easeslb.comQueryType : ATTL : 0Section : AnswerIP4Address : 39.97.9.213 对于上述 Resolve-DnsName 命令的输出的结果解释如下： CNAME 记录： 1a1.easemob.com CNAME 0 Answer a1-v2.easemob.com.x.easeslb.com 这一行表示 a1.easemob.com 是一个别名（CNAME）记录，指向了 a1-v2.easemob.com.x.easeslb.com。 AAAA 记录（IPv6 地址）： 12345Name : a1-v2.easemob.com.x.easeslb.comQueryType : AAAATTL : 0Section : AnswerIP6Address : 2408:4000:200::17 这一行表示 a1-v2.easemob.com.x.easeslb.com 对应的 IPv6 地址是 2408:4000:200::17。 12345Name : a1-v2.easemob.com.x.easeslb.comQueryType : AAAATTL : 0Section : AnswerIP6Address : 2408:4000:200::c5 这一行表示 a1-v2.easemob.com.x.easeslb.com 对应的另一个 IPv6 地址是 2408:4000:200::c5。 A 记录（IPv4 地址）： 12345Name : a1-v2.easemob.com.x.easeslb.comQueryType : ATTL : 0Section : AnswerIP4Address : 47.93.162.59 这一行表示 a1-v2.easemob.com.x.easeslb.com 对应的 IPv4 地址是 47.93.162.59。 12345Name : a1-v2.easemob.com.x.easeslb.comQueryType : ATTL : 0Section : AnswerIP4Address : 39.97.9.213 这一行表示 a1-v2.easemob.com.x.easeslb.com 对应的另一个 IPv4 地址是 39.97.9.213。 综上所述，查询结果显示了域名 a1.easemob.com 的别名记录（CNAME）以及对应的IPv6和IPv4地址。IPv6地址有两个，分别是 2408:4000:200::17 和 2408:4000:200::c5，而IPv4地址有两个，分别是 47.93.162.59 和 39.97.9.213。 再从linux环境下学点不一样的1234567891011121314$ nslookup a1.easemob.comServer: 127.0.0.53Address: 127.0.0.53#53Non-authoritative answer:a1.easemob.com canonical name = a1-v2.easemob.com.x.easeslb.com.Name: a1-v2.easemob.com.x.easeslb.comAddress: 39.97.9.213Name: a1-v2.easemob.com.x.easeslb.comAddress: 47.93.162.59Name: a1-v2.easemob.com.x.easeslb.comAddress: 2408:4000:200::17Name: a1-v2.easemob.com.x.easeslb.comAddress: 2408:4000:200::c5 在linux环境下使用基础命令，还在cmd环境下的输出结果类似，但是比Resolve-DnsName命令输出的结果要简陋许多，但是可以通过添加参数来扩展命令。 type选项nslookup 命令中的 type 参数用于指定查询的记录类型，即你想要获取的特定 DNS 记录的类型。以下是一些常见的 type 参数及其解释： A (Address Record):nslookup example.com type=A 查询域名 example.com 的 IPv4 地址 AAAA (IPv6 Address Record):nslookup example.com type=AAAA 查询域名 example.com 的 IPv6 地址 CNAME (Canonical Name Record):nslookup example.com type=CNAME 查询域名 example.com 的别名记录 MX (Mail Exchange Record):nslookup example.com type=MX 查询域名 example.com 的邮件交换记录 NS (Name Server Record):nslookup example.com type=NS 查询域名 example.com 的域名服务器记录 PTR (Pointer Record):nslookup 192.168.1.1 type=PTR 查询 IP 地址 192.168.1.1 对应的域名 SOA (Start of Authority Record):nslookup example.com type=SOA 查询域名 example.com 的权威区域起始记录 TXT (Text Record):nslookup example.com type=TXT 查询域名 example.com 的文本记录 SRV (Service Record):nslookup _ldap._tcp.example.com type=SRV 查询服务记录，例如 LDAP 服务 ANY (Any Record):nslookup example.com type=ANY 查询域名 example.com 的所有记录类型 这些是常见的 type 参数，可以根据需要选择合适的类型来获取特定类型的 DNS 记录信息。在命令中，type 参数通常是可选的，如果不指定，默认为 A 记录 retry选项该参数用于设置在没有收到响应时的重试次数。如果发送的 DNS 查询请求没有得到响应，nslookup 将会尝试重新发送请求，次数由 -retry 指定 1nslookup -retry=3 example.com 这个示例中，-retry=3 表示设置重试次数为3次。如果第一次查询没有得到响应，nslookup 将尝试再次发送查询请求，最多重试3次 timeout选项该参数用于设置等待响应的超时时间，以秒为单位。如果在指定的超时时间内没有收到响应，nslookup 将认为查询失败 1nslookup -timeout=5 example.com 这个示例中，-timeout=5 表示设置超时时间为5秒。如果在5秒内没有收到查询响应，nslookup 将认为查询失败 使用更强大的dig命令12345678910111213141516171819202122$ dig a1.easemob.com; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; a1.easemob.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 1464;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 65494;; QUESTION SECTION:;a1.easemob.com. IN A;; ANSWER SECTION:a1.easemob.com. 0 IN CNAME a1-v2.easemob.com.x.easeslb.com.a1-v2.easemob.com.x.easeslb.com. 0 IN A 47.93.162.59a1-v2.easemob.com.x.easeslb.com. 0 IN A 39.97.9.213;; Query time: 0 msec;; SERVER: 127.0.0.53#53(127.0.0.53);; WHEN: 一 1月 29 21:53:40 CST 2024;; MSG SIZE rcvd: 117 上述 dig 命令对域名 a1.easemob.com 进行查询的输出结构的含义解释如下： 头部信息 (HEADER 部分): 1-&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 1464 opcode: 查询操作的类型，这里是 QUERY 表示标准查询。 status: 查询的状态，NOERROR 表示没有错误。 id: 查询的唯一标识符，这里是 1464。 标志部分 (flags 部分): 1;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1 qr: 查询响应标志位，这里是 qr 表示响应。 rd: 递归查询标志位，这里是 rd 表示递归查询。 ra: 递归可用标志位，这里是 ra 表示递归可用。 QUERY: 1: 查询的问题部分包含1个查询。 ANSWER: 3: 响应部分包含3个回答。 AUTHORITY: 0: 授权部分不包含任何信息。 ADDITIONAL: 1: 附加部分包含1个附加记录。 OPT PSEUDOSECTION 部分: 1; EDNS: version: 0, flags:; udp: 65494 EDNS: 扩展域名系统 (Extended DNS) 的信息。 version: 0: EDNS 的版本。 flags:;: EDNS 标志，这里为空。 udp: 65494: 用于 DNS 查询和响应的最大 UDP 数据包大小。 请求部分 (QUESTION SECTION): 12;; QUESTION SECTION:;a1.easemob.com. IN A 查询的问题部分，这里是查询域名 a1.easemob.com 的 A 记录。 应答部分 (ANSWER SECTION): 1234;; ANSWER SECTION:a1.easemob.com. 0 IN CNAME a1-v2.easemob.com.x.easeslb.com.a1-v2.easemob.com.x.easeslb.com. 0 IN A 47.93.162.59a1-v2.easemob.com.x.easeslb.com. 0 IN A 39.97.9.213 响应部分包含了查询的回答，这里包含了3个记录。 CNAME 记录：a1.easemob.com 是一个别名，指向 a1-v2.easemob.com.x.easeslb.com。 A 记录：a1-v2.easemob.com.x.easeslb.com 对应两个IPv4地址，分别是 47.93.162.59 和 39.97.9.213。 查询时间 (Query time): 1;; Query time: 0 msec 响应的查询时间，这里是 0 毫秒。 DNS 服务器信息 (SERVER 和 WHEN 部分): 12;; SERVER: 127.0.0.53#53(127.0.0.53);; WHEN: 一 1月 29 21:53:40 CST 2024 SERVER: 显示响应的 DNS 服务器的IP地址和端口号。 WHEN: 显示响应的时间戳。 消息大小 (MSG SIZE): 1;; MSG SIZE rcvd: 117 响应消息的大小，这里是 117 字节。 综上所述，这个 dig 查询输出提供了关于域名 a1.easemob.com 的详细信息，包括查询问题、响应部分（包括 CNAME 和 A 记录）、查询时间、DNS 服务器信息以及响应消息的大小。 总结 使用 nslookup 命令可以用来查询域名对应的IP nslookup a1.easemob.com 使用 nslookup 命令也可以用来查询IP对应的域名 nslookup 8.8.8.8 -ty=PTR 1.1.1.1 nslookup 是一个跨平台的命令，Windows(cmd)、Mac、Linux都可以使用 在PowerShell中等价的命令是 Resolve-DnsName，默认输出信息较为详细 dig 命令也可用于查询域名的详细信息 dig a1.easemob.com ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 曾经沧海难为水，除却巫山不是云，可以被压缩的时间才是生活，不得不花费的时间只是为了活着~]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>Tools</tag>
        <tag>nslookup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个golang小白使用vscode搭建Ununtu20.04下的go开发环境]]></title>
    <url>%2Fblog%2F2024%2F01%2F21%2F%E4%B8%80%E4%B8%AAgolang%E5%B0%8F%E7%99%BD%E4%BD%BF%E7%94%A8vscode%E6%90%AD%E5%BB%BAgo%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[前言先交代一下背景，距离正式接触golang这门语言已经有5年时间，平时偶尔也会用go写写工具和功能，但其实充其量就是语言小白，基本上就是按照教程配置好环境，按照需求写写逻辑，能跑起来就行了。golang随着这几年的变化，这门语言的变化还是非常大的，之前写过一篇《Go环境配置时遇到的GOPATH路径以及包管理问题》，可以看出之前配置的变量很像Java早期的配置环境，从1.11 版本之后go工程不必放到 GOPATH 路径中了，通过go.mod文件来标记并跟踪每个软件包的版本，灵活度大大提高，从此 GOPATH 概念逐渐淡化，在1.18版本新引入一个新功能go.work，它被设计为解决在同一工作空间中处理多个模块的问题，使得go工程的模块管理更加方便。 简单描述下我的开发环境，Windows10 + VSCode1.58.0 + WLS1.0 - Ubuntu20.04，平时工作都是在Ubuntu上，习惯了在Linux上的开发，所以即使是自己家的电脑也安装了子系统，就当成Ubuntu 来用就行了，Vscode安装在Windows上，直接连接到子系统上进行代码编写，以上是前提，本文的重点是在Ubuntu20.04系统上安装go的编译和运行环境。 搭建go环境我选择的go版本是1.19，已经支持 go.sum 和 go.work，同时也为了和工作开发保持一致，没必要用太新的特性，现在的go包管理已经不想早期的Java，不再依赖 GOPATH，和 Python3.x 更像了，只管下载包然后在各自的工程目录下使用就行了，必要时需要指定使用库的版本。 下载go安装包go语言的官网是 https://golang.google.cn/dl/，下载1.19版本的Linux包go1.19.linux-amd64.tar.gz，这个网址通常下载很慢，可以改为go语言中文网https://studygolang.com/dl来下载1.19版本go1.19.linux-amd64.tar.gz 可以通过浏览器下载拷贝到指定位置，也可以通过wget命令下载 12345678910111213141516alberts@home-pc:~$ wget https://golang.google.cn/dl/go1.19.linux-amd64.tar.gz--2024-01-21 17:22:11-- https://golang.google.cn/dl/go1.19.linux-amd64.tar.gzResolving golang.google.cn (golang.google.cn)... 203.208.50.34Connecting to golang.google.cn (golang.google.cn)|203.208.50.34|:443... connected.HTTP request sent, awaiting response... 302 FoundLocation: https://dl.google.com/go/go1.19.linux-amd64.tar.gz [following]--2024-01-21 17:22:12-- https://dl.google.com/go/go1.19.linux-amd64.tar.gzResolving dl.google.com (dl.google.com)... 220.181.174.161, 2401:3800:4001:807::1001Connecting to dl.google.com (dl.google.com)|220.181.174.161|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 148796421 (142M) [application/x-gzip]Saving to: ‘go1.19.linux-amd64.tar.gz’go1.19.linux-amd64.tar.gz 100%[==============================&gt;] 141.90M 3.62MB/s in 31s2024-01-21 17:22:43 (4.54 MB/s) - ‘go1.19.linux-amd64.tar.gz’ saved [148796421/148796421] 解压go压缩包完成安装go的运行环境直接解压拷贝到指定位置就可以了，我把go解压安装到了 /usr/local/go 目录，解压之前也清理掉老的目录，如果之前没安装过，rm这条命令可以不使用 123sudo surm -rf /usr/local/gotar -C /usr/local -xzf go1.19.linux-amd64.tar.gz 配置环境变量这一步有点类似Windows中的修改注册表，我安装了新的程序，需要通知系统当使用 go 命令时需要去哪个目录下寻找可执行程序，具体操作如下 使用vim编辑.bashrc文件 1alberts@home-pc:~$ vim ~/.bashrc 到文件的末尾添加以下代码 1alberts@home-pc:~$ export PATH=$PATH:/usr/local/go/bin 重新加载配置文件 1alberts@home-pc:~$ source ~/.bashrc 查看版本是否安装成功 12alberts@home-pc:~$ go versiongo version go1.19 linux/amd64 查看版本显示出 go1.19 linux/amd64, 至此go语言的基础开发环境就配置好了 编写一个helloword程序123456789alberts@home-pc:/mnt/g$ mkdir wlsworkspacealberts@home-pc:/mnt/g$ cd wlsworkspacealberts@home-pc:/mnt/g/wlsworkspace$ mkdir helloalberts@home-pc:/mnt/g/wlsworkspace$ cd hello/alberts@home-pc:/mnt/g/wlsworkspace/hello$ go mod init github.com/alberts/go/hellogo: creating new go.mod: module github.com/alberts/go/helloalberts@home-pc:/mnt/g/wlsworkspace/hello$ touch hello.goalberts@home-pc:/mnt/g/wlsworkspace/hello$ lsgo.mod hello.go 打开 hello.go 文件编写以下内容： 1234567package mainimport "fmt"func main() &#123; fmt.Println("Hello,World!")&#125; 保存文件后使用 go run 命令运行 12alberts@home-pc:/mnt/g/wlsworkspace/hello$ go run hello.goHello,World! 安装VSCode插件安装智能提示插件为了编写go代码方便，可以安装这个名为Go的插件，这个插件不是必须安装的，但是你在写go代码时VSCode会自动提示你安装这个插件，为了实现Go语法只能提醒，我们可以直接点击提示安装，或者在插件市场里搜索安装 go插件 插件安装完成后会提示下面的报错 The “gopls” command is not available. Run “go get -v golang.org/x/tools/gopls” to install. 说明安装这个插件需要依赖 golang.org/x/tools/gopls 这个包，我们可以根据提示安装 安装go依赖包12alberts@home-pc:/mnt/g/wlsworkspace/hello$ go get -v golang.org/x/tools/goplsgo: module golang.org/x/tools/gopls: Get "https://proxy.golang.org/golang.org/x/tools/gopls/@v/list": dial tcp 142.251.42.241:443: connect: connection refused 直接安装通常会因无法下载而失败，需要修改代理来改变下载地址 修改代理并重新安装依赖包123alberts@home-pc:/mnt/g/wlsworkspace/hello$ go env -w GO111MODULE=onalberts@home-pc:/mnt/g/wlsworkspace/hello$ go env -w GOPROXY=https://goproxy.cn,directalberts@home-pc:/mnt/g/wlsworkspace/hello$ go env -w GOSUMDB="sum.golang.org" 安装依赖包 1234567891011121314151617181920212223242526272829303132alberts@home-pc:/mnt/g/wlsworkspace/hello$ go get -v golang.org/x/tools/goplsgo: downloading golang.org/x/tools v0.17.0go: downloading golang.org/x/tools/gopls v0.14.2go: downloading golang.org/x/tools v0.14.1-0.20231114185516-c9d3e7de13fdgo: downloading golang.org/x/telemetry v0.0.0-20231114163143-69313e640400go: downloading github.com/sergi/go-diff v1.1.0go: downloading honnef.co/go/tools v0.4.5go: downloading mvdan.cc/gofumpt v0.4.0go: downloading mvdan.cc/xurls/v2 v2.4.0go: downloading golang.org/x/mod v0.14.0go: downloading golang.org/x/sync v0.4.0go: downloading golang.org/x/text v0.13.0go: downloading golang.org/x/exp/typeparams v0.0.0-20221212164502-fae10dda9338go: downloading golang.org/x/sys v0.14.0go: downloading golang.org/x/vuln v1.0.1go: downloading github.com/google/go-cmp v0.5.9go: downloading github.com/BurntSushi/toml v1.2.1go: added github.com/BurntSushi/toml v1.2.1go: added github.com/google/go-cmp v0.5.9go: added github.com/sergi/go-diff v1.1.0go: added golang.org/x/exp/typeparams v0.0.0-20221212164502-fae10dda9338go: added golang.org/x/mod v0.14.0go: added golang.org/x/sync v0.4.0go: added golang.org/x/sys v0.14.0go: added golang.org/x/telemetry v0.0.0-20231114163143-69313e640400go: added golang.org/x/text v0.13.0go: added golang.org/x/tools v0.14.1-0.20231114185516-c9d3e7de13fdgo: added golang.org/x/tools/gopls v0.14.2go: added golang.org/x/vuln v1.0.1go: added honnef.co/go/tools v0.4.5go: added mvdan.cc/gofumpt v0.4.0go: added mvdan.cc/xurls/v2 v2.4.0 这次安装成功了，这些依赖包默认安装到了 ~/go/pkg目录下，使用时直接在go文件中引用就行了 import golang.org/x/sys 以上这些包是Go语言的依赖管理工具，它们的作用如下： golang.org/x/tools: 提供了一系列用于开发Go语言的工具，如gopls等 golang.org/x/tools/gopls: 是一个基于Go语言的代码补全和导航工具，它可以帮助开发者更高效地编写和调试Go代码 golang.org/x/telemetry: 提供了一套用于收集和分析应用程序性能数据的库 github.com/sergi/go-diff: 是一个用于比较两个Go源代码文件差异的工具 honnef.co/go/tools: 提供了一些用于处理Go源代码的工具，如goimports（自动导入缺失的包）等 mvdan.cc/gofumpt: 是一个用于格式化Go源代码的工具，它可以自动调整代码格式以符合Go语言的最佳实践 mvdan.cc/xurls/v2: 是一个用于解析URL的库，它可以将URL分解为各个组成部分，方便开发者处理 golang.org/x/mod: 是一个用于管理Go模块的库，它可以帮助你更好地组织和管理项目的依赖关系 golang.org/x/sync: 提供了一组同步原语，如互斥锁、条件变量等，用于实现多线程编程中的同步机制 golang.org/x/text: 提供了一个用于处理Unicode文本的库，它可以帮助你更方便地处理和操作文本数据 golang.org/x/exp/typeparams: 提供了一个用于处理类型参数的库，它可以帮助你更方便地处理泛型编程中的类型参数问题 golang.org/x/sys: 提供了一个用于访问操作系统功能的库，它可以帮助你更方便地处理操作系统相关的任务 golang.org/x/vuln: 提供了一个用于管理已知安全漏洞的库，它可以帮助你更容易地识别和修复项目中的安全漏洞 github.com/stamblerre/gocode: 提供Go语言的代码自动补全功能，用于IDEs和编辑器 github.com/uudashr/gopkgs/v2/cmd/gopkgs: 命令行工具，帮助发现和使用Go语言软件包 github.com/ramya-rao-a/go-outline: VSCode插件，为Go源代码文件生成结构大纲以便于导航 总结一下需要安装的依赖包 go get -v golang.org/x/tools/goplsgo get -v github.com/stamblerre/gocodego get -v github.com/uudashr/gopkgs/v2/cmd/gopkgsgo get -v github.com/ramya-rao-a/go-outline 根据VSCode提示安装后续的Tools，得到的控制台输出信息如下 1234567891011121314151617Tools environment: GOPATH=/home/alberts/goInstalling 6 tools at /home/alberts/go/bin in module mode. gopkgs go-outline dlv dlv-dap staticcheck goplsInstalling github.com/uudashr/gopkgs/v2/cmd/gopkgs (gopkgs) SUCCEEDEDInstalling github.com/ramya-rao-a/go-outline (go-outline) SUCCEEDEDInstalling github.com/go-delve/delve/cmd/dlv (dlv) SUCCEEDEDInstalling github.com/go-delve/delve/cmd/dlv@master (/home/alberts/go/bin/dlv-dap) SUCCEEDEDInstalling honnef.co/go/tools/cmd/staticcheck (staticcheck) SUCCEEDEDInstalling golang.org/x/tools/gopls (gopls) SUCCEEDEDAll tools successfully installed. You are ready to Go :). go.mod 和 go.workgo.modGo语言从1.11版本开始引入了Go Modules，这个版本之后不再需要将项目放到GOPATH中。 Go Modules是Go语言的依赖管理工具，它改变了之前必须使用GOPATH来管理依赖的模式。在Go 1.11及之后的版本中，通过go.mod文件来标记并跟踪每个软件包的版本，这意味着开发者可以在任何地方创建项目，而不必遵循GOPATH目录结构。 此外，Go团队还增加了一个环境变量GO111MODULE来控制模块支持的启用和禁用。这个环境变量的命名中的”111”象征着它是在Go 1.11版本中引入的。 总的来说，Go语言自1.11版本起，通过引入Go Modules，使得开发者不再受限于GOPATH，可以更加灵活地管理项目和依赖。这一变化被广泛认为是Go语言发展中的一个重大改进。 go.work“go.work” 是Go 1.18版本新引入的一个功能，它被设计为解决在同一工作空间中处理多个模块的问题。在之前的版本中，开发者常常会遇到路径问题、版本冲突问题等挑战。为了应对这些情况，他们可能需要对每个模块的go.mod文件进行手动修改，过程繁琐且容易出错。 “go.work”文件的主要作用就是来解决这些问题。它是一个特殊的文件，其中包含了use和replace指令，这些指令会覆盖工作区目录下的每个Go Module的go.mod文件中的内容。因此，开发者无需再对每个Go Module的go.mod文件进行手动修改，极大地简化了操作流程并提高了效率。 此外，”go.work”文件的优先级高于go.mod文件中的定义。也就是说，如果在go.work和go.mod中同时指定了不同的代码仓库路径，那么go.work中的设置将会被优先采用。这一特性进一步加强了”go.work”文件在依赖管理中的重要性。 小试一下go.work当前工作目录下有 base 和 hello 两个包含go.mod的工程，使用 go work init 命令可以初始化一个工作空间 12345678910alberts@home-pc:/mnt/g/wlsworkspace$ lsbase helloalberts@home-pc:/mnt/g/wlsworkspace$ go work init base helloalberts@home-pc:/mnt/g/wlsworkspace$ cat go.workgo 1.19use ( ./base ./hello) 初始化之后工程之间就可以相互引用彼此对外的包内容了 总结 下载go压缩包 wget https://golang.google.cn/dl/go1.19.linux-amd64.tar.gz 初始化工程目录hello go mod init github.com/alberts/go/hello，包名通常是支持https访问的网络地址 初始化工作空间 go work init base hello，base 和 hello 都是包含go.mod文件的go工程 安装依赖包 go get -v golang.org/x/tools/gopls 安装依赖包 github.com/stamblerre/gocode 安装依赖包 github.com/uudashr/gopkgs/v2/cmd/gopkgs 安装依赖包 github.com/ramya-rao-a/go-outline ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 放下助人情节，尊重他人命运~]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>golang</tag>
        <tag>vscode</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐一款通过ssh连接linux服务的开源工具WindTerm]]></title>
    <url>%2Fblog%2F2024%2F01%2F14%2F%E6%8E%A8%E8%8D%90%E4%B8%80%E6%AC%BE%E9%80%9A%E8%BF%87ssh%E8%BF%9E%E6%8E%A5linux%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%BC%80%E6%BA%90%E5%B7%A5%E5%85%B7WindTerm%2F</url>
    <content type="text"><![CDATA[前言工作一入门便是游戏服务器开发，所以常常有连接Linux服务器的需求，之前用的最多的是Xshell，最近这个软件个人版只能免费使用一个月了，超过时间会提示更新无法正常使用，工作当中用的最多的是SecureCRT，不过这个软件是收费的，公司买了授权，可以正常用，但是在家时就不行了，最近我找到了WindTerm这个软件，用起来还不错，记录一下。 WindTerm介绍 A Quicker and better SSH/Telnet/Serial/Shell/Sftp client for DevOps. WindTerm一款使用使用C语言开发的跨平台SSH客户端WindTerm，完全免费开源，软件包只有20多M，性能很好，Windows、Linux、Mac均提供程序包下载，免按照，下载解压即可使用，官方地址为 https://github.com/kingToolbox/WindTerm/releases，最新版本为2.6.0 最近github访问又变慢了，如果你下载不下来，可以从这个平台地址下载CSDN下载-WindTerm_2.6.0_Prerelease_7_Windows_Portable_x86_64.zip，我已经设置成免积分了，方便有需要的人直接取用，简单罗列下它的功能。 多平台支持Linux，Mac，Win，免安装解压即可使用 实施了 SSH v2、Telnet、Raw Tcp、串行、Shell 协议 会话认证时支持 SSH 自动执行 支持 SSH ProxyCommand 或 ProxyJump 支持SSH代理转发。 支持使用密码、公钥、键盘交互、gssapi-with-mic 的 SSH 自动登录 支持直接/本地端口转发、反向/远程端口转发和动态端口转发 支持 XModem、YModem 和 ZModem，可以使用rz、sz上传和下载文件 集成sftp、scp客户端，支持下载、上传、删除、重命名、新建文件/目录 集成本地文件管理器，支持移动到、复制到、复制自、删除、重命名、新建文件/目录 支持 Windows Cmd、PowerShell 和 Cmd、PowerShell 作为管理员 支持Linux bash、zsh、powershell core 支持 MacOS bash、zsh、powershell core 有较好的操作界面 WindTerm使用下载WindTerm_2.6.0_Prerelease_7_Windows_Portable_x86_64.zip 后解压，双击WindTerm.exe即可打开软件，会弹出下面的界面，提示存储配置文件的地方，我就选择放到应用程序目录了 直接通过【会话】-&gt;【新建会话】便可以建立连接Linux服务器的通道，这与之前用的软件都大同小异，基本没有差别，输入主机地址点击【连接】，后续再输入密码就可以开始连接了 如果是第一个使用的时候，会弹出下面这个界面，让自定义一个主密码，其实就是这个软件的使用密码，后续锁屏或者更新会话信息时会用到。 因为我是自己在家使用，我就不设置主密码了，直接点击OK完成，后面会让输入Linux账户和密码，到这一步相信大家都会用了。 主密码和锁屏这个问题我也是查了一些资料才解决的，因为我没有输入主密码，所以每次锁屏时我只要回车就可以了，如果想修改锁屏时间，在【会话】-&gt;【首选项】-&gt;【设置】中可以找到 也可以直接通过配置文件修改，找到刚启动软件时你选择的目录 D:\app\WindTerm_2.6.0\.wind\profiles\default.v10，直接修改配置文件内容，通过字段名也很清楚要改哪些值 12345678&#123; "application.fingerprint" : "c5QR13M1iEkNPGlVNjDBWadi1uS0PiXR4Y+79cp09PAXr7G6WQxg/7WFj9RJ9dIsKKOWqm5Xq2N6w5Jx4v2mTw==", "application.language" : "zh-CN", "application.lockScreenTimeout" : 1, "application.masterPassword" : false, "terminal.loginWizard.defaultAuthType" : "Account", "terminal.reimportShellSessions" : 2&#125; application.lockScreenTimeout：是控制锁屏时间的 application.masterPassword：是代表是否有主密码的 整个页面布局和配色还是挺漂亮的，不喜欢可以自己调整的哈 总结 WindTerm是一款可以通过ssh连接Linux服务器的客户端工具 WindTerm免费开源，支持Windows、Linux、Mac系统，无需安装，解压即可使用 WindTerm集成sftp、scp客户端，支持下载、上传、删除、重命名、新建文件/目录 WindTerm默认需要指定一个主密码，相当于验证使用者身份的功能，当然也可以为空 WindTerm的锁屏时间可以通过首选项菜单设置，也可通过配置修改 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生就是一本账，欠下的东西总要还的，茫茫人生路之后，你是想在历史长河中留下一笔，还是“隐藏”的毫无痕迹呢？]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>运维</tag>
        <tag>ssh</tag>
        <tag>windterm</tag>
        <tag>免安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2023年终总结——你相信光吗]]></title>
    <url>%2Fblog%2F2024%2F01%2F01%2F2023%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E4%BD%A0%E7%9B%B8%E4%BF%A1%E5%85%89%E5%90%97%2F</url>
    <content type="text"><![CDATA[不断追求利益的经济冲动，不停侵蚀人们的生活，压垮、吞噬并统治生活本身，无法言喻的不安和看不到明天的混沌感，让我们在现有社会经济条件下，沦为被金钱和物质所统治的行尸走肉~ 前言最近在看一本叫《饱食穷民》的书，讲述的是泡沫经济年代的日本社会，宛如梦幻泡影一样变化无常，又像断梗浮萍一样飘忽不定，到处充斥着不安的色彩，我试图从中寻找一些当前现实问题的解决方案，上段文字也是出自此书之中，阅读之后我大受震撼，其中描述的有关软件工程师的篇章，简直就是我的复刻版，无论是心境、情感、还是所面临的问题都如出一辙，读完觉得背后隐隐发凉，因为最终那些令人无法接受的结局，确实是我不想看到的。 2023年要结束了，今年的时光过的格外的快，突然想起大年初二时半夜更新游戏版本好像还是昨天发生的事情，结果一眨眼就来到了2023的尾巴，2022年末的时候大家对2023年的发展充满了希望，结果今年快过完了，发现比之前“三年”更难了，零下20度的温度配合着各种“毕业”的声音，让不安的情绪充斥着每一个打工人的内心。 面对除夕上班的决定，一开始大家还是强烈反对，但是渐渐的情绪发生了逆转，除夕上班说明还有工作，很多人在这个寒冷的冬天已经“提前放假”，没有打工的机会了，在这个寒冷的冬天，你还相信光吗？ 回顾2023回顾这一年忙忙碌碌，高强度的工作却没取得预想的成果，各种Flag完成情况更是惨不忍睹。 工作上 FLAG 适应自己身份的转变，提升自己的管理技能，做一名合格的管理者 继续做好本职可开发工作，做好工作内容的总结，推动新项目顺利上线 根据自己的技能树框架，查漏补缺，有针对性的学习和探索 完成度 ：80% 作为一个入门的管理者，今年见识到了更多更复杂的管理情景，Hire and Fire 全都经历过了，管理经验更加完整，尝试跨过了之前一直无法逾越的坎，为了能生存下去，有时不得不做出取舍，参加了一系列的管理课程，学习后发现万物皆可为工具，招人的工具、提升效率的工具、提升凝聚力的工具、提高团队氛围的工具、优化团队的工具，学习过后内心不禁生出一丝悲凉。 本职工作继续充当牛马，还真是个踏踏实实的老黄牛，推动项目一步步往前走，但目前项目暂时停滞了，今年上线是不可能了，不过经历了一次完整的测试过程，算是在失望之中获得了一点点慰藉。 今年的工作强度依旧很大，前三季度基本上12点左右回家，个别时候会工作到后半夜，比上一年能好一点，但是最终的结果却不尽如人意。 技能树框架基本荒废，因为没时间往里边填东西，基本上是被项目推着走，缺什么就补什么，nginx测试和调整配置有了一定的经验，linux系统参数的调优也在压测是接触了不少，还有网络参数配置等等。 对于这一项来说，努力到位了，结果并不能说明什么，可能还是缺少一点点运气了。 学习上 FLAG 继续博客总结，40篇是基础线，2023要超过这个值，尽量一周一篇，放假可休息 新的一年依旧很忙，刷题不强制要求，每周最好有贡献，每月必须有输出 在现有的技能树框架上继续丰富，做到枝繁叶茂，试试找点副业 开源代码还是以项目为驱动，选取经典实现，比如kafka 继续读书，今年书也买好了，数量不多，下半年应该还会买一批 完成度 ：70% 博客总结目标达成，今年在CSDN共有42篇总结，较40篇的及格线多了一点点，年初有篇关于ChatGPT的博文一开始阅读量刷刷刷的涨，后来判定违规就被吞了，整体上博客内容比较基础，大多数来源于工作之中的知识点，也有一些是平时的思考和感悟，以下是近两年的数据对比。 !-- ![2022csdn](https://img-blog.csdnimg.cn/direct/1b019f2ab6114156874451c0f6f20ad7.png#pic_center) ![2023csdn](https://img-blog.csdnimg.cn/direct/77a16b7b2d6c406f9c30eda21fc529bc.png#pic_center) -- 2023年一年的博客访问量突破了60万，相比于去年的40多万上升了不少，文章基数增多了应该是主要原因，去年的10W+文章《float的精度和取值范围》 一年就上涨了6万多，看来高质量的内容才是吸引人的关键，并且会形成头部效应，马太效应，出现强者恒强的情况。 今年的CSDN活动依旧不少，但是我已经很少参加了，一方面没有那么多时间，还有就是看不懂规则，年终的博客之星评选，由于博文的数量没有达标，连报名的资格都没有，有点遗憾。 关于刷题这个Flag约等于没有，别说刷题了，忙的连账号都没登录几次，刚刚看了看提交记录，仅在前两个月提交了十几道题，图我就不截了，有点磕碜，不过新的一年中可能需要多一点了，毕竟工作都不稳定了，得准备准备练练手感了。 技能树框架秃了，其实我也不想秃，但是每天回家总是很晚，我实在是懒得补充了，这明显是一个借口，但却是一个事实，心里知道这个事情要做，所以换了一种方式，改为记笔记的方式来弥补，一开始准备每天问自己一个问题，后来坚持不下来，但是每次有空闲想起来就会认真回答一次，然后就是将工作中遇到的盲点和知识点记录下来，为技能树积累素材。 阅读经典代码这部分还是以项目驱动为主，比如修改easyloging++文件描述符不释放的问题，分析libevent网络数据读取缓慢的问题，扩展框架支持 websocket 连接等，都是从项目需求实际出发，当然也看了一些帧同步小游戏的示例代码，有C#的也有JS的，暂时没有找到能明显解决我对帧同步疑问的项目，目前还在学习和寻找中。 读书一直没有停，由于每天大强度的工作时间，休息之余我已经不读技术类书籍了，现在读的更多是传记、小说、散文等等，通过一页页纸张回顾历史长河，窥探他人的一生。 有的人纷繁的一生落在纸上不过一页，而绝大多数人根部就不会在书本上留下痕迹。以下便是2023年阅读的书籍内容： 时间从来不语却回答了所有问题（2023-2-12 13:56） 一本杂记，更多的是晚年的人生 苏东坡传 —— 一蓑烟雨任平生（2023-2-26 23:57） 人身缘何不快乐，只因未读苏东坡 知行合一王阳明（2023-3-12 20:43） 良知源自内心，无需外求。功名利禄，本是外在的，得之我幸，不得我命。 别让孩子伤在敏感期（2023-3-19 16:32:23） 在孩子处于各种敏感期时，学会给予尊重，顺其自然，适时引导 不要规划孩子必须在哪一个领域取得成就，但只要她喜欢，就提供一个舞台，让她尽情去发挥 我是猫（2023-6-25 00:48:05） 两年的猫生戛然而止，古怪的苦沙弥高朋满座 人间失格（2023-6-24 13:30:50） 一个从小就将自己伪装起来的孩子着实令人害怕，让人回忆起了令人心酸的表哥 虽然一切都已经过去，但还是有些记忆片段会在读书是产生共鸣，从记忆深处涌上来 《斜阳》中的我是贵族是对时代的呐喊，回不去也融不进 《GoodBye》更是写成了幻想小说的范畴 罗生门（2023-8-7 23:13:34） 怪诞、讽刺、描写细致的短篇小说，不过有些故事的寓意看不太懂 浮生六记（2023-11-26 22:35:23） 初中时的课文《童趣》居然来自这本书，据说后两记是伪写的，但最后一记《养生记道》却多次引起共鸣，可能我正处在迷茫期吧。“五百年谪在红尘，略成游戏。三千里击开沧海，便是逍遥。” 月亮与六便士（2023-12-10 17:00:27） 一个完全不顾他人感受的人是否能称之为伟大，疯子还差不多 《苏东坡传》描写了苏东坡的一生，他虽狂放不羁，但有个一直拯救他的弟弟却是它一生的幸福，“人身缘何不快乐，只因未读苏东坡”，其实不止是苏东坡，很多大人物的一生就极其坎坷，如果看看他们的经历，我们的人生才刚刚开始，而如今这个浮躁的社会已经很难让人平静的接受这一切了。 《月亮与六便士》是一本很出名的书籍，从听说这本书到真正的开始读跨过了大概6、7年的时间，但如今读过之后却让与我的预想差距太大，我没想到这是一篇小说，斯特里克兰简直是一个变态，全文描述了他不顾一切的追求艺术，将其称之为伟大，但我觉得这样的伟大毫无意义，即使他的绘画技艺已经出神入化，世界也不缺他一个，但他却真实的伤害了周围的每一个人。 投资上 FLAG 定投和智能条件单保持现状，ETF追加1~2个新的行业赛道 股票池还有几个垃圾票要找机会清理掉，手里的价投股票适当做做趋势 保持对新年经济的乐观，目标收益10个点 完成度 ：50% 今年除了少量增持了之前的“三傻”和科技股之外，追加了医药ETF和光伏ETF，设置了定投单和网格条件单，持仓的FLAG的是达到了，但收益有点一言难尽，手中的垃圾票清掉了部分，有些垃圾真得再等几年，这种票已经不关注了，就留在持仓列表里做个提醒好了。 工业富联算是今天收获颇丰的一个票，也是唯一一个高额正收益的票，去年的FLAG是【目标收益10个点】，可能这个愿望描述的不够具体，就像雍和宫许愿一样，被调剂成了负的，结果股票加基金损失了10个点左右，真是令人头大。 本来前10个月收益基本达标，但最后这一个季度，行情急转直下，价值投资带头向下俯冲，毫无招架之力。 生活上 FLAG 疫情放开了，有事没事都趁着假期回家看看 身体很重要，锻炼的很费时，中和一下，可以跳跳绳 周末了陪娃耍一耍，工作尽量不占用周末的时间 完成度 ：70% 今年回家的次数也不是太多，元旦、春节、劳动节、国庆中秋节总计21天，比2022年的14天多了不少，主要差在了国庆假期上，回家正好收玉米，赶上了丰收的季节。 身体锻炼几乎没有，全年跳绳不超过3次，不过脂肪肝和高尿酸的情况都有所减轻，每天真的懒得不想动。 2023年的周末和工作日分的还是比较清晰的，工作日强度很大，但周末工作的时间很少了，真的是在家陪家人和或者一起出去逛逛，一家人去了之前谈论了8年还没去过的香山公园，看了一圈几乎还没红的枫叶。 去年的总结中给我养的花花拍了照片，今年对比了一下发现一年的时间长了很多，再拍一张照片看看。 2023年初还补充了几盆多肉，从最开始的柔弱养成了郁郁葱葱的“绿油油”，这一年个头没少长，都窜的老高，就是不上颜色，可能是光照太少了？ 年初文章总结了《工作十年对游戏看法的转变历程》，文中提到我买了8090掌上游戏机玩俄罗斯方块，后来趁娃不在又买了Switch准备寻找灵感，但结果就是海关发货太慢了，等娃回来正好赶上游戏机送到，最后Switch就成了她的玩具。 现在每天没时间玩也不敢玩，和娃约定好了周末才可以玩一会，每周末充一次电，和宝宝一起玩一会赛车还是很有意思的，偶尔自己偷偷玩玩塞尔达，似乎这些才能被称之为真正的游戏，哪些哪里亮就点哪里的游戏真的能被称为游戏吗？自己对这类游戏有些厌倦了。 展望2024工作 今年的工作环境极其复杂，切换了新的赛道，工作内容繁杂，需要逐步将内容理顺 继续培养管理技能，面对不同的人采取不同的策略，尝试向上管理的方法 补充技能树框架，采取提问和总结的方式，有针对性的学习和探索，为新的工作机会打好基础 学习 40篇以上博客总结，尽量一周一篇，放假休息，保持知识库一直更新 刷题不要求每周有贡献了，不切实际的FLAG没有意义，希望每月有贡献吧 采取提问回答方式补充技能树，利用好别人总结好的知识，很多知识点没必要从头开始 阅读开源代码解决实际问题，今年想尝试的知识点有【帧同步】和【共享内存】 继续保持读书的习惯，今年的图书已经到位，类型多是历史、传记和小说 投资 垃圾票适时出清 保持定投和智能条件单策略，重点放在科技和医药赛道，光伏赛道等待产能出清，持续观望 环境很差，但依旧相信国运，考虑今年的进入低位，2024年目标正收益15个点（可得说清楚，避免调剂） 生活 假期回家时间不低于去年的21天 想跑跑步，目前仅停留在脑海里，每周末下楼去转转 养养花，新年不打算买新的花了，好好把目前的多肉养出状态 周末陪陪家人，希望不要被家里的神仙宝贝娃娃气炸 总结2023年作为魔幻的一年，本来被给予了美好的希望，没想到放开之后，情况变得越发糟糕，打破了人们的预期，报复性的反弹没有如约而至，很多事情在封闭期间没有发生，却在这个寒冷的冬天尝了个遍，HR在得知自己毕业消息后，需要在临行前通知其他小伙伴毕业，主管实在不想开人，最终不得不自己离开，这些事情听起来很新奇，却在这个寒冬中显得那么的习以为常。 最近看了一些书籍和记录片，已经渐渐的体会到，做人不能太明白，有时糊里糊涂的反而更幸福，如果作为一个明白人还能收放自如的糊里糊涂，那便是境界升华了。 你还相信光吗？我依旧相信！虽然深处寒冬，我心依旧向往光明，信仰的力量支撑着我们前进，不管现在的真实情况是向上还是向下，我相信总有一天会向上突破的，期待这一天早点到来~ ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 旧年伊始志气豪，万物焕新气宇高。谁料寒冬多磨难，心怀光明盼春晓。]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活，投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux环境下通过journal命令查看和管理日志]]></title>
    <url>%2Fblog%2F2023%2F12%2F22%2FLinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%80%9A%E8%BF%87journal%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8B%E5%92%8C%E7%AE%A1%E7%90%86%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[前言就在半月之前，负责打包更新的服务器突然登录不上去了，赶紧找来运维的同事帮忙解决，发现系统日志中有很多 systemd-journald[424]: Failed to open runtime journal: No space left on device 字样的错误，被告知磁盘满了需要清理，但是我当时登录不上去，只能让他们帮忙重启后利用单用户模式上去删除临时文件试试，但重启后发现问题解决了，很是诧异，查询服务器后台监控平台，发现磁盘空间还有50%，inode只占用了0.02%，这怎么会磁盘满了呢？ 问题分析后来经过分析，应该是journal默认将系统日志存储在/run/log/journal目录中，而 /run是一个临时文件系统，通常挂载在内存中，用于存储运行时产生的临时文件，如进程 ID 文件、锁文件等，如果登录所需的文件或进程受到 /run 空间不足的影响，可能会导致登录问题。例如，如果 sshd 进程的运行所需的文件被存储在 /run 中并且空间不足，那么可能无法正常登录。 1234$ df -hFilesystem Size Used Avail Use% Mounted ondevtmpfs 7.8G 0 7.8G 0% /devtmpfs 7.8G 868K 7.8G 1% /run 所以系统重启后，这个空间释放了也就能通过ssh登录了，并且重启后journal日志清空也证实了这一点。 关于这个空间限制多数说的是10%，但是如果真是的所处分区的10%，那么不会导致我登录不上的，所以我倾向于是系统内存10%或者硬盘最大10%，默认最大限制4G，具体限制等我有空时查查文档吧，先贴一段看起来很靠谱的描述 SystemMaxUse=, SystemKeepFree=, SystemMaxFileSize=, SystemMaxFiles=, RuntimeMaxUse=, RuntimeKeepFree=, RuntimeMaxFileSize=, RuntimeMaxFiles=限制日志文件的 大小上限。 以 “System” 开头的选项用于限制磁盘使用量， 也就是 /var/log/journal 的使用量。 以 “Runtime” 开头的选项用于限制内存使用量， 也就是 /run/log/journal 的使用量。 以 “System” 开头的选项仅在 /var/log/journal 目录确实存在且可写时才有意义。 但以 “Runtime” 开头的选项永远有意义。 也就是说， 在系统启动早期 /var 尚未挂载时、 或者系统管理员禁止在磁盘上存储日志的时候， 仅有 “Runtime” 开头的选项有意义。 journalctl 与 systemd-journald 工具会忽略日志目录中 所有后缀名不等于 “.journal” 或 “.journal~” 的文件。 换句话说，日志目录中不应该存在后缀名不等于 “.journal” 或 “.journal~” 的文件， 因为这些文件 永远不会被清理。 SystemMaxUse= 与 RuntimeMaxUse= 限制全部日志文件加在一起最多可以占用多少空间。 SystemKeepFree= 与 RuntimeKeepFree= 表示除日志文件之外，至少保留多少空间给其他用途。 systemd-journald 会同时考虑这两个因素， 并且尽量限制日志文件的总大小，以同时满足这两个限制。 SystemMaxUse= 与 RuntimeMaxUse= 的默认值是10%空间与4G空间两者中的较小者； SystemKeepFree= 与 RuntimeKeepFree= 的默认值是15%空间与4G空间两者中的较大者； 如果在 systemd-journald 启动时，文件系统即将被填满并且已经超越了 SystemKeepFree= 或 RuntimeKeepFree= 的限制，那么日志记录将被暂停。 也就是说，如果在创建日志文件时，文件系统有充足的空闲空间， 但是后来文件系统被其他非日志文件过多占用， 那么 systemd-journald 只会立即暂停日志记录， 但不会删除已经存在的日志文件。 注意，只会删除已归档的日志文件以释放空间。 也就是说，即使在完成日志清理之后， 日志所占用的空间仍然可能大于 SystemMaxUse= 或 RuntimeMaxUse= 的限制。 SystemMaxFileSize= 与 RuntimeMaxFileSize= 限制单个日志文件的最大体积， 到达此限制后日志文件将会自动滚动。 默认值是对应的 SystemMaxUse= 与 RuntimeMaxUse= 值的1/8 ， 这也意味着日志滚动 默认保留7个历史文件。 日志大小 可以使用以1024为基数的 K, M, G, T, P, E 后缀， 分别对应于 1024, 1024², … 字节。 SystemMaxFiles= 与 RuntimeMaxFiles= 限制最多允许同时存在多少个日志文件， 超出此限制后， 最老的日志文件将被删除， 而当前的活动日志文件 则不受影响。 默认值为100个。 以上描述来自 《journald.conf 中文手册》，另外 《Systemd日志管理服务：Journald以及重要配置选项》 这篇博文也提到了默认日志数据将仅存储在内存中，在系统关闭时会删除该数据。 journal 和 syslog 对比提到系统日志除了journal比较常见以外，还有 syslog 也是比较基础的服务，两者都是日志服务，并且在系统中合理分工，相互配合，它们服务于不同的目标并且具有不同的特性。syslog是Linux系统的一套日志框架，它既可以记录日志到本地文件，也可以通过网络发送到接收syslog的服务器，以实现对多个设备的syslog消息进行统一的存储或解析。而journal是改进型的日志管理服务，主要收集来自内核、系统早期的启动阶段的日志以及系统守护进程在启动和运行中的标准输出和错误。 在CentOS 7及更高版本中，journalctl成为了查看和管理系统日志的主要工具，它从/var/log/journal/和/run/log/journal/等路径获取日志信息。同时，systemd-journald会将日志信息写入到socket文件/run/systemd/journal/syslog中，然后由rsyslog服务监听这个socket文件，从而实现对日志的统一管理和处理。因此，可以说syslog和journalctl在日志管理中各司其职，共同保障了Linux系统日志的完整性和可用性。 以上提到了一些名词，先说明一下来清楚其中的关系，journal日志使用journalctl工具管理，依赖systemd-journald服务，syslog日志可以用文本工具来查看，view、more、cat等命令都可以用，也可以借助其他工具查看特定信息，比如用dmesg命令查看syslog中内核相关日志，syslog依赖的服务名通常是 rsyslogd journalctl 和 syslog 都是 Linux 系统中用于处理和查看系统日志的工具，我们从以下几个方面来对比下两者的区别： 存储方式： syslog 使用文本文件来存储日志信息，可以通过文本编辑器查看，通常存储在 /var/log/syslog 或 /var/log/message journalctl 使用 systemd-journald 服务来存储二进制格式的日志文件。通常存储在 /run/log/journal/ 或 /var/log/journal/ 日志内容： syslog 使用文本格式，记录在文本文件中。日志信息包含时间戳、主机名、应用程序名等 journalctl 使用二进制格式，可以存储更多的元数据，例如进程 ID、用户 ID、SELinux 上下文等。这使得日志更加结构化，支持更高级的查询和过滤 查询和过滤： syslog 的查询和过滤通常使用命令行工具（例如 grep）或者专用的工具（例如 logrotate） journalctl 提供了更丰富和强大的查询和过滤功能，可以按时间、服务单元、日志级别等多个条件进行过滤。这使得查找和分析特定事件更加方便 实时查看： syslog 通常使用 tail 命令实时查看日志文件的末尾 journalctl 可以使用 -f 或 --follow 选项来实时查看最新的日志 服务和依赖关系： syslog 是一个通用的日志服务，可以由多个日志守护进程（如 rsyslog、syslog-ng）实现 journalctl 是 systemd 系统中的一部分，依赖于 systemd-journald 服务 配置文件的路径： syslog 的配置文件在 /etc/rsyslog.conf journalctl 的配置文件在 /etc/systemd/journald.conf 总体而言，journalctl 是 systemd 系统的一部分，提供了更现代化、结构化和强大的日志管理功能，但这并不意味着 syslog 是过时的。在一些系统中，两者可能同时存在，而且一些工具和服务可能仍然使用传统的 syslog。选择使用哪一个取决于系统的需求和管理员的偏好。 下面分别展示一下systemd-journald 和 rsyslogd 两个服务查询信息： 123456789101112131415[root@demo ~]# systemctl status systemd-journald● systemd-journald.service - Journal Service Loaded: loaded (/usr/lib/systemd/system/systemd-journald.service; static; vendor preset: disabled) Active: active (running) since Mon 2023-12-04 20:26:34 CST; 2 weeks 3 days ago Docs: man:systemd-journald.service(8) man:journald.conf(5) Main PID: 7099 (systemd-journal) Status: "Processing requests..." CGroup: /system.slice/systemd-journald.service └─7099 /usr/lib/systemd/systemd-journaldDec 04 20:26:34 demo systemd-journal[7099]: Permanent journal is using 8.0M (max allowed 4.0G, trying to leave 4.0G free of 183.5G available → c…imit 4.0G).Dec 04 20:26:34 demo systemd-journal[7099]: Time spent on flushing to /var is 21.108ms for 1873 entries.Dec 04 20:26:34 demo systemd-journal[7099]: Journal startedHint: Some lines were ellipsized, use -l to show in full. 1234567891011121314151617181920[root@demo ~]# systemctl status rsyslogdUnit rsyslogd.service could not be found.[root@demo ~]# systemctl status rsyslog● rsyslog.service - System Logging Service Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2023-12-04 19:46:04 CST; 2 weeks 3 days ago Docs: man:rsyslogd(8) http://www.rsyslog.com/doc/ Main PID: 913 (rsyslogd) CGroup: /system.slice/rsyslog.service └─913 /usr/sbin/rsyslogd -nDec 04 19:46:04 demo systemd[1]: Starting System Logging Service...Dec 04 19:46:04 demo rsyslogd[913]: [origin software="rsyslogd" swVersion="8.24.0-57.el7_9.3" x-pid="913" x-info="http://www.rsyslog.com"] startDec 04 19:46:04 demo systemd[1]: Started System Logging Service.Dec 04 20:26:34 demo rsyslogd[913]: imjournal: journal reloaded... [v8.24.0-57.el7_9.3 try http://www.rsyslog.com/e/0 ]Dec 10 03:22:01 demo rsyslogd[913]: [origin software="rsyslogd" swVersion="8.24.0-57.el7_9.3" x-pid="913" x-info="http://www.rsyslog.com"] rsys...was HUPedDec 15 01:35:01 demo rsyslogd[913]: imjournal: journal reloaded... [v8.24.0-57.el7_9.3 try http://www.rsyslog.com/e/0 ]Dec 17 03:07:01 demo rsyslogd[913]: [origin software="rsyslogd" swVersion="8.24.0-57.el7_9.3" x-pid="913" x-info="http://www.rsyslog.com"] rsys...was HUPedHint: Some lines were ellipsized, use -l to show in full. journal 和 syslog 配置两个服务的配置文件都有很多内容，在此只展示其中部分重要的参数，先看一下 /etc/systemd/journald.conf 12345678[Journal]Storage=persistent #开启持久化存储#Compress=yes #开启压缩功能#Seal=yesSystemMaxUse=40% #设置系统最大占用率为40%StorageOption=size #设置存储选项为按大小进行存储StorageOption=autorotate #设置存储选项为自动轮换StoragePath=/export/log/journal #设置日志文件的存储路径 再看一下 /etc/rsyslog.conf 配置文件 123456789101112131415161718192021222324252627282930313233343536# Include all config files in /etc/rsyslog.d/$IncludeConfig /etc/rsyslog.d/*.conf #将包含 `/etc/rsyslog.d/` 目录下的所有以 `.conf` 结尾的配置文件，可以将配置分散到多个文件使得管理更加灵活# Turn off message reception via local log socket;# local messages are retrieved through imjournal now.$OmitLocalLogging on #不再通过本地日志套接字接收消息，本地消息现在通过 `imjournal`（Journal 日志服务）来检索#### RULES ##### Log all kernel messages to the console.# Logging much else clutters up the screen.#kern.* /dev/console# Log anything (except mail) of level info or higher.# Don't log private authentication messages!*.info;mail.none;authpriv.none;cron.none /var/log/messages #将所有信息级别（info）以及除了邮件、认证、和 cron 之外的其他日志，都记录到 `/var/log/messages` 文件中# The authpriv file has restricted access.authpriv.* /var/log/secure #将所有 authpriv（私密认证）级别的日志记录到 `/var/log/secure` 文件中# Log all the mail messages in one place.mail.* -/var/log/maillog #将所有邮件相关的日志记录到 `/var/log/maillog` 文件中# Log cron stuffcron.* /var/log/cron #将所有 cron 服务相关的日志记录到 `/var/log/cron` 文件中# Everybody gets emergency messages*.emerg :omusrmsg:* #将所有紧急级别（emergency）的日志消息发送到用户消息队列# Save news errors of level crit and higher in a special file.uucp,news.crit /var/log/spooler #将所有 uucp 和 news 的 crit 级别及更高级别的日志记录到 `/var/log/spooler` 文件中# Save boot messages also to boot.loglocal7.* /var/log/boot.log #表示保存启动消息到boot.log文件中 使用journalctl查看和管理日志查看日志123456789101112131415161718192021222324252627282930313233343536373839# 查看所有日志（默认情况下只保存本次启动的日志）$ sudo journalctl# 显示尾部指定行数的日志，默认10行$ sudo journalctl -n 20# 实时滚动显示最新日志$ sudo journalctl -f# 查看指定时间的日志$ sudo journalctl --since yesterday$ sudo journalctl --since="2023-12-22 16:52:18"$ sudo journalctl --since "30 min ago"$ sudo journalctl --since "2023-12-22 16:52:18" --until "2023-12-22 23:52:18"$ sudo journalctl --since 09:00 --until "1 hour ago"# 查看内核日志，过滤掉应用日志$ sudo journalctl -k# 查看系统本次启动的日志$ sudo journalctl -b$ sudo journalctl -b -0# 查看上一次启动的日志$ sudo journalctl -b -1# 查看指定优先级及其以上级别的日志，共有8级 0: emerg 1: alert 2: crit 3: err 4: warning 5: notice 6: info 7: debug# -b 不加任何参数时，表示显示当前引导周期的日志。这意味着只显示自最近一次启动以来的日志。# -b N： N 是一个整数，表示要显示第 N 个引导周期的日志。例如，-b 0 表示显示最新的引导周期，-b 1 表示显示上一个引导周期，以此类推$ sudo journalctl -p err -b# 日志默认分页输出，--no-pager 改为正常的标准输出$ sudo journalctl --no-pager# 以单行 JSON 格式输出$ sudo journalctl -b -u nginx.service -o json# 以多行 JSON 可读性更好的格式输出$ sudo journalctl -b -u nginx.service -o json-pretty 查看指定服务日志1234567891011121314151617181920# 查看指定服务的日志$ sudo journalctl /usr/sbin/sshd# 查看指定进程的日志$ sudo journalctl _PID=1# 查看某个路径的脚本的日志$ sudo journalctl /bin/bash# 查看指定用户的日志$ sudo journalctl _UID=1000 --since today# 查看某个 Unit 的日志# 单元（unit）通常是 systemd 服务的一个抽象，用于表示系统中正在运行的各种服务或任务$ sudo journalctl -u nginx.service$ sudo journalctl -u nginx.service --since today# 合并显示多个 Unit 的日志$ journalctl -u nginx.service -u ssh.service --since today 查看调整存储1234567891011121314# 显示日志占据的硬盘空间$ sudo journalctl --disk-usage# 仅保留500MB大小的日志文减$ sudo journalctl --vacuum-size=500M# 指定日志文件保存多久$ sudo journalctl --vacuum-time=1years# 仅保留最近一个月的日志文件$ sudo journalctl --vacuum-time=1m# 仅保留最近2天的日志文件$ sudo journalctl --vacuum-time=2d 回到文章开头的问题通过梳理知识，我发现通过 systemctl 命令查询 systemd-journald 服务状态可以看到日志存储路径和存储限制，分别展示一下，先看存储到内存的： 123456789101112131415161718192021222324252627root@demo1:~# systemctl status systemd-journald● systemd-journald.service - Journal Service Loaded: loaded (/lib/systemd/system/systemd-journald.service; static; vendor preset: enabled) Active: active (running) since Fri 2023-12-22 10:25:43 CST; 10h ago Docs: man:systemd-journald.service(8) man:journald.conf(5) Main PID: 1132 (systemd-journal) Status: "Processing requests..." Tasks: 1 Memory: 272.0K CPU: 204ms CGroup: /system.slice/systemd-journald.service └─1132 /lib/systemd/systemd-journaldDec 22 10:25:43 demo1 systemd-journald[1132]: Runtime journal (/run/log/journal/) is 8.0M, max 159.9M, 151.9M free.Dec 22 10:25:43 demo1 systemd-journald[1132]: Journal startedroot@demo1:~# df -hFilesystem Size Used Avail Use% Mounted onudev 7.8G 0 7.8G 0% /devtmpfs 1.6G 9.3M 1.6G 1% /run/dev/sda1 83G 29G 51G 36% /tmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup/dev/sdb 1017G 903G 115G 89% /exporttmpfs 1.6G 0 1.6G 0% /run/user/111tmpfs 1.6G 0 1.6G 0% /run/user/1001 默认存储到 /run/log/journal/中，分区一共1.6G，最大max 159.9M，当前用了 8.0M，按这个来推算果然限制是10%，再看看持久化的情况 123456789101112131415161718192021222324252627282930313233root@jenkins2004:~# systemctl status systemd-journald● systemd-journald.service - Journal Service Loaded: loaded (/lib/systemd/system/systemd-journald.service; static; vendor preset: enabled) Active: active (running) since Sun 2023-08-27 09:24:15 CST; 3 months 26 days agoTriggeredBy: ● systemd-journald-dev-log.socket ● systemd-journald.socket ● systemd-journald-audit.socket Docs: man:systemd-journald.service(8) man:journald.conf(5) Main PID: 375 (systemd-journal) Status: "Processing requests..." Tasks: 1 (limit: 19123) Memory: 42.7M CGroup: /system.slice/systemd-journald.service └─375 /lib/systemd/systemd-journald8月 27 09:24:15 jenkins2004 systemd-journald[375]: Journal started8月 27 09:24:15 jenkins2004 systemd-journald[375]: Time spent on flushing to /var/log/journal/cc9ad539032546a1bd39a773d37e415f is 613.827ms for 1229 entries.8月 27 09:24:15 jenkins2004 systemd-journald[375]: System Journal (/var/log/journal/cc9ad539032546a1bd39a773d37e415f) is 1.4G, max 4.0G, 2.5G free.8月 27 09:24:22 jenkins2004 systemd-journald[375]: Forwarding to syslog missed 1 messages.8月 27 09:24:53 jenkins2004 systemd-journald[375]: Forwarding to syslog missed 80 messages.Warning: journal has been rotated since unit was started, output may be incomplete.root@jenkins2004:~# df -h | grep -v snapFilesystem Size Used Avail Use% Mounted onudev 7.8G 0 7.8G 0% /devtmpfs 1.6G 1.6M 1.6G 1% /run/dev/sda5 590G 369G 191G 66% /tmpfs 7.9G 0 7.9G 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup/dev/sda1 511M 4.0K 511M 1% /boot/efitmpfs 1.6G 36K 1.6G 1% /run/user/125tmpfs 1.6G 4.0K 1.6G 1% /run/user/1002 看起来存到了 var/log/journal，最大可用空间居然到了4G，这样来分析这次的问题可能不是journal日志搞的鬼，而是很多进程吧/run目录写满了，导致journal写不进去报了开头的错误，进而引发的无法登录。 总结 syslog和journal在日志管理中各司其职，共同保障了Linux系统日志的完整性和可用性 journal默认将系统日志存储在/run/log/journal目录中，而 /run 通常挂载在内存中，当此目录被写满时可能导致ssh无法登录 将 /etc/systemd/journald.conf 配置中的 Storage 字段设置为 persistent 可以开启日志持久化 sudo journalctl -n 20 可以显示显示尾部最新的20行日志，-n后不加参数默认10行 sudo journalctl -u nginx.service 查看ssh服务器的日志 sudo journalctl --since=&quot;2023-12-22 16:52:18&quot; 查看指定时间以后的日志 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 寒意一波接着一波，总会有看见“光”的时候（30厘米的下跌，摔得可真疼）]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>运维</tag>
        <tag>服务器</tag>
        <tag>journalctl</tag>
        <tag>syslog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab修改代码库的名称、路径和分组]]></title>
    <url>%2Fblog%2F2023%2F12%2F19%2Fgitlab%E4%BF%AE%E6%94%B9%E4%BB%A3%E7%A0%81%E5%BA%93%E7%9A%84%E5%90%8D%E7%A7%B0%E3%80%81%E8%B7%AF%E5%BE%84%E5%92%8C%E5%88%86%E7%BB%84%2F</url>
    <content type="text"><![CDATA[前言工作项目的源码使用gitlab进行管理，通常情况下不会修改代码库和路径，但是非一般的情况总是存在，比如刚刚立项时多次调整项目名，甚至改变管理层级，所以还是有这类需求的，一开始我本打算将旧的代码库和提交记录推送到新的地址和分组，后来在gitlab后台里翻了翻，结合网络大神们的指点，发现可以直接修改代码库的名称和分组，总共需要修改3个配置，一起来看下。 修改过程具体需求假设需求是将项目名从 A 改为 B，同时将访问路径从 git@gitlab.example.cn:APrject/AGroup/A.git 改为 git@gitlab.example.cn:APrject/AGroup/B.git，这里一共有三处需要修改：项目名、访问路径、以及分组，下面依次列举出修改的方法： 修改项目名导航路径【Settings】-&gt;【General】-&gt;【Naming, topics, avatar】-&gt;【Project name】，只需填入新的项目名，点击 “Save changes” 按钮即可，页面展示如下： 修改访问路径做完上一步只是修改了项目名字，如果使用 git clone 命令下载整个库，路径是没有发生变化的，修改访问路径需要调整 “Path” 导航路径【Settings】-&gt;【General】-&gt;【Advanced】-&gt;【Expand】-&gt;【Change path】，只能修改最后一段Path，其中不能再包含 /，修改后点击 “Change path” 即可，页面展示如下： 修改分组上面修改路径这一步只允许修改Path的最后一段，不能直接调整分组路径，想要调整前面的路径需要调整分组 导航路径【Settings】-&gt;【General】-&gt;【Advanced】-&gt;【Expand】-&gt;【Transfer project】，这一步只能选择已有的分组，选择好目标分组，点击 “Transfer project” 按钮即可，页面展示如下： 总结 gitlab后台可以直接修改代码库的名称、路径和分组等信息，不必重新创建项目 在修改名称、路径和分组等信息后需要相关人员更新对原有代码库的引用，否则会找不到 使用 git remote set-url origin &lt;new-url&gt; 命令可以更新已有仓库的URL ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 对未来的迷茫以及内心的不安，让我们在现有社会经济条件下，沦为被金钱和物质所统治的行尸走肉]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>gitlab</tag>
        <tag>修改项目名</tag>
        <tag>修改路径</tag>
        <tag>修改分组</tag>
        <tag>后台管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用websockify将websocket通信转换成tcp]]></title>
    <url>%2Fblog%2F2023%2F12%2F13%2F%E5%88%A9%E7%94%A8websockify%E5%B0%86websocket%E9%80%9A%E4%BF%A1%E8%BD%AC%E6%8D%A2%E6%88%90tcp%2F</url>
    <content type="text"><![CDATA[前言目前遇到一个问题，原本的服务都是利用tcp通信的，但是某些平台只支持websocket，是不是要从头实现一套websocket网络收发逻辑呢？短时间内有没有替代方案呢？在经过一番寻找之后发现了websockify这个神器，可以将websocket协议转换成tcp协议，从而实现不修改应用服务就支持tcp协议的目的，接下来就说说它怎么用，以及探索的过程。 websockifywebsockify 介绍websockify 是一个用于将 WebSocket 流量转发到其他协议的工具，它的主要用途之一是将 WebSockets 转发到基于 TCP 的协议，例如 VNC（Virtual Network Computing），以下是它的一些关键特点和用途： WebSocket 转发： websockify 允许你将 WebSocket 流量转发到其他类型的网络服务，使得你可以通过 WebSocket 连接访问这些服务。 VNC 转发： websockify 最常用于将 WebSocket 转发到 VNC 服务器。这使得你可以通过 Web 浏览器访问运行 VNC 服务的远程计算机的桌面。 安全性： websockify 支持加密通信，因此可以在安全连接上运行，提供更好的数据保护。 协议适配： websockify 提供了一个通用的桥接机制，允许你将 WebSocket 连接转发到支持其他协议的服务，而不仅仅是 VNC。 JavaScript 客户端： websockify 还包括一个 JavaScript 客户端库，可以直接在浏览器中使用，无需额外的插件。 使用 websockify 的典型场景包括在 Web 浏览器中访问远程计算机的桌面，或者通过 WebSocket 连接到其他需要 TCP 连接的服务，在实际应用中，你可以通过命令行使用 websockify，也可以将其嵌入到其他应用程序中。 websockify 使用在Ubuntu系统下可以直接使用以下安装： 1$ sudo apt install websockify 下面是一个使用示例，作用是将原本连接到8765的websocket请求转换成tcp请求，TCP端口4321 1$ websockify 8765 127.0.0.1:12346 启动后原本仅支持TCP协议的服务就可以通过websockify实现对websocket的支持了 探索的过程虽然是短短的一条 websockify 8765 127.0.0.1:12346 命令就可以实现从websocket到tcp的转换，但是探索的过程却不顺利，在发现websockify这个神器之后，首先是测试它是否能满足我们的需求，是否好用，但是直接在原有的服务上测试太费时间，所以尝试写了几个模拟的脚本。 提供基础TCP服务最基本的我们需要一个很能提供TCP服务的程序，这里我使用python写了一个回显的服务器 echoserver.py，绑定本地的12346端口 123456789101112131415161718192021222324252627282930313233import socketdef main(): # 创建一个TCP套接字 server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # 绑定IP地址和端口 server_socket.bind(('127.0.0.1', 12346)) # 监听连接 server_socket.listen(5) print("服务器已启动，等待客户端连接...") while True: # 接受客户端连接 client_socket, client_address = server_socket.accept() print(f"客户端&#123;client_address&#125;已连接") # 接收客户端发送的数据 data = client_socket.recv(1024) print(f"收到来自&#123;client_address&#125;的数据： &#123;data.decode('utf-8')&#125;") # 将接收到的数据发送回客户端 client_socket.send(data) print(f"已将数据发送回&#123;client_address&#125;") # 关闭客户端套接字 client_socket.close() print(f"客户端&#123;client_address&#125;已断开连接")if __name__ == "__main__": main() 测试可用 启动服务 12$ python3 echoserver.py服务器已启动，等待客户端连接... 用nc命令发送tcp数据 12$ echo "Hello server" | nc 127.0.0.1 12346Hello server TCP服务器看到的内容 123456服务器已启动，等待客户端连接...客户端('127.0.0.1', 59214)已连接收到来自('127.0.0.1', 59214)的数据： Hello server已将数据发送回('127.0.0.1', 59214)客户端('127.0.0.1', 59214)已断开连接 TCP服务一切正常 实现Websocket客户端因为python的运行环境是现成的，所以我又用它写了一个websocket客户端 wsclient.py 来发送数据，想运行的话得安装几个包 12python3 -m pip install websocketpython3 -m pip install websocket-client 真实的情况下我是用websocket客户端连接nginx，然后将nginx转发到websockify监听端口，然后websockify将数据转成tcp连接应用服务，但是这个过程太复杂了，不利于说明问题，所以后面的内容我省掉nginx，假装它不存在，直接用websocket客户端连接websockify监听端口 12345678910111213141516171819202122232425262728293031323334# -*- coding: utf-8 -*-# 运行websocket服务器import websocketdef on_message(ws, message): print(f"Received message: &#123;message&#125;")def on_error(ws, error): print(f"Error: &#123;error&#125;")def on_close(ws, close_status_code, close_msg): print(f"Connection closed with status code &#123;close_status_code&#125;")def on_open(ws): print("Connection opened") # 在连接建立后发送一条消息 ws.send("abcd")if __name__ == "__main__": # WebSocket 服务器地址 ws_url = "ws://10.10.49.172:8765" # 创建 WebSocket 连接 ws = websocket.WebSocketApp(ws_url, on_message=on_message, on_error=on_error, on_close=on_close) # 设置连接建立时的回调函数 ws.on_open = on_open # 启动 WebSocket 连接 ws.run_forever() 为了websocket客户端的测试，我写了一个websocket服务器 wsserver.py 来接收数据，不过这不是一开始就测试了，而是在后面发现转换TCP失败查找原因，按流程分段测试时补充的脚本内容 12345678910111213141516171819import asyncioimport websocketsasync def echo(websocket, path): try: async for message in websocket: print(f"Received message: &#123;message&#125;") await websocket.send(message) except websockets.exceptions.ConnectionClosed: print("WebSocket connection closed")async def main(): server = await websockets.serve(echo, "0.0.0.0", 12346) print("WebSocket server is running...") await server.wait_closed()if __name__ == "__main__": asyncio.run(main()) 测试结果websocket客户端发送的数据，websocket服务器可以正常收到。 开始测试websockify功能 启动tcp服务echoserver，监听12346端口 12$ python3 echoserver.py服务器已启动，等待客户端连接... 启动tcp服务websockify，监听8765端口，转换到12346端口 12345$ websockify 8765 127.0.0.1:12346WebSocket server settings: - Listen on :8765 - No SSL/TLS support (no cert file) - proxying from :8765 to 127.0.0.1:12346 运行websocket客户端发送的数据 123$ python3 ./wsclient.pyConnection openedConnection closed with status code 1003 报错了！！！ 再看echoserver输出，虽然连接成功，但是收到的数据为空 客户端(‘127.0.0.1’, 55032)已连接收到来自(‘127.0.0.1’, 55032)的数据：已将数据发送回(‘127.0.0.1’, 55032)客户端(‘127.0.0.1’, 55032)已断开连接 然后看下websockify输出，显示转发连接12346端口成功了，但是没有后续输出 $ websockify 8765 127.0.0.1:12346WebSocket server settings: Listen on :8765 No SSL/TLS support (no cert file) proxying from :8765 to 127.0.0.1:1234610.2.48.36 - - [13/Dec/2023 20:03:11] 10.2.48.36: Plain non-SSL (ws://) WebSocket connection10.2.48.36 - - [13/Dec/2023 20:03:11] connecting to: 127.0.0.1:12346 定位问题吧，期间不断调整nginx配置，查看nginx和websockify日志，调整wsclient写法，调整echoserver写法，分步测试wsclient和echoserver功能都没发现问题，具体测试细节暂且不表，后来我发现可以修改websockify启动参数输出更详细的日志，测试过程如下 再次启动websockify启动websockify再次使用wsclient测试，输出了额外的信息 123456789101112131415161718192021$ websockify 8765 127.0.0.1:12346 --log-file /tmp/ws.log -v --trafficWebSocket server settings: - Listen on :8765 - No SSL/TLS support (no cert file) - proxying from :8765 to 127.0.0.1:1234610.2.48.36: new handler Process10.2.48.36 - - [13/Dec/2023 20:13:42] "GET / HTTP/1.1" 101 -10.2.48.36 - - [13/Dec/2023 20:13:42] 10.2.48.36: Plain non-SSL (ws://) WebSocket connection10.2.48.36 - - [13/Dec/2023 20:13:42] connecting to: 127.0.0.1:12346Traffic Legend: &#125; - Client receive &#125;. - Client receive partial &#123; - Target receive &gt; - Target send &gt;. - Target send partial &lt; - Client send &lt;. - Client send partial&#125;.10.2.48.36 - - [13/Dec/2023 20:13:42] 127.0.0.1:12346: Client closed connection10.2.48.36 - - [13/Dec/2023 20:13:42] 127.0.0.1:12346: Closed target 顺着结尾的两条日志 Client closed connection，我找到了这一篇问题 websockify - Client disconnects immediately after connection #365 结论说是websockify不支持发送文本了 Hmm… Are you trying to send text over that socket? We dropped support for text frames in the big cleanup. Could you print the code and reason from the close in your close handler? 我一看我的wsclient.py可不就是发送的文本嘛，看来有救了，赶紧改成了发送bytes ws.send(b&#39;\x01\x02\x03\x04&#39;)，但错误依旧，尝试各种发送函数无果，我严重怀疑我的换个python的websockets包有问题，所以我用html写了一个wsclient.html： 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt; &lt;title&gt;WebSocket Binary Data Example&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;script&gt; // 替换为你的 WebSocket 服务器地址 const socket = new WebSocket("ws://10.10.49.172:8765"); // 监听连接打开事件 socket.addEventListener("open", (event) =&gt; &#123; console.log("WebSocket connection opened."); // 构造二进制数据，这里使用一个简单的示例 const binaryData = new Uint8Array([49, 50, 51, 52]); // 发送二进制数据 socket.send(binaryData); console.log("Binary data sent successfully."); &#125;); // 监听接收消息事件 socket.addEventListener("message", (event) =&gt; &#123; console.log("Received message:", event.data); &#125;); // 监听连接关闭事件 socket.addEventListener("close", (event) =&gt; &#123; console.log("WebSocket connection closed."); &#125;); // 监听错误事件 socket.addEventListener("error", (event) =&gt; &#123; console.error("WebSocket error:", event); &#125;); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 这次终于正常了，websockify输出变成了由应用服务主动断开 123456789101112131415161718192021$ websockify 8765 127.0.0.1:12346 --log-file /tmp/ws.log -v --trafficWebSocket server settings: - Listen on :8765 - No SSL/TLS support (no cert file) - proxying from :8765 to 127.0.0.1:1234610.2.48.36: new handler Process10.2.48.36 - - [13/Dec/2023 20:24:29] "GET / HTTP/1.1" 101 -10.2.48.36 - - [13/Dec/2023 20:24:29] 10.2.48.36: Plain non-SSL (ws://) WebSocket connection10.2.48.36 - - [13/Dec/2023 20:24:29] connecting to: 127.0.0.1:12346Traffic Legend: &#125; - Client receive &#125;. - Client receive partial &#123; - Target receive &gt; - Target send &gt;. - Target send partial &lt; - Client send &lt;. - Client send partial&#125;&gt;&#123;&lt;10.2.48.36 - - [13/Dec/2023 20:24:29] 127.0.0.1:12346: Target closed connection10.2.48.36 - - [13/Dec/2023 20:24:29] 127.0.0.1:12346: Closed target echoserver也看到了发送的内容【1234】： 123456$ python3 echoserver.py服务器已启动，等待客户端连接...客户端('127.0.0.1', 37438)已连接收到来自('127.0.0.1', 37438)的数据： 1234已将数据发送回('127.0.0.1', 37438)客户端('127.0.0.1', 37438)已断开连接 单独实现一个js版本websocket客户端刚刚用html里的js发送了websocket数据，需要浏览器的帮助，如果是在服务器上局限性很大，所以我调整了一下，改成了js版本websocket客户端，用node.js运行，wsclient.js内容如下： 1234567891011121314151617181920212223242526272829303132const WebSocket = require('ws');// 替换为你的 WebSocket 服务器地址const socket = new WebSocket("ws://10.10.49.172:8765");// 监听连接打开事件socket.addEventListener("open", (event) =&gt; &#123; console.log("WebSocket connection opened."); // 构造二进制数据，这里使用一个简单的示例 const binaryData = new Uint8Array([49, 50, 51, 52]); // 发送二进制数据 socket.send(binaryData); console.log("Binary data sent successfully.");&#125;);// 监听接收消息事件socket.addEventListener("message", (event) =&gt; &#123; console.log("Received message:", event.data);&#125;);// 监听连接关闭事件socket.addEventListener("close", (event) =&gt; &#123; console.log("WebSocket connection closed.");&#125;);// 监听错误事件socket.addEventListener("error", (event) =&gt; &#123; console.error("WebSocket error:", event);&#125;); 运行结果如下： 12345$ node wsclient.jsWebSocket connection opened.Binary data sent successfully.Received message: &lt;Buffer 31 32 33 34&gt;WebSocket connection closed. 至此，websockify的测试就结束了，它可以满足我们的需求，将websocket请求转换成tcp请求 什么是VNC前面在介绍websockify多次提到VNC，其实VNC (Virtual Network Computing) 是一种远程桌面协议，允许用户在一个计算机上通过网络远程控制另一个计算机的桌面。VNC 提供了一种在远程计算机上查看和操作桌面的方式，就像你坐在那台计算机前一样，以下是关于 VNC 的一些关键概念： Server：在 VNC 中，远程计算机上运行的应用程序称为 VNC 服务器。该服务器负责监听连接请求，并将计算机的桌面图像发送给连接的客户端。 Viewer：连接到 VNC 服务器的应用程序称为 VNC 查看器。这是用户使用的远程桌面客户端，允许他们查看和操作远程计算机的桌面。 Port：VNC 服务器通过一个特定的网络端口监听连接请求。通常，VNC 默认使用 5900 端口。如果有多个 VNC 服务器在同一台计算机上运行，它们可能使用不同的端口（5901、5902 等）。 Security：VNC 提供了一些安全性选项，如密码保护和加密。这有助于确保在远程访问时保护计算机的安全。 Authentication：VNC 服务器和查看器之间的连接通常需要身份验证。这可以是使用密码进行简单的身份验证，也可以是更复杂的加密和密钥交换过程。 websockify：对于一些场景，特别是在 Web 浏览器中访问 VNC，你可能会使用工具如 websockify 将 VNC 的协议转换为 WebSocket 协议，以便在浏览器中实现 VNC 远程桌面访问。 总结 websockify可以轻松实现从websocket请求向tcp请求的转换，使用很方便 转换路径 wsclient --ws--&gt; websockify --tcp--&gt; tcp application --tcp--&gt; websockify --ws--&gt; wsclient wss对应的是ws的加密版本，他们的关系是wss=ws+SSL/TSL，类似的https=http+SSL/TSL 无论是nginx还是websockify都可以配置服务器证书，将应用服务器从SSL/TSL中解脱出来 VNC是一种远程桌面协议，允许用户在一个计算机上通过网络远程控制另一个计算机的桌面 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 迈出这一步确实不太容易，一旦行动了就会发现没有想象中的那么难，我们都没有预知未来的能力，无法判定目前的选择是对还是错，既然选择了就要努力走下去]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>网络协议</tag>
        <tag>websockify</tag>
        <tag>websocket</tag>
        <tag>tcp/ip</tag>
        <tag>NVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx配置自建SSL证书]]></title>
    <url>%2Fblog%2F2023%2F12%2F08%2Fnginx%E9%85%8D%E7%BD%AE%E8%87%AA%E5%BB%BASSL%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[前言之前的一篇文章《自建CA并生成自签名SSL证书》中讲到为什么要自建CA和自签名SSL证书，是因为买证书得花钱，对于内部或小规模项目，使用自建SSL证书可能更为方便，不需要支付费用，而且不涉及复杂的验证过程。正式对外的服务一般都是要买公共证书颁发机构（CA）签发的SSL证书的，但是在对外发布前可以先使用自建证书打通流程 配置SSL证书创建SSL证书的流程参考上文中提到的文章吧，本文只讲怎样把自建SSL证书配置到nginx，实际上非常简单。 假设我们的自建证书是 /root/ca/server.crt，服务器私钥是 /root/ca/server.key，nginx配置文件我以《记录一下第一次安装和配置Nginx》 这篇文章的配置文件为例，初始配置为： 1234567891011121314151617upstream go_entrance &#123; server localhost:4101; server localhost:4102;&#125;server &#123; listen 4100; server_name localhost; location / &#123; proxy_pass http://go_entrance; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 4100端口监听http协议转发到本机的4101端口和4102端口，如果把SSL证书配置到这个端口上，就相当于这个端口支持了https，配置修改如下： 1234567891011121314151617181920212223242526upstream go_entrance &#123; server localhost:4101; server localhost:4102;&#125;server &#123; listen 4100 ssl; server_name localhost; ssl_certificate /root/ca/server.crt; ssl_certificate_key /root/ca/server.key; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; proxy_pass http://go_entrance; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 只需要将4100端口后面加上 ssl，再配置几个ssl相关的参数就可以了，含义如下： ssl_certificate： 指定 SSL 证书文件路径 /root/ca/server.crt ssl_certificate_key： 指定私钥文件路径 /root/ca/server.key ssl_session_cache： 配置用于存储 SSL 会话的缓存。shared:SSL:10m 表示使用共享的内存区域，最大占用内存为 10MB ssl_session_timeout： 配置 SSL 会话的超时时间，这里设置为 10 分钟 ssl_protocols： 指定支持的 SSL/TLS 协议版本，这里包括 TLSv1、TLSv1.1 和 TLSv1.2 ssl_ciphers： 指定支持的加密套件，这里配置为 HIGH:!aNULL:!MD5，表示使用高强度的加密套件，不支持空加密和 MD5 SSL证书放在 Nginx 而不放在应用服务器上的好处正如上面的配置一样，4100端口收到https请求后转发到4101和4102上的是http协议，说明使用这种方式一些仅支持http协议的应用服务也可以通过nginx配置证书来达到支持https的目的，具体好处如下： 集中管理： 使用反向代理服务器管理 SSL 证书可以实现集中式管理。这意味着你可以在一个地方管理证书，而不需要在每个应用服务器上都安装和维护证书。这样能够简化证书的更新和维护流程。 简化配置： 通过在反向代理服务器上配置 SSL，你可以简化应用服务器的配置。应用服务器可以专注于处理应用程序逻辑，而无需关心 SSL 配置。这样有助于提高系统的可维护性和简化配置过程。 负载均衡和扩展： 如果你使用负载均衡，SSL终止（SSL Termination）在负载均衡器上执行可以减轻应用服务器的负担。负载均衡器负责处理SSL握手，将非加密的请求转发给后端应用服务器。这样，后端服务器就可以专注于处理业务逻辑，而无需处理加密和解密操作。 性能优化： SSL 握手和加解密操作可能是计算密集型的任务，将这些任务从应用服务器中移除，可以在 SSL 握手和加解密方面提高性能。 统一的安全策略： 通过在反向代理服务器上管理 SSL，可以实施统一的安全策略，确保所有传入和传出的流量都经过相同的安全设置。 Nginx只能转发http协议吗不，Nginx 不仅仅能够转发 HTTP 协议，还支持其他多种协议的代理转发。主要的协议包括： HTTPS协议： 通过在配置中启用 SSL/TLS，Nginx 可以用作安全的 HTTPS 服务器和反向代理，处理加密的 HTTP 流量。 1234567listen 443 ssl;ssl_certificate /path/to/certificate.crt;ssl_certificate_key /path/to/private-key.key;location / &#123; proxy_pass https://backend_server;&#125; TCP协议： 从1.9版本开始 Nginx 可以用于代理 TCP 流量，例如数据库连接、消息队列等。 123456stream &#123; server &#123; listen 3306; proxy_pass backend_server:3306; &#125;&#125; UDP协议： 从Nginx 1.9.13版本开始，开始支持 UDP 代理。这使得它可以用于代理 UDP 流量，如 DNS 请求等。 123456stream &#123; server &#123; listen 53 udp; proxy_pass backend_dns_server:53; &#125;&#125; WebSocket协议： WebSocket 是一种在单个 TCP 连接上提供全双工通信的协议，常用于实时应用程序，如在线游戏、聊天应用等。 1234567891011121314151617map $http_upgrade $connection_upgrade &#123; default upgrade; '' close;&#125;server &#123; listen 80; server_name your_domain.com; location /websocket &#123; proxy_pass http://backend_server; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; &#125;&#125; Nginx转发TCP协议会收到端口限制吗是的，TCP是一种面向连接的全双工通信的协议，当转发TCP消息时，Nginx不仅是一个服务器接受客户端的连接，再它连接应用服务器时还表现成一个客户端，每个连接需要消耗一个端口，以理论值65535个端口来计算，nginx最多转发65535个连接，但是可以通过 proxy_bind 来突破限制，或者配置多个IP或虚拟IP也可以。 这种方式还没测过，感兴趣可以参考官方说明的看一下 https://nginx.org/r/proxy_bind Nginx本身能将Websocket数据转化成TCP数据吗只使用Nginx是做不到的，但是搭配Websockify就可以做到WSS（WebSocket Secure）到 TCP 的转发 安装 Nginx：确保你的系统上已经安装了 Nginx。你可以使用系统包管理器或从 Nginx 官方网站下载并安装 安装 Websockify：安装 Websockify，可以使用 pip 执行以下命令： 1pip install websockify 创建 Websockify 启动脚本：创建一个用于启动 Websockify 的脚本，例如 start_websockify.sh。脚本内容可能如下所示： 12#!/bin/bashwebsockify --web /path/to/webroot 1234 localhost:5678 这里 1234 是用于 WebSocket 连接的端口，localhost:5678 是实际 TCP 服务的地址 配置 Nginx：修改 Nginx 配置文件，将 WSS 请求转发到 Websockify 启动脚本。示例配置如下： 123456789101112131415server &#123; listen 443 ssl; server_name your_domain.com; ssl_certificate /root/ca/server.crt; ssl_certificate_key /root/ca/server.key; location / &#123; proxy_pass http://localhost:1234; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "upgrade"; proxy_set_header Host $host; &#125;&#125; 这里的 listen 443 表示监听 HTTPS 请求，proxy_pass http://localhost:1234 将请求代理到 Websockify 启动脚本 启动服务：启动 Websockify 服务： 12chmod +x start_websockify.sh./start_websockify.sh 启动 Nginx 服务： 1systemctl start nginx 测试：使用支持 WebSocket 的客户端连接到 WSS 地址，例如 wss://your_domain.com，并验证是否成功将 WebSocket 请求转发到 TCP 服务 总结 nginx配置自建SSL证书，只需要修改nginx配置文件，在端口后配置添加 ssl 并指定证书和私钥路径即可 nginx上配置SSL证书可以将证书统一管理，减轻应用服务器加密解密的负担，专注于业务逻辑开发 nginx不仅支持http协议转发，还支持https、tcp、udp、websocket等协议的转发 nginx转发tcp协议时会收到端口号个数限制，理论上限6万，通过proxy_bind可以突破上限 nginx搭配websockify可以做到WSS 到 TCP 的转发 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 寒风终究是刮到我这里了]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>Nginx</tag>
        <tag>nginx ssl</tag>
        <tag>https</tag>
        <tag>websockify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怎么验证公钥和私钥是一对]]></title>
    <url>%2Fblog%2F2023%2F12%2F05%2F%E6%80%8E%E4%B9%88%E9%AA%8C%E8%AF%81%E5%85%AC%E9%92%A5%E5%92%8C%E7%A7%81%E9%92%A5%E6%98%AF%E4%B8%80%E5%AF%B9%2F</url>
    <content type="text"><![CDATA[前言公钥和私钥都是一串字符，长得也没有什么关联性，那么当拿到一个公钥和一个私钥后怎么验证它们两个是一对呢？先说说为什么会有这个疑问，最近在对接一些SDK时经常需要做签名和验证签名的工作，双方要相互提供公钥来完成后续的身份验证，测试过程中生成了多个密钥对，当时就在想如果我用错了怎样才能发现呢？有没有什么方法可以验证公钥和私钥是一对呢？再寻找方法的过程中发现 ssh-keygen 和 openssl 生成的密钥对差别居然这么大，但后来又发现他们居然还有联系，所以本文将刚刚提到的问题都总结一下，方便日后查阅。 解决这个问题的思路我么可以从密钥使用的方式入手，私钥加密公钥解密可以验证身份，公钥加密私钥解密可以秘密传递数据，也就是说公钥和私钥是配合工作的，如果我们用私钥加密一个数据，公钥可以解开就说明他们两个是一对的的。 其实还有一个更简单的办法，公钥是可以从私钥中提取出来的，反之则不行。在公钥密码学中，私钥和公钥之间的关系是一种数学上的单向函数。这种函数的特性使得从私钥生成公钥是可行的，而从公钥还原私钥是非常困难的，这被称为一个单向函数或单向映射，因此被认为是安全的。 有了这个前提我们就可以这样做，从私钥中提取出公钥，然后和要比对的公钥进行比较，如果一致就说明原来的公钥和私钥是一对啦。 ssh-keygen 和 opensslssh-keygen 和 openssl 是两个不同的工具，它们分别用于生成和处理不同类型的密钥和证书。以下是它们的一些主要区别： 功能和用途： ssh-keygen：主要用于生成和管理SSH密钥对，包括RSA、DSA、ECDSA和Ed25519密钥。 openssl：是一个通用的密码学工具，可以用于生成和管理多种密码学对象，包括SSL/TLS证书、密钥、摘要和数字签名等。 支持的算法： ssh-keygen：专注于SSH密钥生成，支持多种SSH密钥算法。 openssl：支持广泛的密码学算法，包括RSA、DSA、ECDSA、Diffie-Hellman、AES、SHA等，用于生成和操作各种密码学对象。 密钥格式： ssh-keygen：生成的SSH密钥通常以OpenSSH格式存储。 openssl：支持多种密钥格式，例如PEM、DER等，可以处理不同类型的密钥和证书。 具体应用场景： ssh-keygen：主要用于SSH连接，生成用于身份验证的密钥对。 openssl：更广泛地用于TLS/SSL证书、数字签名、加密和其他与通用密码学相关的应用。 具体命令和用法： ssh-keygen：用于生成SSH密钥对的常见命令包括： 1ssh-keygen -t rsa -b 2048 -f myid_rsa openssl：用于生成RSA密钥对的常见命令包括： 12openssl genpkey -algorithm RSA -out private-key.pemopenssl rsa -pubout -in private-key.pem -out public-key.pem 以上的ChatGPT提供的内容，还是对比的挺全面的，从 openssl 生成密钥对的过程可以看出，确实可以从私钥中提取出公钥。 ssh-keygen 是我接触的最多的生成密钥对的命令，不管是github还是gitlab，或者一些其他的托管平台都会提到SSH Keys，所以每次我都会用 ssh-keygen 来生成，它是用于生成SSH连接密钥对最直接的选择，如果是想要使用更广泛的密码学操作，包括证书生成、签名和加密等，那么 openssl 提供了更大的灵活性。 两种证书的内容对比前面说了两种命令生成的证书、格式、算法和用途都有区别，我们来看看利用上面的命令生成的证书内容有什么不同，先看 ssh-keygen 生成的私钥 myid_rsa 和公钥 myid_rsa.pub 123456789101112131415161718192021# demo @ demo-ThinkPad-X390 in ~/tmp/rsa [17:58:47]$ ssh-keygen -t rsa -b 2048 -f myid_rsaGenerating public/private rsa key pair.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in myid_rsaYour public key has been saved in myid_rsa.pubThe key fingerprint is:SHA256:eLzG9lryO/AOdd05cDkixvBNCPFZRLRu0wp3bxZ7Vnk demo@demo-ThinkPad-X390The key's randomart image is:+---[RSA 2048]----+| +o =* || =.= . .|| B = + || o . o.=.+|| . S ...=.BE|| o.o .+ + B|| Bo. . .*|| o *o +.|| .o=o |+----[SHA256]-----+ 1234567891011121314151617181920212223242526272829# demo @ demo-ThinkPad-X390 in ~/tmp/rsa [19:22:03]$ cat myid_rsa-----BEGIN OPENSSH PRIVATE KEY-----b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABFwAAAAdzc2gtcnNhAAAAAwEAAQAAAQEAr7gEjCdhA4at0tx0efpLunEOEGvK3ejJCh/dBwLfsvVJelwTeJjKzI2nFvYGkJgndhWLuvwy6+ODnvASeNvXSvZZzXvvXjTF4JP3TRbATayFvS9mZgzOnxh/rUz4P7ecLBfqZZkem+VBrMlv37yaIWf7co59dtrJJuAfZJFXyW//LQhNoEaqbZ/gvZls5pOXr2foMnYg2HV0ayPlvz8cmVF8LIOqaGOc28nMQakVwO+nZhbSv1bGprIGcy4czc7UOp/m+ot6mWJflpq3NBGiImyfaT2kxp1+pIdd6ISqoZqa9C2JzH7ZXVGjevbHuqup871yjUGapxDxlDk9HjdSLwAAA9AlxLUDJcS1AwAAAAdzc2gtcnNhAAABAQCvuASMJ2EDhq3S3HR5+ku6cQ4Qa8rd6MkKH90HAt+y9Ul6XBN4mMrMjacW9gaQmCd2FYu6/DLr44Oe8BJ429dK9lnNe+9eNMXgk/dNFsBNrIW9L2ZmDM6fGH+tTPg/t5wsF+plmR6b5UGsyW/fvJohZ/tyjn122skm4B9kkVfJb/8tCE2gRqptn+C9mWzmk5evZ+gydiDYdXRrI+W/PxyZUXwsg6poY5zbycxBqRXA76dmFtK/VsamsgZzLhzNztQ6n+b6i3qZYl+Wmrc0EaIibJ9pPaTGnX6kh13ohKqhmpr0LYnMftldUaN69se6q6nzvXKNQZqnEPGUOT0eN1IvAAAAAwEAAQAAAP8nH9HjYuE258XMLUnB3vj9ii6XeWiyACLDHlUYOouUTQkPNXEgaOS+psuNUTok97vRjvqNp5mu6/liahGy10AYcMGN/wpM73MfjdKCV455vPAKL4OdNTzML6g97eVYSol7R9foxy8JhwIYdsFyD4xB4+UYnKFS1WP0Q/K5caMaNnazPF0X7JDSenEwBwl1bDAN5OKwZNzSzUpR/96fl1DAgnG9yp1/634WGtIfjXLm13OSMrSHfwazNHoqAPkD0SQVJrm38ppvqArU0tBVfh/kvBkv7CCAE9mzhxusiKjuB0c/jrsRDu45AJX5wuqIWcZdfppATJjK10S20MgN5dkAAACBALYym7mpDVQJ2gedSjv/Pva+O/5b/20jawc3RFZdJAUWSGJTe9UqyHBHPlYj3/bHJVWIQ9S7s5AxpBtAEOsjeQf0VGFaFALQwcUghx5r1uM5sAL06CT/5t3Ze8lcJozNW7yjbVZKjJK13GTDMrnPYTfPOc+I/6N3R+SV2iKOwUQPAAAAgQDp9/2WvHOX2GKXan2BvMaexj6GYIQa8xEsqhtQGY9zxQ2d+9Z7YgDfXnNanxv8XJ61komeP9KmhS+2TCt0trj8sXtQobDsDGjFcDx0jjBobU6zKvO4htoZP3lb8wZARgQoEANxqNOo8R61CsDQW4eM0ReAitKZ0GCcXbqNYCRTGwAAAIEAwEPdhAH4iH6V7IJnQs3pgf9AG9UubDe9mAwVNIircWTFO2EFaBpwP7UKgpNmmZjoD2lWEcr2Zxv6x8ILBlJqeavCpGfLo0hwt2XZHgLRsTx8RmsbmYI2l8qnkMWyz5Lh3XAhNMZM4s9OvSJM6JPWYcedG5KYG1/zbarjwkq+Gn0AAAAVc2h6QHNoei1UaGlua1BhZC1YMzkwAQIDBAUG-----END OPENSSH PRIVATE KEY----- 123# demo @ demo-ThinkPad-X390 in ~/tmp/rsa [19:22:10]$ cat myid_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCvuASMJ2EDhq3S3HR5+ku6cQ4Qa8rd6MkKH90HAt+y9Ul6XBN4mMrMjacW9gaQmCd2FYu6/DLr44Oe8BJ429dK9lnNe+9eNMXgk/dNFsBNrIW9L2ZmDM6fGH+tTPg/t5wsF+plmR6b5UGsyW/fvJohZ/tyjn122skm4B9kkVfJb/8tCE2gRqptn+C9mWzmk5evZ+gydiDYdXRrI+W/PxyZUXwsg6poY5zbycxBqRXA76dmFtK/VsamsgZzLhzNztQ6n+b6i3qZYl+Wmrc0EaIibJ9pPaTGnX6kh13ohKqhmpr0LYnMftldUaN69se6q6nzvXKNQZqnEPGUOT0eN1Iv demo@demo-ThinkPad-X390 接着看看 openssl 生成的私钥 private-key.pem 和公钥 public-key.pem 12345678# demo @ demo-ThinkPad-X390 in ~/tmp/rsa [19:23:01]$ openssl genpkey -algorithm RSA -out private-key.pem......................................................................+++++..................+++++# demo @ demo-ThinkPad-X390 in ~/tmp/rsa [19:25:04]$ openssl rsa -pubout -in private-key.pem -out public-key.pemwriting RSA key 123456789101112131415161718192021222324252627282930# demo @ demo-ThinkPad-X390 in ~/tmp/rsa [19:26:18]$ cat private-key.pem-----BEGIN PRIVATE KEY-----MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDjxEM9ypyLb0fTxk7Ujdvj7efUjbLhzAcbfHNRZqzqif1rlTS4jz+tIDZBX9/HJkDcoz1dIolp/53SCJtz+qBrkHMzsi6dcqd5jXUfT3Ts/FMTVsAKIkwafnw2Gbi2DGQx8WQ8Sn5+LlUPFSCJa+2MTc21taiMP7CtF7iG6jFeGna3xeRTWAjMMS2xk9Zk37KMz8weU4ShdQ9qmX7yuKHuZzUiWHmjbW/bi6abur3dfi/1jRSnFQF+r/gJoH1UXT4st/+fW425nWqOohSV+xw61a09fjECfjVDkMXtwiPCbbGNjD8WjLbilOJK3uBx5e7GIN1jZjVBqlH9NqEk12HVAgMBAAECggEAIlcf8DwRGDDB+psGnaN5yMbA0lo+4y5Yz/sWg+pgHciKTeMkHMN0TUzzAS5MVgFdfCachONFJpckm1sy3OynrTANPnkZ8CEuQPRj+UGIloaAnATTVXPQV5pEbf+4yrHZlXIYs8JjifDAYFLGIhIEsce1QTVPYT3lDGPsLWM36ED6XVnvJYsr00wpLlYy4uWikt4Q8ruKDnWSS8xeE7pj0bBaqYd8Gz4jfrJ94IgNIROQxnXb8aMG/o441ImrF6YXKn8jR/gjZXT1L4ZsD7fGTvLKBxxImBXL2eIoZ8usmTtXeLEfYaJlzyXOcnYKKVtMQxRI+4ANJWWnMDgM8NTBTQKBgQD0hboW//hXGVcng3PuNfD7hUPHQKWWUHbchjRfxvxy/4itqGaWHtUrmc4PTXUxW71YaVA6qm2gvfV+jn83o5k009vVbXYVvztNQq9wC0Fj2OYwQkxHMgAY9JdhZr6ghuEZ0+hNd5Jea9K3w62CcY0mHqlvo55szhWmhuvzFtSUXwKBgQDudTFUjf7+qPsfYEX14udsKtfowrjepFLfnQAbxj16PxpRBSV7S77gdlW1shKEreIPbm083bhU3ypCRs7ktR0Wki/uoQGYekHrb8Nljh+rM9DxY83+HfLHAC/RjP8ZFrWJFgo6Whfr3jhlb8j3a4siLvqMj0d35BRvM+HtsEhWSwKBgQC7M746aZG5LP9akvsofkeWaHpxi366S7QUNPLX+MHBUP1U5j5T5BsTnFbzxDtapH+btlTj9SKlE4llcLbY4/khRqlhBsKG97bw+YLS5usTBDWG8NrxDsigUnGq8jwzLdAcM+3/kPGQNEBIzBiEa2bCrmrNu5hIeKZ+rYDqxKnPRQKBgQChMZuNOwfbAkPvq9bfJuml1zho4eiyqIZVHv6Jx/KbBrk1d7Mkq8l2VsgNsTiPdDPSlqVFdeQ7izg34UHOnRfywBOizjgT6OFS1BRUPETwKHDgHMWNOJV7LIcmspdnUFCbQMjdNTXifi/FclF537sH+pXoGpsHUqj1my7ozDe6DQKBgQDY9tJvXx5DRPVH2t600rl1df3PogoAhwjvK+Q/Y5vLyrL470JxMNPtW546aWXRL6INTg+OvsUqLJXEMlxlHdHIe16orIGBrGcqHNGfkQPfddJVOo0eQCCk4jaLL9EP7+euA83vp6cP38rqgsL70+gt34iMPCP9dEKaJyjZPv+vFQ==-----END PRIVATE KEY----- 1234567891011# demo @ demo-ThinkPad-X390 in ~/tmp/rsa [19:26:21]$ cat public-key.pem-----BEGIN PUBLIC KEY-----MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA48RDPcqci29H08ZO1I3b4+3n1I2y4cwHG3xzUWas6on9a5U0uI8/rSA2QV/fxyZA3KM9XSKJaf+d0gibc/qga5BzM7IunXKneY11H0907PxTE1bACiJMGn58Nhm4tgxkMfFkPEp+fi5VDxUgiWvtjE3NtbWojD+wrRe4huoxXhp2t8XkU1gIzDEtsZPWZN+yjM/MHlOEoXUPapl+8rih7mc1Ilh5o21v24umm7q93X4v9Y0UpxUBfq/4CaB9VF0+LLf/n1uNuZ1qjqIUlfscOtWtPX4xAn41Q5DF7cIjwm2xjYw/Foy24pTiSt7gceXuxiDdY2Y1QapR/TahJNdh1QIDAQAB-----END PUBLIC KEY----- 从私钥提取公钥使用 openssl 命令从私钥提取公钥，上面已经展示过程了，它的公钥就是这样生成的 1openssl rsa -pubout -in private-key.pem -out public-key.pem 使用 ssh-keygen 命令从私钥提取公钥可以这样做 1ssh-keygen -y -f myid_rsa &gt; compare_myid_rsa.pub 结果文件 compare_myid_rsa.pub 和 myid_rsa.pub 的内容一模一样，所以从私钥提取公钥可以作为检验密钥是否匹配的方法 OPENSSH和PEM两种密钥格式相互转化之前说了ssh-keygen 和 openssl两个命令及生成的密钥区别，现在讲讲两种密钥的联系，OpenSSH 和 OpenSSL 使用的密钥对虽然有一些格式上的差异，但可以相互转换。OpenSSH 使用自己的私钥和公钥格式, 而 OpenSSL 通常使用 PEM 格式，可以通过下列命令转化 OpenSSH 格式私钥转换成 OpenSSL PEM 格式私钥1ssh-keygen -p -N "" -f myid_rsa -m PEM $ file myid_rsamyid_rsa: PEM RSA private key OpenSSL PEM 格式私钥转换成 OpenSSH 格式私钥1ssh-keygen -p -N "" -f myid_rsa $ file myid_rsamyid_rsa: OpenSSH private key OpenSSH 格式公钥生成 OpenSSL PEM 格式公钥1ssh-keygen -e -m PEM -f myid_rsa.pub &gt; new-public-key.pem $ file new-public-key.pemnew-public-key.pem: ASCII text OpenSSL PEM 格式公钥生成 OpenSSH 格式公钥1ssh-keygen -i -m PKCS8 -f new-public-key.pem &gt; compare_myid_rsa.pub $ file compare_myid_rsa.pubcompare_myid_rsa.pub: OpenSSH RSA public key 公钥和私钥的匹配使用匹配使用公钥和私钥之前先准备一个测试文件 1echo "Hello World" &gt; data.txt 加密和解密使用公钥加密 1openssl rsautl -in data.txt -out data_enc.txt -inkey public-key.pem -pubin -encrypt 使用私钥解密 1openssl rsautl -in data_enc.txt -out data_dec.txt -inkey private-key.pem -decrypt 两个命令执行后都没有任何控制台输出，文件 data_enc.txt 中是加密后的内容，文件 data_dec.txt 是解密后的内容，成功解密后内容与 data.txt 文件内容一样 签名和验证使用私钥签名 1openssl dgst -sha256 -sign private-key.pem -out signature.bin data.txt 使用公钥验证签名 1openssl dgst -sha256 -verify public-key.pem -signature signature.bin data.txt 验证通过后，会在命令行输出 “Verified OK” 字样，若不通过则输出 “Verification Failure” 字样 总结 验证公钥私钥是否匹配的最便捷的方法是通过私钥提取公钥来和原公钥进行对比 公钥加密私钥解密，私钥签名公钥验证签名，需要把公钥发给别人，私钥自己要保留好 公钥和私钥常用的有OpenSSH和PEM两种格式，这两种格式的密钥可以相互转化 ssh-keygen 和 openssl 是两个不同的工具，前者用于管理SSH密钥更具体，后者使用范围更广更灵活 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 事情一件件去办就好了，『没什么大不了的』，希望你长大后还可以经常乐观的说这句话，一副俾睨天下的姿态]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>openssl</tag>
        <tag>ssh-keygen</tag>
        <tag>pem</tag>
        <tag>公钥</tag>
        <tag>私钥</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自建CA并生成自签名SSL证书]]></title>
    <url>%2Fblog%2F2023%2F11%2F30%2F%E8%87%AA%E5%BB%BACA%E5%B9%B6%E7%94%9F%E6%88%90%E8%87%AA%E7%AD%BE%E5%90%8DSSL%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[前言这是加密与认证系列的第五篇文章了，本来我是想把自建证书和nginx配置https访问总结到一起的，但是在实际操作的过程中我发现了很多细小的知识点，有些还是挺有意思的，这是一个不断自我提问不断寻求答案的过程，随着扩展的内容越来越多，我决定这篇只写自建CA和签名SSL证书这部分，至于nginx配置https访问放到后面再写吧。 相信点进这篇文章的人应该知道为什么要自建CA和自签名SSL证书了，因为买现成的SSL证书挺贵的，SSL证书通常有三种类型：域名级（DV）、企业级（OV）、增强级（EV），价格从每年几百元到上万元不等，再细分的话还有单域名证书、通配符证书、多域名证书等等，有些证书还可以追加域名。 这笔费用对于大厂来说可能不算什么，但是对于小产品来说，即使选择最便宜的证书也是一笔开销，比如在上一篇《根证书的应用和信任基础》提到的12306官网在2017以前使用的也是用的自签名证书，一般正式产品总会咬咬牙买个证书，但是如果是本地测试，或者局域内网的使用的产品使用自建的证书就足够了，相当于我们配了临时证书资源在开发新功能，等到真正对外发布时再替换成购买的证书也来的及，所以接下来我们就一起走一遍自建证书的流程。 一键生成自签名证书总有心急的人想吃热豆腐，所以我把用到的命令写了一个脚本，只要输入几个自定义密码就可以完成CA证书和SSL证书的创建，前提是在你的电脑安装了openssl命令，在ubuntu上系统上默认就有，没有的自己安装一下吧。 命令脚本将下列命令放到shell脚本文件onekeyssl中执行即可 12345678910111213141516171819202122#!/bin/bashread -p "Enter your domain or ip [www.example.com/10.10.49.172]: " INPUTecho "1. Create ca private key..."openssl genrsa -des3 -out selfca.key 2048echo "2. Create ca root certificate..."openssl req -new -x509 -days 3650 -key selfca.key -subj "/C=CN/ST=BJ/L=BJ/O=MyRootCA/OU=MyCA/CN=CA" -out selfca.crtecho "3. Create server key and certificate signing request..."openssl req -newkey rsa:2048 -nodes -keyout server.key -subj "/C=CN/ST=BJ/L=BJ/O=MyRootServer/OU=MyServer/CN=$INPUT" -out server.csrecho "4. Sign SSL certificate..."openssl x509 -req -extfile &lt;(printf "subjectAltName=IP:$INPUT") -days 3650 -in server.csr -CA selfca.crt -CAkey selfca.key -CAcreateserial -out server.crtecho "5. Create end, next work..."echo "Copy server.crt and server.key to your server machine" 执行结果123456789101112131415161718192021222324$ ./onekeyssl.shEnter your domain or ip [www.example.com/10.10.49.172]: 10.10.19.11. Create ca private key...Generating RSA private key, 2048 bit long modulus (2 primes)..................................................+++++.....................................+++++e is 65537 (0x010001)Enter pass phrase for selfca.key:【输入自定义密码】Verifying - Enter pass phrase for selfca.key:【重复密码】2. Create ca root certificate...Enter pass phrase for selfca.key:【重复密码】3. Create server key and certificate signing request...Generating a RSA private key.............................................................+++++..........+++++writing new private key to 'server.key'-----4. Sign SSL certificate...Signature oksubject=C = CN, ST = BJ, L = BJ, O = MyRootServer, OU = MyServer, CN = 10.10.19.1Getting CA Private KeyEnter pass phrase for selfca.key:【重复密码】5. Create end, next work...Copy server.crt and server.key to your server machine 分步来看看自建证书的过程整个过程分为自建CA和自签名SSL证书两部分： 自建CA：并不是要你自己搭建一个CA中心，这里的CA其实指的是创建自己的CA根证书，这样可以给其他人签署证书，但是这个CA根证书是你自己创建的，没有得到互联网的承认，也不会被正规CA认可，所以不具备通用和有效性，一般可以在内部网络使用。 自签名SSL证书：一般来说，我们的证书是要发给权威机构CA进行验证签署的，但是自签证书，就是自己给自己签署生成一份CA证书，或者用自建的CA根证书来签发的SSL证书，同样不具备互联网的通用和有效性，一般只用于测试环境或内部网络。 自建CA根证书自建CA根证书也是分成两步 生成CA私钥1openssl genrsa -des3 -out selfca.key 2048 这条命令使用 OpenSSL 工具生成一个带有Triple-DES（3DES）加密的密码保护的2048位RSA私钥文件，各个参数的含义如下： openssl: OpenSSL 工具的命令行执行器。 genrsa: 生成RSA密钥的命令。 -des3: 使用Triple-DES算法对生成的私钥进行加密，这会在生成私钥时要求你设置一个密码，以便在每次使用私钥时都需要提供密码。 -out selfca.key: 指定生成的私钥的输出文件名为 selfca.key，私钥文件将被保存在当前工作目录中。 2048: 指定生成的RSA私钥的位数为2048位，这是一种常见的安全密钥长度。 其中密码这一项在生成CA证书时我们是想加的，运行命令后需要输入一个自定义密码两次，以后每次使用这个私钥都要输入密码，但是在很多文章中你会发现他们有一步是要删除密码，原因是在一些自动化部署场景中，去掉私钥密码可以避免手动输入密码，使整个过程更加自动化，比如配置到Apache或者Nginx中时不必每次启动时输入密码，所以我们在后面生成服务器私钥时就不使用密码了。 带有密码的私钥内容 —–BEGIN RSA PRIVATE KEY—–Proc-Type: 4,ENCRYPTEDDEK-Info: DES-EDE3-CBC,2D6A221FF66727E0 vEzhQnjsKVBLw6tPNo6Dx7D8CyzhZdZYgfxuZBYP30cLWOORTpsS1q0txNMCaWoyS79k0+qTgENne2oirALiCnPB5Rzf3vO6gv/KSScvEnGrZ6Q57i3xObhLYzCtKOAgE6wSBBfbeC6cXczAFZ2ehnM42+Cv/9BX829X7BnbylNbaV1VXQTaEsmo+uWwx9dZ…C2udyFqENooTB7n1Qbtm3Fsruwgk/0IM3vSRk41/EWADabWs4tR/uXQmCPyxouHIxhT3U4EeabyY8dyjSRCkzKPIEFl2HwnYQHZcDVUPD52uHEvA5M7c1QNgX2VmTXzPAgSijAZHDrh6QWa+R9eqUVQShY4mAN7c1sv1or4ZckV7jQTbIBjUGcjhX1TooxX0/RE/GLodSqD8wCkxjgD7uxy93oOLuV/9iDDsrI7VMrs5jlKhKEuPozc+Y+hERRBmUUMbAE2JBW+jD/JzkmXL8w4AGU8wHeRI/FqKQXLbP6v3h+Yb4zP/aOVZ5mdWlHWT1+BykB3qgKUqcn+FmRbodvK8C1G1opDchyomCToHzCGTDqAAcRPoNiB5z3jB+yPCM0m83wI4rWYPghWL4hT7aZgI8l2xwTJfJfyJ+/6MfBZgh/qa4t703A==—–END RSA PRIVATE KEY—– 不带密码的私钥内容 —–BEGIN RSA PRIVATE KEY—–MIIEpQIBAAKCAQEA6OCPgi1pUWdkS9DdR2mk6QJsE9i6rCgaDuk+xyi8Sdxp2u8rf81ZrK4xUNUNTX8+lnj5WeolJ/Hk1o9I659oPkbWuw7yyuCBFbZ9m3goZCt2w+lccsLw6o6XyGTUiptcgB/GmGcd/ua3REAt3l6uYn32vjeit5oX5xsmsUKbwpIH/B81nMohd/t6m9c0h2mcVnUYDmUsV+hmgdASvkTSmvqHOUwV1qX/pQNgjR+auLiezozaLmVkvtSI9/tX6sqtlyROn7ZFsUHJbYuyfOekqLCiY5Wo5ocSTqAd4n/JYmjA6anIYGDXbLcP+075ZgkHSoR9ab4uFtghStx99QYwywIDAQABAoIBAQDWeWryE2y5wiVH…SdwHzniXCpBpNYB6XoV57bPpQiSCqVyT9Owd0A9csZf4905dOZg+/25K2TFmv7gGfHN/4EkCgYEAz7VrytExyCm1B+7sFln7c1hjy5wzNaW2tYi08szvErJkpe3jz9HHMLbTn+DrXZu76nsiXfQbTl7SpPQJptHctHx7K+9mykDaJNGDDNmJRKdwi8cfINrDyYPab+aojwU1FZtF8EEXKjnzWIvmM8FO71ej+COUOmDxWCsDbpoHyhA=—–END RSA PRIVATE KEY—– 生成CA自签名证书1openssl req -new -x509 -days 3650 -key selfca.key -subj "/C=CN/ST=BJ/L=BJ/O=MyRootCA/OU=MyCA/CN=CA" -out selfca.crt 这条命令使用 OpenSSL 工具生成自签名根证书（Root Certificate），各个参数的含义如下： openssl req： 这是 OpenSSL 工具中用于处理证书签署请求（CSR）的命令。 -new： 表示创建新的 CSR。 -x509： 表示生成自签名的 X.509 证书，而不是生成 CSR。 -days 3650： 设置证书的有效期为 3650 天（10 年）。 -key selfca.key： 指定用于生成证书的私钥文件为 selfca.key。 -subj &quot;/C=CN/ST=BJ/L=BJ/O=MyRootCA/OU=MyCA/CN=CA&quot;： 设置证书主题（Subject）的信息。这里使用了简化的 Distinguished Name (DN)，包括了国家（C=CN）、省/州（ST=BJ）、城市（L=BJ）、组织（O=MyRootCA）、组织单位（OU=MyCA）、通用名称（CN=CA）等信息。 -out selfca.crt： 指定生成的证书文件的输出路径和文件名，这里为 selfca.crt。 至此我们就生成了一个自签名的CA根证书，如果把它加入到操作系统或浏览器的信任列表中，那么之后由他签发的SSL证书都可以被信任了，接下来我们开始用它来签发SSL证书。 自签名SSL证书这个过程主要包括生成服务器密钥、构建签名请求和用CA签名证书三部分，其中前两步可以合并为一步：]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>https</tag>
        <tag>openssl</tag>
        <tag>CA根证书</tag>
        <tag>自签名SSL证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[究竟什么是阻塞与非阻塞、同步与异步]]></title>
    <url>%2Fblog%2F2023%2F11%2F24%2F%E7%A9%B6%E7%AB%9F%E4%BB%80%E4%B9%88%E6%98%AF%E9%98%BB%E5%A1%9E%E4%B8%8E%E9%9D%9E%E9%98%BB%E5%A1%9E%E3%80%81%E5%90%8C%E6%AD%A5%E4%B8%8E%E5%BC%82%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[前言这几个名词在程序开发时经常听到，但是突然问起来各个词的含义一时间还真是说不清楚，貌似这几个词都是翻译过来的，每个人的解释都不太一样，我对这几个词的理解也不是一成不变的，随着开发经验的积累，渐渐有了自己的记忆方式，所以总结一下，不一定准确，有问题可以一起聊一聊。 先说说我的结论：阻塞与非阻塞是指等待执行结果时的状态，同步与异步是指获取执行结果的方式，读起来有点绕口，听起来也迷迷糊糊的，没关系，我们用具体的例子来说明应该就容易理解了。 阻塞与非阻塞先说说『阻塞与非阻塞是指等待执行结果时的状态』这一句，是说在执行某个操作或者某个函数时，在没有拿到我们想要的结果时，我们的状态是怎样的，如果是一直等就是【阻塞】的，如果发现没有结果就去做别的事情了就是【非阻塞】的。 以常见的网络IO为例，服务器对客户端连接的socket调用read函数，试图获取客户端发送的请求数据，但是客户端并不总是有数据发送过来，所以想要获得数据我可以采用【阻塞】方式一直等，也可以采用【非阻塞】方式，在发现此时没有数据时就先去干别的事，一会再来看看。 同步与异步再来说说『同步与异步是指获取执行结果的方式』这一句，以游戏中的常见升级发奖为例，可以主动调用升级函数，在执行完成后返回升级的结果，然后根据结果来发奖励，也就是【同步】写法，也可以注册一个监听等级变化的回调函数，注册完我就不管了，当升级时会将升级的结果通过回调函数传回来，这就是【异步】处理方式。 复杂的网络IO为什么同步和异步没有用网络IO来举例呢？因为网络IO这里的情况更加复杂，虽然你注册了回调函数，但它很可能是个同步IO，究竟怎么回事，一起来看看。 我们知道要想从IO读取数据，需要经历「内核数据准备好」和「数据从内核态拷贝到用户态」两个过程，还是以read函数为例，如果设置为阻塞模式，相当于read函数等待了「内核数据准备好」和「数据从内核态拷贝到用户态」两个过程，然后取到IO数据，如果设置成非阻塞模式，当内核数据没准备好会直接返回，也就是不会等待第一个过程，但是当数据准备好时，会直接等待第二个过程完成后，将结果数据返回。 所以无论是否阻塞，我们都等待了第二个阶段，等着它执行完成后获取结果，所以这两种都是同步IO。 那作为IO多路复用里的“一哥”epoll也是同步IO吗？是的！那封装了select/poll/epoll的libevent可是用了Reactor模式，支持事件回调，它也是同步IO吗？是的！ 真正的异步IO有点惊呆了不是吗？那究竟什么是异步IO呢？还真有！Windows 里实现了一套完整的支持 socket 的异步编程接口 IOCP，而 Linux 是在 2019 年 5.1 版本 内核首次引入的高性能异步I/O 框架 io_uring，我确实都没用过，感兴趣的可以试一下 是否是异步IO就看「数据从内核态拷贝到用户态」这个过程需不需要等待，如果需要逻辑层自己等待这个过程取数据就是同步IO，如果这个过程都不用等，调用回调函数时已经把内核态的数据拷贝出来，并且通过回调将数据进行了回传，这就是异步IO。 IO分类与示例所以总结下来一共有这么几种：同步阻塞IO，同步非阻塞IO，异步IO，为啥不区分异步阻塞IO和异步非阻塞IO呢？你在阻塞时搞个异步试试，办不到吧，所以异步只能与非阻塞搭配，也就习惯只写异步IO了。 一顿理论讲下来可能还是比较抽象，那我们再举个日常生活中的例子，比如中午买饭的过程： 同步阻塞IO就好像，你去食堂吃面条，但是你去这一锅面条还没煮好，然后你就一直在那里等啊等，等了一段时间终于做好了（数据准备的过程），但是你还得继续等工作人员把面条（内核空间）打到你的餐盘里（用户空间），才能找个桌子开始吃饭。 同步非阻塞IO就好像，你又去食堂吃饭，问大叔饭做好了没有，告诉你没有你就离开了，过了一会，你又来饭堂问大叔饭做好了吗，人家说说做好了，于是你等着把饭打到你的餐盘里，后面这个过程你是得等待的。 异步IO就好像，你在十分焦急的写BUG，这时到饭点肚子饿了，给食堂大叔打电话，等饭做好了麻烦给我送一份，等到饭好了真的送来直接就能吃了，一直在抓紧写BUG中间没有等待（做梦中） 我想大部分同学吃午饭都是第一种同步阻塞IO吧，第二种同步非阻塞IO可能也有，但是不是要重新排队啊，如果是第三种异步IO的情况，我只能说大哥/姐，我跟你混了~~ 总结 IO分为同步阻塞IO，同步非阻塞IO，异步IO三类 异步IO有Windows平台的 IOCP 和 Linux 平台的 io_uring 从IO读取数据，需要经历「内核数据准备好」和「数据从内核态拷贝到用户态」两个过程 分析阻塞和非阻塞看是否等待第一个过程，分析同步与异步看是否等待第二个过程 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生的岔路口，不知是机会还是风险，边走边看吧，毕竟路还是在脚下，空想也到不了终点~]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>服务器</tag>
        <tag>同步与异步</tag>
        <tag>阻塞与非阻塞</tag>
        <tag>异步IO</tag>
        <tag>Reactor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git diff对比差异时指定或排除特定的文件和目录]]></title>
    <url>%2Fblog%2F2023%2F10%2F31%2Fgit-diff%E5%AF%B9%E6%AF%94%E5%B7%AE%E5%BC%82%E6%97%B6%E6%8C%87%E5%AE%9A%E6%88%96%E6%8E%92%E9%99%A4%E7%89%B9%E5%AE%9A%E7%9A%84%E6%96%87%E4%BB%B6%E5%92%8C%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言 你一般什么时候会用GPT? 居然会有这种话题，答案就是作为程序员的我天天在用，虽然GPT有个胡说八道的毛病，但试试总没错的，就比如今天题目中这个问题，我也是先查了GPT没好使，但给我提供了查询思路，最后在Stack Overflow上找到解决办法，记录一下 git diff指定或者排除文件我就直接列答案了，没什么可说的，工具参数记住就好 指定文件和目录1git diff dev..feature --stat -- ':*.pb.go' ':*.pb.h' ':*.pb.cc' 以上语句表示对比dev和feature分支差异的文件，但是只选择后缀为以上三种的文件，如果是特定的目录，将冒号后的内容替换成目录即可，-- &#39;:src/thirdparty&#39; 排除文件和目录1git diff dev..feature --stat -- ':!*.pb.go' ':!*.pb.h' ':!*.pb.cc' 在上面的基础上加个取反的感叹号就行，如果是排除目录就在 !直接写目录就可以了，这种排除的写法还有另一种形式，用(exclude)代替感叹号，其中还可以写其他属性，感兴趣可以自己扩展查询一下： 1git diff dev..feature --stat -- ':(exclude)*.pb.go' ':(exclude)*.pb.h' ':(exclude)*.pb.cc' 番外篇今天在某乎上看到一个提问《C++有多难？》，看着大佬们口若悬河、引经据典的在诉说、在吐槽、在愤慨，却引起了我的回忆，我为什么选择C++？ 只因为在人群中多看了一眼吧，有先入为主的因素，也有水到渠成的必然。 我最先接触的一门编程语言是C语言，对于刚刚接触编程的我来说，它是那么令人着迷，虽然开发环境是寒酸可怜的Turbo C，但是丝毫没有影响我的学习热情，刚开始打印出一个杨辉三角都开心的不得了，教科书上的每个例子我都敲了很多遍，虽然那古老的教材现在被大佬们批判的不行，但确实领我在代码的世界里入了门。 后来随着学习的推进，又逐步接触了Java、C#等高级语言，也接触了汇编、单片机等偏底层的领域，为什么没有选Java、C#而一直坚守C语言呢？有份程序员的孤傲，当时觉得C语言很酷，我是底层我牛X，你们这些语言不是带个虚拟机，就是拖个运行时一点都Cool，那为什么不选汇编语言呢？咳咳，确实吼不住。 就这样我带着C语言，C语言伴着我开始一步步经历大学生活，当然期间也从一些“旁门左道”里习得只言片语的C++，只是我不知道它还有这个名字，带着舍我其谁的年轻气盛，我和C语言一起去参加各种编程比赛，结果有好有坏，但实力确实很菜，虽然也拿到过一些区域或者国家级的名次，但如今已成过往云烟。 转眼间就来到了大四实习，一个专业100多人，少部分人选择了苹果应用，绝大多数投身于Java开发，而我独自一人选择了嵌入式C，为什么选择嵌入式，因为只有它带“C”，也是在这短暂的几个月里我再一次听说了C++，但仅仅局限于了解C和C++有什么不同，C++好像有个class和struct挺像的。 开始真正的找工作了，受大学和室友一起打dota的影响，我毅然决然的加入了游戏开发大军，在那个各类游戏百花齐放的年代，C/C++在游戏行业有着根深蒂固的基础，因为我的C语言还不错，所以还是顺利的入了坑。 但随着踏入这个坑，我和C++开始正面交锋了，纠缠至今已经有10多年，中间工作6年之后有一次转Golang开发的机会被我拒绝了，我给那个面试官的回答是因为情怀吧，我不能离开C++。 如今我用C++20的标准在工作，与刚开始接触的C++98真的跨了一个世纪，各种语法糖之外是另一些语法糖，每个版本中标准委员会提到的特性，恨不得要3年后的要下个版本才能开发，庞大语言体系使得一些初入社会的毛头小子才配标注“精通”二字，而像我这种使用十多年的菜鸟只敢承认用过C++，略懂皮毛而已。 C++很难吗？真的很难！可不可以简单一点，其实它也可以很简单，不要拿着各种内存问题和我C++挑战，这不是我的强项，但我同样可以写出没有内存泄漏的程序；也不要拿着各种工具、运行时库和我比开发效率，这也不是我的强项，我难道就没有库吗，开源的世界里百花齐放。 不要一提到C++就是面向对象、STL、模板元编程，它是一门编程语言，一门用于完成任务的可编写逻辑的语言，用它写一个冒泡排序也很简单，不要过分夸大他的难度，每次你都把它最难的地方亮出来，对它来说是不公平的。 如今，我不会再说C++就是牛X其他语言都是垃圾这种脑抽的话，C++只是一种语言，一种我暂时还离不开的语言，但它不再是我的唯一，写个小工具偶尔也会使用Python或者Golang，不是我不再爱C++，而是有了更合适的选择而已。 C++，一门集强大与庞大于一身的语言，学会它容易，但学透它确实很难，我们在一起共同成长。 总结 GPT无处不在，每天必用超过了google git diff 指定文件类型 git diff dev..feature -- &#39;:*.pb.go&#39; &#39;:*.pb.h&#39; &#39;:*.pb.cc&#39; git diff 排除文件类型 git diff dev..feature -- &#39;:!*.pb.go&#39; &#39;:!*.pb.h&#39; &#39;:!*.pb.cc&#39; ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 每天提问自己一个问题，试验一段时间，看看是否有效吧，本质还是眼高手低造成了目前的焦虑]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>diff</tag>
        <tag>gpt</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下使用timeout监控命令执行超过5分钟后发送通知]]></title>
    <url>%2Fblog%2F2023%2F10%2F23%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8timeout%E7%9B%91%E6%8E%A7%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E8%B6%85%E8%BF%875%E5%88%86%E9%92%9F%E5%90%8E%E5%8F%91%E9%80%81%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[前言有一段时间没写了，马上1024了，总得有点输出吧，想到最近用了一个挺有用的命令 timeout ，所以拿出来简单展示一下它的用法，作用就是可以给指定命令设置超时时间，相当于扩展了原命令，对于一般没有超时参数的命令是个方便的扩展方式，可以增加超时处理。 使用方法使用方法也非常简单，下面是一个简单的示例： 123456789101112131415161718192021#!/bin/bash# 设置超时时间（秒）TIMEOUT=300# 要监控的命令./hooks/rsync.sh &amp;# 获取命令的进程IDcommand_pid=$!# 等待命令执行完成，或者直到超时timeout $TIMEOUT tail --pid=$command_pid -f /dev/null# 检查是否超时if [ $? -eq 124 ]; then echo "timeout" iostat -d -x 2 9else echo "normal"fi tail --pid=$command_pid -f /dev/null 这个命令是用来等待指定进程 $command_pid 结束的同时保持脚本的运行，以确保后续的超时检查。 tail 是一个用于显示文件末尾内容的命令，通常用于查看日志文件。 --pid=$command_pid 是 tail 命令的一个选项。它告诉 tail 命令在结束之前等待指定的进程 $command_pid 完成。这是通过监视 /proc/$command_pid/fd/1 文件（进程的标准输出）来实现的，以便在进程结束时 tail 命令也会退出。 -f /dev/null 表示 tail 命令要查看 /dev/null 文件的内容，但 /dev/null 文件是一个空文件，所以它不会产生输出。这里的目的是让 tail 命令一直运行，直到 $command_pid 进程结束。 这种构造的作用是等待命令执行完成，但在等待的过程中不产生输出。一旦 $command_pid 进程结束，tail 命令也会退出，允许脚本继续执行后续操作，如检查是否超时。 if [ $? -eq 124 ]; then ... else ...:这一段是条件语句，用于检查timeout命令的返回状态。如果返回状态是124，意味着rsync.sh脚本执行超时，那么它会打印”timeout”以指示发生了超时，并且运行iostat -d -x 2 9命令来获取磁盘I/O统计信息。 如果timeout命令的返回状态不是124，即没有发生超时，它将执行else部分打印”normal”以指示命令正常执行。 其他有趣的命令类似这种监控或者扩展的命令还有： time，于测量命令执行的时间。它会运行一个命令，并报告命令执行所花费的实际时间（包括用户CPU时间和系统CPU时间），通常用于性能分析和优化。例如 time my_command， at，可以计划执行一次性任务，比如将在15:30执行命令echo &quot;my_command&quot; | at 15:30 nohup，用于在后台运行命令，比如 nohup my_command &gt; output.log 2&gt;&amp;1 &amp; 系统资源查询命令12345date &gt;&gt; /tmp/slow.logiostat -d -x 2 5 &gt;&gt; /tmp/slow.logvmstat -n 2 5 &gt;&gt; /tmp/slow.logsar -B 2 5 &gt;&gt; /tmp/slow.logtop -b -n 3 | grep "md5sum" &gt;&gt; /tmp/slow.log 有时发现系统总是被无故拖慢，有些进程迟迟不能结束，虽然有监控grafana大盘可以查，但是有些参数不太细致，可以上面展示的这些命令来临时查一下，它们的含义如下： iostat -d -x 2 5 &gt;&gt; /tmp/slow.log： iostat 用于报告系统磁盘活动和性能 -d 选项表示只显示磁盘相关的信息 -x 选项用于显示扩展的磁盘统计信息 2 表示每隔2秒报告一次 5 表示报告5次 vmstat -n 2 5 &gt;&gt; /tmp/slow.log： vmstat 用于显示虚拟内存统计 -n 选项用于指定输出中不显示表头 sar -B 2 5 &gt;&gt; /tmp/slow.log： sar 用于系统性能数据采集和报告 -B 选项用于报告缓冲区和页缓存统计信息 top -b -n 3 | grep “md5sum” &gt;&gt; /tmp/slow.log： top 用于实时查看系统中正在运行的进程和其性能 -b 选项表示以批处理模式运行，适用于脚本 -n 3 表示运行3次 top grep &quot;md5sum&quot; 用于筛选包含 “md5sum” 的行，以查找与 “md5sum” 相关的进程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Fri Oct 20 13:36:31 CST 2023Linux 3.10.0-1160.88.1.el7.x86_64 (demo-machine) 10/20/2023 &gt;_x86_64_ (8 CPU)Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz &gt;avgqu-sz await r_await w_await svctm %utilfd0 0.00 0.00 0.00 0.00 0.00 0.00 8.00 &gt;0.00 136.50 136.50 0.00 136.50 0.00sdb 0.00 0.00 1.98 0.02 179.62 6.55 186.84 &gt;0.06 32.16 30.23 265.38 1.45 0.29sda 0.00 0.02 0.01 0.23 0.27 1.98 18.70 &gt;0.00 9.97 103.14 6.89 1.96 0.05Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz &gt;avgqu-sz await r_await w_await svctm %utilfd0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 &gt;0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 1.47 0.00 598.04 0.00 813.33 &gt;0.13 86.33 86.33 0.00 42.33 6.23sda 0.00 0.00 0.49 0.98 3.92 2.70 9.00 &gt;0.01 10.00 30.00 0.00 10.00 1.47Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz &gt;avgqu-sz await r_await w_await svctm %utilfd0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 &gt;0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 0.99 0.00 392.08 0.00 792.00 &gt;0.02 19.00 19.00 0.00 9.50 0.94sda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 &gt;0.00 0.00 0.00 0.00 0.00 0.00Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz &gt;avgqu-sz await r_await w_await svctm %utilfd0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 &gt;0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 4.00 0.00 1776.00 0.00 888.00 &gt;0.06 15.25 15.25 0.00 2.62 1.05sda 0.00 0.00 0.00 1.50 0.00 6.00 8.00 &gt;0.00 2.00 0.00 2.00 0.67 0.10Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz &gt;avgqu-sz await r_await w_await svctm %utilfd0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 &gt;0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 4.23 0.00 2066.15 0.00 976.73 &gt;0.13 31.00 31.00 0.00 3.55 1.50sda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 &gt;0.00 0.00 0.00 0.00 0.00 0.00procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 162364 0 12586664 0 0 23 1 0 1 0 0 100 0 0 2 0 0 158892 0 12565264 0 0 0 0 3209 413 0 8 92 0 0 2 0 0 165976 0 12543900 0 0 0 0 1879 332 0 5 95 0 0 2 0 0 163496 0 12523372 0 0 396 0 2844 678 0 8 92 0 0 3 0 0 162240 0 12502092 0 0 0 0 2038 574 0 7 93 0 0Linux 3.10.0-1160.88.1.el7.x86_64 (demo-machine) 10/20/2023 &gt;_x86_64_ (8 CPU)01:36:48 PM pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s &gt;pgsteal/s %vmeff01:36:50 PM 1674.00 0.00 3098.00 0.00 3431.00 3362.00 &gt;0.00 3362.00 100.0001:36:52 PM 0.00 0.00 2874.50 0.00 2675.00 2650.50 &gt;0.00 2650.50 100.0001:36:54 PM 2822.00 0.00 2478.50 0.00 2666.50 2645.00 &gt;0.00 2645.00 100.0001:36:56 PM 0.00 0.50 1248.50 0.00 156173.00 2642.00 &gt;0.00 2642.00 100.0001:36:58 PM 0.00 0.00 1975.00 0.00 321.50 0.00 &gt;0.00 0.00 0.00Average: 899.20 0.10 2334.90 0.00 33053.40 2259.90 0.00 2259.90 100.00 5667 root 20 0 1455484 54400 1256 R 100.0 0.3 3:54.39 md5sum 5667 root 20 0 1455484 75784 1256 R 100.0 0.5 3:57.40 md5sum 5667 root 20 0 1455484 98752 1256 R 100.0 0.6 4:00.41 md5sum 总结 timeout 可以给其他命令设置超时时间 at 可以计划执行一次性任务 iostat -d -x 2 5 用于报告系统磁盘活动和性能 vmstat -n 2 5 用于显示虚拟内存统计 sar -B 2 5 用于系统性能数据采集和报告 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 心态还是很难调整好，大道理学了这么久，发现都是讲给别人听的，根本治愈不了自己，该歇歇了~]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>运维</tag>
        <tag>服务器</tag>
        <tag>timeout</tag>
        <tag>iostat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[废了九牛二虎之力终于修改了MySQL8.0的root密码]]></title>
    <url>%2Fblog%2F2023%2F10%2F08%2F%E5%BA%9F%E4%BA%86%E4%B9%9D%E7%89%9B%E4%BA%8C%E8%99%8E%E4%B9%8B%E5%8A%9B%E7%BB%88%E4%BA%8E%E4%BF%AE%E6%94%B9%E4%BA%86MySQL8-0%E7%9A%84root%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[前言上次说到在Ubuntu系统上安装MySQL8.0之后默认是没有密码的，如果想设置密码需要写成ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;xxx&#39;; 的形式，其实在用这种方式之前，我还使用功能了mysql_secure_installation这个MySQL自带的程序来设置密码，但是没生效，为什么这个看起来很正规的方式不生效的呢？接下来一起找找原因。 使用mysql_secure_installation使用步骤比较简单，就按照提示来输入就可以了，这是我第二次运行，第一次运行时还要加个 ‘Would you like to setup VALIDATE PASSWORD component?’ 步骤 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657root@w-dev:~# mysql_secure_installationSecuring the MySQL server deployment.Enter password for user root:The 'validate_password' component is installed on the server.The subsequent steps will run with the existing configurationof the component.Using existing password for root.Estimated strength of the password: 50Change the password for root ? ((Press y|Y for Yes, any other key for No) : yNew password:Re-enter new password:Estimated strength of the password: 50Do you wish to continue with the password provided?(Press y|Y for Yes, any other key for No) : yBy default, a MySQL installation has an anonymous user,allowing anyone to log into MySQL without having to havea user account created for them. This is intended only fortesting, and to make the installation go a bit smoother.You should remove them before moving into a productionenvironment.Remove anonymous users? (Press y|Y for Yes, any other key for No) : ySuccess.Normally, root should only be allowed to connect from'localhost'. This ensures that someone cannot guess atthe root password from the network.Disallow root login remotely? (Press y|Y for Yes, any other key for No) : ySuccess.By default, MySQL comes with a database named 'test' thatanyone can access. This is also intended only for testing,and should be removed before moving into a productionenvironment.Remove test database and access to it? (Press y|Y for Yes, any other key for No) : y - Dropping test database...Success. - Removing privileges on test database...Success.Reloading the privilege tables will ensure that all changesmade so far will take effect immediately.Reload privilege tables now? (Press y|Y for Yes, any other key for No) : ySuccess.All done! 那么这个程序具体干了哪些事呢，下面来解释一下。 mysql_secure_installation 是一个 MySQL 提供的命令行工具，用于帮助加强和改善 MySQL 服务器的安全性。当首次安装 MySQL 后，运行此工具可以执行一系列的操作来提高 MySQL 服务器的安全性，并确保默认设置不会导致潜在的安全漏洞。以下是 mysql_secure_installation 工具执行的主要任务： 设置 root 密码： 如果您在安装 MySQL 时没有为 root 用户设置密码，mysql_secure_installation 会提示您设置 root 密码。设置 root 密码是非常重要的，因为 root 用户具有最高的权限，拥有对 MySQL 服务器的完全控制。 删除匿名用户： mysql_secure_installation 会删除默认情况下存在的匿名用户。匿名用户是指没有用户名和密码就能访问 MySQL 服务器的用户。删除这些用户可以减少潜在的安全风险。 禁止远程 root 登录： 默认情况下，MySQL 允许 root 用户从任何主机登录。mysql_secure_installation 会提示您是否禁止 root 用户从远程主机登录，以减少潜在的远程攻击风险。 删除测试数据库： 默认情况下，MySQL 安装包含一个名为 “test” 的测试数据库，这个数据库可能会成为潜在的攻击目标。mysql_secure_installation 会提示您是否删除测试数据库。 刷新权限： 完成上述任务后，mysql_secure_installation 会刷新 MySQL 权限表，以确保更改生效。 mysql_secure_installation 工具有助于在 MySQL 安装完成后采取一些基本的安全措施，以减少潜在的风险和攻击。然而，这只是开始，为了更好地保护 MySQL 服务器和数据，您还应该采取其他安全措施，如定期备份、限制用户权限、监控数据库活动等。 mysql_secure_installation 为什么设置root密码无效看上面的描述这个程序挺靠谱的，为啥设置密码不好使呢？遵循着网络大神们的脚本，我找到了一个 mysql_secure_installation.sh脚本，来自[twitter-forks/mysql]开源库，其中包含这样一段代码： 12345678910111213141516esc_pass=`basic_single_escape "$password1"`do_query "UPDATE mysql.user SET Password=PASSWORD('$esc_pass') WHERE User='root';"if [ $? -eq 0 ]; thenecho "Password updated successfully!"echo "Reloading privilege tables.."reload_privilege_tablesif [ $? -eq 1 ]; then clean_and_exitfiechorootpass=$password1make_configelseecho "Password update failed!"clean_and_exitfi &quot;UPDATE mysql.user SET Password=PASSWORD(&#39;$esc_pass&#39;) WHERE User=&#39;root&#39;;&quot; 这个脚本里居然用了我们之前提到的已经失效的更新方式，但是我发现我在Ubuntu系统下安装的mysql_secure_installation是一个二进制程序，不是shell脚本，所以我又找到了mysql的源码程序，其中包含这样一段： 1234567891011 query = (char *)my_malloc(PSI_NOT_INSTRUMENTED, password_len + 50, MYF(MY_WME)); end = my_stpmov(query, "SET PASSWORD="); *end++ = '\''; end += mysql_real_escape_string_quote(mysql, end, password, (ulong)password_len, '\''); *end++ = '\''; if (mysql_real_query(mysql, query, (ulong)(end - query))) &#123; my_free(query); return false;&#125; 里面包含了&quot;SET PASSWORD=&quot;的字样，看起来不太靠谱的样子，需要注意的是，在 MySQL 5.7 版本之后，SET PASSWORD 命令已被弃用，推荐使用 ALTER USER 命令来修改密码。例如： 1ALTER USER 'root'@'localhost' IDENTIFIED BY '新密码'; 这个命令更加直观，并且与最新版本的 MySQL 兼容。同样，将 &#39;新密码&#39; 替换为您希望设置的新密码。 难道是MySQL 8.0没有更新工具的代码？暂时不得而知，这或许就是mysql_secure_installation设置密码不生效的原因。 ALTER USER再生事端前文提到了一种更加直观的设置新密码的方式 ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;新密码&#39;;，但是在我这行不通，不仅我这行不通，网上也有很多人说设置不了，需要加上插件名称 WITH mysql_native_password，改为 ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;新密码&#39;; 才可以，这又是为什么呢？ 执行 select host,user,plugin,authentication_string from mysql.user; 语句查看发现： 1234567891011mysql&gt; select host,user,plugin,authentication_string from mysql.user;+-----------+------------------+-----------------------+------------------------------------------------------------------------+| host | user | plugin | authentication_string |+-----------+------------------+-----------------------+------------------------------------------------------------------------+| localhost | zone1 | mysql_native_password | *4C7684A2CD28A1C3C55CC832E6O817FCE3C5B5DB || localhost | mysql.infoschema | caching_sha2_password | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED || localhost | mysql.session | caching_sha2_password | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED || localhost | mysql.sys | caching_sha2_password | $A$005$THISISACOMBINATIONOFINVALIDSALTANDPASSWORDTHATMUSTNEVERBRBEUSED || localhost | root | auth_socket | || localhost | admin | mysql_native_password | *F52COBE5FFB5FFCCD5BFD976OADEB81081AE4787 |+-----------+------------------+-----------------------+------------------------------------------------------------------------+ 原来每个账户还有默认的不同的密码管理插件，用于验证和管理用户密码，mysql_native_password、auth_socket和caching_sha2_password这三种插件的含义如下： mysql_native_password 加密算法： 使用较旧的哈希算法，如SHA-1或SHA-256，取决于 MySQL 版本。 认证方式： 用户密码以哈希形式存储在系统表中，服务器在用户登录时将明文密码哈希并与数据库中存储的哈希进行比较。 支持远程登录： 支持远程密码登录，可以从远程主机登录 MySQL。 适用场景： 适用于需要与旧版 MySQL 兼容或需要与其他应用程序或系统进行集成的情况。 caching_sha2_password 加密算法： 使用 SHA-256 进行加密，提供更高级别的安全性。 支持版本： 从 MySQL 5.7.4 开始，caching_sha2_password 成为默认的密码插件。 认证方式： 用户密码以加密形式存储在系统表中，并且在用户登录时，服务器将明文密码哈希并与数据库中存储的哈希进行比较。 支持远程登录： 支持远程密码登录，可以从远程主机登录 MySQL。 auth_socket 认证方式： 不存储密码哈希，而是依赖于操作系统（Linux）的用户认证机制。只有与操作系统用户账户关联的 MySQL 用户可以登录，而不需要密码。 适用场景： 通常用于本地连接，因为它要求数据库用户和操作系统用户名称一致。 安全性： 提供了一定的安全性，因为不需要存储密码哈希，但需要确保服务器的用户账户和 MySQL 用户账户的一致性。 总结来说，这些密码插件之间的区别主要涉及到加密算法、认证方式以及适用场景。caching_sha2_password 提供更高级别的安全性，而 auth_socket 依赖于操作系统用户认证机制，mysql_native_password 则提供与旧版 MySQL 兼容性。使用前应根据安全性需求、用户管理和应用程序集成的要求来选择合适的密码插件。一般来说，推荐使用较新且安全性更高的密码插件，如caching_sha2_password。 测试后发现，MySQL8.0默认安装后root用户的默认插件是 auth_socket，这种插件没办法设置密码，所以指定插件为 mysql_native_password 时就可以设置密码了，但是根据建议，我们应该选择 caching_sha2_password插件。 肯定好使的MySQL初始化方法 apt install mysql-server mysql ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH caching_sha2_password BY &#39;新密码&#39;; flush privileges; exit; 总结 安装MySQL8.0后使用ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH caching_sha2_password BY &#39;xxx&#39;;设置密码 mysql_secure_installation工具不好使的原因目前来看是源代码没有工具的源代码没有及时更新，与MySQL版本不匹配 mysql_secure_installation另外的几项功能，比如删除匿名用户、删除测试数据库、禁止远程登录在安装完貌似都处理好了 综上来看，难道mysql_secure_installation工具已经被废弃了？ ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 你们的一切都是我的，只要留心你将获得无穷的助力，很多东西不必从头开始，既然做不成巨人，就从巨人的肩膀上出发吧~ 证件照尺寸查询一寸 295413px | 2535mm二寸 413579px | 3549mm小一寸 260378px | 2232mm大一寸 390567px | 3348mm小二寸 413531px | 3545mm大二寸 413626px | 3553mm三寸 650992px | 5584mm四寸 8981181px | 76100mm五寸 10501500px | 89127mm 2023-10-8 20:56:28]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>root</tag>
        <tag>mysql_secure_installation</tag>
        <tag>AlterUser</tag>
        <tag>修改密码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++20中的关键字]]></title>
    <url>%2Fblog%2F2023%2F10%2F07%2FC-20%E4%B8%AD%E7%9A%84%E5%85%B3%E9%94%AE%E5%AD%97%2F</url>
    <content type="text"><![CDATA[前言源于上一篇中提到了 decltype是一个关键字，所以去查了现有的关键字列表，其实我对C/C++的关键字还停留在上世纪大概30多个的时代，虽然知道C++加了 auto、constexpr 等关键字，但查询过后才发现原来他扩充到了将近100个，而C语言的关键字也扩到了六七十个，有点震惊了。 关键字列表这份列表来源于cppreference，感兴趣的可以去看原文，下面对这些关键字进行一个简单的介绍，有些还是很陌生的。 关键字含义 关键字 含义 alignas 指定类型或对象的对齐要求 alignof 查询类型的对齐要求 and 用作 &amp;&amp; 的替代写法 and_eq 用作 &amp;= 的替代写法 asm 汇编（asm）声明给予在 C++ 程序中嵌入汇编语言源代码的能力 atomic_cancel (TM TS) 原子块，若抛出异常，则调用 std::abort，除非该异常是用于事务取消的异常之一，这种情况下事务被取消 atomic_commit (TM TS) 原子块，若抛出异常，则正常地提交事务 atomic_noexcept (TM TS) 原子块，若抛出异常，则调用 std::abort auto 可以在声明变量的时候根据变量初始值的类型自动为此变量选择匹配的类型（C++11） bitand 用作 &amp; 的替代写法 bitor 用作 `\ ` 的替代写法 bool 足以存放两个值 true 或 false 之一的类型 break 控制将被转移到紧跟其外围循环或 switch 的下一位置 case 根据条件的值，将控制流转移到若干语句之一 catch 和try联合使用，将一或多个异常处理块（catch 子句）与复合语句关联 char 能在目标系统上最有效地处理的字符表示的类型 char8_t UTF-8 字符表示的类型，要求大到足以表示任何 UTF-8 编码单元（ 8 位） char16_t UTF-16 字符表示的类型，要求大到足以表示任何 UTF-16 编码单元（ 16 位） char32_t UTF-32 字符表示的类型，要求大到足以表示任何 UTF-32 编码单元（ 32 位）， C++ 标准还保证 1 == sizeof(char) &lt;= sizeof(short) &lt;= sizeof(int) &lt;= sizeof(long) &lt;= sizeof(long long) class 声明类 compl 用作 ~ 的替代写法 concept 声明具名类型要求 const 类型限定符，定义类型为常量 consteval 指定函数是立即函数（immediate function），即每次调用该函数必须产生编译时常量 constexpr 指定变量或函数的值可在常量表达式中出现，在 constexpr if 语句中，条件的值必须是可按语境转换到 bool 类型的经转换常量表达式 constinit 断言变量拥有静态初始化，即零初始化与常量初始化 const_cast 用const_cast来去除const限定 continue 导致外围 for、范围 for、while 或 do-while 循环体的剩余部分被跳过 co_await 协程是能暂停执行以在之后恢复的函数 co_return 协程中的 return 语句 co_yield 协程中表达式向调用方返回一个值并暂停当前协程 decltype 检查实体的声明类型，或表达式的类型和值类别 default 1.switch 语句用于声明默认情况标号。2.显式默认化的函数定义令编译器为某个类生成特殊成员函数或比较运算符的显式指令 delete 1.销毁先前由 new 表达式分配的对象，并释放获得的内存区域。2.弃置函数(C++11 起) do do-while 循环，用作循环的声明 double 浮点类型 dynamic_cast 沿继承层级向上、向下及侧向，安全地转换到其他类的指针和引用 else if 语句用作另一分支的声明 enum 声明枚举类型 explicit 1.指定构造函数或转换函数 (C++11 起)或推导指引 (C++17 起)为显式，即它不能用于隐式转换和复制初始化。2.explicit 说明符可以与常量表达式一同使用。当且仅当该常量表达式求值为 true 时函数为显式。 export 1.用于标记模板定义为被导出，这允许在其他翻译单元中声明但不定义同一模板。(C++11 前) 2.不使用并保留该关键词。(C++11 起)(C++20 前) 3.标记一个声明、一组声明或另一模块为当前模块所导出。(C++20 起) extern 1.具有外部连接的静态存储期说明符 2.语言连接说明 3.显示模板实例化声明（或“extern 模板”） false 布尔字面量 float 单精度浮点类型 for 用于该循环的声明 friend 友元声明出现于类体内，并向一个函数或另一个类授予对包含友元声明的类的私有及受保护成员的访问权。 goto 无条件转移控制流。当使用其他语句不可能将控制转移到所需位置时使用。 if 有条件地执行另一条语句。用于需要基于运行时或编译时条件执行的代码。 inline 1.将函数声明为一个 内联（inline）函数。完全在 class/struct/union 的定义之内定义的函数，无论它是成员函数还是非成员 friend 函数，均为隐式的内联函数。2.内联命名空间 int 基本整数类型 long 长整数类型 mutable(1) 1.容许在即便包含它的对象被声明为 const 时仍可修改声明为 mutable 的类成员。2.mutable 用于指定不影响类的外部可观察状态的成员（通常用于互斥体、记忆缓存、惰性求值和访问指令等）。3.从按复制捕获的形参中移除的 const 限定性的 lambda 声明符 (C++11 起) 4.被mutable修饰的变量，将永远处于可变的状态，即使在一个const函数中 namespace 命名空间声明 new .创建并初始化拥有动态存储期的对象，这些对象的生存期不受它们创建时所在的作用域限制 noexcept (C++11 起) 1.noexcept 运算符进行编译时检查，若表达式声明为不抛出任何异常则返回 true 2.指定函数是否抛出异常 not 用作 ! 的替代写法 not_eq 用作 != 的替代写法 nullptr (C++11 起) 代表指针字面量，保持空指针常量的含义 operator 重载的运算符是具有特殊的函数名的函数 or 用作 的替代写法 or_eq 用作 `\ =` 的替代写法 private 该访问说明符之后的各个成员具有私有成员访问 protected 该访问说明符之后的各个成员具有受保护成员访问 public 该访问说明符之后的各个成员具有公开成员访问 reflexpr (反射 TS) 反射是指程序在运行时动态获取对象属性与方法的一种机制 register 1.动存储期，另提示编译器将此对象置于处理器的寄存器。2.不使用并保留该关键词 reinterpret_cast 通过重新解释底层位模式在类型间转换。 requires (C++20 起) 1.对模板形参指定一个求值为要求的常量表达式。 2.于模板声明中指定关联制约 return 终止当前函数并返回指定值（若存在）给调用方函数 short 至少 16 位的宽度（和long一样表示大小） signed 目标类型将拥有有符号表示（若省略则此为默认） sizeof(1) 1.查询对象或类型的大小。2.在需要知道对象的实际大小时使用。3.返回形参包中的元素数量 static 1.声明具有静态存储期和内部连接的命名空间成员。2.定义具有静态存储期且仅初始化一次的块作用域变量。3.声明不绑定到特定实例的类成员 static_assert (C++11 起) 进行编译时断言检查 static_cast 用隐式和用户定义转换的组合在类型间转换 struct 1.复合类型的声明。2.有作用域枚举类型的声明(C++11 起) 3.若存在于作用域中的某个函数或变量所拥有的名字，与某个非联合体类类型的名字相同，则可在其名字之前加上 struct 来消歧义，这产生一个详述类型说明符 switch 根据条件的值，将控制流转移到若干语句之一 synchronized (TM TS) 程序中的所有最外层同步块都以一个单独的全序执行 template 1.声明模板。 2.在模板定义中，template 可用于将某个待决名声明为模板 this this 是一个纯右值表达式，其值是隐式对象形参（在其上调用非静态成员函数的对象）的地址 thread_local (C++11 起) 线程存储期 throw 1.对错误条件发信号，并执行错误处理代码 2.列出函数可能直接或间接抛出的异常 true bool 类型的纯右值，为真 try 将一或多个异常处理块（catch 子句）与复合语句关联 typedef 创建能在任何位置替代（可能复杂的）类型名的别名 typeid 查询类型的信息。用于必须知晓多态对象的动态类型的场合以及静态类型鉴别 typename 1.在模板声明中，typename 可用作 class 的代替品，以声明类型模板形参和模板形参 (C++17 起) 2.在模板的声明或定义内，typename 可用于声明某个待决的有限定名是类型。3.在模板的声明或定义内， (C++11 前)typename 可在非待决的有限定类型名之前使用。此情况下它没有效果。4.在类型要求的要求中。(C++20 起) union 1.联合体类型的声明。2.若存在于作用域中的某个函数或变量所拥有的名字与某个联合体类型的名字相同，则可在该名字之前加上 union 以消歧义，这产生详述类型说明符 unsigned 目标类型将拥有无符号表示 using 1.对命名空间的 using 指令及对命名空间成员的 using 声明。2.对类成员的 using 声明。3.类型别名与别名模板声明 (C++11 起) virtual 1.虚函数说明符。2.虚基类说明符 void 1.void 类型。2.无形参函数的形参列表 volatile 定义类型为易变，确保本条指令不会因编译器的优化而省略，且要求每次直接读值 wchar_t 宽字符表示的类型 while 1.while 循环的声明。2.do-while 循环终止条件的声明 xor 作为 ^ 的替代写法 xor_eq 用作 ^= 的替代写法 总结 C++关键字从C++11开始快速扩张，到C++20标准已经有100个左右了 C语言关键字发展到现在也有60多个了 像 xor_eq、or_eq 这些关键字就是对原来的运算符号换了个写法，感觉上没有什么用，只是不用敲特殊符号了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 好日子都是自己争取来的~ 2023-10-7 23:34:23]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>keywords</tag>
        <tag>xor_eq</tag>
        <tag>co_await</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下如果掌控了系统root账户就能对上面安装的MySQL数据为所欲为了吗]]></title>
    <url>%2Fblog%2F2023%2F10%2F07%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%A6%82%E6%9E%9C%E6%8E%8C%E6%8E%A7%E4%BA%86%E7%B3%BB%E7%BB%9Froot%E8%B4%A6%E6%88%B7%E5%B0%B1%E8%83%BD%E5%AF%B9%E4%B8%8A%E9%9D%A2%E5%AE%89%E8%A3%85%E7%9A%84MySQL%E6%95%B0%E6%8D%AE%E4%B8%BA%E6%89%80%E6%AC%B2%E4%B8%BA%E4%BA%86%E5%90%97%2F</url>
    <content type="text"><![CDATA[前言根据我目前测试的情况来看，这个问题的答案是【确实可以为所欲为】，事情的起因是这样的，一次偶然的机会我发现Ubuntu20.04系统上通过 sudo apt install mysql-server 命令安装的MySQL服务器，使用root账号登录无论输入什么密码都可以成功进入，多次尝试后才知道原来root用户默认没有密码，这可是让我惊呆了，居然还可以这样，为了服务器的安全，我必须给他设置个密码，在不断尝试的过程中引发了标题中的疑问。 修改root密码修改MySQL用户root密码在网络上充斥着各种各样的版本，最常见的这种在MySQL8.0中已经不生效了，错误示例如下： 1UPDATE user SET authentication_string = PASSWORD('iL1jfLqUSfmTECXRi4nO') WHERE User = 'root'; 目前测试在MySQL8.0版本中可行的设置root密码的方式为： 123USE mysql;ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'iL1jfLqUSfmTECXRi4nO';FLUSH PRIVILEGES; 产生疑问root用户的密码是设置好了，这么长的密码估计破解个几年也破解不开，但是我发现安装完毕提示了这样一样配置文件 /etc/mysql/debian.cnf，打开文件内容展示如下： 1234567891011# Automatically generated for Debian scripts. DO NOT TOUCH![client]host = localhostuser = debian-sys-maintpassword = xg9vyvjNly3vUSsmsocket = /var/run/mysqld/mysqld.sock[mysql_upgrade]host = localhostuser = debian-sys-maintpassword = xg9vyvjNly3vUSsmsocket = /var/run/mysqld/mysqld.sock 居然是一个包含了MySQL用户和密码的未加密的文本文件，这不是在大门口的脚垫底下放上了大门的钥匙吗？关键这个debian-sys-maint虽然设计上是用于数据库维护和管理，但权限上与root几乎相同，通常在 Debian 或基于 Debian 的 Linux 发行版比如Ununtu上安装MySQL就会自动生成这个文件，看起来有点不安全啊？ 于是查了相关的处理方法，都说要控制这个文件的权限，只让系统root用户可以访问它，这时我就产生了疑问，这样做安全吗？如果系统root用户被窃取了呢？那是不是完全可以通过这个文件来操控MySQL数据了，答案是很显然的。 渐渐明白虽然说掌握了root用户就可以任意修改MySQL数据让人难以接受，但是换个角度或许就可以释然了，root可是系统里的上帝啊，控制了root用户可以任意执行 rm -rf /*，如果你知道银行卡密码当然就可以从取款机里取钱，对于操作系统也是一样，既然你是root你就掌控了这个系统里的一切。 即便没有这个/etc/mysql/debian.cnf文件，root用户还可以先关闭MySQL服务，然后通过 sudo mysqld_safe --skip-grant-tables &amp;以跳过授权表的方式启动MySQL，此时允许你在不知道密码的情况下以 root 身份登录 MySQL，这还有什么隐私可言。再换种方式，作为root用户的我从磁盘上把MySQL存储的文件拿到其他MySQL服务器上都是可以的，所以你是防不住root上帝的。 所以刚刚那个【大门口的脚垫底下放上了大门的钥匙】的比喻不太恰当，应该是/etc/mysql/debian.cnf就像屋里的桌子上摆着的保险柜的钥匙，而你要保证大门的钥匙（root密码）不要轻易被人搞到手，像 --skip-grant-tables 这种看起来很像是后门，或者叫应急开关，实际上它的设计就是为了提供一种紧急的修复机制，用于在丧失所有密码或无法登录到 MySQL 数据库时进行数据库恢复和修复，比如紧急恢复、密码重置、修复数据库权限等等 这种设计就像是各司其职，我只负责我数据的管理与权限的分配，而用户需要保证系统很牢固，一旦你的系统被攻破我将束手就擒，做的再多也是徒劳。 总结 MySQL8.0修改密码的方式ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;xxx&#39;; 文件/etc/mysql/debian.cnf是Debian系列系统上安装MySQL时自动生成的，其中的用户用于用于数据库维护和管理 如果忘了MySQL中root用户的密码可以通过 sudo mysqld_safe --skip-grant-tables &amp; 启动服务来进行免密登录 控制了系统的root用户，你就掌控了操作系统的一切 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 假期归来又看了很多，也不安分了许多，或许只有通过打破常规才能获得额外的收益，否则只能是一直保持饿不死的状态 2023-10-7 20:47:41]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ubuntu</tag>
        <tag>mysql</tag>
        <tag>root</tag>
        <tag>debian-sys-maint</tag>
        <tag>skip-grant-tables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的decltype、std::declval 和 std::decay_t傻傻分不清楚]]></title>
    <url>%2Fblog%2F2023%2F09%2F05%2FC-%E4%B8%AD%E7%9A%84decltype%E3%80%81std-declval-%E5%92%8C-std-decay-t%E5%82%BB%E5%82%BB%E5%88%86%E4%B8%8D%E6%B8%85%E6%A5%9A%2F</url>
    <content type="text"><![CDATA[前言在C++中提到推导第一个映入脑海的可能是“模板”，当然有人也可能想到 auto，这些都是和推导相关的语言语法，再比如“完美转发”等等，总是就是他们的类型不用明明白白的写出来，可以利用一种简单的方式来定义，比如 candidates 是一个字符串数组， 如果遍历它我们可以写成 for (auto&amp; skillId : candidates)，而不用将 skillId 变量定义成 const std::string&amp;，这在一定程度上带来了便利，今天的几个东东也是用于推导的，我们一起来看看。 它们是什么decltype、std::declval 和 std::decay_t 都是 C++ 标准库中的类型相关工具，这些工具在 C++ 编程中提供了强大的类型推导和处理能力，使得代码更加灵活。 decltype 是一个关键字，用于从一个表达式中推导出其类型。它通常与表达式一起使用，以便在编译时确定表达式的类型。例如： 12int x = 5;decltype(x) y = 10; // y 的类型将被推导为 int decltype 还可以用于推导函数返回类型，以及在泛型编程中处理模板的类型。 std::declval 是一个函数模板，用于在不实际调用构造函数的情况下生成一个对象引用，以便在类型推导中使用。通常在需要引用某个类型的对象但实际无法创建该对象的上下文中使用，例如在函数返回类型推导中： 1234template &lt;typename T&gt;auto createAndProcess() -&gt; decltype(std::declval&lt;T&gt;().process()) &#123; // 此处只是用于类型推导，并不会实际创建 T 的对象&#125; 在这个例子中，std::declval&lt;T&gt;().process() 用于推导函数的返回类型，但并不会实际调用 process 函数。 std::decay_t 是一个类型转换工具模板，用于将给定类型的”衰变”后的类型返回。衰变指的是将类型转换为其”自然”形式，即去掉引用、添加 cv 限定符（const 和 volatile）。例如： 123using T1 = std::decay_t&lt;int&amp;&gt;; // T1 为 intusing T2 = std::decay_t&lt;const int&gt;; // T2 为 intusing T3 = std::decay_t&lt;int[]&gt;; // T3 为 int* std::decay_t 在模板编程中很有用，因为它可以帮助去除类型的多余限定符，使得函数模板的匹配更加精确。 通俗解释如果看了上面的例子还是有点迷糊，那我再解释的通俗一点： decltype 读音为 “dee-kluh-type”，就是返回一个变量的类型，比如你有一个对象 obj 想要再定义一个和它相同各类型的变量就可以写成 decltype(obj) newObj; std::declval 读音为 “standard dee-cl-val”，它能返回类型 T 的右值引用，其实是一个伪实例，不会产生任何临时对象，也不会因为表达式很复杂而发生真实的计算。因为不会真正的进行构造，所以可以实现在元编程时伪构造一个没有定义默认构造函数类，还可以避开纯虚基类不能实例化的问题，说白了它就是假装创建个对象用于推导类型。 std::decay_t 读音为 “standard dee-kay tee”，有点褪去外表直击内心的意味，就是大家一起褪去花里胡哨的修饰符，获得原始的类型，用于函数模板的匹配更加精确，比如判断类型 T是不是int，if constexpr (std::is_same_v&lt;std::decay_t&lt;T&gt;, int&gt;) 总结 decltype 是关键字，用于有了值或对象求类型的 std::declval 是函数模板，用于伪造一个对象但不实际构造的 std::decay_t 是类模板，用于去除修饰符获取原始类型的 没想到decltype 居然是关键字，结果一查发现C++的关键字已经膨胀了，下次总结一下 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当身边出现不和谐声音的时候问问自己，是不是你把他们招来的，用实力让他们闭嘴，而不要视图通过语言让他们明白 2023-9-5 21:42:25]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>decltype</tag>
        <tag>std::declval</tag>
        <tag>std::decay_t</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++定义一个返回值为拥有3个元素的 char 数组引用的函数]]></title>
    <url>%2Fblog%2F2023%2F08%2F28%2FC-%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E8%BF%94%E5%9B%9E%E5%80%BC%E4%B8%BA%E6%8B%A5%E6%9C%893%E4%B8%AA%E5%85%83%E7%B4%A0%E7%9A%84-char-%E6%95%B0%E7%BB%84%E5%BC%95%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言编程语言最怕细节，怼着一个点扣能让你怀疑人生，今天就碰到这么一个问题，“C++定义一个返回值为拥有3个元素的 char 数组引用的函数”，不敢说99%的人不会，但我觉得应该有80%的人写不出来吧，要不你来试试。 分解需求首先得能看出来这是要求定义一个函数，如果这一步就错了，那就没必要往后走了，函数有的参数没有要求，但是返回值得是一个引用，这个引用的原始类型应该为一个拥有3个元素的char类型数组，难道要这样写？ 1234char&amp; [3] func()&#123; //...&#125; 试试吧，肯定编不过的，要是这么容易就哪能让人死去活来，直接给答案了 12345678910111213141516#include &lt;iostream&gt;const char (&amp;getCharArray())[3] &#123; static const char charArray[3] = &#123;'A', 'B', 'X'&#125;; return charArray;&#125;int main() &#123; const char (&amp;result)[3] = getCharArray(); for (char c : result) &#123; std::cout &lt;&lt; c &lt;&lt; " "; &#125; return 0;&#125; 举一反三学习要学会举一反三，把题目中的“引用”换成“指针”，再试一下，依葫芦画瓢还是很容易的 123456789101112131415161718#include &lt;iostream&gt;char (*getCharArrayPointer())[3] &#123; static char charArray[3] = &#123;'A', 'B', 'J'&#125;; return &amp;charArray;&#125;int main() &#123; char (*result)[3] = getCharArrayPointer(); for (int i = 0; i &lt; 3; ++i) &#123; std::cout &lt;&lt; (*result)[i] &lt;&lt; " "; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 仿佛这也不是我们常常见到的形式，对于指针作为返回值，我们常见到的应该是下面这种 1234567891011121314151617#include &lt;iostream&gt;char* getCharArrayPointer() &#123; static char charArray[10] = &#123;'H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l'&#125;; return charArray;&#125;int main() &#123; char* result = getCharArrayPointer(); for (int i = 0; i &lt; 10; ++i) &#123; std::cout &lt;&lt; result[i]; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 他们有什么区别呢？我们从定义层面来对比一下： 进行对比 char (&result)[3] 123 - `result` 是一个引用类型变量，被引用的类型为 `char[3]` - `sizeof(result)` == 3- ```char (*result)[3] = getCharArrayPointer(); result 是一个指针类型变量，指向的类型的类型为 char[3] sizeof(result) == 8 result++ 先后移动24个字节=8*3 char* result = getCharArrayPointer(); result 是一个指针类型变量，被引用的类型为 char sizeof(result) == 8 result++ 先后移动8个字节=8*1 有了上面的对比就可以根据区别来选择定义什么样的返回值，使用引用类型能取到被引用变量的原始大小，使用指针要注意移动的步长。 总结 定义一个返回定长数组引用的函数 const char (&amp;getCharArray())[3] { //... } 定义一个返回定长数组指针的函数 const char (*getCharArray())[3] { //... } 引用不需要内存分配和释放，因为它只是目标对象的别名（存疑，在绝大多数编译器中，引用在底层是通过指针来实现的，但GPT嘴硬说不需要，表示需要继续探索） 引用在底层通过指针实现时，并不需要额外的内存分配。引用被设计为变量的别名，它并不占用额外的内存空间。引用在编译器生成的汇编代码中，会被处理为指向原始对象的指针，但是这个指针对于用户来说是透明的，不需要分配额外的内存来存储引用本身。引用只是目标对象的别名，与指针不同，它没有自己的内存存储空间。因此，无论引用在底层是通过指针实现还是其他方式实现，都不会导致额外的内存分配。这也是为什么引用在使用上更接近于对变量的直接访问。需要注意的是，尽管引用不需要额外的内存分配，但指针本身需要存储指向对象的地址，因此指针会占用一定的内存空间。这也是引用和指针之间的一个重要区别。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 内练一口气，外练筋骨皮 2023-8-28 22:17:27]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>开发语言</tag>
        <tag>指针</tag>
        <tag>引用</tag>
        <tag>函数定义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++编程中的六种内存顺序模型]]></title>
    <url>%2Fblog%2F2023%2F08%2F24%2FC-%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%85%AD%E7%A7%8D%E5%86%85%E5%AD%98%E9%A1%BA%E5%BA%8F%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[前言程序员真是一个活到老学到老的职业，一天不学习就会掉队，『内存顺序模型』对于我来说就是一个新的世界，虽然之前写过多线程的服务器，也处理过死锁和竞态条件等问题，但是从来没考虑过内存顺序问题，所以当我第一次看到这个概念时，整个人都是懵的，经过一段时间的学习和了解有了初步的认识，所以简单总结下来，以备后续查看，不多写，慢慢总结。 为什么要设计内存顺序模型内存顺序模型是为了解决多线程程序中的内存一致性和可见性问题而引入的。在多线程环境下，不同线程可能同时访问和修改共享的内存，这会引发一系列并发性问题，如竞态条件、数据竞争等。内存顺序模型的目的是通过定义不同操作之间的执行顺序和可见性规则，来保证多线程程序的正确性和可预测性。主要原因如下： 多线程并发问题： 在多线程程序中，线程之间可能并发地读取和写入共享内存，导致数据不一致和不可预测的行为。 编译器和处理器优化： 编译器和处理器可能会对代码进行优化，例如重排指令以提高性能。这些优化可能会导致操作的执行顺序与代码中的顺序不一致，从而引发问题。 硬件内存模型： 不同的计算机体系结构有不同的硬件内存模型，即不同的读写操作在不同的条件下可能表现出不同的行为。 数据依赖性： 在某些情况下，某个操作的结果可能会影响后续操作的执行。内存顺序模型可以帮助定义这种数据依赖性。 内存顺序模型通过定义不同操作之间的关系，如同步、重排等，来解决上述问题。不同的内存顺序模型提供了不同的可见性和同步保证，开发者可以根据自己的需求选择适当的模型。总之，内存顺序模型是为了在多线程环境下提供一种标准化的方式来处理内存一致性和可见性问题，从而使多线程编程更加可靠和可预测，重点关注下第2点和第4点。 有人可能会说，我可以用锁来保证顺序，为什么还要设计内存顺序模型呢？ 虽然锁（比如互斥锁）在多线程编程中是一种常见的同步机制，用于保护共享资源，但锁并不能解决所有的并发性问题，而且在某些情况下使用锁可能会引入性能问题。内存顺序模型的设计是为了在不同线程之间定义操作的执行顺序和可见性规则，以解决锁无法解决的一些问题，同时在一些情况下也可以提高性能。 细粒度同步： 锁通常是用于保护共享资源的，但有时候我们需要更细粒度的同步，比如在不涉及共享资源的情况下也需要保证操作的顺序和可见性。 原子操作： 内存顺序模型通过定义原子操作的执行顺序和可见性，可以在不使用锁的情况下确保操作的正确执行。这在一些场景下可以避免锁带来的性能开销。 锁的开销： 锁在某些情况下可能引入较大的性能开销，特别是在高并发环境中。内存顺序模型提供了一种更轻量级的同步机制，可以在一些情况下取代锁，提高性能。 编译器和处理器优化： 编译器和处理器对代码进行优化时可能会引入指令重排，这可能会导致锁保护下的共享资源出现问题。内存顺序模型通过规定操作的执行顺序，可以避免这种问题。 原子操作的组合： 内存顺序模型的原子操作可以灵活地组合，以实现更复杂的同步和顺序要求，而不必仅仅依赖于锁。 总之，内存顺序模型和锁在多线程编程中都有其适用的场景。锁通常用于保护共享资源的访问，而内存顺序模型则用于定义操作的执行顺序和可见性，以确保多线程程序的正确性。在多线程编程中，根据具体需求和性能要求，可以选择合适的同步机制。 如果想理解内存顺序，首先要理解两个东西：同一线程中，谁先执行，谁后执行；不同线程中，切换内存的时是否会及时的把依赖数据带过去，对另一个线程可见。 常见的内存顺序模型C++ 标准库中定义了六种内存顺序模型，用于控制多线程程序中不同操作之间的执行顺序和可见性。这些内存顺序模型通过枚举值表示，从“宽松”到“严格”的次序分别是： std::memory_order_relaxed： 这是最轻量级的内存顺序模型。它不会引入任何额外的同步开销，只保证操作在时间上的顺序是正确的。即使没有明确的同步操作，也不会改变其他线程看到的操作结果。 std::memory_order_consume： 在 C++11 中引入，但在 C++20 中被弃用。它主要用于处理数据依赖关系，但在实际中难以实现，已经不推荐使用。 std::memory_order_acquire： 在执行当前操作之前，确保所有前面的读操作都完成。它提供了一种读操作同步的保证，确保读操作的结果在后续操作中是可见的。 std::memory_order_release： 在执行当前操作之后，确保所有后面的写操作都不会重排到当前操作之前。它提供了一种写操作同步的保证，确保写操作的结果对其他线程是可见的。 std::memory_order_acq_rel： 是 memory_order_acquire 和 memory_order_release 的组合。它同时提供了读和写操作的同步保证，适用于需要同时保证读写操作同步的情况。 std::memory_order_seq_cst： 是最严格的内存顺序模型，提供了全局的、顺序一致的保证。它确保所有操作按照一个全局的顺序执行，不会引入重排，也保证了最高级别的可见性。 根据具体的多线程编程需求，你可以选择适当的内存顺序模型，以确保正确性和性能。不同的内存顺序模型会引入不同程度的同步开销，因此需要根据实际情况权衡选择。了解这些内存顺序模型可以帮助你在多线程编程中更好地控制操作的执行顺序和可见性，每种模型具体的例子我们后面再慢慢总结。 总结 常见的内存顺序模型前三种：std::memory_order_relaxed、std::memory_order_acquire、std::memory_order_acquire 常见的内存顺序模型后三种：std::memory_order_release、std::memory_order_acq_rel、std::memory_order_seq_cst 其中 `std::memory_order_consume`` 在实际中难以实现，已经在 C++20 中被弃用 编译器和处理器对代码进行优化时可能会引入指令重排，这可能会导致锁保护下的共享资源出现问题 内存顺序模型通过规定操作的执行顺序，可以避免上一个问题 内存顺序和原子操作的引入，是为了无锁的并发编程，提高并发编程的效率 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 或许我就不在你规划的未来或许她才是你唯一的偏爱或许我就不该跟你赌未来或许就不会深陷苦海 2023-8-24 22:57:11]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>内存顺序</tag>
        <tag>无锁编程</tag>
        <tag>指令重排</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下Shell脚本中定义函数]]></title>
    <url>%2Fblog%2F2023%2F08%2F15%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8BShell%E8%84%9A%E6%9C%AC%E4%B8%AD%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言说实话，这是我第一次用Shell来定义函数，之前也写了很多shell脚本，但是体量都不大，所以基本上是按照需求罗列了多个命令来实现的，这次也是一样，但是我发现重复的地方太多了，所以还是要拿出看见本事“抽象函数”来解放我懒惰的手指 具体需求 检测特定名称的进程是否存在，若不存在则重新启动这个进程 仿照着之前的类似的脚本我写出了以下函数 check_and_fix_process，仅有一个需要注意的点就是Shell函数不需要定义参数，如果用到参数直接是用 $1、 $2、 $3……依次来代替就可以了 12345678910111213141516171819202122232425262728293031#!/bin/bash# 检查并修复进程的函数function check_and_fix_process() &#123; # $1 进程名 # $2 进程ID列表文件名 # $3 进程ID列表文件md5值 datetime=$(date +%y%m%d%H%M%S) ps aux |grep "$1" |grep -v grep |awk '&#123;print $15&#125;' |awk -F '=' '&#123;print $2&#125;'|sort -n &gt; ~/tmplist a=$(md5sum ~/tmplist |awk '&#123;print $1&#125;') b="$3" if [[ $a = $b ]]; then echo "check [$1 $2 $3] ok" else tmp=$(diff ~/"$2" ~/tmplist |grep ^\&lt; |awk '&#123;print $2&#125;' &gt; /tmp/tmplist) for i in `cat /tmp/tmplist` do cd ~/test &amp;&amp; grep "ID=$i" start.sh | xargs -I &#123;&#125; sh -c '&#123;&#125;' echo "$datetime $i" &gt;&gt;~/fixtxt python3 ~/send_msg.py -c "process:$i crashed; fixtime:$datetime" done fi&#125;# check all process listcheck_and_fix_process "AProcess" "aprocesslist" "a887eedb72472f2ad152e5e91a0727da"check_and_fix_process "BProcess" "bprocesslist" "a887eedb72472f2ad152e5e91a0727db" ChatGPT读代码如果对于这段代码有不理解的地方，可以请ChatGPT来帮忙，以下内容是它的解释，不得不说真是太强大了！身为此段代码的实现者，我都描述不了这么详细 这段代码是一个 Bash 脚本，用于检查指定的进程是否正常运行，并在发现异常时进行修复操作。下面逐行解释代码： function check_and_fix_process() {: 定义了一个名为 check_and_fix_process 的 Bash 函数。 # $1 进程名: 这是一个注释，表示函数的第一个参数是进程名。 # $2 进程ID列表文件名: 这是一个注释，表示函数的第二个参数是进程ID列表文件的文件名。 # $3 进程ID列表文件md5值: 这是一个注释，表示函数的第三个参数是进程ID列表文件的MD5值。 datetime=$(date +%y%m%d%H%M%S): 获取当前日期和时间，并将其格式化为”YYMMDDHHMMSS”的形式，存储在变量 datetime 中。 ps aux |grep &quot;$1&quot; |grep -v grep |awk &#39;{print $15}&#39; |awk -F &#39;=&#39; &#39;{print $2}&#39;|sort -n &gt; ~/tmplist: 这一行通过一系列的管道命令来查找指定进程名 $1 的进程，提取其中的某个列（可能是进程ID），然后排序，将结果保存到文件 ~/tmplist 中。 a=$(md5sum ~/tmplist |awk &#39;{print $1}&#39;): 计算文件 ~/tmplist 的 MD5 值，并将结果保存在变量 a 中。 b=&quot;$3&quot;: 将第三个参数值（进程ID列表文件的MD5值）保存在变量 b 中。 if [[ $a = $b ]]; then: 如果变量 a 的值等于变量 b 的值（即进程ID列表文件的MD5值相匹配），则执行下面的代码块。 echo &quot;check [$1 $2 $3] ok&quot;: 输出一条信息表示检查通过。 else: 如果进程ID列表文件的MD5值不匹配，则执行下面的代码块。 tmp=$(diff ~/&quot;$2&quot; ~/tmplist |grep ^\&lt; |awk &#39;{print $2}&#39; &gt; /tmp/tmplist): 比较进程ID列表文件（路径为 ~/&quot;$2&quot;)和当前的进程ID列表文件 (~/tmplist)，将不匹配的部分提取出来并保存在临时文件 /tmp/tmplist 中。 for i in $(cat /tmp/tmplist): 循环遍历临时文件 /tmp/tmplist 中的每一行内容。 do: 循环体开始。 cd ~/test &amp;&amp; grep &quot;ID=$i&quot; start.sh | xargs -I {} sh -c &#39;{}&#39;: 这一行的作用是切换到目录 ~/test 并在 start.sh 文件中查找包含字符串 “ID=$i” 的行，然后执行找到的行（通常是一些命令）。 echo &quot;$datetime $i&quot; &gt;&gt;~/fixtxt: 将当前日期时间和进程ID添加到文件 ~/fixtxt 中。 python3 ~/send_msg.py -c &quot;process:$i crashed; fixtime:$datetime&quot;: 调用 Python 脚本 send_msg.py 并传递一条消息，该消息包含进程ID以及修复操作的时间。 done: 循环结束。 fi: 条件分支结束。 }: 函数定义结束。 check_and_fix_process &quot;AProcess&quot; &quot;aprocesslist&quot; &quot;a887eedb72472f2ad152e5e91a0727da&quot;: 调用 check_and_fix_process 函数，检查名为 “AProcess” 的进程是否正常运行，使用文件 “aprocesslist” 作为进程ID列表文件，并传递预期的MD5值。 check_and_fix_process &quot;BProcess&quot; &quot;bprocesslist&quot; &quot;a887eedb72472f2ad152e5e91a0727db&quot;: 同上，检查名为 “BProcess” 的进程是否正常运行，使用文件 “bprocesslist” 作为进程ID列表文件，并传递预期的MD5值。 总结 Shell脚本中的函数不需要定义参数，如果用到参数直接是用 $1、 $2、 $3……依次来代替就可以 Shell脚本函数中使用 $1 作为参数，并不会与内部命令的 $1 冲突，因为Shell在不同的上下文中，变量的作用域是不同的 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 看我ge，看我nai，看我jiu，玩的开心吗？ 2023-8-15 22:41:32]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>运维</tag>
        <tag>服务器</tag>
        <tag>Shell函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu删除大量磁盘文件时，内存cache降free升而创造的漂亮曲线]]></title>
    <url>%2Fblog%2F2023%2F08%2F15%2FUbuntu%E5%88%A0%E9%99%A4%E5%A4%A7%E9%87%8F%E7%A3%81%E7%9B%98%E6%96%87%E4%BB%B6%E6%97%B6%EF%BC%8C%E5%86%85%E5%AD%98cache%E9%99%8Dfree%E5%8D%87%E8%80%8C%E5%88%9B%E9%80%A0%E7%9A%84%E6%BC%82%E4%BA%AE%E6%9B%B2%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[前言最近在做压力测试，对磁盘、CPU、内存等指标比较敏感，因为产生了大量的日志文件需要定期删除，于是写了个定时清理磁盘的脚本，今天早上发现内存曲线居然产生了一个漂亮的图形，不知道你们想到了什么？看到这曲线我首先想到的是《编程之美》中那个“让CPU占用率曲线听你指挥” 的面试题，是不是得加一个想内存使用率曲线听你指挥啊？ 图形本尊我所说的漂亮曲线入下图所示，是不是有一种对称美？ 搭配这条曲线的还有当时的磁盘占用曲线，两幅图对比基本可以确定这两个指标是相关的 产生的原因关于实际的原因我不能给出准确的解释，所以跑去问了GPT，它的回答内容如下，如果有不同意的小伙伴欢迎来反驳： 在Ubuntu或其他Linux操作系统中，当你删除大量磁盘文件时，可能会看到内存缓存（cache）降低而”free”（可用内存）升高的情况。这是由于Linux的内存管理机制导致的，这个机制是为了最大程度地提高系统性能。 当你删除文件时，内核会将这些文件从文件系统中删除，但为了提高文件访问的效率，已经读取的数据可能会被保留在内存的缓存中。这样，如果在不久的将来需要访问相同的文件，可以直接从内存缓存中获取，而不必从磁盘读取。这个缓存被称为”page cache”，它占用一部分内存。 当你删除大量文件时，这些文件所占用的内存缓存将被释放，以便更好地利用内存来满足其他正在运行的程序的需求。因此，内存缓存的降低是一个正常现象。 同时，”free”命令在Linux中显示的”free”内存指的是没有被使用的内存，包括未被分配的内存和被内核回收的缓存。因此，当你删除大量文件时，已释放的缓存会被计算在”free”内存中，导致”free”内存升高。 这种行为是Linux内存管理的一部分，它旨在在系统性能和内存使用之间找到平衡。当系统需要更多内存时，它可以释放缓存来满足正在运行的程序的需求。所以，不要担心这种情况，它是Linux操作系统的正常行为。 补一个长周期对比图 总结 想要控制好计算机资源产生自定义曲线，控制好“消耗”就可以了，但是如何控制是门学问，需要考虑的各种相关因素 为了提高文件访问的效率，已经读取的数据可能会被保留在内存的缓存中，其被称为”page cache”，便于下次访问直接从内存缓存中获取 当删除大量文件时，已释放的缓存会被计算在”free”内存中，导致”free”内存升高 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 风急天高猿啸哀，渚清沙白鸟飞回。无边落木萧萧下，不尽长江滚滚来~ 2023-8-12 10:41:18]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>DevOps</tag>
        <tag>服务器性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用游戏运营指标DAU、LTV及参考范围]]></title>
    <url>%2Fblog%2F2023%2F08%2F11%2F%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E8%BF%90%E8%90%A5%E6%8C%87%E6%A0%87DAU%E3%80%81LTV%E5%8F%8A%E5%8F%82%E8%80%83%E8%8C%83%E5%9B%B4%2F</url>
    <content type="text"><![CDATA[前言作为游戏人免不了听到 DAU 、UP值、留存 等名词，并且有些名词听起来还很像，特别是一款上线的游戏，这些游戏运营指标是衡量游戏业务绩效和用户参与度的重要数据，想做一个合格的游戏人得花点时间了解一下，接下来会总结一些常见的游戏运营指标及其简称。 运营指标 DAU - 日活跃用户 (Daily Active Users)：每日登录游戏的独立用户数量 MAU - 月活跃用户 (Monthly Active Users)：每月登录游戏的独立用户数量 ARPU - 平均收入每用户 (Average Revenue Per User)：平均每个用户带来的收入 ARPDAU - 每日平均收入每用户 (Average Revenue Per Daily Active User)：每日平均每个活跃用户带来的收入 ARPPU - 平均付费用户收入 (Average Revenue Per Paying User)：平均每个付费用户带来的收入 Retention Rate - 留存率：测量特定时间段内用户的保留情况，通常以百分比表示 Churn Rate - 流失率：衡量用户流失的速度，通常以月度或年度为周期 Conversion Rate - 转化率：衡量用户从免费用户转化为付费用户的比率 LTV - 用户生命周期价值 (Lifetime Value)：预测一个用户在其使用周期内可能带来的总收入 CAC - 用户获取成本 (Customer Acquisition Cost)：获取一个新用户所花费的成本 ROI - 投资回报率 (Return on Investment)：衡量游戏营销和推广活动的效果 ARPPDAU - 每日平均收入每付费用户 (Average Revenue Per Paying Daily Active User)：每日平均每个付费活跃用户带来的收入。 ACPU - 平均成本每用户 (Average Cost Per User)：获取每个用户的平均成本 Conversion Funnel - 转化漏斗：衡量用户从首次接触游戏到最终付费的整个过程 UGC - 用户生成内容 (User-Generated Content)：用户创造的游戏内容，如自定义角色、地图等，对游戏社区的活跃度有影响 这些指标可以帮助游戏开发者和运营者了解游戏的商业表现和用户行为，从而优化游戏的设计和运营策略。 指标范围参考值知道了有上述的运营指标，但是一款合格的游戏上述指标应该达到多少呢？ 其实每款游戏的合格水平可能会因多种因素而有所不同，包括游戏类型、目标市场、营销策略以及开发成本等，并不是固定的标准，实际情况会受到众多因素的影响，包括游戏的质量、竞争情况、玩家喜好以及运营策略等，下面进给出一些参考值。 DAU - 日活跃用户：数百到数千，具体取决于游戏的类型和受众规模 MAU - 月活跃用户：数千到数万，取决于市场规模和竞争状况 ARPU - 平均收入每用户：几美元到十几美元，视付费玩家比例而定 ARPDAU - 每日平均收入每用户：几分到几毛美元，视玩家付费情况而定 ARPPU - 平均付费用户收入：十美元到数十美元，取决于游戏内购的设计和价值 Retention Rate - 留存率：20%-50% 之间，高于 50% 为很好的表现。 Churn Rate - 流失率：低于 10% 为很好的表现。 Conversion Rate - 转化率：2%-5% 之间，高于 5% 为不错的表现。 LTV - 用户生命周期价值：数十美元到数百美元，取决于玩家的长期付费情况。 CAC - 用户获取成本：通常 LTV 的 1/3 到 1/2，以确保收益能覆盖用户获取成本。 这些值仅供参考，并不适用于所有情况。游戏开发者和运营者需要根据自己的情况制定合适的目标，并随着游戏的运营进行不断优化。同时，密切关注竞争对手的表现也是非常重要的。 留存指标的意义留存率是衡量用户参与度和游戏粘性的重要指标之一。它衡量了一段时间内用户的持续参与情况，通常以百分比表示。针对回合制卡牌游戏，留存率的及格线会受到多种因素的影响，包括游戏的类型、受众群体、市场竞争、游戏质量、运营策略等等 次日留存率：在 30% - 40% 以上被视为相当不错的表现。这意味着有相当比例的玩家在第一次体验游戏后仍然对游戏感兴趣，并愿意在第二天继续游戏。 3日留存率：通常在 20% - 40% 之间被视为良好的表现。这表示在游戏初始安装后的头三天内，有相当比例的玩家愿意继续登录游戏。 7日留存率：较高的及格线通常在 15% - 30% 左右。相比于3日留存，7日留存更能体现游戏对玩家的长期吸引力。 月留存率：这个指标较难界定“及格线”，但在 10% - 20% 以上的范围内被视为不错的表现。月留存更考验游戏的长期吸引力和社区活跃度。 需要注意的是，这些及格线可能会根据不同的情况有所变化。例如，对于新发布的游戏，初始留存率可能会较高，但随着时间推移，可能会逐渐下降。此外，针对特定类型的玩家群体，留存率也可能有所不同。 这三个留存率指标都有助于了解游戏的玩家保持情况，但值得注意的是，不同类型的游戏可能会有不同的留存率标准，而且这些指标可能会受到市场、竞争、游戏类型以及运营策略的影响。提高留存率是游戏开发者和运营者追求的目标，因为更高的留存率通常意味着更好的长期经营和用户忠诚度。 总结 DAU - 日活跃用户 (Daily Active Users)：每日登录游戏的独立用户数量 ARPU - 平均收入每用户 (Average Revenue Per User)：平均每个用户带来的收入 LTV - 用户生命周期价值 (Lifetime Value)：预测一个用户在其使用周期内可能带来的总收入 Retention Rate - 留存率：测量特定时间段内用户的保留情况，通常以百分比表示 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 挥手自兹去，萧萧班马鸣~ 2023-8-10 22:34:28]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>游戏</tag>
        <tag>大数据</tag>
        <tag>运行指标</tag>
        <tag>DAU</tag>
        <tag>LTV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Ubuntu系统下修改limits.conf不生效]]></title>
    <url>%2Fblog%2F2023%2F08%2F10%2F%E5%9C%A8Ubuntu%E7%B3%BB%E7%BB%9F%E4%B8%8B%E4%BF%AE%E6%94%B9limits-conf%E4%B8%8D%E7%94%9F%E6%95%88%2F</url>
    <content type="text"><![CDATA[前言最近遇到的一个问题，在Ubuntu系统下修改/etc/security/limits.conf不生效，查了多种资料都说不用重启，但是我改完就是不生效，多次尝试之后发现Ubuntu系统有毒。 尝试过程通过 ulimit -n 命令可以查询到当前系统单个进程允许打开的文件描述符： 12$ ulimit -n1024 才1024个，放到游戏服务器上显然不太够用，于是使用命令 ulimit -n 65535 手动调大了，但是这种方式只能对当前登录session有效，下次新的连接再启动进程数值又回去了，想要一劳永逸，就得修改/etc/security/limits.conf文件，打开文件加入下面的内容： 12* soft nofile 65535* hard nofile 65535 关闭文件，退出重新登录，运行 ulimit -n 命令发现还是1024，奇怪！ 后续有尝试了网络上常常提到的 pam_limits 配置，按步骤修改后不好使，查找 /etc/systemd/user.conf 文件配置也无法对应，最后查到一种解决方式，将 limits.conf 文件中的 * 写成具体的用户名就好了 12root soft nofile 65535root hard nofile 65535 之前我在CentOS系统上一直是用星号的，每想到在Ubuntu系统上被坑了一次 总结 ulimit -n 65535 命令可以直接将单个进程打开的文件描述符限制扩大到65535 修改 /etc/security/limits.conf 文件不需要重启系统，退出ssh重新登录记录 在CentOS中修改 limits.conf 文件中的用户名可以用 *，但是在Ubuntu中需要写具体用户名 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 高位易跌倒，高位易昏头，面对巨大的诱惑，想要做到不被拉下水是在太难了~ 2023-8-10 22:34:28]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>运维</tag>
        <tag>ubuntu</tag>
        <tag>root</tag>
        <tag>ulimit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下运行脚本时常用＞/dev/null 2＞&1 &这一串的作用]]></title>
    <url>%2Fblog%2F2023%2F08%2F01%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E8%BF%90%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%97%B6%E5%B8%B8%E7%94%A8%EF%BC%9E-dev-null-2%EF%BC%9E-1-%E8%BF%99%E4%B8%80%E4%B8%B2%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言大雨哗哗哗，叫车无人接，把最近用到的命令总结一下，方便日后的拿来主义。 &gt;/dev/null 2&gt;&amp;1&gt;/dev/null 2&gt;&amp;1 &amp; 是一个常见的 Shell 命令行用法，用于在 Linux 或类 Unix 系统上运行一个程序并将其输出（标准输出和标准错误输出）重定向到 /dev/null，同时将程序在后台（background）运行。 接下来可以逐步解释下各部分的作用： &gt;：重定向符号，用于将输出重定向到指定的文件或设备。 /dev/null：在 Unix 系统上，/dev/null 是一个特殊的设备文件，它会将所有写入它的内容都丢弃，相当于一个空黑洞。将输出重定向到 /dev/null 就意味着将输出丢弃，不会显示在终端或记录到文件中。 2&gt;：重定向标准错误输出的符号。2 表示标准错误流，也称为文件描述符 2。 &amp;1：将标准错误输出重定向到标准输出的符号。&amp;1 表示文件描述符 1，而标准输出的文件描述符也是 1。这样，就将标准错误输出和标准输出合并到一起，都重定向到了 /dev/null。 &amp;：在命令行中，&amp; 符号用于将命令置于后台运行，使得当前终端可以继续输入其他命令，而不需要等待前台命令的完成。 综合起来，&gt;/dev/null 2&gt;&amp;1 &amp; 的作用是将某个程序在后台运行，并将其标准输出和标准错误输出都丢弃，不在终端中显示或记录。这样做常常用于运行不需要显示输出的程序或脚本，并且让它在后台运行，不占用终端的输入输出。 去除文件夹下所有文本文件的行末空格想要达到这个目的有多种实现方式，比如： 1find . -type f -exec egrep -l " +$" &#123;&#125; \; | grep -v xxx | xargs sed -i 's/[ ]*$//g' 再比如： 1find xxx -type f -name "*.txt" -exec sed -i 's/[ \t]*$//' &#123;&#125; \; 主要部分的含义： -type f：-type f 参数用于限制 find 命令只查找普通文件（非目录、符号链接等）。 -name &quot;*.txt&quot;：-name &quot;*.txt&quot; 参数用于匹配文件名，这里匹配以 .txt 结尾的文件。您可以根据实际需要修改为其他文件扩展名。 -exec：-exec 参数用于对找到的每个文件执行后面的命令。 sed -i &#39;s/[ \t]*$//&#39; {}：sed 命令用于进行文本替换。-i 参数表示直接在原文件上进行修改（in-place），而不是在终端输出。s/[ \t]*$// 是一个替换表达式，用于匹配行末的空格（包括空格和制表符），将其替换为空。 {}：find 命令中的占位符，代表找到的每个文件名。 \;：find 命令中的结束符，表示 -exec 参数的结束。 还可以利用管道来实现，使用管道 (Pipeline) 去掉一个文件夹下所有文本文件行末的空格，可以结合 find 命令和 sed 命令来完成。管道允许将一个命令的输出作为另一个命令的输入，以实现更复杂的文本处理，如下： 1find xxx -type f -name "*.txt" -print0 | xargs -0 sed -i 's/[ \t]*$//' 各部分的含义： find xxx -type f -name &quot;*.txt&quot; -print0：find 命令查找指定目录 xxx 下的所有以 .txt 结尾的文本文件，并使用 -print0 参数将文件名以空字符作为分隔符打印出来。这是为了处理文件名中可能包含空格或特殊字符的情况。 |：管道符号，将 find 命令的输出传递给下一个命令。 xargs -0：xargs 命令用于接收管道的输入，并将其作为参数传递给后面的命令。-0 参数表示使用空字符作为输入分隔符，与 find 命令的 -print0 参数相对应。 sed -i &#39;s/[ \t]*$//&#39;：sed 命令用于进行文本替换，同样使用 -i 参数进行直接修改。s/[ \t]*$// 是一个替换表达式，用于去掉行末的空格。 与之前的命令相比，这个命令通过管道一次性处理了多个文件，效率可能会更高一些。 总结 将某个程序在后台运行，并将其标准输出和标准错误输出都丢弃，可以在运行命令后加 &gt;/dev/null 2&gt;&amp;1 去除文件行末空格 find . -type f -exec egrep -l &quot; +$&quot; {} \; | grep -v xxx | xargs sed -i &#39;s/[ ]*$//g&#39; ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 机会很重要，选择也很重要，在机会面前做出选择同样很重要，不要过分夸大未选择的那条路，很多时候即使重新选择一次，大部分人的道路不会发生改变，已经定型了… 我也不知道是熬到了司机来，还是把前边排队的人熬走了，现在还有3人排队~ 2023-8-1 01:47:54]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>运维</tag>
        <tag>服务器</tag>
        <tag>dev/null</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[之前好好的distcc突然报错distcc[18595] (dcc_pump_sendfile) ERROR: sendfile failed: Broken pipe怎么办]]></title>
    <url>%2Fblog%2F2023%2F07%2F21%2F%E4%B9%8B%E5%89%8D%E5%A5%BD%E5%A5%BD%E7%9A%84distcc%E7%AA%81%E7%84%B6%E6%8A%A5%E9%94%99distcc-18595-dcc-pump-sendfile-ERROR-sendfile-failed-Broken-pipe%E6%80%8E%E4%B9%88%E5%8A%9E%2F</url>
    <content type="text"><![CDATA[前言distcc 是一个用于分布式编译的工具，它可以将编译任务分发到多台计算机上，从而加快代码编译的速度。distcc 允许多台计算机共同参与编译过程，利用分布式系统的优势来并行处理编译任务，从而提高编译速度和性能。 在传统的编译过程中，代码被发送到一台计算机上，然后由该计算机上的编译器进行编译。这种方式在大型项目中可能会消耗大量时间，特别是当项目变得复杂并且代码量增加时，编译时间会显著增加。 distcc 解决了这个问题，它通过在局域网内或远程服务器上设置 distcc 客户端和 distccd 服务器来实现分布式编译。distcc 客户端将编译任务发送到 distccd 服务器，而服务器上的 distccd 守护进程将任务分发给多个可用的编译器进行并行编译。一旦编译完成，编译结果将发送回到 distcc 客户端，然后再进行链接等后续处理。 通过 distcc 的使用，编译过程可以更快地完成，节省了开发人员的时间，并且利用了局域网或网络上的多台计算机的计算资源，提高了编译效率。 distcc支持多种编程语言，包括C，C ++，Objective-C和Fortran。它是一个开源软件，可以在Linux，Unix和Windows等平台上运行。 问题前面主要简单的叙述了什么是distcc，我遇到的问题就是前几天用的好好的distcc，最近总是报出以下错误： distcc[18595] (dcc_pump_sendfile) ERROR: sendfile failed: Broken pipedistcc[18595] (dcc_readx) ERROR: unexpected eof on fd6distcc[18595] (dcc_r_token_int) ERROR: read failed while waiting for token “DONE”distcc[18595] (dcc_r_result_header) ERROR: server provided no answer. Is the server configured to allow access from your IP address? Is the server performing authentication and your client isn’t? Does the server have the compiler installed? Is the server configured to access the compiler?distcc[18595] Warning: failed to distribute /home/demo/workspace/x_server/gameserver/battle/effect/addbuffroundeffectexecutormodule.cpp to 10.12.7.181, running locally instead 先是Google一番没找到解决方案，又去问了ChatGPT，给出的答案偏向于权限问题，这就很奇怪，我之前用的好好的，为啥突然会出现权限问题，他可能是错误描述的影响： ERROR: server provided no answer. Is the server configured to allow access from your IP address? Is the server performing authentication and your client isn’t? Does the server have the compiler installed? Is the server configured to access the compiler? 解决方案一开始我也是朝着配置和权限来努力的，可是怎样重启服务或是修改配置都不好使，查了防火墙也没问题，telnet 3262端口也是通的，最终我把目光定位到了sendfile failed: Broken pipe 这句话，不会是磁盘满了吧？df一查还真是，唉，折腾了好久~ -exec command ; 和-exec command {} +区别这篇内容有点少，补充一个我今天刚学到的知识点吧~ 在 find 命令中，-exec command ; 和 -exec command {} + 是两种不同的方式来执行指定的命令。 -exec command ; 这种方式将对 find 命令找到的每个文件依次执行一次指定的命令。每找到一个匹配的文件，都会执行一次该命令。例如： 1find /path/to/search -type f -name "*.txt" -exec echo &#123;&#125; \; 上述命令会在 /path/to/search 目录及其子目录中搜索所有扩展名为 .txt 的文件，并依次输出每个文件的名称。 -exec command {} + 这种方式也会执行指定的命令，但是它会将多个找到的文件一起传递给命令进行一次执行。这种方式通常比使用 ; 更高效，因为它可以将多个文件组合在一起，减少了命令的调用次数。例如： 1find /path/to/search -type f -name "*.txt" -exec cat &#123;&#125; + 上述命令会在 /path/to/search 目录及其子目录中搜索所有扩展名为 .txt 的文件，并将它们一起传递给 cat 命令进行合并输出。 如果只需要针对每个文件执行一次命令，可以使用 -exec command ;。如果想要将多个文件组合在一起一次性执行命令，可以使用 -exec command {} +。选择哪种方式取决于具体的需求和效率考虑。 总结 当distcc突然不好使并提示sendfile failed: Broken pipe的时候去看看服务器磁盘是不是满了 使用 -exec command ; 将对每个匹配的文件分别执行一次命令。 使用 -exec command {} + 将多个匹配的文件一起传递给命令执行。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 今朝有今朝醉不是一种豁达，或许是一种无可奈何吧~ 2023-7-21 22:31:00]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>c++</tag>
        <tag>distcc</tag>
        <tag>分布式编译</tag>
        <tag>编译加速</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下rsync命令中两个目录结尾的斜杠‘/‘到底有什么作用]]></title>
    <url>%2Fblog%2F2023%2F07%2F20%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8Brsync%E5%91%BD%E4%BB%A4%E4%B8%AD%E4%B8%A4%E4%B8%AA%E7%9B%AE%E5%BD%95%E7%BB%93%E5%B0%BE%E7%9A%84%E6%96%9C%E6%9D%A0%E2%80%98-%E2%80%98%E5%88%B0%E5%BA%95%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言每次使用文件拷贝 cp 命令或者同步 rsync 命令时都会有些疑惑，不知道路径末尾的 /需不需要加，隐约感觉到有些不同，但具体的区别却不太清楚，需要实际操作一次才能知道了，下面就来试一试。 rsync 命令的基础格式为 rsync [OPTION...] SRC... [DEST]，可用于本地文件的同步，如果想同步远程文件也可以，将 SRC 或者 DEST 改成带有IP的远程地址就可以了，但是最多支持一个远端地址，不能两个目录都写成remote格式，其实这个命令之前也用过好几次了，比如 《linux环境下利用rsync+find实现同步指定时间段文件》，这次来看看结尾的 / 到底有什么影响。 准备测试环境测试环境很简单，只要准备源目录和目标目录就可以了，为了测试各种情况，我需要创建一个比较丰富的目录结构，需要测试的情况有原目录A结尾有无 /，目标目录B结尾有无 /，目标目录B中有无同名A文件夹，A文件夹是否为空，目标目录B中有无同名A文件，目标目录不存在，各种情况组合应该是 2 * 2 * 5 = 20种，这不是全部的情况，但已经覆盖大部分想要测试的问题了，因为使用 rsync 通常是同步目录，暂时不考虑源和目标是文件的情况。 测试目录结构如下： 事先说明，以下的每项测试都是在这个原始的基础上进行的，每项测试完成会将目录还原，用以比较不同命令参数下结果的区别，测试选项统一使用 -a --delete。 rsync测试测试过程分小组进行，便于对比 源目录和目标目录结尾都没斜杠 SRC DEST这种情况的测试结论为，不论目标目录是否存在（没有就创建），都会将源目录拷贝到目标目录之内形成同名文件夹，如果原来有同名文件或文件夹就覆盖掉，如果不加 --delete 就会合并。 源目录结尾有斜杠而目标目录结尾没斜杠 SRC/ DEST为了能更快的看到效果，这次我直接一个命令把5种情况都测试了，结论是如果源目录结尾有斜杠，会将源目录中的内容同步到目标目录之内，覆盖掉目标目录之内原有的内容，如果不加 --delete 就会和目标目录合并。 源目录结尾没有斜杠而目标目录结尾有斜杠 SRC DEST/还是采用了一个命令测试5种情况的方式，与第一组测试结果一致，都是在目标目录下增加了源目录文件夹，内容保持一致。 源目录结尾和目标目录结尾都有斜杠 SRC/ DEST/测试结果与第二组一致，不再赘述。 测试结论 在 rsync 中，执行结果只与原始目录结尾有无斜杠有关，如果不传递原始目录（也称为源目录）的末尾斜杠（/），它将尝试在目标目录中创建源目录的副本，而不是将源目录的内容复制到目标目录中。换句话说，如果没有末尾斜杠，rsync 会尝试在目标目录下创建一个与源目录同名的目录，并将源目录的内容复制到这个同名目录中。 可以简单记忆为，如果原始目录没加末尾斜杠，表示将整个目录拷贝到目标文件夹，如果原始目录加了末尾斜杠，表示将原始目录中的内容拷贝到目标文件夹。 cp和mv两个命令测试看了上面的测试如果你觉得明白了，那么下面的内容可能会让你再次感到混乱，其实每次使用 cp 命令时还不怎么担心，但是使用mv命令时总是害怕把什么东西覆盖了，还是利用上面的目录结构来测试，只是就不贴具体的测试过程了，我把结论写一下。 cp 和 mv 仅考虑参数均为目录的情况，两个命令中无论是原始目录还是目标目录，结尾是否加斜杠 / 没有影响，都表示目录本身。 cp 的目标目录不存在，则创建一个与原始目录内容一致的目标文件夹 cp 的目标目录若存在，则会将原始目录拷贝到目标文件夹下，若在目标文件夹下存在同名目录则合并，若在目标文件夹下存在同名文件，则执行失败，报错 cp: cannot overwrite non-directory &#39;xxx/vvv&#39; with directory &#39;vvv&#39; mv 的目标目录不存在，则创建一个与原始目录内容一致的目标文件夹，并将原始目录移除 mv 的目标目录若存在，则将原始目录移动到目标文件夹之下，若在目标文件夹下存在同名空目录则移动成功，若存在同名非空目录则执行失败，报错 mv: cannot move &#39;vvv&#39; to &#39;zzz/vvv&#39;: Directory not empty，若在目标文件夹下存在同名文件，也执行失败，报错 mv: cannot overwrite non-directory &#39;xxx/vvv&#39; with directory &#39;vvv&#39; 所以这样看来mv用于目录时也不会覆盖掉原始的内容，以后可以放心大胆的用了 总结 rsync 命令只有原始目录结尾的斜杠能影响结果，没有斜杠整个原始目录拷贝过去，有斜杠只拷贝原始目录里的内容到目标文件夹 cp 和 mv 在目标目录不存在是都能执行成功，cp 是复制一份到形成目标目录，mv是将原始目录移动到目标目录 cp 的目标目录若存在，其中不能包含和原始目录同名的文件，如果文件夹同名会将内容合并 mv 的目标目录若存在，其中不能包含和原始目录同名的文件或非空同名文件夹，如果存在同名空文件夹则会将内容合并 mv 命令操作目录时不会随便覆盖，可以放心使用 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 到底是坚持获得了成功，还是成功之后才能坚持下去，其实要看你怎么定义成功了，不怨我们浮躁，很多先机已经被人家占完了~ 2023-7-20 22:32:42]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>运维</tag>
        <tag>rsync</tag>
        <tag>mv</tag>
        <tag>cp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的 i++ 和 ++i 这两种自增运算不是原子操作]]></title>
    <url>%2Fblog%2F2023%2F07%2F04%2FC-%E4%B8%AD%E7%9A%84-i-%E5%92%8C-i-%E8%BF%99%E4%B8%A4%E7%A7%8D%E8%87%AA%E5%A2%9E%E8%BF%90%E7%AE%97%E4%B8%8D%E6%98%AF%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言C++中的 i++ 和 ++i 这两种自增运算是不是原子操作，突然被这么一问竟有点不知所措，这么“简单”的操作应该是原子的吧，但是好像有读又有写应该不是原子操作，原子操作就是那种刷一下就能完成的操作，准确来描述就是一个操作不可再分，要完成都完成不能存在中间态，咦？怎么听起来和事务这么像？那么 i++ 和 ++i 是不是原子操作我们看它是否满足不可再分就行了。 原子操作怎么看是否可再分呢？想到一个办法，看一个操作是否可再分，直接看汇编是不是就行了，比如一个赋值语句： 12345int main()&#123; int i = 110; return 0;&#125; 使用 x86-64 gcc 13.1编译后生成的汇编： 1234567main: push rbp mov rbp, rsp mov DWORD PTR [rbp-4], 110 mov eax, 0 pop rbp ret int i = 110; 被汇编成了 mov DWORD PTR [rbp-4], 110 看起来是一句，没啥问题，再看 i++: 123456int main()&#123; int i = 110; i++; return 0;&#125; 12345678main: push rbp mov rbp, rsp mov DWORD PTR [rbp-4], 110 add DWORD PTR [rbp-4], 1 mov eax, 0 pop rbp ret i++ 也被汇编成了一句 add DWORD PTR [rbp-4], 1，居然也是一句，那么这是原子的吗？我们换个编译器看看，使用 x64 msvc 19.35 生成的汇编如下： 123456789101112i$ = 0main PROC$LN3: sub rsp, 24 mov DWORD PTR i$[rsp], 110 ; 0000006eH mov eax, DWORD PTR i$[rsp] inc eax mov DWORD PTR i$[rsp], eax xor eax, eax add rsp, 24 ret 0main ENDP 看吧，这里被翻译成了3句，这肯定不是原子操作了，那返回来看在gcc编译时生成 add DWORD PTR [rbp-4], 1的这一句，就是原子操作吗？ 准确来表述是这样的 add DWORD PTR [rbp-4], 1这条汇编指令本身是原子的，但是在多线程环境中，对于变量的自增操作需要使用适当的同步机制（如互斥锁、原子类型等）来确保原子性和线程安全性。 如果在单核机器上，上述不加锁不会有问题，但到了多核机器上，这个不加锁同样会带来意外后果，两个CPU可以同时执行这条指令，但是两个执行以后，却可能出现只自加了一次 证明++i不是原子操作的例子写个简单的例子，两个线程同时执行i++自增操作，看最后的结果是否符合预期： 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;thread&gt;int val = 0;void f1(int n)&#123; for (int i = 0; i &lt; n; ++i) ++val;&#125;int main(int argc, char* argv[])&#123; int n = 100000000; if (argc &gt; 1) n = atoi(argv[1]); std::thread t1(f1, n); std::thread t2(f1, n); t1.join(); t2.join(); std::cout &lt;&lt; "The final value is [" &lt;&lt; val &lt;&lt; "] for 2 threads running [" &lt;&lt; n &lt;&lt; "] times." &lt;&lt; std::endl; return 0;&#125; 执行结果如下： 1234567891011$ ./iplusplus 1000The final value is [2000] for 2 threads running [1000] times.$ ./iplusplus 10000The final value is [20000] for 2 threads running [10000] times.$ ./iplusplus 100000The final value is [117784] for 2 threads running [100000] times.$ ./iplusplus 1000000The final value is [1271769] for 2 threads running [1000000] times. 从运行结果得知，起初1000次和10000次还没出现竞态条件问题，当次数扩大到100000次时，2个线程最终自增的结果只有117784 保证原子操作还是上面的例子，怎样改成原子操作呢？这时可以利用 std::atomic 模板类，只需将上述例子中的 val 变量修改成 std::atomic&lt;int&gt; val(0); 即可： 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;atomic&gt;std::atomic&lt;int&gt; val(0);void f1(int n)&#123; for (int i = 0; i &lt; n; ++i) ++val;&#125;int main(int argc, char* argv[])&#123; int n = 100000000; if (argc &gt; 1) n = atoi(argv[1]); std::thread t1(f1, n); std::thread t2(f1, n); t1.join(); t2.join(); std::cout &lt;&lt; "The final value is [" &lt;&lt; val &lt;&lt; "] for 2 threads running [" &lt;&lt; n &lt;&lt; "] times." &lt;&lt; std::endl; return 0;&#125; 再编译运行试试： 1234567891011$ ./iplusplus 100The final value is [200] for 2 threads running [100] times.$ ./iplusplus 10000The final value is [20000] for 2 threads running [10000] times.$ ./iplusplus 1000000The final value is [2000000] for 2 threads running [1000000] times.$ ./iplusplus 100000000The final value is [200000000] for 2 threads running [100000000] times. 这样就解决了i++不是原子操作的问题，这里还可以将 ++val 写成 val.fetch_add(1) 表示原子加，其实 std::atomic 类实现了 operator++ 调用的就是 fetch_add(1)： 1234567_GLIBCXX_ALWAYS_INLINE value_typeoperator++(int) const noexcept&#123; return fetch_add(1); &#125;value_typeoperator++() const noexcept&#123; return __atomic_impl::__add_fetch(_M_ptr, value_type(1)); &#125; 总结 i++ 和 ++i 不是原子操作，执行命令时包含内存读取，变量递增，回写内存三步，所以存在 data race 即使被汇编成一句 add DWORD PTR [rbp-4], 1一句代码在多核CPU上也会导致结果的不确定性或错误 想要 i++ 变成原子操作只需要定义成 std::atomic 模板类的对象即可，逻辑代码几乎无需修改 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 我们常常把求之不得的东西称之为理想~ 2023-7-4 09:51:32]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>automic</tag>
        <tag>竞态条件</tag>
        <tag>自增操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++访问std::variant类型数据的几种方式]]></title>
    <url>%2Fblog%2F2023%2F06%2F23%2FC-%E8%AE%BF%E9%97%AEstd-variant%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言std::variant（可变体） 是 C++17 中引入的一个新的类模板，提供了一种存储不同类型的值的方式，类似于之前版本中的 union（联合体），但可以存储非 POD 类型和类对象，能够在运行时进行类型检查和转换，但具有更多的功能和更高的类型安全性，今天来看一下存储在std::variant中的数据要怎么读取。 variant的简单使用可以参考cppreference网站的使用示例，也可以看看下面这个例子： 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;variant&gt;#include &lt;string&gt;int main()&#123; std::variant&lt;int, double, std::string&gt; value; value = 110; std::cout &lt;&lt; "The value is an integer: " &lt;&lt; std::get&lt;int&gt;(value) &lt;&lt; std::endl; value = 0.618; std::cout &lt;&lt; "The value is a double: " &lt;&lt; std::get&lt;double&gt;(value) &lt;&lt; std::endl; value = "hello world"; std::cout &lt;&lt; "The value is a string: " &lt;&lt; std::get&lt;std::string&gt;(value) &lt;&lt; std::endl; // value = true; // Compilation error: cannot convert type bool to any of the alternative types // std::get&lt;int&gt;(value) // std::bad_variant_access exception: value holds a different type return 0;&#125; 在示例程序中，定义了一个 std::variant 对象 value 来存储整型、浮点型和字符串类型中的任意一种，然后分别将 value 赋值为整型、浮点型和字符串类型，并使用 std::get 来获取对应的值，此时可以正常打印 value 对象中存储的值 当我们试图将 value 赋值为其它未在定义变量时指定的类型时，编译器将会报编译错误，而当我我们试图获取 value 中不存在的类型的值时，程序将会抛出 std::bad_variant_access 异常，可以使用 try-catch 已经捕获。 通过这段代码我们可以得知，使用std::variant可以方便地存储多种类型的数据，并且能够在运行时进行类型检查和转换，这使得代码更加清晰易读，便于维护。 variant相关函数和类 成员函数 index：返回 variant 中保存用类型的索引下标 valueless_by_exception：返回 variant 是否处于因异常导致的无值状态 emplace：原位构造 variant 中的值 非成员函数 visit：通过调用variant保存类型值所提供的函数对象获取具体值 holds_alternative：检查某个 variant 是否当前持有某个给定类型 std::get：以给定索引或类型读取 variant 的值，错误时抛出异常 get_if：以给定索引或类型，获得指向被指向的 variant 的值的指针，错误时返回空指针 辅助类 monostate：用作非可默认构造类型的 variant 的首个可选项的占位符类型（预防一些类型不提供无参构造函数） bad_variant_access：非法地访问 variant 的值时抛出的异常 variant_npos：非法状态的 variant 的下标 访问std::variant数据从前面提到的例子和函数说明，我们可以看到有多种方式来访问std::variant数据，接一下来一起总结一下： std::get搭配index函数使用123456789101112#include &lt;iostream&gt;#include &lt;variant&gt;int main()&#123; std::variant&lt;double, int&gt; value = 119; if (1 == value.index()) std::cout &lt;&lt; "The value is: " &lt;&lt; std::get&lt;1&gt;(value) &lt;&lt; std::endl; return 0;&#125; 先用 index() 查询 variant保存的类型索引，然后在通过 std::get&lt;NUMBER&gt;() 获取其中的值 std::get搭配std::holds_alternative函数使用123456789101112#include &lt;iostream&gt;#include &lt;variant&gt;int main()&#123; std::variant&lt;double, int&gt; value = 119; if (std::holds_alternative&lt;int&gt;(value)) std::cout &lt;&lt; "The value is: " &lt;&lt; std::get&lt;int&gt;(value) &lt;&lt; std::endl; return 0;&#125; 先通过 std::holds_alternative() 查询 variant保存的类型，然后在通过 std::get&lt;TYPE&gt;() 获取其中的值 std::get_if函数123456789101112#include &lt;iostream&gt;#include &lt;variant&gt;int main()&#123; std::variant&lt;double, int&gt; value = 119; if (auto p = std::get_if&lt;int&gt;(&amp;value)) std::cout &lt;&lt; "The value is: " &lt;&lt; *p &lt;&lt; std::endl; return 0;&#125; 直接使用 std::get_if 函数获取对应值的指针，如果类型不匹配会返回空指针 std::visit函数使用函数visit函数访问时，有点像使用std::sort这类函数，可以搭配自定义的结构（排序）重写operator()，让其变成可以被调用的函数对象，也可以定义lambda自带可执行特性。 自定义访问结构的写法1234567891011121314151617#include &lt;iostream&gt;#include &lt;variant&gt;int main()&#123; std::variant&lt;double, int&gt; value = 119; struct VisitPackage &#123; auto operator()(const double&amp; v) &#123; std::cout &lt;&lt; "The value is: " &lt;&lt; v &lt;&lt; std::endl; &#125; auto operator()(const int&amp; v) &#123; std::cout &lt;&lt; "The value is: " &lt;&lt; v &lt;&lt; std::endl; &#125; &#125;; std::visit(VisitPackage(), value); return 0;&#125; 定义lambda函数组重载1234567891011121314151617#include &lt;iostream&gt;#include &lt;variant&gt;template&lt;class... Ts&gt; struct overloaded : Ts... &#123; using Ts::operator()...; &#125;;template&lt;class... Ts&gt; overloaded(Ts...) -&gt; overloaded&lt;Ts...&gt;;int main()&#123; std::variant&lt;double, int&gt; value = 119; std::visit(overloaded &#123; [] (const double&amp; v) &#123; std::cout &lt;&lt; "The value is: " &lt;&lt; v &lt;&lt; std::endl; &#125;, [] (const int&amp; v) &#123; std::cout &lt;&lt; "The value is: " &lt;&lt; v &lt;&lt; std::endl; &#125; &#125;, value); return 0;&#125; 这种方式将多个lambda放到一起形成重载，进而达到了访问variant数据的目的。 overloaded是什么上文例子中的最后一个中使用到了 overloaded，这令人眼花缭乱的写法着实很诡异，不过我们可以从头来分析一下，最后两个例子中等价的两部分是 1234567struct VisitPackage&#123; auto operator()(const double&amp; v) &#123; std::cout &lt;&lt; "The value is: " &lt;&lt; v &lt;&lt; std::endl; &#125; auto operator()(const int&amp; v) &#123; std::cout &lt;&lt; "The value is: " &lt;&lt; v &lt;&lt; std::endl; &#125;&#125;;VisitPackage() 与 1234567template&lt;class... Ts&gt; struct overloaded : Ts... &#123; using Ts::operator()...; &#125;;template&lt;class... Ts&gt; overloaded(Ts...) -&gt; overloaded&lt;Ts...&gt;;overloaded &#123; [] (const double&amp; v) &#123; std::cout &lt;&lt; "The value is: " &lt;&lt; v &lt;&lt; std::endl; &#125;, [] (const int&amp; v) &#123; std::cout &lt;&lt; "The value is: " &lt;&lt; v &lt;&lt; std::endl; &#125;&#125; 要想理解它们为什么等价，我们首先的得弄清楚lambda表达式是什么，在剖析lambda之前先来看看 std::visit 函数需要的参数是什么，分析std::visit的参数，先看 struct VisitPackage 结构更容易一些。 std::visit的第一个参数通俗的来说std::visit的第一个参数需要的是一个可执行的对象，如果对象能被执行就需要实现 operator() 这个操作符，看起来像函数一样，这就是为什么在 struct VisitPackage 中定义了 operator()，并且定义了两个形成了参数不同的静态重载，作用就是为了在访问 variant 对象时适配不同的类型，在访问variant 对象时会选择最匹配的 operator() 函数，进而实现了访问variant中不同类型值行为不同的目的。 那lambda表达式能实现这个目的吗？我们接着往下看 lambda 是什么自从 C++11 引入lambda之后，对它赞美的声音不绝于耳，那lambda表达式究竟是怎样实现的呢？真的就是一个普通的函数吗？我们看一个小例子： 1234567891011int main() &#123; int x = 5, y = 6; auto func = [&amp;](int n) &#123; return x + n; &#125;; func(7); return 0;&#125; 这是一个使用lambda表达式简单的例子，代码中定义了一个int类型参数的返回值也是int的lambda函数，作用就是将外部变量x与函数参数的和返回，我们使用 cppinsights.io 网站来将此段代码展开 1234567891011121314151617181920212223242526int main()&#123; int x = 5; int y = 6; class __lambda_4_15 &#123; public: inline /*constexpr */ int operator()(int n) const &#123; return x + n; &#125; private: int &amp; x; public: __lambda_4_15(int &amp; _x) : x&#123;_x&#125; &#123;&#125; &#125;; __lambda_4_15 func = __lambda_4_15&#123;x&#125;; func.operator()(7); return 0;&#125; 可以发现我们虽然定义了一个lambda函数，但是编译器为它生成了一个类 __lambda_4_15，生成了 int&amp; 类型的构造函数，并实现了 operator操作符，再调用lambda函数时先生成了 __lambda_4_15类的对象，再调用类的 operator()函数 func.operator()(7);，看到这里是不是似曾相识，虽然还不是很明白，但是和struct VisitPackage的定义总是有种说不清道不明的血缘关系。 弄清楚了lambda函数的本质，现在要实现的是怎么把多个lambda函数合成一个对象，并且让他们形成重载，因为lambda函数本质上存在一个类，只需要定义一个子类，继承多个lambda表达式就可以了，其实 overloaded 这个模板类就是为了实现这个目的。 overloaded剖析12template&lt;class... Ts&gt; struct overloaded : Ts... &#123; using Ts::operator()...; &#125;;template&lt;class... Ts&gt; overloaded(Ts...) -&gt; overloaded&lt;Ts...&gt;; 一时间看起来很难理解，它来自 en.cppreference.com 中介绍 std::visit 访问 std::variant 的例子，可以换行看得更清楚一点： 123456// helper type for the visitor #4template&lt;class... Ts&gt;struct overloaded : Ts... &#123; using Ts::operator()...; &#125;;// explicit deduction guide (not needed as of C++20)template&lt;class... Ts&gt;overloaded(Ts...) -&gt; overloaded&lt;Ts...&gt;; template struct overloaded : Ts… { using Ts::operator()…; }; 这是一个类模板的声明，模板的名字是overloaded 分步拆解来看： template struct overloaded 表示类的模板参数为可变长的参数包 Ts假设 Ts 包含 T1, T2, … , TN，那么这一句声明可以展开为：template struct overloaded struct overloaded : Ts… 表示类的基类为参数包 Ts 内所有的参数类型假设 Ts 包含 T1, T2, … , TN，那么这一句声明可以展开为：struct overloaded : T1, T2, …, TN { using Ts::operator()…; }; 这是一个函数体内的变长 using 声明假设 Ts 包含 T1, T2, … , TN，那么这一句声明可以展开为：{ using T1::operator(), T1::operator(), …, TN::operator(); }经过这步声明，overloaded 类的参数包 Ts 内所有的参数作为基类的成员函数operator()均被 overloaded 类引入了自己的作用域 template overloaded(Ts…) -&gt; overloaded; 这是一个自动推断向导说明，用于帮助编译器根据 overloaded 构造器参数的类型来推导 overloaded 的模板参数类型，C++17需要，C++20已不必写它告诉编译器，如果 overloaded 构造器所有参数的类型的集合为Ts，那么 overloaded 的模板参数类型就是 Ts 所包含的所有类型如果表达式a1, a2, …, an的类型分别为T1, T2, …, TN，那么构造器表达式overloaded x{a1, a2, …, an} 推导出，overloaded的类型就是 overloaded 经过这些解释，我们可以认为在最后一个例子中可能产生了类似这样的代码： 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;variant&gt;class __lambda_12_7&#123;public: inline /*constexpr */ void operator()(const double &amp; v) const &#123; std::operator&lt;&lt;(std::cout, "The value is: ").operator&lt;&lt;(v).operator&lt;&lt;(std::endl); &#125;&#125;;class __lambda_13_7&#123;public: inline /*constexpr */ void operator()(const int &amp; v) const &#123; std::operator&lt;&lt;(std::cout, "The value is: ").operator&lt;&lt;(v).operator&lt;&lt;(std::endl); &#125;&#125;;template&lt;&gt;struct overloaded&lt;__lambda_12_7, __lambda_13_7&gt; : public __lambda_12_7, public __lambda_13_7&#123; using __lambda_12_7::operator(); // inline /*constexpr */ void ::operator()(const double &amp; v) const; using __lambda_13_7::operator(); // inline /*constexpr */ void ::operator()(const int &amp; v) const;&#125;;int main()&#123; std::variant&lt;double, int&gt; value = std::variant&lt;double, int&gt;(119); std::visit(overloaded&#123;__lambda_12_7(__lambda_12_7&#123;&#125;), __lambda_13_7(__lambda_13_7&#123;&#125;)&#125;, value); return 0;&#125; 总结 std::variant 可以存储多个类型的值，并且它会自动处理类型转换和内存分配 std::variant 可以存储非 POD 类型和类对象，能够在运行时进行类型检查和转换，具有更高的类型安全性 可以使用 std::visit 全局函数来访问 std::variant 中存储的值，该函数根据存储的值的类型自动选择调用哪个函数对象 可以使用 std::holds_alternative 函数来检查变量中是否存储了特定的类型 定义lambda函数时，编译器会为其生成一个类 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 代码是程序世界的运行规则，一个个字母的敲击正在规定着程序世界的运行秩序，无论代码写的天花乱坠，究其本质还是规定这一步要做什么，下一步继续做什么而已~ 2023-6-23 21:13:04]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>variant</tag>
        <tag>Union</tag>
        <tag>visit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[试试InsCode这个在线跑项目代码的平台]]></title>
    <url>%2Fblog%2F2023%2F06%2F18%2F%E8%AF%95%E8%AF%95InsCode%E8%BF%99%E4%B8%AA%E5%9C%A8%E7%BA%BF%E8%B7%91%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%E7%9A%84%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[前言最近看CSDN上线了InsCode这个在线跑项目代码的平台，放在了主菜单主推位置，上次这个位置好像是Gitee，在上次应该是代码片，CSDN 作为我一直使用的产品，最近十年我经历了他所有的变革，就像我刚刚提到的这些主菜单的变化，很多的东西已经被抛弃了，希望这次的InsCode能长久的支持下去。 代码开源平台并不好做，Github最大最活跃但访问并不顺畅，国内最早用的TaoCode在2018年关闭，后来Gitee用的比较多，但审查严格时候有多个开源项目被隐藏，CSDN现在主推的是GitCode，希望也能长久的支持下去，我并不太可能做出轰轰烈烈的开源项目分享，但分享一些代码世界的只言片语也是我的愿望，但愿能帮助一些恰好需要的人。 先了解一下InsCodeInsCode服务是一个在线编程工具，旨在为开发者提供一个便捷的编写、运行和分享代码的环境，让开发者无需在本地搭建编程环境，即可快速编写和运行代码。 InsCode支持多种编程语言，包括Java、Python、C++、JavaScript等代码，支持一些方便的项目模板，但目前还不支持C++模板希望后面能补充上。它提供了完整的运行环境，让代码在网页上直接运行并输出结果，即时调试，方便快捷，并且还提供了现实免费的InsCode AI Chat，帮助我们快速实现功能。同时，Inscode还提供了分享功能，可以轻松地将代码分享给其他人。 写一个Python示例本来想写一段C++示例的，但是因为目前还不支持C++模板，所以先写个Python的例子展示一下使用方式吧，比如想打印本机IP地址，可以借助 InsCode AI Chat 生成，直接点击插入即可。 直接点击运行按钮查看效果，打印出外网IP是116.198.41.215 整个过程我真的是一行代码都不用写了，最终还可以将作品打包放到博客中展示，因为这个示例项目还在审核中，等审核通过后我会把效果贴上来。 总结 InsCode服务是一个在线编程工具，旨在为开发者提供一个便捷的编写、运行和分享代码的环境 InsCode支持多种编程语言，包括Java、Python、C++、JavaScript等代码，并支持一些方便的项目模板 使用InsCode真的可以做到不写一行代码来发布自己的产品，虽然不是很完善，但应该会越来越好 各大平台都在AI赛道里竞争起来，真正做出相关产品的并不多，我们一起期待吧 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 三个人确实就比两个人好看，这其中隐含的就是团圆的情感~ 2023-06-18 23:26:00]]></content>
      <categories>
        <category>Platform</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>InsCode</tag>
        <tag>在线运行</tag>
        <tag>CSDN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Telegraf、InfluxDB和Grafana铁三角形成的监控可视化解决方案]]></title>
    <url>%2Fblog%2F2023%2F06%2F17%2F%E5%88%9D%E8%AF%86Telegraf%E3%80%81InfluxDB%E5%92%8CGrafana%E9%93%81%E4%B8%89%E8%A7%92%E5%BD%A2%E6%88%90%E7%9A%84%E7%9B%91%E6%8E%A7%E5%8F%AF%E8%A7%86%E5%8C%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[前言数据监控目前用于各行各业，服务器开发也不例外，进程什么时候被杀死了？内存占用已经达到98%了怎么早点没发现，网络带宽已经占满了怪不得玩家反馈说登录不上去了，这些都是服务器开发人员常常听到的“抱怨”，他们都涉及到服务器资源的使用情况，如果有一种方便的途径可以高效的监控各个服务器的情况就好了，单靠人去各个服务器查看是不现实的，今天就来简单介绍一种我刚刚学到的方案。 原始的监控靠人盯如果服务器只有一台或者很少，可以每隔一段时间去服务器上看看，top 看一下整体运行情况，free 查一下内存详情，df 看一下硬盘还够不够，iftop 看一下网络带宽情况，不要觉得不可能，有次测试我真的就是开了好几个远程窗口来盯着各个指标，不过人的精力有限，这样做毕竟不是长久之计。 进化的监控靠批处理脚本随着服务器的增多，靠人盯的方式的已经达不到要求了，看看上面的这些命令都是已知的，并且只需要定期得到这些值就可以了，完全可以使用脚本定期执行来做呀，把上面提到的命令封装成脚本然后定期执行得到结果列表，然后发送到指定的地址，我们就可以只分析这些结果，没必要登录到各个服务器上去查询了。 高端的监控靠完整的可视化解决方案懒惰是科学发展的推动力，使用脚本收集数据就不用登录服务器去实时查询了，那能不能帮我自动分析结果，或者在异常时直接给出提醒，因为我连分析的操作都不想做，肯定有人这样想当然也有人这样做了，并且形成了成熟的解决方案，我们只需要简单搭建和配置就可以早点下班了，半夜收到服务器报警电话时起来解决就可以了。 Telegraf、InfluxDB和Grafana铁三角Telegraf是一个用于收集、处理和发送指标数据的代理，它可以与各种数据源和监控系统集成，定期收集数据并将其发送到目标位置。 InfluxDB是一个开源的时间序列数据库，用于存储和查询大量时间序列数据。它被广泛用于存储来自Telegraf等数据收集器的指标数据。 Grafana用于创建交互式仪表板，从各种数据源中提取数据，并将数据以图表、图形和表格等形式展示出来。 通过将Telegraf用作数据收集代理，将数据发送到InfluxDB中存储，然后使用Grafana从InfluxDB中提取数据进行可视化，这个铁三角组合提供了一个强大的监控和数据可视化解决方案。 TelegrafTelegraf是一个开源的代理程序，用于收集、处理和发送指标数据。它设计用于高效地收集各种类型的数据，并将其发送到不同的目标位置，如时序数据库、消息队列等，具有以下是关键特点和功能： 插件架构：Telegraf采用模块化的插件架构，具有丰富的输入和输出插件。它支持多种输入插件，如系统性能指标、日志文件、数据库查询、API调用等，以便从不同的数据源中收集数据。同样，Telegraf还支持多种输出插件，如InfluxDB、Elasticsearch、Kafka等，用于将收集到的数据发送到不同的目标位置。 多平台支持：Telegraf可在多个操作系统上运行，包括Linux、Windows和macOS，以适应不同的部署环境和需求。 轻量级和高性能：Telegraf被设计为轻量级和高性能的代理程序，可以在资源受限的环境中高效运行。它使用Golang编写，具有低的内存和CPU占用，并能够处理高并发的数据收集和处理任务。 数据转换和处理：Telegraf提供了灵活的数据转换和处理功能，可以对收集到的数据进行过滤、转换、聚合等操作。它支持各种数据处理任务，如标签添加、数据格式转换、聚合计算等，以便对数据进行预处理和清洗，使其适应目标存储或分析平台的要求。 插件生态系统：Telegraf拥有一个活跃的插件生态系统，用户可以根据自己的需求选择和配置适当的插件。社区提供了大量的插件，覆盖了各种数据源和目标位置，使得Telegraf具备广泛的适用性和灵活性。 InfluxDBInfluxDB是一个开源的高性能时序数据库，专门用于存储和处理时间序列数据。它被设计为能够高效地处理大规模的时间相关数据，并提供快速的数据插入、查询和聚合能力，具有关键特点和功能： 时序数据存储：InfluxDB专注于存储和管理时间序列数据，如传感器数据、监控指标、日志事件等。它使用特定的数据结构和索引机制，以优化对时间序列数据的写入和查询操作。 高性能和可扩展性：InfluxDB被设计为具有高吞吐量和低延迟的数据库系统。它采用了一系列的优化策略，包括压缩算法、分片存储、数据分区等，以实现高效的数据处理和查询性能。此外，InfluxDB还支持水平扩展，可以通过添加更多节点来提高性能和容量。 查询语言：InfluxDB提供了InfluxQL和Flux两种查询语言。InfluxQL是类似于SQL的查询语言，专门用于时间序列数据的查询和聚合操作。Flux是一种功能更强大和灵活的查询语言，它支持更复杂的数据处理和转换操作，并具有扩展性更好的能力。 数据保留策略：InfluxDB允许定义数据保留策略，以指定数据在数据库中的保存期限和存储策略。这样可以根据数据的时效性需求和存储资源限制来管理数据的保留和清理。 数据一致性和持久性：InfluxDB采用了写时复制（Write Ahead Log）的机制，确保数据的一致性和持久性。它使用持久化引擎来保证数据在故障情况下的可靠性，同时提供数据备份和恢复的功能。 整合生态系统：InfluxDB具有丰富的整合生态系统，可以与各种数据源和工具集成。它支持与Telegraf等数据收集代理的无缝集成，以及与Grafana等数据可视化工具的联合使用，提供完整的监控和数据处理解决方案。 GrafanaGrafana是一个开源的数据可视化和监控平台，用于创建、展示和分析实时指标和数据的交互式仪表板。它提供了丰富的可视化选项和灵活的配置，使用户能够轻松地创建各种类型的仪表板，并实时监控和分析数据，具有以下关键特点和功能： 数据源支持：Grafana支持广泛的数据源，包括各种数据库（如MySQL、PostgreSQL、InfluxDB）、时序数据库（如Prometheus、Graphite）、日志文件、API等。它提供了内置的数据源插件和API接口，以便与不同的数据源进行集成和数据获取。 丰富的可视化选项：Grafana提供多种可视化选项，如折线图、柱状图、仪表盘、热力图等，可以根据数据的特点选择适合的图表类型。它还支持动态图表、图表注释、数据标记等功能，以增强数据的可视化效果和交互性。 交互式仪表板：Grafana的仪表板具有交互式的功能，用户可以自由地缩放、选择时间范围、过滤数据等。它还支持实时数据更新和自动刷新，以确保仪表板上的数据保持最新。 告警和通知：Grafana提供了强大的告警功能，用户可以根据自定义的条件设置告警规则，并通过电子邮件、Slack等渠道接收告警通知。这样可以及时发现数据异常或重要指标的变化。 用户管理和权限控制：Grafana支持多用户和团队管理，可以设置不同的用户角色和权限，以控制对仪表板和数据的访问和操作。它还提供了LDAP、OAuth等身份验证和单点登录功能，与企业的用户管理系统集成更加便捷。 社区支持和插件生态系统：Grafana拥有庞大的开源社区支持和活跃的插件生态系统。用户可以从社区中获取各种主题、模板和插件，以扩展和定制Grafana的功能和外观。 Grafana仪表板展示说实话第一次看到Grafana仪表板还是挺震撼的，展示的信息很多，可以帮助服务器维护人员分析很多问题，下面来展示一些主要的面板。 服务器资源总览 负载和内存使用 网络带宽 磁盘IO IO延迟 其他指标 进程信息 总结 Telegraf、Grafana和InfluxDB通常被称为”铁三角”，因为它们经常一起使用，形成一个完整的监控和数据可视化解决方案 Telegraf是一个用于收集、处理和发送指标数据的代理，它可以与各种数据源和监控系统集成，定期收集数据并将其发送到目标位置 InfluxDB是一个开源的时间序列数据库，用于存储和查询大量时间序列数据。它被广泛用于存储来自Telegraf等数据收集器的指标数据 Grafana用于创建交互式仪表板，从各种数据源中提取数据，并将数据以图表、图形和表格等形式展示出来 Grafana的仪表板真的挺漂亮的 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 把烦恼痛了吞了认了算了不对别人讲谁还没有辜负几段 昂贵的时光 2023-6-17 19:39:56]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>服务器</tag>
        <tag>Telegraf</tag>
        <tag>InfluxDB</tag>
        <tag>Grafana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下熟悉又陌生的sudo命令]]></title>
    <url>%2Fblog%2F2023%2F05%2F26%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%86%9F%E6%82%89%E5%8F%88%E9%99%8C%E7%94%9F%E7%9A%84sudo%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言sudo命令应该是任何一个服务器使用人员都绕不开的命令，比如那个大名鼎鼎的删库跑路命令 sudo rm -rf /*，虽然经常使用，也知道用它能做一些普通用户无法完成的事情，那么它究竟是什么意思，有什么作用和好处，和超级无敌的 root 用户又有什么关系呢？接下来我们来简单梳理一下。 关于sudo的一些疑问sudo是什么sudo 是一种在类 Unix 系统中使用的命令，用于以超级用户（superuser）的身份执行其他命令。它是 “Super User Do” 的缩写。超级用户通常是系统管理员，具有对操作系统的完全控制权。 当你以普通用户的身份登录到系统时，可能没有足够的权限来执行某些敏感的系统操作或访问某些文件。在这种情况下，可以使用 sudo 命令在临时提升权限的情况下执行特定命令。 使用 sudo 命令时，需要输入管理员密码以验证你的身份。一旦验证成功，就可以以超级用户的权限执行命令。这样可以确保系统安全，因为只有经过授权的用户才能执行特权操作。 例如，如果想要安装软件包或修改系统设置，但这些操作需要超级用户权限，你可以在命令前加上 sudo，然后输入密码以执行该命令。 以下是一个sudo命令使用的示例：1sudo apt-get install git 这个命令将使用 apt-get 工具安装一个软件包，由于它需要对系统进行修改，所以需要超级用户权限。通过使用 sudo，可以在普通用户的身份下临时获取这些权限来执行该操作。 sudo 和 root 的区别和联系sudo 和 root 在类 Unix 系统中具有不同的角色和功能。以下是它们之间的区别和联系： 权限级别： root 是超级用户，也称为系统管理员。拥有 root 权限意味着拥有对整个系统的完全控制权，可以执行所有操作并访问所有文件。相比之下，sudo 是一个命令，允许普通用户在需要时以超级用户的身份执行特定的命令。 使用方式： root 用户可以直接登录到系统并拥有持久的超级用户权限。而 sudo 是一种临时提升权限的机制，允许普通用户在需要时以超级用户的身份执行特定命令。普通用户可以使用 sudo 命令来运行特权命令，但在执行时需要输入管理员密码进行身份验证。 安全性： sudo 的使用可以提高系统的安全性。通过限制普通用户的特权，只有在需要执行特定任务时才临时提升权限，可以减少意外或恶意操作造成的潜在风险。相比之下，root 用户拥有系统的完全权限，因此使用 root 用户执行命令时需要格外小心，以免对系统造成损坏。 日常操作： 为了提高系统安全性，通常建议使用 sudo 来执行需要特权的命令，而不是直接使用 root 用户。使用 sudo 可以在普通用户的身份下执行特权命令，并避免了在常规操作中长时间保持 root 权限，从而减少了潜在的错误或安全风险。 总结来说，root 是系统的超级用户，拥有完全的权限，而 sudo 是一种临时提升权限的机制，允许普通用户以超级用户的身份执行特定命令。使用 sudo 可以提高系统的安全性，同时避免在日常操作中过多使用 root 用户带来的潜在风险。 使用 sudo 的好处使用 sudo 有以下几个好处： 安全性： sudo 增加了系统的安全性。通过限制普通用户的特权，只有在需要执行特定任务时才临时提升权限，可以减少潜在的错误或恶意操作对系统造成的风险。普通用户在执行需要特权的命令时需要输入管理员密码，这种身份验证确保只有经过授权的用户才能执行特权操作。 权限控制： sudo 允许系统管理员细粒度地控制用户对系统资源和命令的访问权限。管理员可以通过配置 sudo 的策略文件，精确地指定哪些用户可以执行哪些命令以及以何种方式执行。这种权限控制有助于实施最小权限原则，即用户只能以最低权限执行任务，从而降低了潜在的安全风险。 审计和日志记录： sudo 提供了审计和日志记录功能，可以记录执行特权操作的用户、时间和执行的命令。这对于追踪系统中的活动、故障排查和安全审计非常有价值。通过审计和日志记录，管理员可以了解特权操作的细节，以及哪些用户在何时执行了哪些命令。 限制特权操作： 使用 sudo 可以限制特权操作的范围。普通用户只有在需要执行特定命令时才能使用 sudo 提升权限。这种限制可以避免用户误操作或滥用超级用户权限，从而减少了对系统的潜在风险。 总而言之，使用 sudo 可以提高系统的安全性、实施最小权限原则、提供审计和日志记录功能，并限制特权操作的范围。这使得系统管理员能够更好地管理用户权限并降低潜在的风险。 怎样查看sudo命令日志可以使用适当的文本编辑器打开 sudo 日志文件：选择适用于你的系统的文本编辑器（如 nano、vim、或者其他）以超级用户权限打开 sudo 日志文件。日志文件的位置和名称可能会因操作系统的不同而异。下面是一些常见的 sudo 日志文件路径： Ubuntu、Debian 等基于 Debian 的系统： 1sudo nano /var/log/auth.log CentOS、Fedora、Red Hat 等基于 Red Hat 的系统： 1sudo nano /var/log/secure macOS： 1sudo nano /var/log/system.log 查看 sudo 日志文件通常需要管理员或超级用户权限。确保你具有足够的权限才能访问和查看日志文件。此外，一些操作系统可能还提供了特定的命令或工具来查看系统日志，例如 journalctl 命令在某些 Linux 发行版中可用于查看系统日志。你可以根据你的操作系统和日志管理方式来选择适合的方法来查看 sudo 日志。 修改sudo配置sudo的配置文件是 /etc/sudoers 但是不建议直接用文本工具打开它，一旦修改出错可能导致很多严重问题，可以使用 visudo 命令进行打开编辑，使用 visudo 命令编辑 sudoers 文件时，系统会执行语法检查以确保配置的正确性，以避免可能的权限问题，下面展示一个简单配置样例： 简单样例1234567891011# User privilege specificationroot ALL=(ALL:ALL) ALL# Allow members of group sudo to execute any command%sudo ALL=(ALL:ALL) ALL# Allow members of group admin to execute any command%admin ALL=(ALL:ALL) ALL# Allow specific user to run specific command as another userjohndoe ALL=(appuser) /usr/local/bin/git visudo 是一个用于编辑 sudoers 文件的命令，该文件包含了与 sudo（以管理员权限执行命令）相关的配置信息。在 sudoers 文件中，每一行都由多个列组成，其中包含不同的字段和含义。以下是 sudoers 文件中各列的一般含义： 列1：用户列表 该列指定了可以使用 sudo 命令的用户或用户组。 可以是单个用户名，如 “bob”。 可以是用户组前缀 “%” 后跟用户组名，如 “%admin”。 列2：主机列表 该列指定了可以在哪些主机上使用 sudo 命令。 可以是单个主机名，如 “localhost”。 可以是主机名前缀 “ALL”，表示适用于所有主机。 列3：=(运行 as) 列表 该列指定了可以以哪个用户身份来运行命令。 可以是单个用户名，如 “root”。 可以是用户组前缀 “ALL”，表示可以以任何用户身份运行命令。 =(ALL:ALL)：表示可以以任何用户身份 (ALL) 在任何终端 (ALL) 上运行命令。 列4：NOPASSWD: 标志 该列指定了是否需要输入密码才能使用 sudo 命令。 如果列中包含 “NOPASSWD:”，则表示不需要密码。 如果列为空或未包含 “NOPASSWD:”，则需要输入密码。 列5及以后：命令列表 该列指定了可以使用 sudo 命令执行的具体命令。 可以是完整的命令路径，如 “/usr/bin/apt-get”。 可以是通配符，如 “/usr/bin/*” 表示该目录下的所有命令。 复杂样例及解释下面是一个复杂一些的 visudo 配置示例，其中包含了多个用户、用户组和命令的授权规则： 1234567891011121314151617181920212223242526272829# User privilege specification## Allow root to run any commands on all hostsroot ALL=(ALL:ALL) ALL## Allow members of the admin group to run any commands on all hosts%admin ALL=(ALL:ALL) ALL## Allow specific user to run specific commands as root without passwordjohndoe ALL=(root) NOPASSWD: /bin/ls, /usr/bin/apt-get## Allow specific user to run commands in specific directories as another userjohndoe ALL=(appuser) /opt/app/bin/*johndoe ALL=(backupuser) /var/backup/scripts/*## Allow specific user to run specific command with arguments as rootjohndoe ALL=(root) /usr/sbin/usermod [a-z]*## Deny specific user to run any commandbanneduser ALL=(ALL:ALL) ALL, !/usr/bin/*## Allow members of the wheel group to run any commands on localhost without password%wheel localhost=(ALL:ALL) NOPASSWD: ALL## Allow users in the developer group to run specific commands with arguments%developer ALL=(ALL:ALL) /usr/bin/git *, /usr/bin/make clean## Allow users in the support group to restart specific services%support ALL=(root) /usr/sbin/service apache2 restart, /usr/sbin/service nginx restart 上述配置说明不考虑注释行，含义如下： 第1行表示允许 root 用户在所有主机上以任何用户身份执行任何命令。 第3行表示允许 johndoe 用户作为 root 用户以无需密码的方式执行 /bin/ls 和 /usr/bin/apt-get 命令。 第4行和第5行表示允许 johndoe 用户作为 appuser 用户或 backupuser 用户在特定目录下执行命令。 第6行表示允许 johndoe 用户作为 root 用户执行带有特定参数的 /usr/sbin/usermod 命令。 第7行表示禁止 banneduser 用户执行除 /usr/bin/* 之外的所有命令。 第8行表示允许 wheel 组的用户在 localhost 主机上以任何用户身份执行任何命令，且无需密码。 第9行表示允许 developer 组的用户执行带有特定参数的 /usr/bin/git 和 /usr/bin/make 命令。 第10行表示允许 support 组的用户作为 root 用户重启特定服务。 使用sudo用户创建用户并添加到 sudo 组在 Linux 上创建用户并将其添加到 sudo 组，可以按照以下步骤进行操作： 打开终端：在操作系统中打开终端或命令行界面。 使用 adduser 命令创建新用户： 1sudo adduser newuser 这将提示你输入新用户的密码和其他详细信息。 将新用户添加到 sudo 组。可以使用 usermod 命令执行此操作： 1sudo usermod -aG sudo newuser 这将将新用户 newuser 添加到 sudo 组中。 验证用户是否成功添加到 sudo 组。你可以通过执行以下命令来检查用户所属的组： 1groups newuser 如果 sudo 组在列表中显示，表示用户已成功添加到 sudo 组。 现在，新创建的用户已经添加到 sudo 组，拥有 sudo 权限。他们可以使用 sudo 命令以管理员权限执行特权操作。请确保将适当的权限授予新用户，以确保系统的安全性和合规性。 查看是否有sudo权限可以通过id命令查看指定用户是否在 sudo 组内，比如下面的用户就在27(sudo)这个组内： 12root@jenkins1604:~# id gyeeuid=1004(gyee) gid=1004(gyee) groups=1004(gyee),27(sudo) 也可以查看(sudo)组内是否包含当前用户：12root@jenkins1604:~# getent group sudosudo:x:27:admin,gyee getent 是一个命令行工具，用于从系统数据库（如密码、组、主机、服务等）中获取条目的信息。它可以用于检索和显示与系统用户、组、网络服务和其他系统资源相关的信息 ，getent 命令的基本语法如下： 1getent database entry 其中，database 表示要查询的数据库，而 entry 则表示要获取的数据库条目。 以下是一些常见的 getent 命令示例： 获取系统中所有用户的信息： 1getent passwd 获取特定用户的信息： 1getent passwd &lt;username&gt; 获取系统中所有组的信息： 1getent group 获取特定组的信息： 1getent group &lt;groupname&gt; 获取系统中所有网络服务的信息（/etc/services 文件中定义的服务）： 1getent services 没有sudo权限时执行会被记录当没有sudo权限时，执行命令会输出以下信息，显示此操作被记录在案： 123gyee@jenkins1604:/root$ sudo usermod -aG sudo gyee[sudo] password for gyee:gyee is not in the sudoers file. This incident will be reported. 可以查询系统安全日志 12root@jenkins1604:~# cat /var/log/auth.log | grep gyeeMay 26 22:07:13 machine sudo: gyee : user NOT in sudoers ; TTY=pts/1 ; PWD=/root ; USER=root ; COMMAND=/usr/sbin/usermod -aG sudo gyee 总结 sudo 是一种在类 Unix 系统中使用的命令，用于以超级用户（superuser）的身份执行其他命令 root 是超级用户，拥有对整个系统的完全控制权，sudo 是一个允许普通用户在需要时以超级用户的身份执行特定的命令 adduser 和 useradd 都是在 Linux 系统上用于创建用户的命令，但 adduser命令更方便友好，尤其适用于交互式的用户创建 getent 是一个命令行工具，用于从系统数据库（如密码、组、主机、服务等）中获取条目的信息 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 大家所谓的不满，是不满自己没成为人上人吧，至于温饱，很多人已经绰绰有余了~ 2023-5-26 22:08:38]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>服务器</tag>
        <tag>sudo</tag>
        <tag>adduser</tag>
        <tag>getent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下使用daemon函数实现守护进程]]></title>
    <url>%2Fblog%2F2023%2F05%2F12%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8daemon%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言daemon进程是后台守护进程，比如linux 下server都是daemon进程，像httpd、sshd等这些服务你肯定听说过，如果我们想写一个后台运行程序应该怎么做呢？其实非常容易，只要添加头文件 #include &lt;unistd.h&gt;，然后在调用 daemon(1, 0)函数即可。 daemon函数说明12345/* Put the program in the background, and dissociate from the controlling terminal. If NOCHDIR is zero, do `chdir ("/")'. If NOCLOSE is zero, redirects stdin, stdout, and stderr to /dev/null. */extern int daemon (int __nochdir, int __noclose) __THROW __wur;#endif /* Use misc. */ 头文件中已经说得很清楚了，当 nochdir为零时，将当前目录变为根目录，否则不变，当 noclose为零时，标准输入、标准输出和错误输出重导向为/dev/null不输出任何信息，否则照样输出。 daemon函数的实现关于找daemon函数的实现还是费了一些功夫的，我发现我根本不会在github上查代码，一开始以为是linux源码所以去翻了Linus Torvalds的Linux源码结果没找到，最后还是在google收到了一些，整理如下： 安卓源码 https://android.googlesource.com/platform/bionic.git/+/donut-release/libc/unistd/daemon.c 123456789101112131415161718192021222324252627282930313233#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;int daemon( int nochdir, int noclose )&#123; pid_t pid; if ( !nochdir &amp;&amp; chdir("/") != 0 ) return -1; if ( !noclose ) &#123; int fd = open("/dev/null", O_RDWR); if ( fd &lt; 0 ) return -1; if ( dup2( fd, 0 ) &lt; 0 || dup2( fd, 1 ) &lt; 0 || dup2( fd, 2 ) &lt; 0 ) &#123; close(fd); return -1; &#125; close(fd); &#125; pid = fork(); if (pid &lt; 0) return -1; if (pid &gt; 0) _exit(0); if ( setsid() &lt; 0 ) return -1; return 0;&#125; 苹果源码 https://opensource.apple.com/source/Libc/Libc-167/gen.subproj/daemon.c.auto.html 12345678910111213141516171819202122232425262728293031323334#include &lt;fcntl.h&gt;#include &lt;paths.h&gt;#include &lt;unistd.h&gt;intdaemon(nochdir, noclose) int nochdir, noclose;&#123; int fd; switch (fork()) &#123; case -1: return (-1); case 0: break; default: _exit(0); &#125; if (setsid() == -1) return (-1); if (!nochdir) (void)chdir("/"); if (!noclose &amp;&amp; (fd = open(_PATH_DEVNULL, O_RDWR, 0)) != -1) &#123; (void)dup2(fd, STDIN_FILENO); (void)dup2(fd, STDOUT_FILENO); (void)dup2(fd, STDERR_FILENO); if (fd &gt; 2) (void)close (fd); &#125; return (0);&#125; glibc源码 https://elixir.bootlin.com/glibc/glibc-2.37/source/misc/daemon.c 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#if defined(LIBC_SCCS) &amp;&amp; !defined(lint)static char sccsid[] = "@(#)daemon.c 8.1 (Berkeley) 6/4/93";#endif /* LIBC_SCCS and not lint */#include &lt;errno.h&gt;#include &lt;fcntl.h&gt;#include &lt;paths.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/stat.h&gt;#include &lt;device-nrs.h&gt;#include &lt;not-cancel.h&gt;intdaemon (int nochdir, int noclose)&#123; int fd; switch (__fork()) &#123; case -1: return (-1); case 0: break; default: _exit(0); &#125; if (__setsid() == -1) return (-1); if (!nochdir) (void)__chdir("/"); if (!noclose) &#123; struct __stat64_t64 st; if ((fd = __open_nocancel(_PATH_DEVNULL, O_RDWR, 0)) != -1 &amp;&amp; __glibc_likely (__fstat64_time64 (fd, &amp;st) == 0)) &#123; if (__builtin_expect (S_ISCHR (st.st_mode), 1) != 0#if defined DEV_NULL_MAJOR &amp;&amp; defined DEV_NULL_MINOR &amp;&amp; (st.st_rdev == makedev (DEV_NULL_MAJOR, DEV_NULL_MINOR))#endif ) &#123; (void)__dup2(fd, STDIN_FILENO); (void)__dup2(fd, STDOUT_FILENO); (void)__dup2(fd, STDERR_FILENO); if (fd &gt; 2) (void)__close (fd); &#125; else &#123; /* We must set an errno value since no function call actually failed. */ __close_nocancel_nostatus (fd); __set_errno (ENODEV); return -1; &#125; &#125; else &#123; __close_nocancel_nostatus (fd); return -1; &#125; &#125; return (0);&#125; glibc github地址 https://github.com/lattera/glibc/blob/master/misc/daemon.c 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#if defined(LIBC_SCCS) &amp;&amp; !defined(lint)static char sccsid[] = "@(#)daemon.c 8.1 (Berkeley) 6/4/93";#endif /* LIBC_SCCS and not lint */#include &lt;errno.h&gt;#include &lt;fcntl.h&gt;#include &lt;paths.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/stat.h&gt;#include &lt;device-nrs.h&gt;#include &lt;not-cancel.h&gt;intdaemon (int nochdir, int noclose)&#123; int fd; switch (__fork()) &#123; case -1: return (-1); case 0: break; default: _exit(0); &#125; if (__setsid() == -1) return (-1); if (!nochdir) (void)__chdir("/"); if (!noclose) &#123; struct stat64 st; if ((fd = __open_nocancel(_PATH_DEVNULL, O_RDWR, 0)) != -1 &amp;&amp; (__builtin_expect (__fxstat64 (_STAT_VER, fd, &amp;st), 0) == 0)) &#123; if (__builtin_expect (S_ISCHR (st.st_mode), 1) != 0#if defined DEV_NULL_MAJOR &amp;&amp; defined DEV_NULL_MINOR &amp;&amp; (st.st_rdev == makedev (DEV_NULL_MAJOR, DEV_NULL_MINOR))#endif ) &#123; (void)__dup2(fd, STDIN_FILENO); (void)__dup2(fd, STDOUT_FILENO); (void)__dup2(fd, STDERR_FILENO); if (fd &gt; 2) (void)__close (fd); &#125; else &#123; /* We must set an errno value since no function call actually failed. */ __close_nocancel_nostatus (fd); __set_errno (ENODEV); return -1; &#125; &#125; else &#123; __close_nocancel_nostatus (fd); return -1; &#125; &#125; return (0);&#125; 挺有意思的写法 https://github.com/bmc/daemonize/blob/master/daemon.c 1234567891011121314151617181920212223242526272829303132333435363738int daemon(int nochdir, int noclose)&#123; int status = 0; openlog("daemonize", LOG_PID, LOG_DAEMON); /* Fork once to go into the background. */ if((status = do_fork()) &lt; 0 ) ; /* Create new session */ else if(setsid() &lt; 0) /* shouldn't fail */ status = -1; /* Fork again to ensure that daemon never reacquires a control terminal. */ else if((status = do_fork()) &lt; 0 ) ; else &#123; /* clear any inherited umask(2) value */ umask(0); /* We're there. */ if(! nochdir) &#123; /* Go to a neutral corner. */ chdir("/"); &#125; if(! noclose) redirect_fds(); &#125; return status;&#125; nginx中实现 https://github.com/nginx/nginx/blob/master/src/os/unix/ngx_daemon.c 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960ngx_int_tngx_daemon(ngx_log_t *log)&#123; int fd; switch (fork()) &#123; case -1: ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, "fork() failed"); return NGX_ERROR; case 0: break; default: exit(0); &#125; ngx_parent = ngx_pid; ngx_pid = ngx_getpid(); if (setsid() == -1) &#123; ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, "setsid() failed"); return NGX_ERROR; &#125; umask(0); fd = open("/dev/null", O_RDWR); if (fd == -1) &#123; ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, "open(\"/dev/null\") failed"); return NGX_ERROR; &#125; if (dup2(fd, STDIN_FILENO) == -1) &#123; ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, "dup2(STDIN) failed"); return NGX_ERROR; &#125; if (dup2(fd, STDOUT_FILENO) == -1) &#123; ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, "dup2(STDOUT) failed"); return NGX_ERROR; &#125;#if 0 if (dup2(fd, STDERR_FILENO) == -1) &#123; ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, "dup2(STDERR) failed"); return NGX_ERROR; &#125;#endif if (fd &gt; STDERR_FILENO) &#123; if (close(fd) == -1) &#123; ngx_log_error(NGX_LOG_EMERG, log, ngx_errno, "close() failed"); return NGX_ERROR; &#125; &#125; return NGX_OK;&#125; chatgpt生成123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;int daemon(int nochdir, int noclose)&#123; pid_t pid; // 创建子进程并退出父进程 if ((pid = fork()) &lt; 0) &#123; return -1; &#125; else if (pid != 0) &#123; exit(0); &#125; // 创建新的会话 if (setsid() &lt; 0) &#123; return -1; &#125; // 忽略 SIGHUP 信号 signal(SIGHUP, SIG_IGN); // 再次创建子进程并退出父进程 if ((pid = fork()) &lt; 0) &#123; return -1; &#125; else if (pid != 0) &#123; exit(0); &#125; // 更改当前工作目录 if (!nochdir) &#123; if (chdir("/") &lt; 0) &#123; return -1; &#125; &#125; // 关闭文件描述符 if (!noclose) &#123; int fd; if ((fd = open("/dev/null", O_RDWR, 0)) != -1) &#123; dup2(fd, STDIN_FILENO); dup2(fd, STDOUT_FILENO); dup2(fd, STDERR_FILENO); if (fd &gt; STDERR_FILENO) &#123; close(fd); &#125; &#125; &#125; return 0;&#125; daemon函数fork两次的原因以上这些开源实现中，有的fork一次，有的fork两次，不过fork一次的多一些，对于这个问题我还查了不少资料 http://www.cppblog.com/justin-shi/archive/2014/10/10/208530.htmlhttps://blog.csdn.net/shaoye_csdn1/article/details/94599271https://m.xp.cn/b.php/75000.html 关于为什么要fork两次，我找到了上面几篇相关的解释，总结如下： 第一次fork使父进程终止，让子进程中进入后台执行，同时保证了会话id与当前子进程id不同，可以调用setsid函数创建新的会话，保证了子进程是会话组长(sid==pid)，也是进程组组长(pgid == pid)。 第二次fork目的是防止进程再次打开一个控制终端，因为打开一个控制终端的前提条件是该进程必须是会话组长，再fork一次可以保证子进程pid != sid（sid是进程父进程的sid），所以也无法打开新的控制终端。 关于第一次fork我是理解的，第二次fork我也做过实验，无论fork一次还是两次，都可以再次打开新的终端，也就是说即使保证了pid != sid，一样可以打开新的终端，而有很多daemon函数的开源的实现中也只是调用了一次fork，包括我使用的Ubuntu20.04实验结果也是调用了一次fork，所以调用两次fork的原因成谜。 https://www.cntofu.com/book/46/linux_system/153.md 而这篇文章总结了两次fork技巧，不过和我们本文中所说的daemon两次fork前提不一样，在父进程不退出时会使用两次fork技巧避免出现僵尸进程，但时在daemon函数中fork只之后的父进程都是直接退出的，所以应该不是这个原因。 关于这个疑问我还专门去ChatGPT上问了一下，它给的答案如下： 在早期的Unix系统中，使用 daemon() 函数通常需要调用两次 fork() 函数，以确保子进程成为孤儿进程（orphan process），即与任何终端无关联，并且不会收到终端信号。孤儿进程通常是由父进程创建的子进程，但父进程已经退出或被杀死。 以下是调用两次 fork() 函数的工作原理： 调用第一次 fork() 函数：创建子进程1。父进程退出，子进程1成为孤儿进程，也就是没有父进程的进程。此时，子进程1仍然与原来的控制终端相关联。 子进程1调用 setsid() 函数创建一个新的会话，并成为新会话的领头进程和新进程组的组长进程。由于孤儿进程已经成为新会话的领头进程，因此它无法再重新关联到终端。此时，子进程1仍然是当前进程组的组长进程，并且与新的控制终端相关联。 调用第二次 fork() 函数：创建子进程2。由于子进程1是新会话的领头进程，因此子进程2不再与任何控制终端相关联。此时，子进程2成为孤儿进程，并与新会话和新进程组分离。 子进程2关闭不需要的文件描述符，改变工作目录和文件掩码，最后运行守护进程。 &gt; 需要注意的是，在现代的Linux系统中，使用 daemon() 函数只需要调用一次 fork() 函数和一些必要的操作（如 setsid()、关闭文件描述符等），就可以将进程变成守护进程，而不需要调用两次 fork() 函数。 自己做实验的代码自己在测试 daemon 函数的时候写了一些代码，记录一下方便再次拿来用，请原谅我c/c++混用，为了验证某个问题我已经各种尝试了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;limits.h&gt;#include &lt;fcntl.h&gt; //open O_RDWR#include &lt;sys/stat.h&gt; //umask#include &lt;iostream&gt;int my_daemon (int nochdir, int noclose)&#123; pid_t pid; /* Fork and exit parent process. */ if ((pid = fork ()) &lt; 0) return -1; else if (pid != 0) _exit (0); /* Detach ourselves from the parent process group. */ if (setsid () &lt; 0) return -1; /* Fork again to ensure we are not session leader. */ if ((pid = fork ()) &lt; 0) return -1; else if (pid != 0) _exit (0); /* Change the file mode mask. */ umask (022); /* If nochdir is zero, change to the root directory. */ if (!nochdir) chdir ("/"); /* If noclose is zero, redirect standard input, standard output, and standard error to /dev/null. */ if (!noclose) &#123; int fd; fd = open ("/dev/null", O_RDWR, 0); if (fd != -1) &#123; dup2 (fd, STDIN_FILENO); dup2 (fd, STDOUT_FILENO); dup2 (fd, STDERR_FILENO); if (fd &gt; 2) close (fd); &#125; &#125; return 0;&#125;void showpidinfo(const char* info)&#123; std::cout &lt;&lt; info &lt;&lt; "\n" &lt;&lt; "getpid=" &lt;&lt; getpid() &lt;&lt; "\n" &lt;&lt; "getppid=" &lt;&lt; getppid() &lt;&lt; "\n" &lt;&lt; "getpgrp=" &lt;&lt; getpgrp() &lt;&lt; "\n" &lt;&lt; "getpgid(pid)=" &lt;&lt; getpgid(getpid()) &lt;&lt; "\n" &lt;&lt; "getsid(pid)=" &lt;&lt; getsid(getpid()) &lt;&lt; "\n" &lt;&lt; "getuid=" &lt;&lt; getuid() &lt;&lt; "\n" &lt;&lt; "geteuid=" &lt;&lt; geteuid() &lt;&lt; "\n" &lt;&lt; "getgid=" &lt;&lt; getgid() &lt;&lt; "\n" &lt;&lt; "getegid=" &lt;&lt; getegid() &lt;&lt; "\n" &lt;&lt; "ttyname(0)=" &lt;&lt; ttyname(0) &lt;&lt; "\n" &lt;&lt; std::endl;&#125;void open_new_terminal()&#123; system("gnome-terminal -- /bin/bash -c 'date; exec bash'"); //system("konsole -e './v'"); //system("xterm -e './v'");&#125;;int main(int argc, char *argv[])&#123; char strCurPath[PATH_MAX]; showpidinfo("===before daemon:"); //while(1); int n = daemon(0, 1); //int n = my_daemon(0, 1); open_new_terminal(); printf("daemon result %d\n", n); showpidinfo("===after daemon:"); if(n &lt; 0) &#123; perror("error daemon.../n"); exit(1); &#125; sleep(2); if(getcwd(strCurPath, PATH_MAX) == NULL) &#123; perror("error getcwd"); exit(1); &#125; printf("cur path [%s]\n", strCurPath); //打印当前目录 return 0;&#125; showpidinfo 函数用于输出进程id相关信息 open_new_terminal 函数用于打开一个新的控制台 my_daemon 函数是自定义的daemon实现，采用fork两次的形式 perror 可以输出详细的报错信息 12345678910111213141516171819202122232425262728293031# demo @ ThinkPad-X390 in ~/cpp/daemontest [23:30:53]$ g++ a.cpp$ ./a.out===before daemon:getpid=352638getppid=62351getpgrp=352638getpgid(pid)=352638getsid(pid)=62351getuid=1000geteuid=1000getgid=1000getegid=1000ttyname(0)=/dev/pts/15# demo @ ThinkPad-X390 in ~/cpp/daemontest [23:30:57]$ Unable to init server: Could not connect: Connection refused# Failed to parse arguments: Cannot open display:daemon result 0===after daemon:getpid=352639getppid=1getpgrp=352639getpgid(pid)=352639getsid(pid)=352639getuid=1000geteuid=1000getgid=1000getegid=1000ttyname(0)=/dev/pts/15cur path [/] 试验结果不论是fork一次还是两次，都可以打开新的控制台，这与网络上很多文章是相悖的，真相还需继续检验。运行结果中的报错”Unable to init server: Could not connect: Connection refused” 是因为我远程连接无法打开GUI。 总结 想实现后台守护进程，只需添加头文件 #include &lt;unistd.h&gt;，然后在调用 daemon(1, 0)函数即可 daemon 的开源实现有很多种，早期很多说是fork两次，但是查了很多最新的版本都是fork一次 c/c++代码中打开控制台可以使用 system(&quot;gnome-terminal -- /bin/bash -c &#39;date; exec bash&#39;&quot;); 前提是可以正常打开用户界面的电脑，远程操作是不可以的 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 除了死亡，都是擦伤~ 2023-5-12 22:38:40]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>服务器</tag>
        <tag>daemon</tag>
        <tag>gnome-terminal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下使用dd命令测试磁盘的读写速度]]></title>
    <url>%2Fblog%2F2023%2F04%2F26%2Flinux%E4%B8%8B%E4%BD%BF%E7%94%A8dd%E5%91%BD%E4%BB%A4%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98%E7%9A%84%E8%AF%BB%E5%86%99%E9%80%9F%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[前言最近打包更新总是特别慢，其中有一步使用md5sum命令计算MD5值的操作有时居然要卡一个小时，虽说计算MD5比较耗时，但通常计算5个G数据的md5值也就1分多钟，卡一个多小时有点说不过去了，所以在卡顿时使用 top 命令查看了进程情况，发现md5sum进程在卡顿时总是处于 D 状态，这个还是第一次遇到，所以查了些资料记录一下。 D状态进程D 状态进程(TASK_UNINTERRUPTIBLE)，处于不可中断的睡眠状态，指进程不响应异步信号。绝大多数情况下，进程处在睡眠状态时，总是应该能够响应异步信号的。而D状态存在的意义就在于，内核的某些处理流程是不能被打断，比如进程调用read系统调用对某个设备文件进行读操作，一般需要使用D状态对进程进行保护，以避免进程与设备交互的过程被打断，造成设备陷入不可控的状态。这种情况下的D状态总是非常短暂的，通过ps命令基本上不可能捕捉到。 但是我捉到了，在执行md5sum命令时经常捉到，D状态往往是由于 I/O 资源得不到满足，而引发等待~ 这是个关键点，既然IO得不到满足就查一下IO，计算md5也就从磁盘上读一些文件，所以怀疑问题出现在磁盘的读写上，进而想测一些磁盘的读写速度，这就用到了今天想说的 dd 命令。 dd命令磁盘的读写速度需要通过实际读写文件来测试，dd 命令就可以完成这种测试，配合 time 命令可以统计命令执行时间 仅测读取速度 12345678# time dd if=/tmp/test of=/dev/null bs=16k1000000+0 records in1000000+0 records out16384000000 bytes (16 GB) copied, 42.7044 s, 384 MB/sreal 0m42.718suser 0m0.387ssys 0m8.951s 仅测写入速度 12345678# time dd if=/dev/zero of=/tmp/test bs=16k count=10000001000000+0 records in1000000+0 records out16384000000 bytes (16 GB) copied, 35.7006 s, 459 MB/sreal 0m38.428suser 0m0.356ssys 0m12.608s 测试读写速度 12345678# time dd if=/tmp/test of=/tmp/test2 bs=16k1000000+0 records in1000000+0 records out16384000000 bytes (16 GB) copied, 87.3214 s, 188 MB/sreal 1m27.331suser 0m0.430ssys 0m15.799s 问题原因最后测试发现硬盘读写速度不是很稳定，找运维沟通发现这台服务器使用的网络硬盘，存储带宽有限，当执行太多磁盘任务时，存储带宽会被占满，会造成硬盘读写缓慢，进而影响了之前提到的计算MD5的操作步骤，换了块本地硬盘就好了。 总结 进程显示D状态时处于不可中断的睡眠状态，往是由于 I/O 资源得不到满足而引发了等待 仅测试写入速度 time dd if=/dev/zero of=/tmp/test bs=16k count=1000000 仅测试读取速度 time dd if=/tmp/test of=/dev/null bs=16k 测试读写速度 time dd if=/tmp/test of=/tmp/test2 bs=16k /dev/zero 是一个特殊的文件，当你读它的时候会得到无限的空字符，可用来初始化文件，不产生IO /dev/null 也是一个特殊的文件，它丢弃一切写入其中的数据，被称为黑洞，也不产生IO 网络硬盘受存储带宽的影响，一旦带宽被打满，硬盘读写速度也就慢了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 好像遥不可及，但突然有那么一瞬间又感觉触手可及，貌似也没什么退路，只有往前冲了 2023-4-26 22:28:10]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>运维</tag>
        <tag>服务器</tag>
        <tag>硬盘读写</tag>
        <tag>dd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx返回静态的json字符串]]></title>
    <url>%2Fblog%2F2023%2F04%2F24%2FNginx%E8%BF%94%E5%9B%9E%E9%9D%99%E6%80%81%E7%9A%84json%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[前言自从上次安装完nginx我把服务器对外开发的端口都通过它来转发了，真香，最近要加一个获取最新版本号的功能，一开始还想写在服务器里，后来想想可不可以直接通过nginx配置呢？经过一番尝试果然是可以的，这样就不用单独编写代码来响应请求了，并且可以减少服务器资源占用，同时响应性能也非常快。 修改配置文件还是在上次《记录一下第一次安装和配置Nginx》的配置文件 project.conf基础上改吧，修改下面这一段 123456789101112server &#123; listen 4100; server_name localhost; location / &#123; proxy_pass http://login_entrance; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 返回简单json字符串如果是简单的json字符串，可以直接写在配置文件中，比较方便： 1234location /version/en &#123; default_type application/json; return 200 '&#123;"code":0, "version":"1.6"&#125;';&#125; 返回json文件内容如果json内容较多，，是无法完全放到配置中的，因为 nginx 的 config buffer 有4kb大小限制，这时就可以将json内容放到文件中： 1234location /version/cn &#123; default_type application/json; alias /data/update/cn_version.json;&#125; 综合到一起的配置文件就变成了： 12345678910111213141516171819202122server &#123; listen 4100; server_name localhost; location /version/en &#123; default_type application/json; return 200 &apos;&#123;&quot;code&quot;:0, &quot;version&quot;:&quot;1.6&quot;&#125;&apos;; &#125; location /version/cn &#123; default_type application/json; alias /data/update/cn_version.json; &#125; location / &#123; proxy_pass http://login_entrance; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 总结 通过nginx可以直接配置返回一些静态文本或json信息，这样不用写额外的处理逻辑，同时效率比较高 在返回json内容时如果数据比较多，可以保存到文件中直接在配置文件中引用文件名即可 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 好汉不提当年勇，如今落寞谁能懂，唯有鹤立鸡群中，任他必敬又必恭。 2023-4-24 22:40:16]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>配置文件</tag>
        <tag>json</tag>
        <tag>静态文本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN通过备份、过滤、再导入的方式彻底删除废弃目录]]></title>
    <url>%2Fblog%2F2023%2F04%2F21%2FSVN%E9%80%9A%E8%BF%87%E5%A4%87%E4%BB%BD%E3%80%81%E8%BF%87%E6%BB%A4%E3%80%81%E5%86%8D%E5%AF%BC%E5%85%A5%E7%9A%84%E6%96%B9%E5%BC%8F%E5%BD%BB%E5%BA%95%E5%88%A0%E9%99%A4%E5%BA%9F%E5%BC%83%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言SVN占用的空间随着项目版本迭代越来越大，因为保存了历史记录中的各个版本，所以即使本地把废弃的目录删掉提交，也不会释放出多余的空间，大概率因为操作删除增加了一个版本号，使得占用的空间更大了。 怎么才能让它的空间变小的，最简单粗暴的方式就是丢弃历史记录，在最新的版本中剔除的废弃的目录直接上传创建一个新的仓库，这样操作后所有的人需要重新下载才行，如果想保留历史记录，就要用到今天说的这种备份、过滤、再导入的方式了。 简要步骤假设svn库A在服务器上的路径是：/data/svndata/repos/A，需要删除的文件路径为 /arts/tmp/pictures，注意要过滤的文件夹路径以 / 开头，其实代指起始目录 A。 操作的步骤如下： 1234567891011121314151617//1.备份svnadmin dump /data/svndata/repos/A &gt; A.dump//2.过滤掉废弃目录cat A.dump | svndumpfilter exclude /arts/tmp/pictures &gt; B.dump//3.创建新的库svnadmin create /data/svndata/repos/B//4.导入新库svnadmin load /data/svndata/repos/B &lt; B.dump//5.重命名老库mv A A_backup//6.重命名新库代替老库mv B A 操作示例操作过程使用的是一个真实的SVN仓库，就是按照上面的步骤试了一次，主要想看下是不是传说中的那么耗时，仓库名为R，数据量115G，共10843个版本。 导出备份文件 12345# svnadmin dump/data/svndata/repos/R &gt; r.dump* Dumped revision 0.* Dumped revision 1.* Dumped revision 2.... 完整导出用了2小时40分钟，导出文件大小为514G，数据量翻了近5倍。 过滤掉指定目录 12345678910# cat r.dump | svndumpfilter exclude /arts/tmp/pictures &gt; r-exclude.dumpExcluding prefixes: &apos;/arts/tmp/pictures&apos;Revision 0 committed as 0.Revision 1 committed as 1.Revision 2 committed as 2.Revision 3 committed as 3.Revision 4 committed as 4.... 将/arts/tmp/pictures目录过滤掉，总共用时58分钟，过滤后的备份文件大小442G 创建新的临时仓库 1# svnadmin create r-new 将过滤后的备份文件导入新库 12345678910111213# svnadmin load ./r-new &lt; r-exclude.dump&lt;&lt;&lt; Started new transaction, based on original revision 1 * editing path : arts ... done. * editing path : develop ... done.------- Committed revision 1 &gt;&gt;&gt;&lt;&lt;&lt; Started new transaction, based on original revision 2 * editing path : develop/client ... done. * editing path : develop/server ... done.------- Committed revision 2 &gt;&gt;&gt;... 导入工作从第一个版本依次应用，总共用时4小时。 这个过程真的是很慢，完整用时近8小时，并且对磁盘空间要求很大，另外还有个接近1万个版本号大小为800G的库，在24核机器上仅导出就花了41小时，备份文件大小4.5T，后面我就没试了，磁盘快满了。 总结 备份 svnadmin dump /data/svndata/repos/A &gt; A.dump 过滤 cat A.dump | svndumpfilter exclude /arts/tmp/pictures &gt; B.dump 建新 svnadmin create /data/svndata/repos/B 导入 svnadmin load /data/svndata/repos/B &lt; B.dump ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 朝气蓬勃的孩子真的可爱，可是我还是想长大，这样才有机会掌控节奏，虽然现在看起来像个提线木偶，但正在尝试突破束缚~ 2023-4-21 21:56:28]]></content>
      <categories>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>服务器</tag>
        <tag>SVN</tag>
        <tag>备份</tag>
        <tag>数据清理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++代码使用 gperftools 工具进行性能分析]]></title>
    <url>%2Fblog%2F2023%2F04%2F19%2FC-%E4%BB%A3%E7%A0%81%E4%BD%BF%E7%94%A8-gperftools-%E5%B7%A5%E5%85%B7%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言一直想用 gperftools 做一下性能方面的尝试，之前一直忙着开发，目前已经到了后期，忙里抽闲亲自操作一遍，从安装到分析做个简单的记录，以便后续拿来直接用。 gperftools 是什么gperftools 是Google开发的用来进行代码性能分析工具，其实他是一系列高性能多线程 malloc() 实现的集合，同时添加了一些精巧的性能分析工具。 使用gperftools工具可以通过采样的方式生成上面这种图形化的代码性能分析结果，便于我们分析程序性能瓶颈。 使用方法C++程序按照代码插桩的方式引入了gperftools工具，不过这个工具需要单独安装，为了生成图形化的分析结果，还需要安装一些依赖库，下面简述以下使用功能步骤。 安装工具 安装编译所需基础软件 1sudo apt install autoconf automake libtool 安装graphviz，用于图形化显示分析结果 1sudo apt install graphviz 安装libunwind, 这个库提供了可用于分析程序调用栈的 API 123456789cd /tmpwget https://github.com/libunwind/libunwind/releases/download/v1.6.2/libunwind-1.6.2.tar.gztar -zxvf libunwind-1.6.2.tar.gzcd libunwind-1.6.2./configuremake -j4sudo make installcd /tmprm -rf libunwind-1.6.2.tar.gz libunwind-1.6.2 安装gperftools 123456789cd /tmpwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.10/gperftools-2.10.tar.gztar -zxvf gperftools-2.10.tar.gzcd gperftools-2.10./configuremake -j4sudo make installcd ~rm -rf gperftools-2.10.tar.gz gperftools-2.10 刷新动态装入程序所需的链接和缓存文件 1sudo ldconfig 代码插桩引入工具代码修改主要在源程序中引入头文件，并且在待测试逻辑前后添加启动分析和结束分析的语句就行了，对于服务类程序，因为要一直运行，可以通过kill信号通知来开启和关闭性能分析。 关键代码123456#include &lt;gperftools/profiler.h&gt; // 引入头文件...ProfilerStart("cpp_demo.prof"); // 启动分析...ProfilerStop(); // 结束分析... 完整示例12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;gperftools/profiler.h&gt;static void sig(int sig) // kill -10 pid to trigger&#123; static bool b = false; if (!b) ProfilerStart("cpp_demo.prof"); else ProfilerStop(); b= !b;&#125;int main(int argc, char* argv[])&#123; signal(SIGUSR1, sig); while (true) &#123; std::this_thread::sleep_for(std::chrono::milliseconds(1)); //.. std::cout &lt;&lt; std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;( std::chrono::system_clock::now().time_since_epoch()).count() &lt;&lt; std::endl; &#125; return 0;&#125; 编译链接编译时我们需要将 profiler 库和 libunwind 库链接到可执行程序，如果使用 cmake来构建，那么 CMakeLists 文件中的语句为： 1target_link_libraries($&#123;PROJECT_NAME&#125; profiler unwind) 启动分析程序 正常启动游戏服务器，通过ps命令查找到要分析的进程id，比如查找到demoserver的进程是 7217 12$ ps -ef | grep demoserverdemo 7217 1 22 21:51 ? 00:00:18 ./demoserver-d 通过kill命令传递自定义信号10的方式启动和关闭分析程序，第一次运行命令是启动，第二次运行相同的命令是关闭，两次命令之间是分析的时间段 1$ kill -10 7217 关闭分析程序之后，会在可执行程序所在目录生成 cpp_demo.porf 文件，可以使用下面命令将结果图形化 1234$ pprof --pdf demoserver cpp_demo.prof &gt; demoserver.pdfUsing local file demoserver.Using local file cpp_demo.prof.Dropping nodes with &lt;= 1 samples; edges with &lt;= 0 abs(samples) 最终生成的 demoserver.pdf 文件就是我们要用的分析结果，如文章开头所示。 数据分析上面提到了生成pdf图，其实可以生成txt文本的，只要修改生成选项就可以，比如像这样： 123456789# pprof --text demoserver cpp_demo.profUsing local file demoserver.Using local file cpp_demo.prof.Total: 13 samples 3 21.4% 21.4% 3 21.4% SpinLock::Unlock (inline) 3 21.4% 42.9% 3 21.4% __GI_madvise 2 14.3% 57.1% 2 14.3% SpinLock::Lock (inline) 1 7.1% 64.3% 1 7.1% TCMalloc_PageMap2::get (inline) ... 上述文本数据每行包含6列数据，依次为: 分析样本数量（不包含其他函数调用） 分析样本百分比（不包含其他函数调用） 目前为止的分析样本百分比（不包含其他函数调用） 分析样本数量（包含其他函数调用） 分析样本百分比（包含其他函数调用） 函数名(或者类名+方法名) 样本数量相当于消耗的CPU时间，整个函数消耗的CPU时间相当于包括函数内部其他函数调用所消耗的CPU时间，如果是分析最上面的pdf图，每个节点代表一个函数，包含2~3行数据： 函数名(或者类名+方法名) 不包含内部函数调用的样本数 (百分比) of 包含内部函数调用的样本数 (百分比) #如果没有内部调用函数则不显示总结 gperftools 是可以通过采样的方式进行代码性能分析工具，可生成图形化结果便于我们分析程序性能瓶颈 待分析程序中引入gperftools非常方便，但是需要单独安装这个工具 程序引入时只需要添加头文件，在目标位置插入 ProfilerStart(&quot;cpp_demo.prof&quot;); 和 ProfilerStop(); 语句即可 对于服务类程序通常不会直接结束，可以通过 kill 命令传递信号的方式来启动和关闭分析程序 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 可能终于有一天 刚好遇见爱情可能永远在路上 有人奋斗前行可能一切的可能 相信才有可能可能拥有过梦想 才能叫做青春 2023-4-19 23:22:53]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>服务器调优</tag>
        <tag>linux</tag>
        <tag>gperftools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下查看因内存占用过大被杀掉的进程]]></title>
    <url>%2Fblog%2F2023%2F04%2F04%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9F%A5%E7%9C%8B%E5%9B%A0%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E8%BF%87%E5%A4%A7%E8%A2%AB%E6%9D%80%E6%8E%89%E7%9A%84%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言最近发生两次游戏服务器进程突然消失的事件，查询日志上下文没有找到有用的信息，日志显示运行到某处戛然而止，此处代码逻辑简单，排除异常逻辑导致的服务器进程崩溃，所以联想到之前同一台服务器上部署多个进程，因占用内存太大被内核杀死的情况，猜测本次也是这个原因。 查询方法通过搜索引擎找到了两种查询方法，一种是直接过滤系统日志，一种是借助 dmesg 命令，备注以下命令在 CentOS 7 上测试通过 通过系统日志查找1234[root@demo]# grep "Out of memory" /var/log/messagesApr 4 10:32:30 hk-dev kernel: Out of memory: Kill process 2434 (Game9) score 212 or sacrifice childApr 4 10:33:53 hk-dev kernel: Out of memory: Kill process 2476 (git) score 381 or sacrifice childApr 4 10:33:53 hk-dev kernel: Out of memory: Kill process 2777 (git) score 381 or sacrifice child 通过dmesg命令查找123456[root@demo]# dmesg -T | grep "Out of memory"[Mon Aug 30 12:06:25 2021] Out of memory: Kill process 22437 (git) score 400 or sacrifice child[Wed Sep 22 20:23:52 2021] Out of memory: Kill process 29780 (Game6) score 161 or sacrifice child[Wed Mar 29 15:54:31 2023] Out of memory: Kill process 29093 (git) score 388 or sacrifice child[Tue Apr 4 10:24:05 2023] Out of memory: Kill process 2434 (Game9) score 212 or sacrifice child[Tue Apr 4 10:25:29 2023] Out of memory: Kill process 2476 (git) score 381 or sacrifice child 进程被杀的原因Linux 内核有个机制叫OOM killer，全称为 Out Of Memory killer，很形象的一个名字——内存溢出杀手，这个机制会监控那些占用内存过大，尤其是瞬间占用内存很快的进程，为防止机器内存耗尽而主动把该进程杀掉。 当内核检测到系统内存不足、挑选并杀掉某个进程的过程可以参考内核源代码 linux/mm/oom_kill.c（2023-4-4 23:24:07确认了此文件存在），当系统内存不足的时候，out_of_memory() 函数被触发，然后调用 select_bad_process() 函数选择一个进程杀掉，这个选择的过程是通过调用 oom_badness() 函数实现的，挑选的算法和想法都暴力但朴实：就是找到最占用内存的进程。 源码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * oom_badness - heuristic function to determine which candidate task to kill * @p: task struct of which task we should calculate * @totalpages: total present RAM allowed for page allocation * * The heuristic for determining which task to kill is made to be as simple and * predictable as possible. The goal is to return the highest value for the * task consuming the most memory to avoid subsequent oom failures. */long oom_badness(struct task_struct *p, unsigned long totalpages)&#123; long points; long adj; if (oom_unkillable_task(p)) return LONG_MIN; p = find_lock_task_mm(p); if (!p) return LONG_MIN; /* * Do not even consider tasks which are explicitly marked oom * unkillable or have been already oom reaped or the are in * the middle of vfork */ adj = (long)p-&gt;signal-&gt;oom_score_adj; if (adj == OOM_SCORE_ADJ_MIN || test_bit(MMF_OOM_SKIP, &amp;p-&gt;mm-&gt;flags) || in_vfork(p)) &#123; task_unlock(p); return LONG_MIN; &#125; /* * The baseline for the badness score is the proportion of RAM that each * task's rss, pagetable and swap space use. */ points = get_mm_rss(p-&gt;mm) + get_mm_counter(p-&gt;mm, MM_SWAPENTS) + mm_pgtables_bytes(p-&gt;mm) / PAGE_SIZE; task_unlock(p); /* Normalize to oom_score_adj units */ adj *= totalpages / 1000; points += adj; return points;&#125; 总结 如果你发现运行了一段时间的进程突然不见了，那可能是内核嫉妒生恨把它给干掉了 查询内存溢出被杀掉的进程可以直接通过系统日志来查 grep &quot;Out of memory&quot; /var/log/messages 也可以通过专门的命令查找 dmesg -T | grep &quot;Out of memory&quot; 刚刚看了linus 的 linux 代码库，昨天还在提交，真的是更新无止境 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 做人一辈子 快乐没几天一条大路分两边 随你要走哪一边不怕不怕就不怕 我是年轻人风大雨大太阳大 我就是敢打拼 2023-4-4 23:31:40]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>服务器</tag>
        <tag>dmesg</tag>
        <tag>oom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单谈谈图形界面和命令行的区别]]></title>
    <url>%2Fblog%2F2023%2F04%2F03%2F%E7%AE%80%E5%8D%95%E8%B0%88%E8%B0%88%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E5%92%8C%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言随着总结的文章越写越多，我发现自己是一个“追求完美”的人，但同时也是一个“虎头蛇尾”的人，这两者看起来冲突，却可以融于一身。去写一个问题是总想从三皇五帝时写起，总想把各个分支情况都写清楚，这就会创造一个“虎头”，但写着写着就发现时间不允许了，或者已经触及了自己的知识边界，最后不得不退化成了“蛇尾”。 这对于总结和传播知识是不合适的，对于知识的总结应该做到详略得当，这一点上我还有很大的提升空间，推荐一本这方面做的比较好的书籍《图解HTTP》，这本书从始至终让人感觉到很丰满，没有哪一部分让人感觉是凑数的，特别的讲到电信号在同轴电缆中的传播，给人一种知识体系非常完整的体验。 言归正传，今天我要谈谈图形用户界面和命令行的区别，工作久了以后总会有点自己的心得，灵光一闪时就想记录下来，算是给今后的自己保存一点点瞬时的记忆，今天就不从三皇五帝开始讲了，我们从计算机的诞生开始说起。 世界世界上第一台电子计算机 ENIAC，是一个占地170平方米，重达30吨的庞然大物，慢慢的计算机的体积越来越小，但主要用于特定领域，并未进入寻常人家，此阶段的计算机多用于完成特定任务，也就是输入指令完成任务，我们看到的更多的是黑框框，后来随着windows系统（当然苹果系统也出现了）的普及，鼠标点点点极大的降低了入门的难度，也使得图形用于界面深入人心。 关于Windows内置的四款游戏据说每一款都有其特殊的目的与作用，纸牌希望让用户熟悉拖动和释放鼠标；扫雷是为了训练用户使用鼠标进行精准的点击，并掌握左键与右键的区别；红心大战是为了鼓励用户使用互联网与其他玩家交流；空当接龙是为了测试32位的数据处理子系统是否正确安装。这些游戏的出现让人们快速适应了图形用户界面，甚至这是他们接触电脑的第一印象，可能他们并不懂得什么是命令行。 对比图形用户界面和命令行不能说哪一种更好，只能说哪一种更合适，一些情况下图形界面会给你更直观的感受，一些情况下命令行可以代替重复性的操作。 图形用户界面图形用户界面，简称GUI，现在绝大多数的用户面对的都是图形界面，而传统意义上的命令行总被看成是上世纪的东西，图形界面首先看起来很直观，就像给你一堆散列的数据很难看出关系，但是如果画一个折线图，立马就能看出数据的走向。 命令行它的门槛要高一些，虽然使用起来不如图形界面清晰，但是在一些自动化程序中，命令行可以简化操作，省去了点点点的麻烦操作 举个例子我们以git为例，他在被Linus发明出来的时候肯定只有命令行，但是后来git的图形界面工具多到数不清，一个又一个方便的软件被创造出来，但是git命令行一直没有消失，而且我每天都在用，是因为我不会用图形界面吗？当然不是，因为在我使用的大部分需求中，我只要敲一两个命令就可以完成了，不必拿起鼠标右键菜单再去等几秒中的界面刷新。 还有一方面我需要经常编写自动化脚本，使用Jenkins部署服务器程序，这些环境下是没办法使用图形界面的，只能用命令行还编写逻辑。 适用场景图形界面适用于需要直观显示的情景，以及需要根据显示结果反复调整输入的情景，比如过滤commit，我可以在界面上不断调整和尝试过滤的关键词，而结果会随着我的调整发生改变。 命令行适用于简单命令和机械化的操作，最好是那种写完一遍可以用一辈子的逻辑，如果是类似过滤commit这种需求，我们必须不断的重新输入命令参数，也就是说命令行在我输入命令之后他的参数都确定了，绝大多数不能再调节了。 总有例外命令行的参数输入之后真的不能调吗？大多数是这样的，但总有例外，这种例外要求命令结果处于定时刷新状态，比如 top 命令，因为它的结果是可以刷新的，所以当你按下键盘上的 c 、P 等按键时，输出结果是会发生变化的。 总结 图形界面和命令行两者没有更好，只有更合适 图形界面适用于直观显示需求，可以根据用户输入随时调整输出结果 命令行更适用于自动化场景，可以将指令编写到操作逻辑中，通常来说更加高效 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 发现了一种我称之为反围城的状态：外面的人不想进去，里面的人不想出来~ 2023-4-3 22:31:29]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>图形用户界面</tag>
        <tag>命令行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7通过rpm包离线安装git]]></title>
    <url>%2Fblog%2F2023%2F03%2F21%2FCentOS7%E9%80%9A%E8%BF%87rpm%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85git%2F</url>
    <content type="text"><![CDATA[前言如果能连接外网，谁甘愿在离线状态下折腾，重新安装git的起因是服务器上的git版本(1.8.3.1)过低，Mac客户端升级后git版本(2.40.0)太高，导致无法验证密钥来进行拉取操作，所以要把服务器上的git升级一下，而git的1.8.3.1版本貌似是CentOS7的默认版本，我在多篇文章中看到了这个版本号，本来以为分分钟搞定，结果的折腾到半夜。 初次安装失败开始觉得挺顺利的，如果能升级就直接升级，不能升级就直接移除掉然后安装就行了，然后我就直接通过 sudo yum remove git 删除了git，刷的一下很快啊。 据说在 CentOS 7 上安装新版本 git 最快的方法是通过 End Point 库，所以我跟着找到的教程输入下面的命令企图添加 “End Point Package Repository”，但是报错： 12yum install https://packages.endpointdev.com/rhel/7/os/x86_64/endpoint-repo.x86_64.rpmyum install git 官网 packages.endpointdev.com/有专门讲解安装git 2.x的帖子，看起来很轻松，但是这条路我可能走不通了 我这里提示 “curl:(6) Could resolve host: packages.endpointdev.com; Unknown error”，我的天，难道这台机器不能访问外网？ 尝试修改软件源然后我尝试ping了几个常见的域名均已失败告终，所以我只能离线安装了？后来我发现它不是不能访问外网，只是不能访问域名，所以我自作聪明的把软件源/etc/yum.repos.d/CentOS-Base.repo里的域名都改成了IP: 1234567891011121314151617181920212223#mirrors.cloud.aliyuncs.com被我改成了100.100.2.148enabled=1failovermethod=prioritybaseurl=http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/gpgcheck=1gpgkey=http://mirrors.cloud.aliyuncs.com/centos/RPM-GPG-KEY-CentOS-7[updates]name=CentOS-$releaseverenabled=1failovermethod=prioritybaseurl=http://mirrors.cloud.aliyuncs.com/centos/$releasever/updates/$basearch/gpgcheck=1gpgkey=http://mirrors.cloud.aliyuncs.com/centos/RPM-GPG-KEY-CentOS-7[extras]name=CentOS-$releaseverenabled=1failovermethod=prioritybaseurl=http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/gpgcheck=1gpgkey=http://mirrors.cloud.aliyuncs.com/centos/RPM-GPG-KEY-CentOS-7 接着我就发现这样是无法使用的，这个域名的IP一直在变，然后我又找了公司内部的软件源直接把IP替换上去，发现访问超时，看来得解决无法解析域名的问题，后来查到的修改 /etc/resolv.conf，于是在文件中添加一下内容： 12nameserver 114.114.114.114nameserver 8.8.8.8 保存之后可以解析域名了，将软件源修改为域名的形式依旧访问超时，这条路彻底走不通了。 离线安装git离线安装类似于Window上安装一个.msi文件，不过git貌似没有特别方便的安装包，看看Linux下这些安装方式，基本都是需要联网下载的，其中CentOS还是最麻烦的，需要编译安装。 1234567891011121314151617181920212223242526272829303132333435Debian/UbuntuFor the latest stable version for your release of Debian/Ubuntu# apt-get install gitFor Ubuntu, this PPA provides the latest stable upstream Git version# add-apt-repository ppa:git-core/ppa # apt update; apt install gitFedora# yum install git (up to Fedora 21)# dnf install git (Fedora 22 and later)Gentoo# emerge --ask --verbose dev-vcs/gitArch Linux# pacman -S gitopenSUSE# zypper install gitMageia# urpmi gitNix/NixOS# nix-env -i gitFreeBSD# pkg install gitSolaris 9/10/11 (OpenCSW)# pkgutil -i gitSolaris 11 Express# pkg install developer/versioning/gitOpenBSD# pkg_add gitAlpine$ apk add gitRed Hat Enterprise Linux, Oracle Linux, CentOS, Scientific Linux, et al.RHEL and derivatives typically ship older versions of git. You can download a tarball and build from source, or use a 3rd-party repository such as the IUS Community Project to obtain a more recent version of git.Slitaz$ tazpkg get-install git 但是编译git需要有依赖项，还好它依赖的不多，很多教程上来就装一堆，不管有用没有了： 1yum install -y zlib-devel bzip2-devel openssl-devel ncurses-devel gcc perl-ExtUtils-MakeMaker package 但是我的机器上 yum 命令已经废了，无法联网安装新的东西，所以这些依赖项我必须找到安装包来离线安装，这就要用到rpm包了。 什么是rpmrpm（redhat package manager） 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎，逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 上的软件易于安装和升级，说白了就是可以通过它来安装软件包。 RPM包里面包含可执行的二进制程序，还包括程序运行时所需要的文件有时除了自身所带的附加文件保证其正常以外，还需要其它特定版本文件，这就是软件包的依赖关系，这和Windows的软件包结构类似。 安装准备 下载指定版本git源码 source code 下载zlib-devel-1.2.7-15.el7.x86_64.rpm RPM库 机器上本来有gcc，如果没有自己想办法装吧 幸运的是我的机器上有gcc，并且经过实践发现只要成功安装了zlib-devel就可以完成git的编译，如果没安装会报编译错误： 12345cache.h:21:18: fatal error: zlib.h: No such file or directory #include &lt;zlib.h&gt; ^compilation terminated.make: *** [fuzz-commit-graph.o] Error 1 在有网的情况下，CentOS使用 yum install zlib-devel 解决，Ubuntu安装 apt-get install zlib1g-dev 解决 安装zlib-devel如果版本正确通过rpm安装还是挺快的，但是我一开始下载了另一个版本 zlib-devel-1.2.7-18.el7.x86_64.rpm，安装时提示我： 123[root@demo software]# rpm -i zlib-devel-1.2.7-18.el7.x86_64.rpmerror: Failed dependencies: zlib = 1.2.7-18.el7 is needed by zlib-devel-1.2.7-18.el7.x86_64 我又去下载了 zlib-1.2.7-18.el7.x86_64.rpm ，结果安装时出现冲突： 12[root@demo software]# rpm -i zlib-1.2.7-18.el7.x86_64.rpm file /usr/lib64/libz.so.1.2.7 from install of zlib-1.2.7-18.el7.x86_64 conflicts with file from package zlib-1.2.7-15.el7.x86_64 最后找了兼容的版本 rpm -i zlib-devel-1.2.7-15.el7.x86_64.rpm 解决了这个问题。 编译git 解压git源码 tar -zxf git-2.28.0.tar.gz 进入解压后的目录，执行安装前检查 make configure 指定安装目录 ./configure --prefix=/usr/local 执行编译安装 make install 查看git版本 git --version 终于装完了，呼~ 总结 服务端和客户端的git版本差距过大时可能导致无法验证密钥的问题 在CentOS上git可以采用yum安装，也可以使用 “End Point Package Repository” 来安装 如果想通过编译安装git需要有gcc，还要能安装上zlib-devel依赖项 参考文档：https://zhuanlan.zhihu.com/p/352955774 补充yum失败终于找到了我的yum更新源一直超时的原因，是因为我通过 Wandisco GIT存储库 方式尝试安装过 git，步骤如下： 先写入新yum存储库配置文件启用Wandisco GIT存储库，在终端输入 vim /etc/yum.repos.d/wandisco-git.repo 文件中加入如下内容，保存退出 123456[wandisco-git]name=Wandisco GIT Repositorybaseurl=http://opensource.wandisco.com/centos/7/git/$basearch/enabled=1gpgcheck=1gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco 导入存储库GPG密钥 sudo rpm --import http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco 安装Git yum install git 结果 opensource.wandisco.com 无法正常访问，导致 yum update 报错，把 wandisco-git.repo 文件删掉就好了 rpm 和 yum 的关系之前讲到rpm是由Red Hat公司开发的软件包管理方式，使用它可以方便的进行软件的安装、查询、卸载、升级等工作。但处理rpm软件包之间的依赖性问题往往会很繁琐，尤其是当软件由多个rpm包组成时，比如我们在离线安装git时遭遇到的问题。 Yum（Yellow dog Updater, Modified）是一个在Fedora和RedHat等系统上基于RPM包管理前端软件包管理软件，能够从指定的服务器自动下载RPM包并且安装，并自动处理软件包的依赖关系，可以一次安装所有依赖的软体包。 总结来说就是, rpm命令只能安装已经下载到本地机器上的rpm包，并且自己解决依赖问题，yum能在线下载并安装rpm包、更新内核、并自动处理包与包之间的依赖问题。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 死去元知万事空，但悲不见九州同~ 2023-3-21 23:11:12]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>rpm</tag>
        <tag>yum</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下利用rsync+find实现同步指定时间段文件]]></title>
    <url>%2Fblog%2F2023%2F03%2F18%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%88%A9%E7%94%A8rsync-find%E5%AE%9E%E7%8E%B0%E5%90%8C%E6%AD%A5%E6%8C%87%E5%AE%9A%E6%97%B6%E9%97%B4%E6%AE%B5%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言这几天一直在处理shell脚本，作为服务器开发人员免不了要部署一些环境，数据备份和同步工作也是家常便饭，最近常搞的几个命令有 find、rsync、sed 等，之前也写过一篇 《linux环境下使用rsync命令完成数据同步》，不过这次有新的需求了。 插曲前段时间发现通过 rysnc 同步游戏版本数据时，在数据同步前有一个较长的等待时间，大概12G数据需要等待4分钟，多方面查找原因后定位到应该是 rsync 的 -c 参数导致的，参数全称是 --checksum 打开校验开关，强制对文件传输进行校验，多次实验后发现只要把 -c 参数省略，这个检验所花的4分钟就能省掉，同步前校验只会根据修改的时间戳和文件大小来判断，这在大多数情况下足够了。 根据时间段同步现在想同步日志文件，因为日志文件是每小时产生一个，之前的日志文件绝大多数情况也不会改动，所以再同步时没必要检测所有的日志文件，只需要同步最近一段时间的就可以了，这样可以减少文件比对的时间。 查询了 sync 命令的所有参数，没有找到指定同步时间段的参数，但是有个 --files-from=FILE 看起来能实现这个需求，man手册中是这样写的： 12345678910111213Using this option allows you to specify the exact list of files to transfer (as read from the specified FILE or - for standard input). It also tweaks the default behavior of rsync to make transferring just the specified files and directories easier: o The --relative (-R) option is implied, which preserves the path information that is specified for each item in the file (use --no-relative or --no-R if you want to turn that off). o The --dirs (-d) option is implied, which will create directories specified in the list on the destination rather than noisily skipping them (use --no-dirs or --no-d if you want to turn that off). o The --archive (-a) option’s behavior does not imply --recursive (-r), so specify it explicitly, if you want it. o These side-effects change the default state of rsync, so the position of the --files-from option on the command-line has no bearing on how other options are parsed (e.g. -a works the same before or after --files-from, as does --no-R and all other options). The filenames that are read from the FILE are all relative to the source dir -- any leading slashes are removed and no &quot;..&quot; references are allowed to go higher than the source dir. For example, take this command: rsync -a --files-from=/tmp/foo /usr remote:/backup 基于参数的描述，我只要提供一个包含指定时间段的待同步的所有文件名的文件就可以了，比如将指定时间段需要同步的文件名放到 /tmp/foo 文件中，然后以 --files-from=/tmp/foo 形式来指定就行了。 其实还有一个方便的写法，就是直接在参数后面利用 find 写过滤文件的命令，比如只同步最近3天修改过的文件就可以写成： 1rsync -avz --files-from=&lt;(find /var/log/ -mtime -3) /usr remote:/backup 其中 find /var/log/ -mtime -3 命令的含义就是找出 /var/log/ 目录下最近3天修改的文件。 按时间过滤文件有了上面的基础命令，想要同步不同时间段的文件只需要修改 find 命令参数就可以了，比较常用的参数就是 mtime 了。 使用 mtime 参数查找mtime 表示文件的修改时间，用在 find 查找时是以天为单位的，最小间隔24小时，数字是几就表示几天前，带有 + 表示几天前之外，带有 - 表示今天前之内，下面举几个例子： 查找修改时间在3天之前的文件（修改时间距今大于96小时） 1find /var/log/ -mtime +3 查找修改时间在3天之内的文件（修改时间距今小于72小时） 1find /var/log/ -mtime -3 查找修改时间在3天前当天的文件（修改时间距今大于72小时，小于96小时） 1find /var/log/ -mtime 3 如果对mtime参数的使用有点疑惑，可以看一下下面的示意图： 除了上面提到的 -mtime 参数，还有类似的 -atime 和 -ctime 参数，这三个参数的含义如下，使用时可根据定义来选择： 123-atime：文件访问时间，文件被读取或执行的时间。-ctime：属性改变时间，文件的inode被修改的时间-mtime：内容修改时间 使用 newermt 进行更精确查找find -newermt 的通用形式是find -newerXY reference，目的是找到一些X属性比variable的Y属性更早的文件，其中X代表find的目标文件属性，Y代表参照属性。X可选a,c,m，Y可选a,c,m,t，其中t代表客观绝对时间，只作为参照属性存在，具体详细的用法参照man手册： 12345678910-newerXY reference Compares the timestamp of the current file with reference. The reference argument is normally the name of a file (and one of its timestamps is used for the comparison) but it may also be a string describing an absolute time. X and Y are placeholders for other letters, and these letters select which time belonging to how reference is used for the comparison. a The access time of the file reference B The birth time of the file reference c The inode status change time of reference m The modification time of the file reference t reference is interpreted directly as a time Some combinations are invalid; for example, it is invalid for X to be t. Some combinations are not implemented on all systems; for example B is not supported on all sys‐tems. If an invalid or unsupported combination of XY is specified, a fatal error results. Time specifications are interpreted as for the argument to the -d option of GNU date. If you try to use the birth time of a reference file, and the birth time cannot be determined, a fatal error message results. If you specify a test which refers to the birth time of files being examined, this test will fail for any files where the birth time is unknown. 如果要找到修改时间在2023-3-18 20:00:00 和 2023-3-18 22:00:00 之间的文件可以写成： 1find /var/log -newermt '2023-3-18 20:00:00' ! -newermt '2023-3-18 22:00:00' 总结 rsync 命令本身不能指定同步的时间段，可搭配 find 命令来同步指定时间段的文件 rsync -avz --files-from=&lt;(find /var/log/ -mtime -3) /usr remote:/backup 同步修改时间在3天内的文件 使用 newermt 可按时间精确查找，形式如 find /var/log -newermt &#39;2023-3-18 20:00:00&#39; ! -newermt &#39;2023-3-18 22:00:00&#39; 假设变量 KEY=birthday、FILE=myfile.log，查找KEY所在的行号可以使用 grep -rn $KEY&quot; $FILE | awk -F &#39;:&#39; &#39;{print $1}&#39; 还是在上面的假设 sed -n &#39;/${KEY}=/&#39; $FILE 无法获得行号，使用 sed -n &#39;/birthday=/&#39; $FILE 就可以 上面这种sed命令中使用变量不生效的问题目前还没找到解决方法，有解决方案的大佬请指教 不过 sed -e &quot;${LINE},${LINE}s%$OLD_VAL%$NEW_VAL%g&quot; $FILE 这种替换命令就支持变量，但sed不支持非贪婪模式，目前有点迷糊，还在探索中 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 不要规划孩子必须在哪一个领域取得成就，但只要她喜欢，就提供一个舞台，让她尽情去发挥~ 2023-3-19 21:01:51]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>find</tag>
        <tag>shell</tag>
        <tag>rsync</tag>
        <tag>sed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ununtu环境下的判断字符串相等出现sh: xxx: [: xxx: unexpected operator的问题]]></title>
    <url>%2Fblog%2F2023%2F03%2F17%2FUnuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84sh-xxx-xxx-unexpected-operator%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言这是个小问题，本质原因是判断字符相等使用两个 == 需要在 bash 环境下，而Ubuntu默认不是这个Shell环境，所以就会出现在 CentOS 上运行的好好的Shell，放到Ubuntu上就运行出错的问题，我遇到的问题本质也是这个原因，不过在复杂的状况下却不能一眼找到是哪里导致的。 现场还原为了找出这个问题的原因，我把无关逻辑都去掉了，出现了下面a.sh 和 b.sh 两个脚本： 12345#!/bin/bashVAL=$1sh ./b.sh $VAL 123456789#!/bin/bashVAL=$1if [ $VAL == "1" ] ; then echo "find it"fiecho "end" 考虑到兼容问题，我还在开头写了 #!/bin/bash，让它默认的用bash来执行，这样我无论在 Ununtu 还是其他的 Linux 版本都应该没问题了，可事实却不是这样，运行结果如下： 123root@demo:~# ./a.sh fly./b.sh: 5: [: fly: unexpected operatorend 如果眼尖的话一眼就看出来了，问题出在 sh ./b.sh $VAL 这一句，导致 #!/bin/bash 没有起到作用，使用 sh 这个shell 来执行的 b.sh 脚本，所以用 == 判断字符串相等会报错。 这个例子里很好发现，但是放在实际工作中的复杂脚本中就比较难发现了，再看报错 ./b.sh: 5 没有发现任何问题，因为查到可能是 Ubuntu 系统下的shell 语法导致的，我还尝试了多次改成单等号的写法，其他比较字符串的方式，依旧报错，但是当我发现问题的所在之后，无论单个等号还是两个等号我都能让他正确执行，这就是忙中出乱，静下来就好了 总结 使用 == 判断字符串相等是 bash 中的语法 Shell 脚本开头的 #!/bin/bash 表示执行脚本时若没有指定Shell类型，则用说明的这种Shell执行 遇到问题时需要静下心来慢慢解决，切忌忙中出乱 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有的人活下来就已经花光了所有力气，还要靠什么去抗争命运~ 2023-3-17 22:55:15]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>shell</tag>
        <tag>字符串判断</tag>
        <tag>等号问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一下第一次安装和配置Nginx]]></title>
    <url>%2Fblog%2F2023%2F03%2F14%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%8B%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AENginx%2F</url>
    <content type="text"><![CDATA[前言Nginx虽不是第一次接触，但是使用较少，今天算是完整的安装了配置了一次，前段时间想写一写的《Nginx的安装与常用配置》，因突发状况没有写完，今天补充一下。 安装 nginxnginx 安装比较方便，我安装的环境是拥有root权限的Ubuntu 20.04，安装只需要以下两个命令： 12apt updateapt install nginx 安装完默认启动，查询运行状态的命令 systemctl status nginx 123456789101112131415161718192021# systemctl status nginx● nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2023-03-14 21:09:35 CST; 2h 55min ago Docs: man:nginx(8) Process: 943307 ExecStartPre=/usr/sbin/nginx -t -q -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Process: 943314 ExecStart=/usr/sbin/nginx -g daemon on; master_process on; (code=exited, status=0/SUCCESS) Main PID: 943315 (nginx) Tasks: 5 (limit: 9132) Memory: 10.6M CGroup: /system.slice/nginx.service ├─943315 nginx: master process /usr/sbin/nginx -g daemon on; master_process on; ├─943316 nginx: worker process ├─943317 nginx: worker process ├─943318 nginx: worker process └─943319 nginx: worker processMar 14 21:09:35 w-mi-dev systemd[1]: Starting A high performance web server and a reverse proxy server...Mar 14 21:09:35 w-mi-dev snoopy[943307]: [hostname:w-mi-dev ssh:((undefined)) uid:0 username:root pid:943307 ppid:1 rpname:(nginx) sid:943307 tty:(none) cwd:/ filename:/usr/sbin/nginx]: /usr/sbin/ng&gt;Mar 14 21:09:35 w-mi-dev snoopy[943314]: [hostname:w-mi-dev ssh:((undefined)) uid:0 username:root pid:943314 ppid:1 rpname:(nginx) sid:943314 tty:(none) cwd:/ filename:/usr/sbin/nginx]: /usr/sbin/ng&gt;Mar 14 21:09:35 w-mi-dev systemd[1]: Started A high performance web server and a reverse proxy server. 要想测试的话通过浏览器访问 http://YOUR_IP，通常会看到 “Welcome to nginx” 字样的界面，但这对于我是没用的，我的默认端口访问不了。 查找nginx安装和配置文件的位置我的nginx安装在 /usr/sbin/nginx 目录，配置文件主目录在 /etc/nginx/nginx.conf，找的方法比较简单就是用 which 命令 12# which nginx/usr/sbin/nginx 找配置文件时利用 nginx 检查配置文件的 -t 参数就可以了 123# nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful 如果使用 which 命令找不到，可以利用 ps -ef | grep nginx 找到进程id，在利用 ll /proc/nginx_pid 来找到安装的位置。 修改配置文件进入 /etc/nginx 目录，可以看到多个配置文件和目录 12345678910111213141516171819# cd /etc/nginx/# lltotal 64drwxr-xr-x 2 root root 4096 Mar 14 20:56 conf.d/-rw-r--r-- 1 root root 1077 Feb 4 2019 fastcgi.conf-rw-r--r-- 1 root root 1007 Feb 4 2019 fastcgi_params-rw-r--r-- 1 root root 2837 Feb 4 2019 koi-utf-rw-r--r-- 1 root root 2223 Feb 4 2019 koi-win-rw-r--r-- 1 root root 3957 Feb 4 2019 mime.typesdrwxr-xr-x 2 root root 4096 Nov 10 14:38 modules-available/drwxr-xr-x 2 root root 4096 Mar 9 21:30 modules-enabled/-rw-r--r-- 1 root root 1490 Feb 4 2019 nginx.conf-rw-r--r-- 1 root root 180 Feb 4 2019 proxy_params-rw-r--r-- 1 root root 636 Feb 4 2019 scgi_paramsdrwxr-xr-x 2 root root 4096 Mar 9 21:30 sites-available/drwxr-xr-x 2 root root 4096 Mar 9 21:30 sites-enabled/drwxr-xr-x 2 root root 4096 Mar 9 21:30 snippets/-rw-r--r-- 1 root root 664 Feb 4 2019 uwsgi_params-rw-r--r-- 1 root root 3071 Feb 4 2019 win-utf 其中 nginx.conf 是主配置文件，内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384user www-data;worker_processes auto;pid /run/nginx.pid;include /etc/nginx/modules-enabled/*.conf;events &#123; worker_connections 768; # multi_accept on;&#125;http &#123; ## # Basic Settings ## sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; # server_tokens off; # server_names_hash_bucket_size 64; # server_name_in_redirect off; include /etc/nginx/mime.types; default_type application/octet-stream; ## # SSL Settings ## ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3; # Dropping SSLv3, ref: POODLE ssl_prefer_server_ciphers on; ## # Logging Settings ## access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; # Gzip Settings ## gzip on; # gzip_vary on; # gzip_proxied any; # gzip_comp_level 6; # gzip_buffers 16 8k; # gzip_http_version 1.1; # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*;&#125;#mail &#123;# # See sample authentication script at:# # http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript## # auth_http localhost/auth.php;# # pop3_capabilities "TOP" "USER";# # imap_capabilities "IMAP4rev1" "UIDPLUS";## server &#123;# listen localhost:110;# protocol pop3;# proxy on;# &#125;## server &#123;# listen localhost:143;# protocol imap;# proxy on;# &#125;#&#125; 这个文件是默认的配置文件，我们可以先不管，先注意几点就行： 访问日志在 /var/log/nginx/access.log 错误日志在 /var/log/nginx/error.log 主配置文件包含了 /etc/nginx/conf.d/*.conf 这个目录下所有配置 所以我们在 /etc/nginx/conf.d/ 目录下新建一个 project.conf 文件，编写如下内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344upstream login_entrance &#123; server localhost:4101; server localhost:4102;&#125;upstream exit_entrance &#123; server localhost:6101; server localhost:6102;&#125;server &#123; listen 4100; server_name localhost; location / &#123; proxy_pass http://login_entrance; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125;server &#123; listen 6100; server_name localhost; location / &#123; proxy_pass http://exit_entrance; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125;server &#123; listen 8100; server_name localhost; location / &#123; root /usr/share/nginx/html; &#125;&#125; 配置文件的含义： 访问 8100 端口可以看到 nginx 欢迎页 将发送给4100端口的数据转发给4101和4102端口 将发送给6100端口的数据转发给6101和6102端口 重启nginx服务重启可采用关闭服务器再开启的方式，也可以直接使用以下命令： systemctl restart nginx nginx -s reload 查看报错当nginx启动不起来是可以通过 systemctl status nginx 命令查看，也可以直接查看 errorlog 文件，比如可能会看到如下内容： 12342023/03/14 20:51:06 [emerg] 937503#937503: bind() to 0.0.0.0:4100 failed (98: Address already in use)2023/03/14 20:51:06 [emerg] 937503#937503: bind() to 0.0.0.0:4100 failed (98: Address already in use)2023/03/14 20:51:06 [emerg] 937503#937503: bind() to 0.0.0.0:4100 failed (98: Address already in use)2023/03/14 20:51:06 [emerg] 937503#937503: bind() to 0.0.0.0:4100 failed (98: Address already in use) 一看就应该知道是端口4100被占用了。 负载均衡配置负载均衡是nginx一个常见的应用，只需要在配置文件中加入一些参数即可： 轮询这是默认的策略，把每个请求按次序逐一分配到不同可用的server，如果分配到的server不可用，则自动分配到下一个 1234upstream good.cc &#123; server localhost:4101; server localhost:4102;&#125; 权重weight默认值为1，值越大则代表被访问的几率越大，下面配置中4102端口的访问几率是4101的二倍 1234upstream good.cc &#123; server localhost:4101 weight=1; server localhost:4102 weight=2;&#125; 最少连接顾名思义就是把请求分配到连接数最少的server 12345upstream good.cc &#123; least_conn; server localhost:4101; server localhost:4102;&#125; IP哈希根据访问客户端ip的hash值分配，这样同一客户端的请求都会被分配到同一个server上，适合有状态的服务 12345upstream good.cc &#123; ip_hash; server localhost:4101; server localhost:4102;&#125; 总结 nginx 在 Ununtu 20.04 上安装非常简单，使用 apt install nginx 就行啦 nginx 的默认主配置文件是 /etc/nginx/nginx.conf 会包含同级目录下 conf.d/*.conf 所有配置 nginx 服务可以用 systemctl status nginx 方式查看 nginx 的重启方式 systemctl restart nginx 或 nginx -s reload nginx 作为负载均衡服务器有轮询、权重、最少连接、ipHash等多种配置方式 我也是刚刚入门，关于使用方法和配置文件还有很多东西要学，后面慢慢总结吧 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 无善无恶心之体，有善有恶意之动，知善知恶是良知，为善去恶是格物，愣凑数啊~]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>负载均衡</tag>
        <tag>反向代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git配置中的core.autocrlf和core.safecrlf]]></title>
    <url>%2Fblog%2F2023%2F03%2F04%2Fgit%E9%85%8D%E7%BD%AE%E4%B8%AD%E7%9A%84core-autocrlf%E5%92%8Ccore-safecrlf%2F</url>
    <content type="text"><![CDATA[前言使用git作为版本管理工具的朋友们，有没有遇到更新完同事写的脚本无法运行的问题？逻辑正确的脚本换台机器不能运行了多半是换行符捣的鬼，修改git中的core.autocrlf设置即可解决问题，那么autocrlf是什么意思呢？我来简单件解释下。 core.autocrlfautocrlf 是三部分的缩写 auto-cr-lf，分开来看应该是3组词 auto/carriage-return/linefeed，直译的话就是自动回车换行。 还是说下前面的问题，这是因为 Windows 使用回车（CR）和换行（LF）两个字符来结束一行，而 MacOS 和 Linux 只使用换行（LF）一个字符来结束一行。 当你在Windows下用常见编辑器写一个Shell脚本是，直接拿到Linux去执行很可能会报错，而在Linux编辑的文件拿到Windows环境下打开通常会显示成一行。 git 作为一个多人共同使用的版本管理工具，使用者的系统可能是Windows、Linux或者是Mac中的任意一种，所以在共同操作一个文件时就会造成换行符混乱，这时就要用到 core.autocrlf参数了。 core.autocrlf 参数不仅影响检出时的换行符装换，还会影响提交时的换行符转换，共有 true、false、input 三种可选值 12345678# 提交时转换为LF，检出时转换为CRLFgit config --global core.autocrlf true# 提交和检出时都不转换git config --global core.autocrlf false# 提交时转换为LF，检出时不转换git config --global core.autocrlf input 根据他们的定义我们可以选择一种保险的方式，在windows系统上设置为 true，在 Linux 和 Mac 上设置为 input，如果所有使用者仅在统一系统下工作，设置成 false 就好了。 core.safecrlf前一个 autocrlf 大家听到的应该比较多，而这个safecrlf可能很多人是第一次听到，它的默认只是false，若果改成true表示拒绝提交混合换行符的代码，也就是说如果你的代码中同时包含两种系统中不同的换行格式会被禁止提交，如果设置成 warn 表示只提出警告但可以提交。 总结 多人合作的git库如果下载之后，其中的脚本不能直接运行先看看是不是换行符搞的鬼 Windows平台建议设置 git config --global core.autocrlf true Linux平台建议设置 git config --global core.autocrlf input 可以使用工具 dos2unix 与 unix2dos 将 LF 和 CRLF 相互转化 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 一代文豪东坡居士，却写多篇祈雨文~]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>换行符</tag>
        <tag>配置</tag>
        <tag>autocrlf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工作十年对游戏看法的转变历程]]></title>
    <url>%2Fblog%2F2023%2F02%2F05%2F%E5%B7%A5%E4%BD%9C%E5%8D%81%E5%B9%B4%E5%AF%B9%E6%B8%B8%E6%88%8F%E7%9C%8B%E6%B3%95%E7%9A%84%E8%BD%AC%E5%8F%98%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言现在一提到玩游戏很多人的第一反应就是『手游』『网游』，是社会的发展将“游戏”这个词狭义化了，甚至在搜索引擎中输入『游戏』一词，得到的前几页搜索结果都是手机游戏推荐，而游戏本来的定义是指体育手段的一种，也是文化娱乐的一种形式。以一定的形式反映人类社会劳动、军事、文化等方面的活动。 称之为游戏论之父的豪伊金格说：“游戏是在明确规定的时间、空间里所进行的行为或者活动。它是按照自发接受的规则来进行的。这种规则一旦被接受就具有绝对的约束力。游戏的目的就存在于游戏行为自身之中，它伴有紧张和喜悦的感情与日常生活不同。” 现在也很少听到“网瘾少年”这个词了，在网络刚刚兴起时，这个词可是让很多父母闻风丧胆，当网络开始和游戏相结合时，又让多少自制力差的孩子深陷其中，所以当时的网络、游戏都被看做是大老虎，是一切美好事物的对立面，但随着时间的推移，互联网开始普及并渗透到生活中方方面面。 人们理智了很多，认识到了我们的生活离不开网络，而孩子们从小生活在这样的环境中，对一切都习以为常，所以很难出现深陷其中无法自拔的情况，之前是因为没有接触过，所以一旦陷入这种虚拟环境中很容易无法自控，而如今社会网络无处不在，没有而来环境落差也就不容易沉迷了，就像一个穷苦孩子落入富豪家就会沉迷，而富二代的公子哥只觉得眼前的生活无趣而已。 初识游戏我们从来到这个世界就在和游戏打交道，三岁前父母家人是我们最亲密的游戏玩伴，上了幼儿园又有了其他的小伙伴，而我对这些记忆已经模糊了，除了捉迷藏、丢沙包、跳皮筋这些常规游戏外，记得还算清楚的一个游戏应该是小学时玩的，名字叫『鸭子嘴』，就是一群孩子追来追去，那时的孩子们玩这些游戏很开心的。 那时的电脑还是一个昂贵的稀罕物，我记得经常在电视购物广告里看到，“电脑只要9999轻松搬回家”，当时并不知道电脑能做什么，其实当时的家用电脑还真做不了什么，方方的大头，粗糙的98。 后来老爸给买了一台学习机，就是小霸王横行的那个年代，这是我第一次接触电子游戏，虽然他的目的是让我来学习打字的，可是我的盲打真的很烂，还是工作之后才慢慢好的，为此还被同事嘲笑过。 这台学习机是我对电脑的启蒙，真正接触电脑是初中的微机课，每周只有一节课，大头电脑没有网络，操作完Excel和Word能让玩一会扫雷都开心的不得了，当然，课业紧张时微机老师总是恰好来不了。 在高中时我家买了台电脑，当时整个村子应该也没有几台，而我对电脑也渐渐熟悉了，当时对游戏没有那么痴迷，但是对QQ太阳过分迷恋，整天的挂着QQ升等级，没事养养动物偷偷菜，电脑里的红色警戒我就没玩明白过。 计算机专业因为接触过电脑觉的很有意思，所以大学想学计算机专业，而我也很幸运的被第一志愿的计算机专业录取，大学军训的刚结束就跟着班长去网吧了，当时好像没玩游戏，去网吧是登QQ跟家人视频了，说出来可能很多人不信，那是我第一次去网吧，那天晚上视频时老爸还说呢，刚上大学就学会上网吧了，你这是要学坏吧，看了吧，这就是那个谈『网吧』色变的时代。 其实当时学计算机也不是为了游戏，但是因为学计算机，让我更多的接触到了游戏。前面提到说我要挂QQ等级、要偷菜、要升级农场，这就为我后来的游戏风格建立的开端，这样游戏都是养成类，也就是你的付出在游戏中一直有积累，比较偏向于RPG类型游戏。 所以我在上大学后，同学邀我一起玩对战游戏dota时，我觉得简直不可思议，花了半天时间攒的金钱装备，下一把游戏时一点用都没有了，这游戏有什么意思啊。 但是人总是会变的，当融入兄弟环境中，一起开黑打dota，打CF，我也渐渐迷上了这种MOBA类风格，但是我是个手残，技术特别烂，技能通常是一顿乱按，就是喜欢和大家凑热闹，一起在游戏里玩而已，我就是传说中越菜越爱玩的菜狗。 步入游戏开发行业大学时光转瞬即逝，时间来到大四实习阶段，从几份Offer中选择了工资较低但比较感兴趣的游戏公司，找工作之前并没有想着一定要开发游戏，但是缘分来了挡都挡不住，众多工作中只有一份游戏开发，而我就选择了它，在一家小公司工资只有可怜的3K。 那是2013年页游风靡的时代，我们也不落俗套，正正经经的微端页游，类型MMORPG。后来在同一家公司接触了端游，手游，虽然形式不同，但类型一直是MMORPG，没想到这家签了毕业三方的小公司，我一待就是6年多。 长时间同质化游戏的开发，使我对游戏的兴趣大大降低，从刚开始的新奇，到后来的麻木，我发现我对这些游戏不再感冒，有时候甚至都不想伸手点一点了。 这一阶段我还能坚持玩的两款游戏，一个是之前玩的《dota》，一个是后来通了几千关的《开心消消乐》，重度的RPG我真的不想再玩，因为我真的有点肝吐了，各种游戏玩法上线前我都测了几百遍，上线后真的不想玩了。 转变游戏开发类型肝不动的RPG影响了心情，在2020年我跳出了原来的舒适圈，开始了回合制游戏的开发，新换的环境必然会带来新鲜感，我开始了新的征程，体会着回合制与RPG游戏的差距。 但是近几年回合制游戏变了，除了战斗模式，回合制变得越来越RPG化，重度的培养系统慢慢加入，导致回合制游戏也很肝，工作3年之后我觉得之前很多的兴趣点也没有了。 现在的对于我这些哪里亮光点哪里的游戏，真的感觉有些麻木，难道我做出来的游戏就是引导大家一直点亮光？难道就是让大家熬夜点亮光？难道就是让大家把钱放到游戏中不断点亮光？ 手机游戏应该什么样最近我经常有些错觉，我之前真的是做游戏的？我做的那些真的叫游戏吗？我总有一种感觉，这种跟着指引点点点的东西没有游戏性，根本不能叫游戏，甚至是扫雷、蜘蛛纸牌、俄罗斯方块这些才能叫做游戏，一个RPG游戏整天无脑点点点到底有什么游戏性可言？ 回归游戏的本质，“游戏的目的就存在于游戏行为自身之中，它伴有紧张和喜悦的感情与日常生活不同”。也就是说这些东西是不是游戏要分人的，如果玩这个东西能让你感觉到快乐、感觉到兴趣使然，那么它就是游戏，而对于我来说，这种东西我真的是不爱玩了。 现在的我想去试一试休闲游戏的开发，因为我觉得那才是我现在的兴趣点，是能给我带来快乐的游戏类型。 我的历程人生进行到现在和游戏密不可分，接触电脑 -&gt; 报计算机专业 -&gt; 和同学一起打电脑游戏 -&gt; 毕业进行游戏开发 -&gt; MMORPG -&gt; 回合制卡牌。 喜欢的游戏类型也在悄然发生变化，小游戏 -&gt; 模拟经营养成类 -&gt; MOBA、FPS -&gt; MMORPG、回合制卡牌 -&gt; dota、休闲类，因为现在的网络游戏太肝了，所以渐渐只玩随时可以暂停的小游戏了。 现在的我也很迷茫，觉得自己并不是一个游戏开发人员，起码是不合格的，除了十几前年的学习机让我体验了几把游戏，后来的游戏机我是一窍不通，所以我也想回归游戏本质，去年年末我在便利店买了一个8090掌上游戏机，就是最原始的俄罗斯方块，玩了几次感觉挺好的。 这是我曾经梦寐以求却得不到的，现在终于有能力把它买回家好好玩了，经典的游戏永不过时。 买年货时又和老婆在商场的游戏厅体验了一下当年的手柄游戏，感觉这才有游戏的乐趣。 为了不与时代脱轨，我前段时间又在网上下单了台Switch游戏机，真的是查了很多资料才弄清楚要买什么，已经在海关卡了好几天了，估计还得至少半个月才能到我手中，希望我能从这个现代的游戏机中找到一点灵感。 总结 游戏本来的定义是指体育手段的一种，也是文化娱乐的一种形式，如今的社会将游戏狭义化成了网游 我与游戏有缘，从接触电脑、选择专业，到毕业后找到游戏开发工作，因为喜欢dota、所以选择游戏 我对于我开发游戏的喜欢不是因为游戏性，而是有一种作为自己产品的感情，所以我自己做的游戏我都有充钱，但并不是因为它好玩 同一款游戏中玩法思路也在变，之前玩dota总喜欢玩战士或刺客，享受对拼的感觉，而现在更喜欢玩法师，变着花样搞怪更有意思 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 最近一周从好几个地方看到“人生的意义是什么”这个问题，有些问题我也做了回答，甚至很多人去请教了ChatGPT，但问这种问题真的有意义吗？问这种问题的人目前应该处于迷茫期，想要过来人提供一些答案，可人生必须要有意义吗？每个人的追求不同，人生意义的解释也就不同。我现在觉得清晨能看见一缕阳光，静静的呼吸着清晰的空气这就够了，如果还要奢侈一点，那么我想说“有事做，有人爱，有所期待”。 2023-2-5 19:03:15]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>游戏</tag>
        <tag>想法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跟风试试ChatGPT]]></title>
    <url>%2Fblog%2F2023%2F02%2F05%2F%E8%B7%9F%E9%A3%8E%E8%AF%95%E8%AF%95ChatGPT%2F</url>
    <content type="text"><![CDATA[前言其实现在也不算是跟风了，从 ChatGPT 出现至今已经有几个月的时间，这股风似乎已经刮过去了，虽然各种新闻铺天盖地，但因为懒一直没有亲手试一试，这两天尝试了一下，发现要想使用还是有点门槛的，它使用方式很简单，可以把它想象成一个智能机器人，可以回答你的各种提问，当然它比淘宝机器人客服智能多了，它的门槛在于地域限制非常强，中国用户要想使用须得费一番功夫。 什么是ChatGPTChatGPT 以其强大的信息整合和对话能力惊艳了全球，是人工智能研究实验室OpenAI新推出的一种人工智能技术驱动的自然语言处理工具，使用了Transformer神经网络架构，也是GPT-3.5架构，能够在输入序列中捕捉长期依赖性。它还使用了大量的语料库来训练模型，这些语料库包含了真实世界中的对话，以便模型能够更好地理解人类语言。 ChatGPT怎么玩要想使用 ChatGPT 主要有以下几步： 注册 ChatGPT 账号 完成 ChatGPT 手机号验证后登录 输入任意话题，探索 ChatGPT 的强大功能 注册 需要将网络环境切换成国外IP完成整个注册步骤（美国、加拿大、日本等，大陆IP肯定不行，香港澳门 IP 据说也不行） 访问 https://chat.openai.com/auth/login 链接并使用自己的邮箱进行账号注册，我就偷了个懒使用 Microsoft Account 直接登录的 不管是用邮箱注册，还是第三方账号登录都需要打开邮箱查收 OpenAI 账号验证邮件，并完成邮箱验证 验证 登录成功之后需要验证手机号，这一步中国大陆手机号不好使，需要借助短信接码平台 sms-activate.org 具体使用方法比较简单，可以自己试试或者百度一下，其实就是提供一个外国号码给我们验证使用，在网站上可以收验证码，把收到的验证码填到 ChatGPT 平台就可以完成验证了 使用前需要先用邮箱在 sms-activate.org 注册账号后充值，用支付宝可以自动转汇率充值，佣金2.3%，我充值的这段过年时间好像免佣金 一般充值0.2美元就可以，因为免佣金我充了0.5，方便以后再用 选择号码地区时尽量使用印度尼西亚，这个地区验证过了，没有问题 使用直接在最下方的框中输入想问的问题就可以了，下面随便截两个问题，有些问题它还是答不上来的，我曾让它用 Python 画一个猪猪，每次代码生成约10分钟就卡主了，让它画一个兔子结果只画了耳朵。 设计型 开发型 强人所难型 Python调用ChatGPT先去 https://beta.openai.com/account/api-keys 点击 Create new secret key 按钮创建一个API密钥，然后赋值给下面代码中的 openai.api_key 变量直接运行就可以了 1234567891011121314151617import openaidef use_openai(): # Set your API key openai.api_key = "YOUR_API_KEY" # Use the GPT-3 model completion = openai.Completion.create( engine="text-davinci-002", prompt="今天中午吃什么", max_tokens=1024, temperature=0.5 ) # Print the generated text print(completion.choices[0].text)if __name__ == '__main__': use_openai() 总结 ChatGPT 是经过训练的语言模型，以GPT-3.5为基础，其强大的信息整合和对话能力惊艳了全球 ChatGPT 有时很聪明，仿佛是一个拥有了自己的智力，可以应对许多问题，甚至可以直接写代码 ChatGPT 有时却很蠢，特意回避了一些问题，并认为加了一些限制，所以目前来看局限性还很大 ChatGPT 注册登录地址 https://chat.openai.com/auth/login ChatGPT 短信验证可使用的第三方平台 https://sms-activate.org ChatGPT 创建密钥地址 https://beta.openai.com/account/api-keys ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 试图窥探整个世界的运行规律，至今一无所获~]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>OpenAI</tag>
        <tag>人工智能</tag>
        <tag>智能机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows10彻底关闭自动更新]]></title>
    <url>%2Fblog%2F2023%2F02%2F05%2FWindows10%E5%BD%BB%E5%BA%95%E5%85%B3%E9%97%AD%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[前言在写这篇总结之前，我已经尝试过多次关闭Win10的自动更新了，一般来说更新之后可以使操作系统更加健壮，但总会有一些原因让我们不想更新，比如我最近总是需要远程操作电脑，如果它临时更新会导致我无法继续使用，所以我一般会关掉更新，然后在一个空闲的时间让他尽情更新个够。 网上有很多关于停止自动更新的文章，有些操作起来比较简单，有些则非常繁琐，但大多数教程都不太好使，很多时候就是临时有效，过段时间就又开始自动更新了，这让我想起了很久之前设计的流氓软件。 无赖一个软件会有一个正常的A进程，然后还有一个默默守护着A进程的B进程，当B进程发现一段时间内A进程不见了，就自动启动A进程，如果担心B进程被发现，还可以搞一个默默守护B进程的C进程，子子孙孙无穷尽也。 操作系统的自动更新当然不会这么直白，但是禁用之后还能自动启动，说明一定有其他的东西在默默的守护着他，如果找到这些守护者，把他们干掉，自动更新也就被真正禁止了。 步骤关闭自动更新服务按下快捷键 Win+R 打开运行对话框，输入services.msc，点击『确定』按钮或者直接回车打开服务页面 如图所示，找到 Windows Update 双击打开属性 在 Windows Update 属性页面的『常规』选项卡中，将启动类型设置为选择『禁用』，一次点击『停止』『应用』按钮 我们点击切换到『恢复』选项卡中，将失败后的操作均设置为『无操作』后，点击『应用』按钮 按照相同的方式处理 Update Orchestrator Service 服务，有些电脑中名字是 更新 Orchestrator 服务，也是按照上面的方法设置『常规』和『恢复』两个选项卡就好。 关闭自动更新策略上面的步骤是很多教程中都提到的，一开始耗时，过一段时间发现又开始自动更新了，所以要按照后面的步骤继续操作一下。 按下快捷键 Win+R 打开运行对话框，输入gpedit.msc，点击『确定』按钮或者直接回车打开组策略编辑器 依次打开树形结构中『计算机配置』-『管理模板』-『Windows组件』-『Windows更新』，双击打开右侧『配置自动更新』选项 在弹出的界面中选择『已禁用』，然后点击『应用』『确定』按钮 继续在右侧找到『删除使用所有Windows更新功能的访问权限』选项，双击打开 在弹出的界面中选择『已启用』，然后点击『应用』『确定』按钮 至此就设置完了，以后Windows10就不会偷偷摸摸的自动更新了 效果我司的电脑上安装有安全监测软件，之前我关闭自动更新服务器时该软件只是给我提示个警告，提醒我要保持自动更新的好习惯，但是当我设置完更新组策略之后，安全软件直接提示我已经高危风险了，并且提示将在10分钟后给我自动断网，说明这次的禁用真的生效了，无奈，为了上网我还是还原了更新组策略的设置。 总结 网上的教程虽然多，但要注意甄别，找到好使的方案记下来，方便日后重复操作 关闭Windows自动更新，不仅要关闭自动更新的服务，还要设置更新组策略，防止死灰复燃 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 得不到的永远在躁动，被偏爱的都有恃无恐~]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>update</tag>
        <tag>关闭自动更新</tag>
        <tag>自动更新</tag>
        <tag>service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2022年终总结——人生中最美好的一站]]></title>
    <url>%2Fblog%2F2023%2F02%2F05%2F2022%E5%B9%B4%E7%BB%88%E7%BB%93%E2%80%94%E2%80%94%E4%BA%BA%E7%94%9F%E4%B8%AD%E6%9C%80%E7%BE%8E%E5%A5%BD%E7%9A%84%E4%B8%80%E7%AB%99%2F</url>
    <content type="text"><![CDATA[有一种责任与压力，叫做上有老下有小，但有一种幸福也叫做上有老下有小，当你遭遇挫折与困难时，这些“老小”以及那个同龄的“她”是你坚实的后盾，同时也是你最后的港湾~ 前言2022年已经接近尾声，在经历了多次封控之后终于趁着元旦假期回到老家，和家人聚在了一起，这种平淡的幸福正是我想要的，作为普通平淡的人，我没有拯救世界的梦想，也没达到忧国忧民的高度，我就是芸芸众生中的一员，渴望过着普通而平淡的人生。 我深知这种平凡的生活来之不易，更知许多人为了这份平凡在负重前行，感恩我们这个和平的环境和为了这份和平在努力付出的人，我会好好珍惜。 回顾2022去年依旧设立了不少目标，虽然总体上完成的不太好，但还是要勇敢面对，接下来会逐个回顾一下。 工作上 FLAG 继续踏实做好本职工作，做好工作内容的总结，落实到纸上 学习和了解常见中间件的使用，更多的参与设计的工作 完成度 ： 90% 个人的性格决定了我不能摆烂，所以对待任何事情都比较认真，对待各项工作也是比较谨慎的，一直在踏踏实实的完成，做好日报记录和周报总结，好记性不如烂笔头，有些问题记录下来再回过头查找时会非常方便。 时代在发展，落实到纸上已经不单单指的是“纸”，而是一切可以记录新的媒介，电子版的记录还有一个非常大的好处就是查找信息方便，这可以大大加快我们解决问题的速度，不过我读书时还是喜欢写在纸上，不太喜欢电子版的书籍。 今年工作上有一个比较大的变化就是，从一个被动的执行者转变成了一个初级的管理者，这个身份的转变给我带来了机会，同时也带来了不小的挑战，关于设计工作也一直在进行，因为目前的身份导致我必须做一些决定，所以这部分工作是逃不开的。 通过一些课程学习和与大佬的沟通的交流后发现，管理重点不在“管”而在“理”，管理者并没有身份上的优越感，而在于合理的分配自己拥有的资源，巧妇难为无米之炊，手上什么也没有，最后肯定完不成任务。 在管理者的岗位上，自己能调动的一切皆为“资源”，这些包括时间、金钱、合作、甚至是人员，合理的调配这些才能达到最大的效果，否则就会产生浪费，效果也大大折扣。 这个Flag中的扣分项应该是学习和了解常见中间件的使用，今年确实使用和配置过一些中间件，但是没有好好的系统的学习，这方面后面要加油了。 今年关于工作上的付出基本达到最大化了，最近一段时间为了阶段性版本，大多数都是后半夜才能回家了，很累，但是并不排斥，我喜欢自己做的东西，为了取得好的结果，目前的付出是值得的。 学习上 FLAG 博客总结不能落，继续保持一周一更，完成40篇基础目标 刷题不用太频繁，每周总得有贡献，不可抗拒因素除外 建立自己的技能树，搭配工作总结，统计出自己到底会什么 选取经典开源代码学习，代码量要少一点，毕竟精力有限了 读书、读书、读书，书都买好了 完成度 ： 70% 博客总结flag达标，依旧是40篇踩线通过，今年CSDN改变挺大的，但不影响我在上面记录工作学习内容的初衷，40篇博客内容比较基础，大多数来源于工作之中，也有一部分来源与生活需求，简单记录，记忆珍存，以下是近两年的数据对比。 今年的博客还诞生了第一篇浏览量过10W的文章，《float的精度和取值范围》，目前浏览量到达了117805，这篇总结当初确实花了不少时间，也侧面说明了高质量的文章才能得到粉丝的认可。 今年的CSDN玩出了不少新花样，特别是与博主的互动上，经常会发一些实体的证书，虽然这些东西不值什么钱，但是对博主来说得到认可还是很开心的。 关于刷题这个Flag今年的情况有点拉跨，别说参加比赛了，每周的刷题数也保证不了，时间真的不太够用，虽然这听起来像是在找理由，但确实是客观存在的，毕竟从早上9点工作到凌晨4点，我是真的抽不出时间来刷题了，还是要命的，活着才有未来，噶了就什么都没了。 建立自己的技能树，这方面已经着手开始做了，目前只列举了一个大概的知识框架，今后会随着工作和学习继续补充，主要是总结自己，为今后的职业发展打好基础。 读取经典开源代码这个Flag今年完成的最差了，几乎没有什么进展，阅读代码一直被项目牵着走，代码确实没少读，一直在Code Review，完整的开源项目几乎没有读，不过发现了一个 magic_enum 项目还不错，把他按照项目需求改造了一下，目前正在使用。 读书一直在进行，中间因为身体原因和项目忙的原因暂停过一段时间，不过去年Flag列举的书单都翻了翻，毕竟书都买回来了，但是有些真的不适合精读，比如民法典，有空了翻一翻还是比较有意思的。 股票大作手回忆录（2022-02-12 22:58:39） 顺势而为，不与大盘做对，永远朝着阻力小的方向操作 世界很烦，但你要很可爱（2022-02-28 07:09:45） 不管别人对你如何，请记得一定要对自己好一点儿 洛克菲勒写给儿子的38封信 商人就是靠创造资源、掠夺他人的资源，甚至逼迫他人转让自己的资源来致富的。 两次全球大危机的比较研究 经济发展比较注重效率，收入分配比较关注公平 创华为 —— 任正非传（2022-07-17 11:00:19） 你不要说未来有什么贡献，万一活不到未来，未来的贡献和我们没有关系 孙子兵法与三十六计（没看完） 历史总是在重演，只有胜利的人才能诉说 世界经济简史（看了一点） 原始的经济就不纯粹 民法典（翻阅） 没想到这么薄薄的一本干掉了那么多大部头 活着（2022-12-17 11:22:10） 生活是属于每个人自己的感受，不属于任何别人的看法 《创华为》是一部任正非的传记，这是继《乔布斯传》之后我读的有一本比较厚的传记书籍了，大佬们的经历总是能鼓舞人心，虽然有些描写我觉得不真实，但是所有的经历不太可能都是伪造的，去伪存真，相信那些正能量的内容。 《活着》这本书很出名，但是我读了一遍感觉这本书宣传时给我的印象完全不同，整本书的描写朴实无华，就真的像我等小老百姓在村头树下闲聊，很多情景并没有刻意描写，“她死了”——仅仅三个字就断送了别人姓名，并没有更多的叙述。读过之后我并没有觉得富贵有多惨，这就是那个时代的一个缩影，他不仅不惨，而且也经历过很多人没有经历过的幸福，有一个衣食无忧的童年，有一个不离不弃的妻子，经历战乱却活了下来… 投资上 FLAG 基金和ETF继续定投，适当配置债券固收 股票池里把曾经“瞎选”的股票逐渐出清，依旧拥抱大白马，少折腾 目标收益不太高，8个点，希望不要打脸 完成度：85% 今年的投资严格按照Flag策略执行，基金和ETF持续定投，目前有周定投和月定投两套策略，中概、恒生、红利还设置了智能条件单，不主动干预，实现被动调仓，只是最后的收益结果没有达标(ಥ_ಥ) 曾经瞎选的股票也出清了一部分，有一只最多赔了90%的票拿了5年，今年回本之后也卖掉了，所以只要股票不退市都有回本的机会。 持仓依旧银地保三傻和大白马——万安招富，虽然没挣钱，但今年的回撤控制的还可以，做了一波过山车，4月份感觉股市要崩了，6月份股市要飞了，不仅回本还有了盈利，10月份股市要炸了，12月份又飘起来了，可谓一波三折。 本来在2021年已经预测2022在加息周期中，能保住本金就可以了，没想到又遭遇俄乌战争，疫情反复，股市受到了比较大的冲击，不过结果还是不错的，虽然Flag想达到8%，最终收益达到了6.72%，也还可以接受，不过基金和ETF有点浮亏，吃掉了股票的部分收益，整体上略有盈余。 生活上 FLAG 疫情结束了多回几次老家吧，去看看那些想我又不愿说出口的亲人 注重身体的保养，锻炼提上日程，降低亮红灯的指标 完成度60% 生活上的Flag是完成的最差的，本来想今年疫情结束多回几次老家，结果今天封闭的时间最长，刚刚统计完2022年算上春节假期一共回家14天，达到了历史最低值。 身体的保养和锻炼更是拉跨，年初的时候因为胸腔疼痛，3、4月份吃了两个月的中药来调理，医生告诉我不要熬夜，晚上10点准时睡觉，这几乎是不可能完成的任务，每天有太多的事情需要忙了，不过今年体检各项指标比较稳定，注意血压、视力和肝脏。 有趣的是这个中医岁数不大，但是找他看病的人很多，每次门诊都持续到晚上12点钟，医生门诊持续到12点这还是我第一次遇到，所以我每次挂号都看一下时间，大概每次都赶在周六晚上9点去找他，当有一次他告诉我要10点钟睡觉时，我反问还在忙碌的他为什么不10点睡，他只说了一句：“你比我幸福”。其实就是生活所迫呗，他因为工作无法按时睡觉，我又何尝不是呢？ 今年陪娃的时间不太多，娃娃也顺利的上了幼儿园，认识了很多小伙伴，虽然因为疫情只上了1个多月，但每天还是开开心心的。我因为有两个月封控在家，在一起相处的时间还蛮多的，但是也在忙于工作，居家办公导致生活和工作融在一起，无法分割，个人不太喜欢长时间处在这种环境下，还是喜欢公司办公，回家陪娃的日子。 去年总结中提到的那批多肉，今年一年过得不错，长势喜人，已经成了我的小花园中重要的组成部分，今年12月初的时候又买了一批，还买了几颗文竹，结果花盆到了、营养土到了，这都新年了多肉和文竹还没到，看来得等到春天才能发货了。 展望2023工作 适应自己身份的转变，提升自己的管理技能，做一名合格的管理者 继续做好本职可开发工作，做好工作内容的总结，推动新项目顺利上线 根据自己的技能树框架，查漏补缺，有针对性的学习和探索 学习 继续博客总结，40篇是基础线，2023要超过这个值，尽量一周一篇，放假可休息 新的一年依旧很忙，刷题不强制要求，每周最好有贡献，每月必须有输出 在现有的技能树框架上继续丰富，做到枝繁叶茂，试试找点副业 开源代码还是以项目为驱动，选取经典实现，比如kafka 继续读书，今年书也买好了，数量不多，下半年应该还会买一批 投资 定投和智能条件单保持现状，ETF追加1~2个新的行业赛道 股票池还有几个垃圾票要找机会清理掉，手里的价投股票适当做做趋势 保持对新年经济的乐观，目标收益10个点 生活 疫情放开了，有事没事都趁着假期回家看看 身体很重要，锻炼的很费时，中和一下，可以跳跳绳 周末了陪娃耍一耍，工作尽量不占用周末的时间 总结 旧的一年虽然看起来很拉跨，但我认为是人生中最美好的一站，因为照片人上的人都还在，而我也在成长 新的一年稳中向好，不盲目乐观，也不宜过分悲观，投资环境应该会转好 身体方面自己要多注意，有些情况并不是自己要熬夜，其实多为生活所迫，若衣食无忧，谁甘愿拼搏 珍惜周末时光，珍惜和家人在一起的日子，收起自己的坏脾气 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 往昔总是回忆情，未来光景亦可期。若问此生弥珍贵，还看今朝不分离。]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活，投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[圣诞平安夜了还要继续敲代码吗]]></title>
    <url>%2Fblog%2F2022%2F12%2F25%2F%E5%9C%A3%E8%AF%9E%E5%B9%B3%E5%AE%89%E5%A4%9C%E4%BA%86%E8%BF%98%E8%A6%81%E7%BB%A7%E7%BB%AD%E6%95%B2%E4%BB%A3%E7%A0%81%E5%90%97%2F</url>
    <content type="text"><![CDATA[此时不敲，更待何时~ 前言先不说圣诞节算不算我们的节日，单纯就节日还要不要敲代码这个问题来说，每个程序猿/媛都有自己的答案。如果说你把工作敲代码当做一种负担，那么节假日正好可以作为自己休息的理由，好好休息一下也是不错的选择；如果把敲代码作为一种乐趣，那么节假日难得清净，少了很多杂事的打扰，正好是敲代码的黄金时间段，想一下自己的状态很容易就应该明白了。 正文节日圣诞节很明显不是我们的传统节日，应该被归为“洋节”一类，有些人对这些外来节日特别喜欢，同时也有人对此特别排斥，每个人都有自己的理由，其实不止是对这些节日，人们对于其他任何事物都存在着截然相反的评价，这些评价通常是基于自身利益、过往经验而给出的，它代表了每个人的想法，这些想法不是一成不变的，随着时间的推移，人们对它的评价甚至会走到相反的对立面。 我觉得不必过分排斥这些节日，一些人推崇这些节日自然有他们的目的，比如一些商家为了销售业绩往往会推一些节日活动，如果我们也能在这些活动中获得优惠，那么何乐而不为呢？ 无论是中国节还是“洋节”出现的原因基本都是用来纪念一些事件或者寄托某种情感的，但是节日的这些属性在如今这个快节奏的社会环境下渐渐淡化，人们更多的是希望在节日假期中能好好的放松一下。 代码与技术写代码对我来说算是一种爱好，没有达到痴迷的程度，但是有空了还是想随便写点东西，不过今天只写了一点点，毕竟今天周末，多花了点时间陪伴家人，最近一段时间太忙了，心中总是有所亏欠，暂时停下来歇一歇。 每次进入新的领域时总是一头雾水，当渐渐入门以后便会发现一些规律，最近发现linux中也有很多蹭热度的命令，比如 ls 这个命令实在太常用了，可以展示指定目录下的所有文件，lscpu、lsgpu、lsmem、lspci、lsusb 这些命令的出现我觉得就是在蹭 ls 的热度。 除了 ls 还有一个 top 命令用来展示正在运行的进程信息，iftop、iotop、htop、atop 这些命令自然是蹭了它的热度，起一个相似度很高的名字，然后便于自己推广。 上面 ls 和 top 两个被蹭热度的命令，与后续命令还有有相似的功能，比如 ls 展示目录下的所有文件，而 lscpu 是用来展示 cpu 信息的，那么 “ls” 变成了一个基础功能的代表，就是用来展示信息，类似于英语中的词根，以及汉字中的偏旁。 而编程语言的历史上，还有一对完全为了蹭热度没有任何关系的语言，那就是 java 和 javascript 语言，javascript 在当时为了推广自己蹭了流行语言 java 的热度，它俩实际的关系就像是周杰和周杰伦的关系，仅仅是名字长得像罢了。 总结 ls 家族有 lscpu、lsgpu、lsmem、lspci、lsusb 等命令 top 家族有 iftop、iotop、htop、atop 等命令 把写代码当做一种兴趣爱好，其实没有什么节假日，想写就写咯 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当下的今天就是最好的，我不想回退到之前人生中的任何一个时间点。我一直有这种感觉，今天是最好的，所以珍惜现在拥有的一切，我不想让自己产生回退到过去的想法，因为那样肯定是发生了痛苦的事情。恰好今天和她看了《想见你》这部穿越剧，更加深了我的这种想法，今天就是最好的。 2022-12-25 01:37:25]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
        <tag>linux</tag>
        <tag>节日</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx的安装与常用配置]]></title>
    <url>%2Fblog%2F2022%2F12%2F19%2FNginx%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言Nginx 是开源、高性能、高可靠的 Web 和反向代理服务器，而且支持热部署，几乎可以做到 7 * 24 小时不间断运行，出镜率很高，从2004年发布至今，凭借开源的力量，日趋完善。其占用内存少、并发能力强、并且Nginx 是免费的还可以商业化，配置使用也比较简单。 常用功能反向代理Nginx 可以用作正向代理和反向代理，而在做反向代理时，能提供性能稳定，且配置灵活的转发功能 负载均衡Nginx 对于负载均衡策略提供了内置策略和扩展策略。内置策略有轮询，加权轮询，IP哈希。而扩展策略，就可以随心所欲了，只要遵循规则就可以搞出各种花样。 静态资源服务Nginx 可以通过本地文件系统提供服务 web缓存服务器Nginx 可以对不同的文件做缓存处理，配置灵活，支持FastCGI_Cache，可用于对 FastCGI 的动态程序进行缓存。配合着第三方的ngx_cache_purge可以对制定的URL缓存内容进行增删管理。 插播快讯本来今天打算总结一下Nginx的安装和使用的，结果中午的几个未接电话打破了宁静，游戏居然进不去了，看到几个未接电话就觉得不妙了，了解完情况才知道居然是阿里云香港C区的服务器挂了，本来觉得很快就能恢复，但随着时间的推移和官方的不表态，渐渐发现事情没有这么简单，虽然在过去好几个小时以后更新了公告，但这反应速度也太慢了。 现在已经过去了10多个小时（2022-12-18 10:47:22开始），尽管最新的公告说机房已经恢复，产品功能还在陆续恢复，但我们的游戏现在（2022-12-19 02:10:06）依旧处于停服状态，目前的云服务器依旧不可用，我还在等实在是写不下去了，关于Nginx的使用后面再补充吧，现在只希望云服务器能快点恢复。 【已恢复】阿里云香港地域电讯盈科机房制冷设备故障尊敬的客户： 您好！阿里云监控发现香港地域某机房设备异常，影响香港地域可用区C的云服务器ECS、云数据库PolarDB等云产品使用，阿里云工程师已在紧急处理中，非常抱歉给您的使用带来不便，若您有任何问题，请随时联系我们。—进展更新—尊敬的客户： 您好！经排查，阿里云香港地域故障确认系香港PCCW机房制冷设备故障所致，影响香港地域可用区C的云服务器ECS、云数据库、存储产品（对象存储、表格存储等）、云网络产品（全球加速、NAT网关、VPN网关等）等云产品使用。这一故障也影响了香港地域控制台访问和API调用操作，目前阿里云工程师在配合PCCW机房工程师加速处理，部分制冷设备正在恢复中。非常抱歉给您的使用带来不便。若您有任何问题，请随时联系我们。—进展更新—尊敬的客户： 您好！目前阿里云所租用的香港电讯盈科公司机房已修复制冷设备故障，阿里云香港地域所有可用区云产品功能正在陆续恢复正常。对于受本次故障影响的产品，阿里云将根据相关产品的SLA协议进行赔付。非常抱歉给您的使用带来的不便 ，若您有任何问题，请随时联系我们。 总结 Nginx服务器配置方便、高效可靠，可以用作反向代理、负载均衡、静态资源服务、Web缓存服务等 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 生活是属于每个人自己的感受，不属于任何别人的看法~ 阿根廷夺冠了，虽然我从不看球，但这场比赛真的跌宕起伏，不到最后一秒永远不知道会发生什么~ 2022-12-19 02:14:22]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>安装</tag>
        <tag>负载均衡</tag>
        <tag>反向代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动推送消息时附带图片的一种实现方式]]></title>
    <url>%2Fblog%2F2022%2F12%2F03%2F%E8%87%AA%E5%8A%A8%E6%8E%A8%E9%80%81%E6%B6%88%E6%81%AF%E6%97%B6%E9%99%84%E5%B8%A6%E5%9B%BE%E7%89%87%E7%9A%84%E4%B8%80%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言本文是之前总结 pushplus 使用的这篇文章 《借助第三方工具网站完成消息自动推送》 的后续，当时说使用pushplus的微信公众号渠道不允许直接发图片，但是可以将图片链接放到 image 标签中，这就要求我们必须要有一个图床，今天也是从这里开始。 实现途径 注意：请勿使用base64编码的方式把本地图片放到内容中，您可以将本地的图片上传到七牛云等云存储空间上来获取图片的外链地址。 这段内容来自pushplus官网，限制了直接发送图片的方法，并给出了使用图片外链的建议，实现代码非常简单，示例如下： 1234567891011import requestsdef post_wechat_msg(token): url = 'http://www.pushplus.plus/send' new_json = &#123; "token": token, "title": "图片示例", "content": "content":"这是一张图片&lt;br/&gt;&lt;img src='http://www.pushplus.plus/doc/img/push.png' /&gt;", "topic": "9caigroup" &#125; result = requests.post(url, json=new_json) 代码量非常的少，想发别图片就直接修改img标签中的内容就可以了，问题是怎么获取我们图片的外链。 图床与外链如果你没听说过这两个词说明你对建站、发文等了解的比较少，不过也没有关系，这两个词很好理解 图床：就是放置图片的床，可以理解为图片的仓库，里面存储了很多图片外链：就是访问图床里图片的一个链接，可以通过一个网址读取到想要的图片 以上面代码中的例子来看，http://www.pushplus.plus/doc/img/push.png 就是图片的外链，图片push.png被放在 http://www.pushplus.plus/doc/img/ 这个“图床”下面。 如果没有自己创建网站或者跨平台转发博文的经历，可能不太理解图床有啥用，我举个例子试着说明一下，假如你在A平台发了一篇技术文章，在文章中需要一些图片来丰富文章内容，你可以把这些图片在A平台的文章中上传。同时你想在另一B平台上也同步这篇文章，此时把A平台上这篇文章完全复制过去是不行的，因为其中的图片是在A平台上传的，A平台作为这些图片的图床一般会给图片加防盗链，这些图片链接复制到B平台上是显示不出来的，这也是为什么之前的新浪博客图片加了防盗链以后，很多平台上的文章图片都挂了的原因。 现在有一种实现方式就是自己建一个图床，获取得到图片链接就是固定的，也可以限制可以在域名下显示，这样同一篇文章无论是A平台，还是B平台都引用自己图床里的图片，文章同步就方便了很多，但是因为建立自己的图床功能单一，没有cdn加速，维护成本较高，所以一般使用第三方的图床服务，我只用过七牛和Github这两个图床，不过Github有时不太稳定，有需求可以按自己的喜好来选择。 七牛图床说起七牛我用的还比较早，大概在2017年左右就用过了，当时建了一个静态的小破站，需要引用一些图片资源，就了解到了图床与七牛，免费提供10个G的空间，同时可以获取外链，使用比较方便，图片更新有一套方便的API，已经过去5年了，我前两天使用之前的脚本更新时发现还能用，说明还是比较稳定的。 后来出了一个事情导致我放弃了七牛，转身选择了Github图床。这件事就是七牛的规则调整，曾经使用的外链全都过期，要求必须绑定备案过的域名才能使用，当时替换了文章中所有的图片链接，同时兴冲冲的买了一个域名，结果只有域名是不能备案的，必须还要买服务器，各种要求晕晕乎乎的一直没太弄懂，申请了几次的公安备案也因为资料不全未审核未通过，最后无奈放弃了。 1、测试域名有使用限制，在创建空间的时候自动分配，到期30天会自动回收，因此建议您绑定自己的已备案域名2、根据工信部的有关规定，国内接入cdn服务，域名必须要完成备案 这个是七牛官方给出的解释，起初我认为想要达到这个要求完全不用每个使用者绑定自己的域名，其实这些要求无非就是想让cdn使用者对自己上传和分享的资源负责，禁止传播非法内容，这个要求本身是好的，绑定了自己备案的域名如果出现非法内容，可以立马找到使用者，可是不仅仅只有绑定自己域名这一条实现方式。 要求七牛给每个使用者分配一个1级域名不太现实，但是可以做成主域名备案，然后给每个使用者分配二级、甚至三级域名，其实这就是一开始提供的测试域名，使用这些域名分享资源的用户要求必须在平台实名，这样再出现非法内容的时候可以定位到个人，也能达到监管的目的，可是为啥平台放弃了测试域名的长期使用，反而要求每个使用者绑定自己域名呢？ 最近和七牛的工程师反复沟通了解到，其实分配给用户的测试域名是合规的，因为它的主域名肯定完成了备案，但是因为有些用户违规使用，这就对这个主域名造成了影响，一些产品可以和社交软件因为该域名分享的图片资源出现过非法内容，可能对这个域名的资源自动屏蔽，这就对正常用户的使用造成了影响，这也是一个平台发展壮大之后必须面临的问题，林子大了什么鸟都有。 所以七牛把原来提供的域名被定义为测试域名，给30天的使用期限，仅用于调通功能流程，到期自动回收，即使被一些平台限制了也没什么关系，只要确定后续要继续使用，绑定自己的域名就行了，违规使用的用户的不会影响到其他人。 所以我觉的这件事挺无奈的，如果可以一直使用测试域名，对于使用者无疑是很方便的，对于平台方也降低了用户门槛，便于产品的推广，但是这些美好的愿望总因为一些“极个别人”无法实现了，我们只能接受现实，采用目前这种方案。 备案流程因为最近想要在发送通知消息中增加图片，所以不得不再次使用图床了，你问我为啥不用Github的图床，那是因为微信把它给屏蔽用不了了（捂脸），所以我不得不把目光又转回七牛，经过不懈努力，终于把绑定域名的事情搞定了，记录一下，希望能帮助到有同样需求的人。 备案前的准备 购买一个域名 购买一台云服务器 搭建好个人网站 身份证正反照片 手持身份证的照片 开始备案购买域名推荐阿里云和腾讯云，因为后续备案的流程会有通知提醒，可以免去很多查找资料的麻烦，两个平台我都买过域名，但是只在腾讯云上完成过备案，所有后续的流程也是以腾讯云备案为基础的。 购买域名需要先实名，腾讯云上购买之前需要先提交实名模板，审核通过后才能买域名，域名的购买和网上购物没啥区别，挑选自己喜欢的加到购物车，然后付款就可以了，购买成功后会有站内信提醒，通知后续的备案流程，这也是比较方便的地方。 买完域名要买服务器，其实备案不仅仅是对一个域名的备案，而是对这个域名提供的服务内容进行备案，而要提供内容和服务就需要有一台带有外网IP的服务器，然后将域名和服务器进行绑定，统一写到备案内容中，这样你提供的服务和提供服务的这个域名就登记在册了。 腾讯云买服务器同样跟买菜一样，如果只是为了备案，买一台最低配的就可以了，然后再上面搭建一个简易网站，就可以后续的备案流程了，备案的第一步叫管局备案，也叫工信部备案，也是我们常听说的ICP备案。 在提交备案资料后，腾讯云会先进行审核，其中要注意规避游戏内容，提及游戏的网站很容易被退回，不同地区的管局对网站内容要求不一样，比如北京地区现在不允许提交个人博客类网站了，这个再提交前先看一下不同地区的限制，手机号和紧急电话必须时刻保持畅通，提交资料后3-4天会有电话确认，按要求修改好审核通过后就会被提交到管局审核。 提交管局审核之前，要关停域名解析，在审核通过前不允许域名到服务器的解析，提交之后提示20个工作日会收到结果，马上会收到一条短信验证通知，按要求回复后只要等着就可以了，我是在第7天收到的审核通过的通知，速度还是比较快的。 当ICP备案成功后会得到一个 【京/冀/沪 ICP 备 xxxxxxx 号 - 1】的网站身份证，之后腾讯云站内信会通知在30个工作日内完成公安备案，这是不同于ICP备案的另一种备案流程，这一步要求的内容会比较多，不过好在大平台这些基础信息都是提供好的，目前已经按照流程提交了申请，正在等待审核中，希望不要再出什么差错。 图床绑定域名这一步在完成ICP备案通过之后就可以进行了，参考 官方文档 - 如何配置域名的 CNAME 操作就可以了，注意二级域名的命名和使用，步骤并不复杂，我因为拼写错误还耽误了一些时间，绑定成功之后就可以愉快的使用了。 pushplus发图片的实现 申请一个七牛图床账号 将图片上传到七牛平台 复制图片的外链 临时使用可以使用测试域名 长期使用需要绑定备案域名 实名购买一个域名 购买云服务器，搭建一个网站 提交资料，完成ICP备案 在七牛绑定备案好的域名 编写发送图片的代码，在img标签中引用图床中图片的外链 总结 使用 pushplus 发送微信通知消息时可以使用img标签引用图床外链的方式发送包含图片的消息 图床可以选择 pushplus 官方推荐的七牛，可以暂时使用测试域名来提供外链，想长久使用可以绑定自己已经备案的域名 备案域名时需要先买一个域名和云服务器，推荐在阿里云和腾讯云上购买，大平台上购买后的续通知提醒和备案流程会非常顺畅 对于非盈利性的网站备案需要有ICP备案和公安备案，ICP备案购买域名后平台会帮助进行，而公安备案中需要的信息平台方也会提供 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 神州归家，腾飞中华，眼中所见都是大事，逐渐接受自身的平庸，一个普普通通的人，想要过一段平凡的生活，在柴米油盐中慢慢步入黄昏~ 2022-12-4 21:57:22]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pushplus</tag>
        <tag>自动推送</tag>
        <tag>图片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下统计目录下所有文件的行数]]></title>
    <url>%2Fblog%2F2022%2F11%2F30%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%BB%9F%E8%AE%A1%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%89%80%E6%9C%89%E6%96%87%E4%BB%B6%E7%9A%84%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言工作中时常有统计数据行和代码行的需求，虽然能依次打开每个文件数一下，但是这样操作效率太低了，如果是在linux环境中，可以使用一个常用的命令 wc，一起记录一下常见的用法吧。 wc命令wc命令，（全称water closet，洗手间，哦，错了），全称word count？我也没找到，作用就是输出每个文件包含的行数、单词数和字节数，这三项是基础信息也是最常用的信息，此外它还可以输出文件中字符数量、输出文件中最长行的长度。 使用方法非常简单，直接在命令末尾添加文件名就行了 12$ wc README.md 35 60 2460 README.md 上面的信息显示，README.md文件中包含25行内容，存在60个单词，共有2460个字节，可能是因为该文件中包含中文，所以单词数统计不太准确。 常用选项该命令各选项含义如下： - l: 统计行数 - w: 统计字数 - c：统计字节数 - m：统计字符数 - L：统计最长行的长度 其中 - l 使我们统计行数的利器 统计文件行数wc 命令搭配一些其他的命令能更方便的完成统计工作 统计1个文件行数12$ wc -l README.md35 README.md 统计2个文件行数wc 命令后名跟多个文件名时会打印出每个文件的行数，并且在最后一行显示所有文件总行数 1234$ wc -l README.md test.txt 35 README.md 3 test.txt 38 total 统计当前目录下python代码文件数量12$ find ./ -name "*.py" | wc -l317 统计当前目录下python代码文件行数12$ find ./ -name "*.py" | xargs cat | wc -l38538 统计当前目录下python代码文件非空行数12$ find ./ -name "*.py" | xargs cat | grep -v ^$ | wc -l35320 实现的方式就是在输出文件内容的过程中过滤掉空行，但我认为统计代码行数时不应该过滤空行，适当的留白也是优秀代码的一部分 统计当前目录下2种代码文件的总行数一个项目中使用的代码往往不止一种，所以在统计代码行数时要考虑多种代码文件，可以写多个-name选项或者使用正则表达式 123456$ find ./ -regex '.*\.\(py\|h\)' | xargs cat | wc -l39795$ find ./ -regextype posix-extended -regex '.*\.(py|h)' | xargs cat | wc -l39795$ find ./ -name "*.py" -o -name "*.h" | xargs cat | wc -l39795 总结 wc 命令是用来查询文件行数的核心命令，不加选项时默认输出文件行数、单词数、字节数 wc 命令配合 find、xargs、cat 等命令可以统计出目录内指定类型文件的总行数 find 查找多个文件的常用写法 find ./ -regex &#39;.*\.\(py\|h\)&#39; 或 find ./ -name &quot;*.py&quot; -o -name &quot;*.h&quot; 查项目全部代码量终极大招：find ./ -regex &#39;.*\.\(py\|lua\|go\|h\|hpp\|cpp\|c\|cc\)&#39; | xargs cat | wc -l ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 确实挺难的，再坚持一下~ 2022-12-1 17:16:29]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>grep</tag>
        <tag>wc</tag>
        <tag>统计函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[借助第三方工具网站完成消息自动推送]]></title>
    <url>%2Fblog%2F2022%2F11%2F19%2F%E5%80%9F%E5%8A%A9%E7%AC%AC%E4%B8%89%E6%96%B9%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99%E5%AE%8C%E6%88%90%E6%B6%88%E6%81%AF%E8%87%AA%E5%8A%A8%E6%8E%A8%E9%80%81%2F</url>
    <content type="text"><![CDATA[前言寻找消息推送的起因是之前买过一台云服务器，用于开发环境搭建和学习，最近想用它进行一些数据分析，而数据分析的结果如果每次都需要登录服务器来看就有点“太老土”了，所以想找一种使用方便的消息推送模式。提到之前的消息通知最常用的是短信和邮件，但是随着时代的发展，它们的弊端也逐渐暴露出来，短信的内容主要以文本为主，想要发送图片就比较困难，邮件查看起来不是太方便。而微信作为每天都用的社交软件，用它来接收消息就很方便了，所以朝着这个方向寻找，我发现了 pushplus 这个第三方工具。 pushplus是什么pushplus(推送加)是集成了微信、短信、邮件、企业微信、HiFlow连接器、钉钉、飞书等渠道的信息推送平台，只需要调用一个简单的API接口，即可帮助你迅速完成消息推送，使用简单方便。 我主要想用它的微信推送，一开始也想试试短信推送的，但是需要支付每条0.1元的费用所以放弃了（什么年代了，短信收费还这么贵），这里先简单说下它的微信推送的原理，利用的就是微信公众号的消息通道。 什么是消息通道，我的理解就是获取消息的途径，比如短信、邮件、微信、电话等等，这个pushplus本身并没有搭建新的消息通道，而是整合了已有消息通道的使用方式，让消息通知变得简单了，而微信消息推送利用的就是微信公众号这个途径。 我们知道微信公众号可以接收消息，并且微信公众平台也开放了后台接口，允许公众号运营方通过程序接口发送消息给关注者，这就是微信公众号的消息通道。pushplus就是使用了这个通道，他们把微信公众平台的接口进行了封装，开放给所有关注 pushplus 的人，而这些人就可以利用这个通道给自己发送消息，给群组发送消息，甚至给所有的关注者发送消息，当然这些消息都是显示在这个公众号里，要想接收消息必须先关注公众号，或者按照pushplus的规则加入群组。 既然是利用别人家的通道，必然要收到这个通道原有的限制，pushplus也不例外，首先当前微信公众号对模板消息推送有当日100万次的上限，超过这个上限有当日将无法发送模板消息，也就是所有pushplus的用户都将无法使用微信渠道的推送功能，所以pushplus在单日推送次数上做了部分限制。每人每日可用推送条数为200条，当大于200条的时候消息将不在推送。 目前这个每天200条是免费的，但随着用户量的增大，这个限制必然是会更加严格的，按现在计算没人推送200条也只能服务于5000人，不过大部分人是不会发这么多的，短时间应该没问题。另外该网站目前还推出了绑定自己公众号的服务，必须是经过认证的非个人公众号才可以哦，腾讯就是这么要求的，它也没办法，当前还提供了会员制度，可以适当放宽消息推送的次数。 我觉得大家不要反感会员制度，人总是要恰饭的，即使不是为了赚钱，也需要付出成本的，免费提供这项服务是需要服务器资源的，总不能一直永爱发电吧，收个合理的费用方便广大开发者我觉得无可厚非，就像我买的服务器1年一百多，续费成本每年要700多，如果对外提供服务器不收钱总不能靠西北风活着吧。 pushplus测试使用pushplus 的官方网站是 www.pushplus.plus，直接微信扫码就注册成功，同时会关注微信公众号用于接收消息，对于一对一消息和一对多消息都提供了消息发送界面，输入内容直接点击发送按钮就可以了，非常方便。 官网介绍如下： 一对一消息发送 一对一消息发送 微信公众号里看到的消息通知 代码发送一对一发送消息的代码非常简单，也就下面这几行，更复杂的格式还需要研究下： 123456789import requestsdef send_wechat_msg(title, content): token = '2c358fyearacb4581bc92f0c320c728fb' url = 'http://www.pushplus.plus/send?token='+token+'&amp;title='+title+'&amp;content='+content requests.get(url)if __name__ == "__main__": send_wechat_msg('python推送测试消息', '测试消息详细内容') 注意的问题 限制问题。前面已经提到，使用pushplus公众号，所有用户共享100万条消息的限制，随着公司后期的发展这可能不够用 合规问题。因为是自定义推送内容，所以平台要注意合规问题，不仅公众号要进行内容审查，pushplus也需要进行内容审查，否则可能因为一个用户的违规导致所有用户都发不了消息。 消息格式。目前pushplus支持多种消息格式，包括 text、html、json、markdown 等格式，这能满足大部分用户的要求了 发送图片。pushplus不支持直接发送图片，具体方式通过html的 &lt;img&gt; 标签来实现，可以将本地的图片上传到七牛云等云存储空间上来获取图片的外链地址。 总结 消息推送可以选择 pushplus 官网扫码即可使用 pushplus 的微信推送借用了微信公众号的消息通道，会受到的平台原有的100万条消息限制 pushplus 提供给普通用户每天有200条消息的使用权限，仅通过http请求即可方便的使用api 注意后续的平台发展，很可能发展到一定程度后增加更严格的使用限制 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 一道残阳铺水中，半江瑟瑟半江红。可怜九月初三夜，露似真珠月似弓~ 2022-11-20 13:01:16]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>消息推送</tag>
        <tag>微信</tag>
        <tag>短信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下使用rsync命令完成数据同步]]></title>
    <url>%2Fblog%2F2022%2F11%2F13%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8rsync%E5%91%BD%E4%BB%A4%E5%AE%8C%E6%88%90%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[前言linux 环境下说到数据复制第一个映入脑海的命令还是 cp，毕竟它用起来很方便，但是只能完成本地数据的拷贝，如果想实现本地和远程主机之间的数据拷贝可以使用 scp，有些文章会把它翻译成 “ssh copy”，但实际上它的全称是 “secure copy”，不过它确实利用了ssh协议，缺点就是只能全量拷贝，如果想完成增量拷贝可以选择 rsync命令，官网 rsync.samba.org 比较简陋，但很符合GNU的风格。 rsync的特点 可实现增量同步，即只同步发生变化的数据 可保持原文件或目录的权限、时间、软硬连接、属主、组等所有属性均保持不变 支持拷贝特殊文件，如连接文件、设备等 排除指定文件或目录同步的功能，相当于打包命令tar的排除功能 rsync本身不对数据加密，可使用rcp/rsh/ssh等方式来配合传输文件 支持匿名的活认证的进程模式传输，可实现方便安全的数据备份和镜像 rsync 传输模式 本地方式，类似cp，可完成本机数据的拷贝复制 远程方式，类似scp，但可实现增量复制，可以将数据从本地推送至服务端，也可以将数据从服务端拉取到本地 守护进程方式，区分客户端和服务端，可以启动 rsyncd 服务（未使用过需研究） rynsc 命令格式1234567891011Local: rsync [OPTION...] SRC... [DEST]Access via remote shell: Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST:DESTAccess via rsync daemon: Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST] rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST::DEST rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 常用的还是第2种和第3种，第一种是本地复制，一般用cp代替就够了，4~7用于守护进程模式，至今我还没用过 rsync 命令选项12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061-v, --verbose 详细模式输出。-q, --quiet 精简输出模式。-c, --checksum 打开校验开关，强制对文件传输进行校验。-a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD。-r, --recursive 对子目录以递归模式处理。-R, --relative 使用相对路径信息。-b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。--backup-dir 将备份文件(如~filename)存放在在目录下。-suffix=SUFFIX 定义备份文件前缀。-u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件，不覆盖更新的文件。-l, --links 保留软链结。-L, --copy-links 想对待常规文件一样处理软链结。--copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结。--safe-links 忽略指向SRC路径目录树以外的链结。-H, --hard-links 保留硬链结。-p, --perms 保持文件权限。-o, --owner 保持文件属主信息。-g, --group 保持文件属组信息。-D, --devices 保持设备文件信息。-t, --times 保持文件时间信息。-S, --sparse 对稀疏文件进行特殊处理以节省DST的空间。-n, --dry-run现实哪些文件将被传输。-w, --whole-file 拷贝文件，不进行增量检测。-x, --one-file-system 不要跨越文件系统边界。-B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节。-e, --rsh=command 指定使用rsh、ssh方式进行数据同步。--rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息。-C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件。--existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件。--delete 删除那些DST中SRC没有的文件。--delete-excluded 同样删除接收端那些被该选项指定排除的文件。--delete-after 传输结束以后再删除。--ignore-errors 及时出现IO错误也进行删除。--max-delete=NUM 最多删除NUM个文件。--partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输。--force 强制删除目录，即使不为空。--numeric-ids 不将数字的用户和组id匹配为用户名和组名。--timeout=time ip超时时间，单位为秒。-I, --ignore-times 不跳过那些有同样的时间和长度的文件。--size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间。--modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0。-T --temp-dir=DIR 在DIR中创建临时文件。--compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份。-P 等同于 --partial。--progress 显示备份过程。-z, --compress 对备份的文件在传输时进行压缩处理。--exclude=PATTERN 指定排除不需要传输的文件模式。--include=PATTERN 指定不排除而需要传输的文件模式。--exclude-from=FILE 排除FILE中指定模式的文件。--include-from=FILE 不排除FILE指定模式匹配的文件。--version 打印版本信息。--address 绑定到特定的地址。--config=FILE 指定其他的配置文件，不使用默认的rsyncd.conf文件。--port=PORT 指定其他的rsync服务端口。--blocking-io 对远程shell使用阻塞IO。-stats 给出某些文件的传输状态。--progress 在传输时显示传输过程。--log-format=formAT 指定日志文件格式。--password-file=FILE 从FILE中得到密码。--bwlimit=KBPS 限制I/O带宽，KBytes per second。-h, --help 显示帮助信息。 常用组合从上面描述的选项可以看到，这个命令的参数特别多，但实际使用时只需要掌握一些常用组合即可： 保留文件原属性拷贝1rsync -avz jerry@82.156.125.169:/data/logs /data/backuplogs 从远程主机将目录 /data/logs 拷贝到本地，对应目录为 /data/backuplogs，保留文件原有属性，包括权限、时间、软硬连接、属主、组等 删除目标目录中S再原目录不存在的文件1rsync -avz --delete --progress /data/logs 82.156.125.169:/data/backuplogs 将本地 /data/logs 目录推送到远程目录 82.156.125.169:/data/backuplogs 下，同时删除在原目录下不存在的文件，这个参数组合特别适合用来完全同步两个目录的内容 限制网速传输1rsync -avz --bwlimit=1024 /data/logs 82.156.125.169:/data/backuplogs/ 同步数据时限制传输速度为1024kBytes/s（就是1MB/s），防止占用过多带宽，可以根据需要调整数值，注意末尾加了一个斜杠 / 表示将原目录同步到目标目录下，而不是与目标目录完全同步，这一点在实践中要注意 断点续传1rsync -avz --partial /data/logs root@82.156.125.169:/data/backuplogs 传输时保留那些因故没有完全传输的文件，来是加快随后的再次传输 总结 本地拷贝数据用 cp，远程拷贝选 scp，如果想实现增量复制拷贝可以使用 rsync rsync 可以将数据从本地推送至服务端，也可以将数据从服务端拉取到本地，也可以使用 daemon 模式 rsync 最常用的命令组合就是 avz，可以保留文件原有属性进行递归拷贝 使用 rsync 命令传输数据时注意目录末尾的斜杠 /，加上它表示目录下的内容，使用时要注意不要覆盖错了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== “人类一思考，上帝就发笑”。虽然我们已经总结了万物，终究还是渺小和无知的代表，但我们可以利用这些渺小的总结，在有限的生命内过的更好，一些看似无法到达和接触的领域，在被划分整理之后显得那么苍白，他想笑就让他去笑吧，毕竟我们只需要为自己的有限生命而负责~ 2022-11-14 00:29:36]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>rsync</tag>
        <tag>cp</tag>
        <tag>数据同步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下查询主板、CPU、内存等硬件信息]]></title>
    <url>%2Fblog%2F2022%2F11%2F06%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9F%A5%E8%AF%A2%E4%B8%BB%E6%9D%BF%E3%80%81CPU%E3%80%81%E5%86%85%E5%AD%98%E7%AD%89%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言如果是在windows系统下，查询电脑硬件会容易的多，可以通过电脑属性、计算机管理等多种图形化界面中查到，如果安装了各种电脑管家，那查询这类信息就更方便了，但如果在linux系统下通常要使用命令来解决，特别是查询服务器配置时，一般不会给服务器安装图形化界面，所以掌握必要的查询命令对于合理使用服务器资源很有必要。 dmidecode这个命令是偶然发现的，之前一般是查询电脑硬件资源的使用情况，比如 top 来看各个进程消耗的CPU和内存，使用 free -h 查询内存总体使用情况，最近电脑内存不太够了，想查询一下内存插槽状况，所以找到了这个命令 dmidecode，英文解释为 “DMI table decoder”，也就是DMI表解码器，可以理解为DMI信息的解释器。 在学习什么是DMI之前，先了解一下 SMBIOS 的概念，BOIS 是英文”Basic Input Output System”的缩略词，它是一组固化到计算机内主板上一个ROM芯片上的程序，它保存着计算机最重要的基本输入输出的程序、开机后自检程序和系统自启动程序，并可以从CMOS中读写系统设置的具体信息，为计算机提供最底层的、最直接的硬件设置和控制。 SMBIOS 全称是”System Management Basic Input/Output System”，表示系统管理基本输入输出系统，是主板或系统制造者以标准格式显示产品管理信息所需遵循的统一规范。 DMI 全称 “Desktop Management Interface”，用于帮助收集电脑系统信息的管理系统，DMI信息的收集必须在严格遵照SMBIOS规范的前提下进行，其设计适用于任何的平台和操作系统，充当了管理工具和系统层之间接口的角色。它建立了标准的可管理系统，更加方便了电脑厂商和用户对系统的了解，同时提供更为友好的用户工作环境。SMBIOS 和 DMI 都是由行业指导机构 “Desktop Management Task Force (DMTF)” 起草的开放性的技术标准。 常用参数 -q：显示会简单点，一些未知的、不活动的和oem指定的条目不显示，元数据和句柄引用被隐藏。 -t: 一个非常重要的参数，用于显示指定类型的条目，可以使用数字，也可以使用一些代表数字组合的单词 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849DMI TYPES The SMBIOS specification defines the following DMI types: Type Information ──────────────────────────────────────────── 0 BIOS 1 System 2 Baseboard 3 Chassis 4 Processor 5 Memory Controller 6 Memory Module 7 Cache 8 Port Connector 9 System Slots 10 On Board Devices 11 OEM Strings 12 System Configuration Options 13 BIOS Language 14 Group Associations 15 System Event Log 16 Physical Memory Array 17 Memory Device 18 32-bit Memory Error 19 Memory Array Mapped Address 20 Memory Device Mapped Address 21 Built-in Pointing Device 22 Portable Battery 23 System Reset 24 Hardware Security 25 System Power Controls 26 Voltage Probe 27 Cooling Device 28 Temperature Probe 29 Electrical Current Probe 30 Out-of-band Remote Access 31 Boot Integrity Services 32 System Boot 33 64-bit Memory Error 34 Management Device 35 Management Device Component 36 Management Device Threshold Data 37 Memory Channel 38 IPMI Device 39 Power Supply 40 Additional Information 41 Onboard Devices Extended Information 42 Management Controller Host Interface 数字组合的单词代表： 123456789101112131415161718Keyword Types──────────────────────────────bios 0, 13system 1, 12, 15, 23, 32baseboard 2, 10, 41chassis 3processor 4memory 5, 6, 16, 17cache 7connector 8slot 9Keywords are matched case-insensitively. The following command lines are equivalent:· dmidecode --type 0 --type 13· dmidecode --type 0,13· dmidecode --type bios· dmidecode --type BIOS -s：可以输出一些指定关键词相关信息，可用关键词如下： 123456789101112131415161718192021222324Valid string keywords are: bios-vendor bios-version bios-release-date system-manufacturer system-product-name system-version system-serial-number system-uuid system-family baseboard-manufacturer baseboard-product-name baseboard-version baseboard-serial-number baseboard-asset-tag chassis-manufacturer chassis-type chassis-version chassis-serial-number chassis-asset-tag processor-family processor-manufacturer processor-version processor-frequency -t参数测试以查看内存信息为例，可以使用 sudo dmidecode -t memory 命令查看内存所有信息，根据man手册知道这个命令包括5、6、16、17四项内容，但实际上我查询到的5、6为空，这个命令在我的电脑上只包含16、17两项 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283$ sudo dmidecode -t memory# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present.Handle 0x0002, DMI type 16, 23 bytesPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 32 GB Error Information Handle: Not Provided Number Of Devices: 2Handle 0x0003, DMI type 17, 84 bytesMemory Device Array Handle: 0x0002 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: ChannelA-DIMM0 Bank Locator: BANK 0 Type: DDR4 Type Detail: Synchronous Speed: 2667 MT/s Manufacturer: Micron Serial Number: 00000000 Asset Tag: None Part Number: 4ATF51264HZ-2G6E1 Rank: 1 Configured Memory Speed: 2667 MT/s Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: Not Specified Module Manufacturer ID: Bank 1, Hex 0x2C Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 4 GB Cache Size: None Logical Size: NoneHandle 0x0004, DMI type 17, 84 bytesMemory Device Array Handle: 0x0002 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: ChannelB-DIMM0 Bank Locator: BANK 2 Type: DDR4 Type Detail: Synchronous Speed: 2667 MT/s Manufacturer: Micron Serial Number: 00000000 Asset Tag: None Part Number: 4ATF51264HZ-2G6E1 Rank: 1 Configured Memory Speed: 2667 MT/s Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: Not Specified Module Manufacturer ID: Bank 1, Hex 0x2C Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 4 GB Cache Size: None Logical Size: None 可以直接指定单独的数字查询，比如查询内存阵列： 12345678910111213$ sudo dmidecode -t 16# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present.Handle 0x0002, DMI type 16, 23 bytesPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 32 GB Error Information Handle: Not Provided Number Of Devices: 2 查询内存设备（内存条）信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374$ sudo dmidecode -t 17# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present.Handle 0x0003, DMI type 17, 84 bytesMemory Device Array Handle: 0x0002 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: ChannelA-DIMM0 Bank Locator: BANK 0 Type: DDR4 Type Detail: Synchronous Speed: 2667 MT/s Manufacturer: Micron Serial Number: 00000000 Asset Tag: None Part Number: 4ATF51264HZ-2G6E1 Rank: 1 Configured Memory Speed: 2667 MT/s Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: Not Specified Module Manufacturer ID: Bank 1, Hex 0x2C Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 4 GB Cache Size: None Logical Size: NoneHandle 0x0004, DMI type 17, 84 bytesMemory Device Array Handle: 0x0002 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 4096 MB Form Factor: SODIMM Set: None Locator: ChannelB-DIMM0 Bank Locator: BANK 2 Type: DDR4 Type Detail: Synchronous Speed: 2667 MT/s Manufacturer: Micron Serial Number: 00000000 Asset Tag: None Part Number: 4ATF51264HZ-2G6E1 Rank: 1 Configured Memory Speed: 2667 MT/s Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: 1.2 V Memory Technology: DRAM Memory Operating Mode Capability: Volatile memory Firmware Version: Not Specified Module Manufacturer ID: Bank 1, Hex 0x2C Module Product ID: Unknown Memory Subsystem Controller Manufacturer ID: Unknown Memory Subsystem Controller Product ID: Unknown Non-Volatile Size: None Volatile Size: 4 GB Cache Size: None Logical Size: None 一共有2个存储插槽，每个插槽上装有一个4G的内存条，最大支持32G内存 -q参数测试测试查询内存信息命令 sudo dmidecode -t 16 时可以看到开头有一些版本信息，查询其他类型时也包含这些信息 123# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present. 如果想屏蔽这些信息就可以使用 -q 参数，还可以屏蔽未知信息，可以对比一下： 1234567$ sudo dmidecode -t 16 -qPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 32 GB Number Of Devices: 2 12345678910111213$ sudo dmidecode -t 16# dmidecode 3.2Getting SMBIOS data from sysfs.SMBIOS 3.2.0 present.Handle 0x0002, DMI type 16, 23 bytesPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 32 GB Error Information Handle: Not Provided Number Of Devices: 2 -s参数测试查询BOIS信息: 12345678$ sudo dmidecode -s bios-vendorLENOVO$ sudo dmidecode -s bios-versionN2SET18P (1.12 )$ sudo dmidecode -s bios-release-date04/01/2020 查询系统信息： 12345678$ sudo dmidecode -s system-uuida2d29e4c-2f08-11b2-a85c-8252d66b70f9$ sudo dmidecode -s system-familyThinkPad X390$ sudo dmidecode -s system-manufacturerLENOVO 查询机箱主板信息 1234567891011121314151617$ sudo dmidecode -s chassis-typeNotebook$ sudo dmidecode -s chassis-versionNone$ sudo dmidecode -s chassis-serial-numberPC1JX6X4$ sudo dmidecode -s baseboard-manufacturerLENOVO$ sudo dmidecode -s baseboard-product-name20SDA01ACD$ sudo dmidecode -s baseboard-versionSDK0L77769 WIN 查询处理器信息 12345$ sudo dmidecode -s processor-familyCore i5$ sudo dmidecode -s processor-versionIntel(R) Core(TM) i5-10210U CPU @ 1.60GHz 总结 DMI 在遵守SMBIOS规范的前提下帮助收集电脑系统信息，适用于任何的平台和操作系统，可用 dmidecode 命令查询 dmidecode 最重要的是 -t 参数，可以指定的特定数字类型0-42，可以使用具有组合命令的单词， 这些预定单词不区分大小写： bios、system、baseboard、chassis、processor、memory、cache、connector、slot 还有查询硬件使用情况的 top、htop、free 命令，查询静态信息的 lscpu、lsgpu、lsmem、lspci、lsusb命令等待挖掘 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 管理的本质是管人理事。真诚待人，踏实做事，坚信自己的目标可以实现并全力以赴，用行动表明自己的态度，不要忘了沟通交流，了解每个人的核心诉求，不要大包大揽，学会做应该做的事而不是所有事~ 2022-11-6 23:03:25]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>dmidecode</tag>
        <tag>lscpu</tag>
        <tag>硬件信息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk处理/etc/passwd文件]]></title>
    <url>%2Fblog%2F2022%2F10%2F30%2Fawk%E5%A4%84%E7%90%86-etc-passwd%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言awk 是 linux 环境下的一个强大的编程工具，用于对文本和数据进行处理。数据可以来自标准输入、一个或多个文件，或其它命令的输出。同时它支持用户自定义函数和动态正则表达式等先进功能，可以被作为一种编程语言，可以很方便的在命令行中使用，但更多是在脚本来使用，为了熟悉这个命令，我们可以用它来尝试处理 /etc/passwd 文件学习一下常见用法。 awk处理流程awk 这个名字来源于它的三个作者姓氏的第一个字母，分别是Alfred Aho、Brian Kernighan、Peter Weinberger，常用的版本是 gawk， 它是awk的GNU版本，提供了Bell实验室和GNU的一些扩展。 awk 工作流程可分为三个部分： 读输入文件之前执行的代码段（GEGIN块，由BEGIN关键字标识，可选） 主循环执行输入文件的代码段（BODY块，可选） 读输入文件之后的代码段（END块，由END关键字标识，可选） 命令结构: 1awk &apos;BEGIN&#123; commands &#125; pattern&#123; commands &#125; END&#123; commands &#125;&apos; 开始块（BEGIN）1BEGIN &#123;awk-commands&#125; 开始块部分是可选的，可以没有开始块部分，如果存在必须以 BEGIN 开头 开始块就是在程序启动的时候执行的代码部分，并且它在整个过程中只执行一次，我们可以在开始块中初始化一些变量。 主体块（BODY）1pattern &#123;awk-commands&#125; 默认情况下，对于输入的每一行，awk 都会执行命令，我们可以通过 pattern 将其限定在指定的模式中，这部分也是可选的，不存在也是可以的，不过不存在主体的脚本用处不大。 结束块（END）1END &#123;awk-commands&#125; 结束块部分是可选的，可以没有结束块部分，是在程序结束时执行的代码。 如果存在必须以 END 开头 /etc/passwd文件起初 /etc/passwd 文件包含了用户名和密码，但是由于该文件允许所有用户读取，易导致用户密码泄露，因此 Linux 系统将用户的密码信息从 /etc/passwd 文件中分离出来，并单独放到了/etc/shadow 文件中，此文件只有 root 用户拥有读权限，其他用户没有任何权限，这样就一定程度上保证了用户密码的安全性。 /etc/passwd 配置文件的内容用冒号 : 隔开分为7段，分别为： 用户名：账户名字 x：早期这个部分放的是用户登入密码，现在的密码是放入/etc/shadow中的 UID：用户ID，0表示系统管理员，1~999保留给系统使用的ID，1000以上给一般使用者 GID：组ID，0表示系统管理员，1~999保留给系统使用的ID，1000以上给一般使用者 使用者的信息说明 用户家目录：用户登入时所在的目录 默认shell：用户在登入的时候，默认使用的shell类型，如果不能使用shell，则会显示/sbin/nologin /etc/shadow 配置文件的内容用冒号 : 隔开分为9段，分别为： 用户名 经过加密的密码( * 表示用户被锁定，！表示无加密) 最近更改过密码的日期：Linux中的日期是经过1970年1月1号开始累计的日期 密码不能修改的天数，0表示随时可以修改 密码需要重新被修改的天数，通过修改该值可以强制修改密码 密码需要变更的告警天数，7表示系统会向用户发出警告的天数 密码到期后帐号可以使用的时间 帐号失效日期，通过1970年1月1号开始累加的日期，到了时间后无论密码是否过期，该账号就不能再使用了 保留的 如果忘记自己的账户密码，该怎么处理呢？ 对于普通账户的密码遗失，可以通过 root 账户使用 passwd 命令重新设置密码解决，passwd username。 如果 root 账号的密码遗失，则需要重新启动进入单用户模式，系统会提供 root 权限的 bash 接口，此时可以用 passwd 命令修改账户密码。或者通过挂载根目录，修改 /etc/shadow 文件将 root 密码清空，无密码即登陆后再使用 passwd 命令配置 root 密码。 awk 常用处理字段分割及相关变量 $1,$2,$3…$n：awk中用该顺序形式表示files中每行以间隔符号分割的各列的不同字段 $0：表示文本本身 NF：表示当前记录的字段数（列数） $NF：最后一列 $(NF-1)：倒数第二列 FNR/NR：行号 FILENAME：文件名 “\t”：制表符 RS：换行符 “”： 打印字符串 FS：定义间隔符 ~：匹配，与==相比不是精确比较 !~：不匹配，不精确比较 ==：等于，必须全部相等，精确比较 /[0-9][0-9]+/：两个或两个以上数字 -F’[:#]’：定义两个分隔符 常用处理添加表头12345678[root@VM-0-3-centos ~]# awk -F: 'BEGIN&#123;print"用户名\t密码\tUID\tGID\t信息\t家目录\tshell"&#125;&#123;printf $1"\t"$2"\t"$3"\t"$4"\t"$5"\t"$6"\t"$7"\n"&#125;' /etc/passwd用户名 密码 UID GID 信息 家目录 shellroot x 0 0 root /root /bin/bashbin x 1 1 bin /bin /sbin/nologindaemon x 2 2 daemon /sbin /sbin/nologinadm x 3 4 adm /var/adm /sbin/nologinlp x 4 7 lp /var/spool/lpd /sbin/nologinsync x 5 0 sync /sbin /bin/sync 添加行号1234567[root@VM-0-3-centos ~]# awk -F: '&#123;printf NR"\t"$0"\n"&#125;' /etc/passwd1 root:x:0:0:root:/root:/bin/bash2 bin:x:1:1:bin:/bin:/sbin/nologin3 daemon:x:2:2:daemon:/sbin:/sbin/nologin4 adm:x:3:4:adm:/var/adm:/sbin/nologin5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin6 sync:x:5:0:sync:/sbin:/bin/sync 打印第三行第1列和第7列12[root@VM-0-3-centos ~]# awk -F: 'NR==3&#123;print $1"\t"$7&#125;' /etc/passwddaemon /sbin/nologin 打印文件总行数12[root@VM-0-3-centos ~]# awk -F: 'END&#123;print FILENAME"\t"NR&#125;' /etc/passwd/etc/passwd 28 查询不能登录的用户数12[root@VM-0-3-centos ~]# awk -F: 'BEGIN&#123;x=0&#125; &#123;if($7=="/sbin/nologin") x++&#125; END&#123;print FILENAME"\t"x&#125;' /etc/passwd/etc/passwd 21 12[root@VM-0-3-centos ~]# awk '/nologin/' /etc/passwd | wc -l21 查询可以登录的用户123[root@VM-0-3-centos ~]# awk -F: '$NF !~ /nologin$/&#123;print $1&#125;' /etc/passwdrootsync 打印UID小于300的用户1234567[root@VM-0-3-centos ~]# awk -F: '&#123; if($3&lt;300) print $1"\t"$3 &#125;' /etc/passwdroot 0bin 1daemon 2adm 3lp 4sync 5 打印UID在2和6之间的用户1234[root@VM-0-3-centos ~]# awk -F: '($3&gt;2&amp;&amp;$3&lt;6)&#123;print $1"\t"$3 &#125;' /etc/passwdadm 3lp 4sync 5 进行算术运算1234[root@VM-0-3-centos ~]# awk -F: '($3*2 &lt; 6)&#123;print $1"\t"$3 &#125;' /etc/passwdroot 0bin 1daemon 2 利用条件判断统计各类用户数1234[root@VM-0-3-centos ~]# awk -F: '&#123;if($3==0)&#123;i++&#125; else if($3&gt;999)&#123;k++&#125; else&#123;j++&#125;&#125; END&#123;print "管理员个数: "i; print "普通用个数: "k; print "系统用户数: "j&#125;' /etc/passwd管理员个数: 1普通用个数: 1系统用户数: 26 每行数据打印3遍12345678910111213141516[root@VM-0-3-centos ~]# awk -F: '&#123;for(i=1;i&lt;=3;i++) print NR"\t"i"\t"$0&#125;' /etc/passwd1 1 root:x:0:0:root:/root:/bin/bash1 2 root:x:0:0:root:/root:/bin/bash1 3 root:x:0:0:root:/root:/bin/bash2 1 bin:x:1:1:bin:/bin:/sbin/nologin2 2 bin:x:1:1:bin:/bin:/sbin/nologin2 3 bin:x:1:1:bin:/bin:/sbin/nologin3 1 daemon:x:2:2:daemon:/sbin:/sbin/nologin3 2 daemon:x:2:2:daemon:/sbin:/sbin/nologin3 3 daemon:x:2:2:daemon:/sbin:/sbin/nologin4 1 adm:x:3:4:adm:/var/adm:/sbin/nologin4 2 adm:x:3:4:adm:/var/adm:/sbin/nologin4 3 adm:x:3:4:adm:/var/adm:/sbin/nologin5 1 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin5 2 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin5 3 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 统计/etc/passwd中各种类型shell的数量1234567[root@VM-0-3-centos ~]# awk -F: '&#123;shells[$NF]++&#125;END&#123; for(i in shells)&#123;print i,shells[i]&#125;&#125;' /etc/passwd/bin/sync 1/bin/bash 3/sbin/nologin 21/sbin/halt 1/bin/false 1/sbin/shutdown 1 网站访问状态统计1234[root@VM-0-3-centos ~]# netstat -ant | grep :80 | awk '&#123;access_stat[$NF]++&#125;END&#123;for(i in access_stat)&#123;print i, access_stat[i]&#125;&#125;'LISTEN 3CLOSE_WAIT 1ESTABLISHED 1 生成清空arp命令的文本12345678[root@VM-0-3-centos ~]# arp | awk '!/Address/&#123;print "arp -d " $1&#125;'arp -d 169.254.0.2arp -d 169.254.0.3arp -d 169.254.0.79arp -d 172.18.0.3arp -d 169.254.128.8arp -d 169.254.128.9arp -d 10.10.0.17 直接在末尾加管道 bash 就可以执行了，arp | awk &#39;!/Address/{print &quot;arp -d &quot; $1}&#39;| bash 这是一种通用的生成处理命令的方式，先生成命令文本，然后作为bash输入，执行即可。 总结 awk 处理流程分为3个阶段，BEGIN块、BODY块、END块，其中BEGIN块和END块都执行一次，BODY块对每一行输入执行一次 awk 的3个处理块都是可选的，对于待处理的文件，一般至少会包含BODY块 awk 中BODY块的 pattern 用于行过滤，/pattern/ 表示匹配，/!pattern/ 表示不匹配 awk 中BODY块的中也可对指定字段判断是否包含，&#39;$1 ~ /root/&#39; 表示第一列包含root，&#39;$NF !~ /nologin$/&#39; 表示最后一列不包含nologin awk 还可用于待处理命令文本的预处理，先将过滤文本拼装成命令文本，然后利用管道直接通过bash执行 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 最近有些急躁，虽知欲速则不达，但压力之下确实难以平静。之所以这么拼，并不是为了卷谁，也不是为了表现给别人看，只是想有所提升超越昨天的自己，目的也非常俗套，通过提升自身来给自己的小家一个更好的生活。“一室之不治，何以天下家国为？”如果每个人都能把自己的小日子过好，则天下太平! 2022-10-30 21:24:10]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>awk</tag>
        <tag>passwd</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后台运行一个go程序]]></title>
    <url>%2Fblog%2F2022%2F10%2F23%2F%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E4%B8%80%E4%B8%AAgo%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言后台运行在日常开发中比较常用，特别是在部署服务器上，一般都是通过ssh连接到服务器，然后启动后台运行程序，如果程序不支持后台运行，那么当终端断开时程序也就退出了，所以掌握常用的后台运行方式还是比较有用的。 概念提到后台运行，通常会想到 daemon 模式，日常开发时也常常混着说，不过通过查询资料时发现，这两个概念还有些区别： 后台运行：是指进程在操作系统中非显示运行，未关联到任何命令行终端或程序界面，这种方式运行的进程则被称为后台进程。 daemon模式：也叫守护进程，它首先是后台运行，然后它还有守护的职责，若异常退出，可以自动重启服务程序。 所以说 daemon 不仅要时候后台运行，还有守护进程职责，像Windows 和 Linux 中的各种服务，比如MySQL、防火墙、SSH服务等都是后台运行的进程。 常用方式很多产品会部署在linux服务器上，所以相比较而言，后台运行在linux上更常用，而 nohup、&amp;、setsid 等命令就基本上可以达到后台运行的目的，之前写过一篇总结 《linux环境下运行程序常用的nohup和&amp;的区别》，可以简单回忆下： nohup 是no hang up的缩写，就是不挂断的意思，忽略SIGHUP信号，在关闭命令终端后程序依旧运行 &amp; 是只后台运行，即忽略SIGINT信号，也就是按Ctrl+C不会终止程序，但是关闭命令行终端程序终止 而 setsid 是新学到的命令，使用起来也非常的简单，只需要加在待执行命令的前面即可： 1[root@VM-0-3-centos ~]# setsid ping www.baidu.com &gt; out.log 此时关闭当前终端，重新打开另一终端会发现 ping 的进程，同时文件 out.log 文件也一直在更新 12345678[root@VM-0-3-centos ~]# ps -ef | grep pingroot 1692 1 0 23:36 ? 00:00:00 ping www.baidu.comroot 1707 1279 0 23:36 pts/1 00:00:00 grep --color=auto ping[root@VM-0-3-centos ~]# tail -f out.log64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=8 ttl=251 time=9.38 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=9 ttl=251 time=9.36 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=10 ttl=251 time=9.35 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=11 ttl=251 time=9.37 ms 小技巧&amp; 不能让进程永久在后台执行，但是如果在命令前后加上()括起来，一样可以实现nohup ..&amp;的功能，命令就能永久在后台执行了。 12345[root@VM-0-3-centos ~]# (ping www.baidu.com &gt; t.log &amp;)[root@VM-0-3-centos ~]# ps -ef | grep pingroot 3410 1 0 23:44 pts/0 00:00:00 ping www.baidu.comroot 3450 1279 0 23:44 pts/1 00:00:00 grep --color=auto ping 代码级别实现虽然上面的方式很方便，但毕竟只能在 linux 上使用，如果可以通过修改 go 代码在 windows 和 linux 上都实现后台运行那再好不过了，很幸运查到一个go的库 github.com/codyguo/godaemon，使用起来非常方便，只需要在代码中引入这个库，然后启动程序是加入 -d 参数就可以后台运行了 123import ( _ "github.com/codyguo/godaemon") 不过我在使用的过程中发现两个问题，一个是传递的后续参数会莫名消失，另一个是好像关闭终端会导致程序退出，所以我打算看看源码，结果发现源码就只有几行： 1234567891011121314151617181920212223package godaemonimport ( "flag" "fmt" "os" "os/exec")func init() &#123; goDaemon := flag.Bool("d", false, "run app as a daemon with -d=true.") flag.Parse() if *goDaemon &#123; cmd := (os.Args[0], flag.Args()...) if err := cmd.Start(); err != nil &#123; fmt.Printf("start %s failed, error: %v\n", os.Args[0], err) os.Exit(1) &#125; fmt.Printf("%s [PID] %d running...\n", os.Args[0], cmd.Process.Pid) os.Exit(0) &#125;&#125; 这些就是源码的全部了，是不是很吃惊，其实弄懂原理就很好明白了，其中利用了 exec.Command 函数，因为go 中没有 fork 的便利实现，所以可以利用 exec.Command 启动新的进程，这样新启动的进程在当前进程退出后就被系统进程接管了，只不过它处理的参数有点问题，flag.Args()会把所有 - 开头的参数都消耗掉，自己按需实现就可以了 另一个关闭终端会导致程序退出的问题，可以在传入 -d 参数的情况下调用 signal.Ignore(syscall.SIGHUP) 忽略掉 SIGHUP 信号即可 总结 在操作系统中非显示运行，未关联到任何命令行终端或程序界面的进程则被称为后台进程 linux环境下常用来后台运行程序的命令有 hohup、&amp; 和 setsid github.com/codyguo/godaemon 是一个极简的后台运行可用库，仅添加修改一行代码 若想更丰富的功能可以参考 github.com/sevlyar/go-daemon 和 github.com/zh-five/xdaemon 两个库 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生若只如初见，何事秋风悲画扇。等闲变却故人心，却道故人心易变~ 2022-10-24 00:04:29]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>daemon</tag>
        <tag>nohup</tag>
        <tag>setsid</tag>
        <tag>后台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由逆序对数引发的解题目录]]></title>
    <url>%2Fblog%2F2022%2F10%2F04%2F%E7%94%B1%E9%80%86%E5%BA%8F%E5%AF%B9%E6%95%B0%E5%BC%95%E5%8F%91%E7%9A%84%E8%A7%A3%E9%A2%98%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言如果你恰巧看到了我的上一篇总结《排序算法系列之（七）——分分合合的归并排序》会发现我把搁置了3年半的排序系列又续更了，起因是最近刷题时遇到了逆序对数求解，而解这类问题常用的方法之一就是归并排序，究竟是怎样的一道题呢？我们可以先试着解决一下。 逆序对数题目非常简短，描述内容如下： 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。 示例 1: 12输入: [7,5,6,4]输出: 5 限制： 10 &lt;= 数组长度 &lt;= 50000 作者：LeetCode-Solution链接：https://leetcode.cn/problems/shu-zu-zhong-de-ni-xu-dui-lcof/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 求解过程题目要求不难理解，即使你不知道什么是逆序对也能很容易的根据描述写出下面暴力的解法： 123456789101112class Solution &#123; int ans = 0;public: int reversePairs(vector&lt;int&gt;&amp; nums) &#123; for (int i = 0; i &lt; nums.size(); i++) &#123; for (int j = i + 1; j &lt; nums.size(); j++) &#123; if (nums[i] &gt; nums[j]) ++ans; &#125; &#125; return ans; &#125;&#125;; 不出意外的获得了 TLE，看看限制范围也能猜到这题暴力肯定不让过，如果数组长度小于 100 还可以考虑搏一搏，而这道题必须用更巧妙的方法才行，而归并排序就是解法之一： 12345678910111213141516171819202122232425262728293031class Solution &#123; int ans = 0;public: void mergeCnt(vector&lt;int&gt;&amp; v, int left, int right, vector&lt;int&gt;&amp; t) &#123; if (left &gt;= right) return; int i = left, mid = (left+right) / 2, j = mid + 1, k = 0; mergeCnt(v, left, mid, t); mergeCnt(v, mid+1, right, t); while(i &lt;= mid &amp;&amp; j &lt;= right) &#123; if (v[i] &lt;= v[j]) t[k++] = v[i++]; else &#123; t[k++] = v[j++]; ans += mid - i + 1; // 相比归并排序只多了这一行 &#125; &#125; while (i &lt;= mid) t[k++] = v[i++]; while (j &lt;= right) t[k++] = v[j++]; copy(t.begin(), t.begin() + right - left + 1, v.begin() + left); &#125; int reversePairs(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;int&gt; t(n); mergeCnt(nums, 0, n-1, t); return ans; &#125;&#125;; 相比原始的归并排序只增加了一行代码，就得到了一个优于暴力解法O(N^2)的O(NlogN)的解法，具体解释可以去看相关题解，本文的总结目的不是解一道题，而是想给自己保留一个有趣的解题目录。 解题目录刷题大计最近两年断断续续的在进行着，题解也有写过一些，尝试过多种方式，内容比较零散，复习时比较头疼，没有一个完整的大纲和复习线路，所以打算单开一篇总结，持续收集一些有意思的题目，方便后续的复习和拿来即用。 题目的标签主要有两个方面，一是本身的题目知识点类型，另一种是解题用到的解法类型，比如上面提到的这道题，从题目看归为逆序对数，从解法看可以归为排序解法和离散化树状数组解法，所以后面可以会看到一个题目出现在多个目录中的情况，只是分类依据不同而已。 题目分类逆序对数 51. 数组中的逆序对 解法分类归并排序 51. 数组中的逆序对 离散化树状数组 51. 数组中的逆序对 差分数组 2406. 将区间分为最少组数 滑动窗口 2379. 得到 K 个黑块的最少涂色次数 Trie树 2416. 字符串的前缀分数和 1233. 删除子文件夹 动态规划 19. 秋叶收藏集 ……持续补充 总结 有时看似无关的两件事居然关系紧密，比如归并排序加一行代码就可以求解逆序对数 分类、总结、重新分类，在不断分类中重新认识这个世界 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 不积跬步，无以至千里；不积小流，无以成江海。放眼于未来，着眼于脚下，一味计划而不行动，最终醒来只会发现是梦一场~ 2022-10-5 00:49:14]]></content>
      <categories>
        <category>OJ</category>
      </categories>
      <tags>
        <tag>题解</tag>
        <tag>OJ</tag>
        <tag>解题目录</tag>
        <tag>逆序对数</tag>
        <tag>归并排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法系列之（七）——分分合合的归并排序]]></title>
    <url>%2Fblog%2F2022%2F10%2F03%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B9%8B%EF%BC%88%E4%B8%83%EF%BC%89%E2%80%94%E2%80%94%E5%88%86%E5%88%86%E5%90%88%E5%90%88%E7%9A%84%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言再一次总结基础的排序算法，印象里距离上一次总结排序也没过多久，查询后才发现上一篇总结《排序算法系列之（六）——逐步砍掉树杈的堆排序》到现在已经过去了3年多的时间，真是时光荏苒，岁月如梭啊，这次想起总结排序起因并不是排序，而是查找逆序数对，而解决逆序数对通常使用的两种方式是归并排序和离散化树状数组，所以我又把归并排序捡起来了 归并排序温故而知新，知识有时候就是这么神奇，多年以后再次看到归并排序，才发现以前掌握的归并排序并不全面，之前理解的归并排序主要放在“并”这个操作，但是其实归并排序中还有很重要的一环，那就是“分”，先分后合才是完整的归并排序。 归并排序是建立在归并操作上的一种有效，稳定的排序算法，该算法是采用分治法的一个非常典型的应用，首先将整个序列一分为二得到两个序列，然后将分出的两个序列排好序后再合并，得到完全有序的序列。对于子序列如果元素大于1则继续进行一分为二，分别排序再合并的操作，直至有序，算法本身是通过一个递归的概念定义。当然反过来通过递推也可以实现，相邻区间两两合并，最终至全部有序也是可以的。 提到一分为二很容易联想到快速排序，在最优的情况下待排序的数组每次被一分为二，将小于中间值的元素全部移到数组前半段，将大于中间值的元素全部移到数组后半段，完成一趟快排，然后对每段分别采用相同的策略从而达到整理有序，这是快排的思想，听起来有些部分确实和归并排序很像，但是区别也是很大的，比如快排不需要合并的操作，另外快排是一种不稳定的排序。 为了简单一点，我们还是采用递归的版本来描述一下归并排序，先画一个图，直观的看下归并排序中二分天下是怎么操作的，先分裂： 12345678910111213141516171819graph TB A[2,3,9,4,7,12,6,1,11,5]--&gt;B[2,3,9,4,7]; A[2,3,9,4,7,12,6,1,11,5]--&gt;C[12,6,1,11,5]; B[2,3,9,4,7]--&gt;D[2,3,9]; B[2,3,9,4,7]--&gt;E[4,7]; C[12,6,1,11,5]--&gt;F[12,6,1]; C[12,6,1,11,5]--&gt;G[11,5]; D[2,3,9]--&gt;H[2,3]; D[2,3,9]--&gt;I[9]; E[4,7]--&gt;J[4]; E[4,7]--&gt;K[7]; F[12,6,1]--&gt;L[12,6]; F[12,6,1]--&gt;M[1]; G[11,5]--&gt;N[11]; G[11,5]--&gt;O[5]; H[2,3]--&gt;P[2]; H[2,3]--&gt;Q[3]; L[12,6]--&gt;R[12]; L[12,6]--&gt;S[6]; idx_0 idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 2 3 9 4 7 12 6 1 11 5 排序后再合并： 12345678910111213141516171819graph BT A[1,2,3,4,5,6,7,9,11,12]--&gt;B[2,3,4,7,9]; A[1,2,3,4,5,6,7,9,11,12]--&gt;C[1,5,6,11,12]; B[2,3,4,7,9]--&gt;D[2,3,9]; B[2,3,4,7,9]--&gt;E[4,7]; C[1,5,6,11,12]--&gt;F[1,6,12]; C[1,5,6,11,12]--&gt;G[5,11]; D[2,3,9]--&gt;H[2,3]; D[2,3,9]--&gt;I[9]; E[4,7]--&gt;J[4]; E[4,7]--&gt;K[7]; F[1,6,12]--&gt;L[6,12]; F[1,6,12]--&gt;M[1]; G[11,5]--&gt;N[11]; G[11,5]--&gt;O[5]; H[2,3]--&gt;P[2]; H[2,3]--&gt;Q[3]; L[6,12]--&gt;R[12]; L[6,12]--&gt;S[6]; 排序过程上面的图可以很清楚的看出怎样划分以及合并的结果，但是对于描述怎样每个子数组怎样从无序变成有序不太明显，同时也忽略了合并的算法，所以下面用语言简单描述下，其实每个子数组怎样从无序变成有序这个并不需要关心，因为数组是一个元素是必定有序，当数组包含两个元素时，说明它是由两个单元素数组“归并”而成，所以我们需要掌握的是归并的算法。同理，4个元素的数组是由两个双元素有序数组归并而成，同样说明需要掌握的只有归并的逻辑。 下面我们具体操作一下，以 [2,3,4,7,9] 和 [1,5,6,11,12] 两个子数组归并成最终有序数组为例，模拟一次从小到大排序。 首先我们假设有两个指针L 和 R 分别指向两个数组的首个元素，另有一个数组 M 为结果数组，初始为空 M = []，下面开始操作，初始数据如下： 123[2(L),3,4,7,9][1(R),5,6,11,12][] 比较两个指针元素，找出两个指针所指的更小的元素，放入结果数组后指针加一，R 指针元素更小，元素1放入结果数组，指针向后加一 123[2(L),3,4,7,9][1,5(R),6,11,12][1] 继续步骤1，这次 L 指针元素更小，元素2放入结果数组，指针向后加一 123[2,3(L),4,7,9][1,5(R),6,11,12][1,2] 继续比较，还是 L 指针元素更小，元素3放入结果数组，指针向后移动 123[2,3,4(L),7,9][1,5(R),6,11,12][1,2,3] 继续比较，仍然是 L 指针元素更小，元素4放入结果数组，指针向后移动 123[2,3,4,7(L),9][1,5(R),6,11,12][1,2,3,4] 继续比较，这次变成 R 指针元素更小，元素5放入结果数组，指针向后移动 123[2,3,4,7(L),9][1,5,6(R),11,12][1,2,3,4,5] 继续比较，仍然是 R 指针元素更小，元素6放入结果数组，指针向后移动 123[2,3,4,7(L),9][1,5,6,11(R),12][1,2,3,4,5,6] 继续比较，现在是 L 指针元素更小，元素7放入结果数组，指针向后移动 123[2,3,4,7,9(L)][1,5,6,11(R),12][1,2,3,4,5,6,7] 继续比较，仍是 L 指针元素更小，元素9放入结果数组，指针向后移动 123[2,3,4,7,9](L)[1,5,6,11(R),12][1,2,3,4,5,6,7,9] 现在 L 指针已经走到了数组的最后，而 R 所在的数组还有元素，说明 R 中剩余的元素肯定都比 L 最后一个元素都大，所以直接把 R 中剩余元素11和12放到结果数组就完成了排序 123[2,3,4,7,9](L)[1,5,6,11,12](R)[1,2,3,4,5,6,7,9,11,12] 从排序过程也可以看出，使用归并排序需要一个额外的结果数组来完成合并操作，下面我们用代码来实现一下这个算法。 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/*功能： 交换两个变量参数： element1--被交换的第一个元素的地址 element2--被交换的第二个元素的地址返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void swap_data(int* element1, int* element2)&#123; int middle_value = *element1; *element1 = *element2; *element2 = middle_value;&#125;/*功能： 将数组的两个有序段[left,mid]和[mid+1,right]合并成[left,right]区间完整有序参数： array--表示待排序的数组，此处会退化成指针 left--数组第一段开始的索引 mid--数组第一段结束的索引 right--数组第二段结束的索引返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void merge(int array[], int left, int mid, int right)&#123; int* temp = (int*)malloc((right-left+1)*4); int i = 0, l = left, r = mid + 1; while (l &lt;= mid &amp;&amp; r &lt;= right) &#123; if (array[l] &lt;= array[r]) temp[i++] = array[l++]; else temp[i++] = array[r++]; &#125; //第一段仍有元素没加到结果 while (l &lt;= mid) temp[i++] = array[l++]; //第二段仍有元素没加到结果 while (r &lt;= right) temp[i++] = array[r++]; //结果赋值回原数组 for (int j = 0; j &lt;= right - left; j++) array[left+j] = temp[j]; free(temp);&#125;/*功能： 归并排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 left--数组第一个待排序元素索引 right--数组最后一个待排序元素索引返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void merge_sort(int array[], int left, int right)&#123; if (right &gt; left) &#123; int mid = (right + left) / 2; merge_sort(array, left, mid); merge_sort(array, mid+1, right); merge(array, left, mid, right); &#125;&#125; 代码分析归并排序的核心的是分割和归并，分割时采取递归一分为二就可以，然后归并才是体现算法精髓的地方，合并时通常使用两个指针来分别指向数组的两段，通过不断比较元素大小将两段有序数组合并成一段，在排序过程中使用了额外的空间，这也是归并排序的劣势，例子中为了方便每次合并时都申请了新数组，其实可以优化一下，在排序开始申请一个临时数组就可以，中间合并时使用同一个临时数组就可以。 运行测试在线编辑器是一个很方便的测试代码的环境，如果想本地调试一下，也可以直接下载归并排序–源码，在本地编译后进行调试，其实边看代码边调试是理解算法思路很有效的方式。 总结 归并排序的核心思想是不断二分后合并两段有序子数组达到最终有序 归并排序因为是优先比较相邻的元素，所以是稳定的排序算法 归并排序使用了临时空间，最大与原数组等长，优化时可以在排序前申请一个就可以服务于整个排序过程 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 生活的点点滴滴随时间流淌，它们不是逝去，恰恰是它们组成了我们的生活，我们可以选择躺平，可以选择奋斗，可以选择全力以赴，当我们还可以选择的时候，请感恩你所拥有的一切吧，特别是对已经攥在手里的东西，深深地表达一下感激，勿等失去徒伤悲。 2022-10-4 02:20:05]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
        <tag>归并</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本来打算完整安装一次redmine最终还是放弃了]]></title>
    <url>%2Fblog%2F2022%2F10%2F01%2F%E6%9C%AC%E6%9D%A5%E6%89%93%E7%AE%97%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85%E4%B8%80%E6%AC%A1redmine%E6%9C%80%E7%BB%88%E8%BF%98%E6%98%AF%E6%94%BE%E5%BC%83%E4%BA%86%2F</url>
    <content type="text"><![CDATA[前言redmine 是最近工作中经常使用的项目管理软件，因为平时主要使用跟踪一些开发进度和bug修复情况，平时使用并不算重度，体验还不错，最近因为需要停电重启，所以想借着这个机会学习一下怎么安装，看了不少教程，各种安装方法，因为依赖项确实有点多，至今也没按照官网wiki安装成功（战术捂脸），不过从中发现了很多新的知识点，还是记录一下。 安装参考其实官方的wiki写的就不错，但是有关依赖项的部分提及的很少，还有就是没有说怎么安装 ruby，先把写的比较好的文档列举在此，有时间再试试，其实也快安装成功了，就是云主机内存太低，有一步安装需要编译，总是内存不足… redmine/wiki/redmineinstall CentOS 7 安装 Redmine 4.1 Linux下redmine安装使用 centos安装ruby及更换gem的源 bitnami redmine一键式安装 centos 安装 redmine mysql ruby on rails How to setup Redmine 4 on Ubuntu 18.04 Linux 下安装 Redmine linux搭建redmine：bitnami-redmine-4.1.1-4-linux-x64-installer.run Redmine安装神器：Bitnami redmine 的安装指导（Linux的安装方法） Redmine packaged by Bitnami 以上列举的教程前几个还不错，虽然没安装完整，但是接触了ruby、nginx、mysql这些依赖项，也了解了不少新东西，大概知道运行 redmine 需要哪些东西了，完成安装只是时间和硬件问题，咳咳~ 其中有几篇文章建议利用 bitnami 来简化 redmine 的安装，看步骤确实方便了很多，但是不幸的是 Bitnami 在2021年6月30日之前停止对大多数Linux本地安装程序的支持，我已经找不到安装程序 bitnami-redmine-4.0.5-0-linux-x64-installer.run了 Bitnami plans on discontinuing the support for the majority of Native Installers for Linux by June 30th 2021. Please read this blog post for more information. Ruby已经多次接触到 Ruby 了，上次是升级gitlab服务器的时候，新版本的gitlab需要安装ruby依赖项，这次安装redmine同样需要安装Ruby，对于Ruby的安装省事的方法可以通过Linux对应包管理工具直接安装，但是版本通常达不到要求，所以我们总是需要采用复杂的方式才可以安装Ruby，先了解下Ruby是什么。 Ruby 是一种跨平台、面向对象的动态类型编程语言。Ruby 体现了表达的一致性和简单性，它不仅是一门编程语言，更是表达想法的一种简练方式。 Ruby 的作者于 1993 年 2 月 24 日开始编辑 Ruby，直至 1995 年 12 月才正式公开发布。之所以称为 Ruby，是因为 Perl 的发音与 6 月的诞生石 pearl（珍珠）相同，因此 Ruby 以 7 月的诞生石 ruby（红宝石）命名。 Ruby 的特性与 Smalltalk、Perl 和 Python 类似。Perl、Python 和 Smalltalk 是脚本语言。Smalltalk 是一个真正的面向对象语言。Ruby，与 Smalltalk 一样，是一个完美的面向对象语言。使用 Ruby 的语法比使用 Smalltalk 的语法要容易得多。 RailsRuby on Rails（官方简称为 Rails，亦被简称为 RoR），是一个使用 Ruby 语言写的开源 Web 应用框架，它是严格按照 MVC 结构开发的。它努力使自身保持简单，来使实际的应用开发时的代码更少，使用最少的配置。 Rails 的设计原则包括“不做重复的事”和“惯例优于设置” 。 Ruby on Rails 是一种结合 Ruby 语言与 Rails 平台的一种网页程序框架，Ruby 语言以自然、简洁、快速著称，全面支持面向对象程序设计，而 Rails 则是 Ruby 广泛应用方式之一，在 Rails 平台上设计出一套独特的 MVC 开发架构，采取模型（Model）、视图（View）、控制器（Controller）分离的开发方式，不但减少了开发中的问题，更简化了许多繁复的动作。 RubyGemsRubyGems 是 Ruby 的一个包管理器，它提供一个分发 Ruby 程序和库的标准格式，还提供了一个管理程序包安装的工具，它将一个 Ruby 应用程序打包到一个 gem 里，作为一个安装单元，旨在方便地管理 gem 安装的工具，以及用于分发 gem 的服务器，这类似于 Ubuntu 下的apt-get，Centos 的 yum，Python 的 pip，大约创建于2003年11月，从Ruby 1.9版起成为Ruby标准库的一部分。 RVM在linux上安装Ruby在之前提到了，可以通过对应系统的包管理软件来直接安装，比如在CentOS上可以直接运行 sudo yum install ruby 命令来安装，但是这种方式通常安装的版本比较低，无法满足需要，所以你也可以采用源码编译的方式安装。 如果从源码级别安装已经超出了你的能力范围还可以使用 RVM 来安装，它是 Ruby 的版本管理工具，类似 nodejs的 nvm 工具，可以方便的安装和配置当前系统使用的Ruby版本，安装RVM的命令如下： 12$ gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB$ curl -sSL https://get.rvm.io | bash -s stable 若安装报错先输入以下命令再继续： 12$ command curl -sSL https://rvm.io/mpapis.asc | gpg2 --import -$ command curl -sSL https://rvm.io/pkuczynski.asc | gpg2 --import - 之后就可以利用 rvm 命令来安装 Ruby 了 12$ rvm install 2.6$ ruby -v 另辟蹊径安装 redmine 很困难，难道我们就没办法了吗？不！我们还有 docker，一条命令 docker run -d --name some-redmine -p 8080:3000 redmine 搞定，首先会下载所需要的镜像，之后直接启动，通过 http://IP:8080/ 直接就可以访问了，真香~ 总结 redmine 是一个项目管理web软件，依赖Rails、MySQL、Nginx等组件或服务 Ruby on Rails 是一个使用 Ruby 语言写的开源 Web 应用框架，严格按照 MVC 结构开发 可以通过RVM安装Ruby， RVM 是 Ruby 的版本管理工具，可以方便的安装和配置当前系统使用的Ruby版本 如果参照官方的wiki实在安装不上redmine，可以通过 docker 来安装，虽然看起来像个玩具，但是真的能用啊 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 黄河远上白云间，一片孤城万仞山。羌笛何须怨杨柳，春风不度玉门关。 2022-10-3 02:33:26]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
        <tag>redmine</tag>
        <tag>ruby</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如期而至的SVN服务器迁移引来一个大瓜XAMPP]]></title>
    <url>%2Fblog%2F2022%2F09%2F13%2F%E5%A6%82%E6%9C%9F%E8%80%8C%E8%87%B3%E7%9A%84SVN%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%81%E7%A7%BB%E5%BC%95%E6%9D%A5%E4%B8%80%E4%B8%AA%E5%A4%A7%E7%93%9CXAMPP%2F</url>
    <content type="text"><![CDATA[前言之前对于版本控制工具 svn 和 git 更多的是操作命令的使用，而最近逐步开始接触使用环境的搭建了，对于一些私有项目还是需要在内部服务器上搭建一个 svn 或者 git 服务器的，前段时间因为 SVN 服务器硬盘空间告急，所以 SVN 数据迁移被提上了日程，经过几个月的准备，终于要动手了，期间也讨论了几个方案，结果最终迁移完发现，和我想的完全不一样啊，看来还是我太年轻了，世界真大~ 方案评估进行重大行动前总要有个可行性分析，虽然这不算特别重大，但终归会影响一个项目组的工作，所以要尽可能考虑周全，避免造成数据损失，影响开发进度，所以一开始基于目前数据仓库的现状提出了几种迁移的方案： 原机器增加硬盘：经运维同学确认机器太老，不支持扩展了，此路不通 采用 svnadmin dump 这种官方推荐的备份方式：这种方式采用 svnadmin dump、svndumpfilter、svnadmin load 等命令可以实现仓库备份、还原、甚至定向过滤等目的，但这种方式适合数据少短历史的仓库，如果版本比较大，如版本数增长数十万，那么dump的过程将非常慢，恢复过程更耗时，另外还有一个难点，这种方式要求磁盘空间足够，但是目前仓库占用90%，剩余的10%用来备份绝对不够的，所以此种方法也基本行不通 丢弃提交历史，本地客户端更新最新数据，然后提交到新配的服务器上，做0版提交：此种方法可以绕过磁盘不足的问题，但这种方式丢掉了历史记录，并且改变了仓库的uuid，测试时发现无论是 switch 还是 relocate 命令都无法直接切换，基本上要重新下载一份数据了。 把整个系统连同数据直接镜像到新配置的服务器上：这种方式可以保留完整的提交记录，同时不需要老硬盘提供太大的空间，但操作难度上还需运维同学评估。 其他：使用 svnadmin hotcopy 进行全量拷贝，备份过程较快，灾难恢复也很快；使用 svnsync 制作2个镜像库，须在 svn1.4 版本以上使用。这些基本都是备份的方式，是保证数据安全应该采取的策略，可以作为参考。 所以综上优先选择全盘镜像，其次选择用最新的数据做0版的方式，除此之外如果用 svnadmin 因硬盘受限就只能划分成一个个子目录来迁移了。 前奏之前我接触到的svn服务器基本就是安装svnserve就好，修改用户的访问权限可以直接在服务器上编辑access配置文件，后来了解到了 svnmanager 才知道原来有个可以修改这些配置文件的网页啊，用起来方便了许多，这次迁移之前我还是停留在这个层面上，结果运维同学搭建好环境后，我用 ps 命令找了半天也没找到 svnserve 进程，然后在历史记录里发现了 xampp 这货 接着便查找了很多关于 xampp 的知识，才发现之前还是弱了，这些基本都没了解过，经过这一次迁移，接触到了几十个命令、相关工具和配置文件，特此记录一下整个过程，方便今后复习和拿来就用。 最终迁移的方案选择的是数据整体拷贝，通过网络从旧机器发送到新机器上，然后在新机器上搭建svn服务器环境，这种方式好处很多：不依赖原机器剩余的硬盘空间、传输方便、可以完整保留历史记录、不用重新配置权限文件、修改域名绑定后对所有使用者几乎无感，即使需要重新认证，输入自己的svn用户名密码就搞定了，非常方便。 svn客户端访问服务端时可以通过svnserve、svnserver+ssh、Apache等多种方式访问。svnserve是一个小巧、轻便的服务器程序，设置简单，可以使用 subversion 专有的协议进行访问；但因为本身不提供加密通讯的功能，安全性低，可以通过ssh建立在安全隧道后调用svnserve程序；当然也可以通过http访问，利用Apache通过mod_dav_svn访问版本库，进而进行svn的操作。 我平时用的较多的也是通过 http 协议来访问和更新 svn 仓库的，可以直接搭建 Apache 来实现，也可以使用很方便的 xammp 来实现，本质上访问svn数据和使用svnmanager管理用户和权限都是网页服务，所以这两项都可以利用xammp，那么接下来简单了解下什么是 xammp XAMMPXAMPP（Apache+MariaDB+PHP+PERL）是一个功能强大的建站集成软件包。这个软件包原来的名字是 LAMPP（Linux+Apache+MySQL+PHP+PERL），但是为了避免误解，最新的几个版本就改名为 XAMPP 了。 XAMMP 的出现源于人们对安装网页服务时的挫败感，许多人通过他们自己的经验认识到安装 Apache 服务器是件不容易的事儿，如果您想添加 MySQL、PHP 和 Perl等环境那就更困难了。XAMPP 是一个易于安装且包含 MySQL、PHP 和 Perl 的 Apache 发行版，非常容易安装和使用：只需下载，解压缩，启动即可，最近的版本把解压缩这一步替换成了运行脚本安装，其实和解压缩一样。 因为软件包非常全，一下就解决了访问、数据存储、数据管理、插件安装管理等多个方面，我查看了其中一些软件的配置文件，比如MySQL，无论是配置路径还是数据存储路径都是在 XAMPP 安装路径下，如果想在新的机器上部署一套完全一样的服务，只需要拷贝整个安装目录到新机器上就行了（前提是一些安装时创建的新用户要有），保险的做法是在新机器上安装一次，再用旧机器数据覆盖上去就行了。 前面夸了 XAMPP 这么久，那么它有没有缺点呢？当然有！缺点就是不安全，XAMPP仅用于开发目的，它具有某些配置设置（比如MySQL没有密码），使本地开发变得容易，如果你想让你搭建的XAMPP可以从互联网访问，这样并不是一个好的做法，可以使用WAMP, MAMP或LAMP等替换方案，这些是类似的软件包，更适合生产环境。 不知道是不是因为仅用于开发环境，网络上很难找到在Linux 环境下安装 XAMPP + subversion + svnmanager 的完整教程，在 windows 下安装倒是有几篇，所以我追随运维同学使用历史命令列表，尝试完整搭建一次 svn 服务，这可是从几百个命令列表中不断尝试出来的。 搭建svn服务系统环境如下： [root@VM-0-3-centos /]# hostnamectl Static hostname: VM-0-3-centos Icon name: computer-vm Chassis: vm Machine ID: 5467bde017714ffcad6d449b4a1fbbbc Boot ID: 8aa2f3bd14104190bc11e39bf2831052 Virtualization: kvm Operating System: CentOS Linux 7 (Core) CPE OS Name: cpe:/o:centos:centos:7 Kernel: Linux 3.10.0-1127.19.1.el7.x86_64 Architecture: x86-64 准备软件包可以自行去官网下载： xampp-linux-x64-5.6.40-1-installer.run subversion-1.10.8-bin.tar.gz svnmanager-1.10.tar.gz 安装必要环境和工具123yum install -y epel-releaseyum install vim net-tools telnet wget bind-utils ipmitool ntp rsyncyum install libserf 安装xampp给脚本添加可执行命令后，直接运行即可： 12chmod +x xampp-linux-x64-5.6.40-1-installer.run./xampp-linux-x64-5.6.40-1-installer.run 安装过程还真是挺容易的，一直输入 Y 就可以了 12345678910111213141516171819202122232425262728293031323334[root@VM-0-3-centos software]# ./xampp-linux-x64-5.6.40-1-installer.run----------------------------------------------------------------------------Welcome to the XAMPP Setup Wizard.----------------------------------------------------------------------------Select the components you want to install; clear the components you do not wantto install. Click Next when you are ready to continue.XAMPP Core Files : Y (Cannot be edited)XAMPP Developer Files [Y/n] :YIs the selection above correct? [Y/n]: Y----------------------------------------------------------------------------Installation DirectoryXAMPP will be installed to /opt/lamppPress [Enter] to continue:----------------------------------------------------------------------------Setup is now ready to begin installing XAMPP on your computer.Do you want to continue? [Y/n]: Y----------------------------------------------------------------------------Please wait while Setup installs XAMPP on your computer. Installing 0% ______________ 50% ______________ 100% #########################################----------------------------------------------------------------------------Setup has finished installing XAMPP on your computer. 运行xampp进入到目录下启动服务 12cd /opt/lampp./xampp start 报下面的错误： 1234567891011121314151617181920212223242526272829# ./xampp start/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/bash: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directoryid: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/share/xampp/xampplib: line 11: test: -ne: unary operator expected/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directoryXAMPP: netstat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directorynetstat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/bin/httpd: error while loading shared libraries: librt.so.1: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directoryXAMPP: hostname: error while loading shared libraries: libnsl.so.1: cannot open shared object file: No such file or directorynetstat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directoryXAMPP: netstat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/bin/sh: error while loading shared libraries: libdl.so.2: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory/opt/lampp/bin/gettext: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directorycat: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory 编辑xampp1vim xammp 将文件中 export LD_ASSUME_KERNEL=2.2.5 内容改成 export LD_ASSUME_KERNEL=2.8.0，再次启动时正常 12345[root@VM-0-3-centos lampp]# ./xampp startStarting XAMPP for Linux 5.6.40-1...XAMPP: Starting Apache...ok.XAMPP: Starting MySQL...ok.XAMPP: Starting ProFTPD...ok. 访问xampp浏览器输入IP后回车，显示页面如下，XAMPP 就安装好了，是不是很方便 安装subversion直接解压，移动到指定的位置即可 123tar zxvf subversion-1.10.8-bin.tar.gzmv subversion /usr/local/cp /usr/local/subversion/*.so /opt/lampp/modules/ 安装svnmanagersvnmanager 依赖 VersionControl_SVN-0.5.1，所以需要先安装这个组件： 12cd /opt/lampp/bin./pear install VersionControl_SVN-0.5.1 输出信息如下： 123456[root@VM-0-3-centos bin]# ./pear install VersionControl_SVN-0.5.1WARNING: channel "pear.php.net" has updated its protocols, use "pear channel-update pear.php.net" to updatedownloading VersionControl_SVN-0.5.1.tgz ...Starting to download VersionControl_SVN-0.5.1.tgz (31,559 bytes).........done: 31,559 bytesinstall ok: channel://pear.php.net/VersionControl_SVN-0.5.1 解压 svnmanager-1.10.tar.gz 移动到指定位置： 12tar zxvf svnmanager-1.10.tar.gzmv svnmanager-1.10 /opt/lampp/htdocs/svnmanager 创建svn仓库目录新建svn必要的目录，并调整目录权限 1234mkdir -p /export/svnrepos/configmkdir -p /export/svnrepos/reposmkdir -p /export/svnrepos/trashchown -R daemon:daemon /export/svnrepos/ 修改配置文件首先修改Apache的主配置文件： 12cp /opt/lampp/etc/httpd.conf /opt/lampp/etc/httpd.conf.bakvim /opt/lampp/etc/httpd.conf 搜索 LoadModule 在后面添加如下内容 12345678910111213141516171819# add for svnLoadModule dav_svn_module modules/mod_dav_svn.soLoadModule authz_svn_module modules/mod_authz_svn.so&lt;Location /svn&gt; DAV svn SVNParentPath /export/svnrepos/repos # Allow the index page to list all the repositories it contains SVNListParentPath On # Require SSL connection for password protection. # SSLRequireSSL AuthType Basic # Message to give to the committer AuthName "AW SVN" # File listing users with write (commit) access AuthzSVNAccessFile /export/svnrepos/config/svn_access_file AuthUserFile /export/svnrepos/config/svn_passwd_file Require valid-user&lt;/Location&gt; 修改svnmanger配置文件 12cp /opt/lampp/htdocs/svnmanager/config.php.linux /opt/lampp/htdocs/svnmanager/config.phpvim /opt/lampp/htdocs/svnmanager/config.php 按照自己的本地目录修改成以下配置： 12345678910111213141516171819//Shell command's$htpassword_cmd = "/opt/lampp/bin/htpasswd";$svn_cmd = "/usr/local/subversion/bin/svn";$svnadmin_cmd = "/usr/local/subversion/bin/svnadmin";//Subversion locations$svn_config_dir = "/export/svnrepos/config/";$svn_repos_loc = "/export/svnrepos/repos/";$svn_passwd_file = "/export/svnrepos/config/svn_passwd_file";$svn_access_file = "/export/svnrepos/config/svn_access_file";//If the following is set, removing a repository will cause it to be//moved to this location rather than being deleted.$svn_trash_loc = "/export/svnrepos/trash/";//Data Source Name (only tested with mysql and sqlite!!)//$dsn = "mysqli://svnmanageruser:xxxpass@localhost/svnmanager"; 基本上看注释和变量名都能明白各个配置的作用，简单解释几个： $htpassword_cmd：用于加密svn用户密码的，svn支持明文密码，使用加密后密码更安全一些 $svn_passwd_file：保存SVN仓库所有用户的密码文件，svnmanager增删改用户时不仅修改自己的数据库，也会同步修改此文件 $svn_access_file：保存SVN仓库所有权限访问规则，svnmanager增删改用户和组的权限时不仅修改自己的数据库，也会同步修改此文件 $dsn：svnmanager自己访问MySQL的用户名、密码、及数据库名字 为svnmanager创建MySQL用户直接在命令行用mysql命令就可以登录MySQL，无需密码，这也是前面提到的XAMPP不安全的地方，好在设置的是只允许本地访问 12cd /opt/lammp/bin./mysql 展示如下： 12345678910[root@VM-0-3-centos bin]# ./mysqlWelcome to the MariaDB monitor. Commands end with ; or \g.Your MariaDB connection id is 13Server version: 10.1.38-MariaDB Source distributionCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.MariaDB [(none)]&gt; 为svnmanager创建MySQL用户，并授予权限： 1234CREATE DATABASE svnmanager;CREATE USER 'svnmanageruser'@'localhost' IDENTIFIED BY 'xxxpass';GRANT ALL PRIVILEGES ON db . * TO 'svnmanageruser'@'localhost';FLUSH PRIVILEGES; 执行结果如下： 12345678910111213141516171819202122232425262728293031MariaDB [(none)]&gt; CREATE DATABASE svnmanager;Query OK, 1 row affected (0.00 sec)MariaDB [(none)]&gt; CREATE USER 'svnmanageruser'@'localhost' IDENTIFIED BY 'xxxpass';Query OK, 0 rows affected (0.04 sec)MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON svnmanager . * TO 'svnmanageruser'@'localhost';Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || phpmyadmin || svnmanager || test |+--------------------+6 rows in set (0.01 sec)MariaDB [(none)]&gt; use svnmanagerDatabase changedMariaDB [svnmanager]&gt; show tables;Empty set (0.00 sec)MariaDB [svnmanager]&gt; 重启xammp服务1/opt/lampp/xammp restart 展示如下： 12345678[root@VM-0-3-centos ~]# /opt/lampp/xampp restartRestarting XAMPP for Linux 5.6.40-1...XAMPP: Stopping Apache...ok.XAMPP: Stopping MySQL...ok.XAMPP: Stopping ProFTPD...ok.XAMPP: Starting Apache...ok.XAMPP: Starting MySQL...ok.XAMPP: Starting ProFTPD...ok. 访问svnmanager在浏览器输入IP地址/svnmanager/ 访问如下： 登录svnmanagersvnmanager安装后默认使用admin用户登录，密码也是admin，这个可以在 /opt/lampp/htdocs/svnmanager/config.php 文件中配置，当我们添加新用户后，admin这个自动废弃。 添加新用户的步骤：『admin登录』-&gt; 『User Admin』 -&gt; 『Add a new user to the repository system』 -&gt; 『输入新用户信息和自己的密码』 -&gt; 『Confirm』 添加完新用户就可以使用新用户登录了，还可以创建新的仓库并给用户授予权限 创建新仓库的步骤：『Repository Admin』 -&gt; 『Create a new Repository』 -&gt; 『输入新库名和描述』 -&gt; 『Confirm』 给用户授予访问仓库的读写权限步骤：『Repository Admin』 -&gt; 『Change User Privileges of a Repository』 -&gt; 『Repository Name select』 -&gt; 『选择用户、路径，勾选读写权限』 -&gt; 『Confirm』 此外还有一些svnmanager操作可以查询官方文档，一般会把所有用户分成组管理，并把各个库的权限授予不同的组，这个实践一下就清楚了。 可能遇到的错误如果你遇到了以下错误，一定是配置访问MySQL服务的用户或权限不匹配了，检查纠正过来就好 1234567891011121314151617Fatal Error[2] mysqli_real_connect(): (HY000/1044): Access denied for user &apos;svnmanageruser&apos;@&apos;localhost&apos; to database &apos;svnmanager&apos; (@line 86 in file /opt/lampp/htdocs/svnmanager/prado-2.0.3/framework/Data/adodb/drivers/adodb-mysqli.inc.php).Debug Backtrace#1 -- pradoErrorHandler(...)#2 adodb-mysqli.inc.php:86 -- mysqli_real_connect(...)#3 adodb.inc.php:416 -- ADODB_mysqli-&gt;_connect(...)#4 adodb.inc.php:3713 -- ADOConnection-&gt;Connect(...)#5 TAdodb.php:318 -- ADONewConnection(...)#6 TAdodb.php:113 -- TAdodb-&gt;open()#7 DataModule.php:31 -- TAdodb-&gt;__call(...)#8 DataModule.php:31 -- TAdodb-&gt;MetaTables()#9 TApplication.php:584 -- DataModule-&gt;onLoad(...)#10 TApplication.php:629 -- TApplication-&gt;loadModule(...)#11 TApplication.php:482 -- TApplication-&gt;loadPage(...)#12 index.php:5 -- TApplication-&gt;run() 查看服务器目录信息查看一下服务器目录下信息，之前我们建立了三个空文件夹 config、repos、trash，现在已经有很多数据了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@VM-0-3-centos export]# tree svnrepossvnrepos|-- config| |-- auth| | |-- svn.simple| | |-- svn.ssl.client-passphrase| | |-- svn.ssl.server| | `-- svn.username| |-- config| |-- README.txt| |-- servers| |-- svn_access_file| `-- svn_passwd_file|-- repos| `-- firstrepos| |-- conf| | |-- authz| | |-- hooks-env.tmpl| | |-- passwd| | `-- svnserve.conf| |-- db| | |-- current| | |-- format| | |-- fsfs.conf| | |-- fs-type| | |-- min-unpacked-rev| | |-- revprops| | |-- revs| | |-- transactions| | |-- txn-current| | |-- txn-current-lock| | |-- txn-protorevs| | |-- uuid| | `-- write-lock| |-- format| |-- hooks| | |-- post-commit.tmpl| | |-- post-lock.tmpl| | |-- post-revprop-change.tmpl| | |-- post-unlock.tmpl| | |-- pre-commit.tmpl| | |-- pre-lock.tmpl| | |-- pre-revprop-change.tmpl| | |-- pre-unlock.tmpl| | `-- start-commit.tmpl| |-- locks| | |-- db.lock| | `-- db-logs.lock| `-- README.txt`-- trash |-- testrepos1-2022-09-17T10:51:04+02:00 | |-- conf | | |-- authz | | |-- hooks-env.tmpl | | |-- passwd | | `-- svnserve.conf | |-- db | | |-- current | | |-- format | | |-- fsfs.conf | | |-- fs-type ... ... config 目录下增加了管理权限的文件 svn_access_file 和用户密码的文件 svn_passwd_file，repos目录下新增了刚刚新增加的库 firstrepos，trash 目录下是我刚刚删除的无用的仓库。 检出新增加的svn库使用subversion客户端填写 https://IP/svn/firstrepos 地址可以下载我们最新的库 firstrepos，需要输入刚刚授予了权限的用户和密码。 注意事项各个组件的配置文件 The main XAMPP configuration files are located as follows:Apache configuration file: \xampp\apache\conf\httpd.conf, \xampp\apache\conf\extra\httpd-xampp.confPHP configuration file: \xampp\php\php.iniMySQL configuration file: \xampp\mysql\bin\my.iniFileZilla Server configuration file: \xampp\FileZillaFTP\FileZilla Server.xmlApache Tomcat configuration file: \xampp\tomcat\conf\server.xmlApache Tomcat configuration file: \xampp\sendmail\sendmail.iniMercury Mail configuration file: \xampp\MercuryMail\MERCURY.INI 配置svn服务过程可能修改的文件123/opt/lampp/etc/httpd.conf/opt/lampp/htdocs/svnmanager/config.php/opt/lampp/etc/extra/httpd-ssl.conf 这配置文件 /opt/lampp/etc/httpd.conf 中还可以通过 LimitRequestBody、LimitXMLRequestBody 等参数限制访问和提交的数据量 配置服务器时间如果发现svn提交记录时间不匹配，一般是因为svn服务器时间错误导致的，可以使用 date 命令修改，也可以配置一个 ntp 服务器来解决 ntp update -s xxx.xxx.xxx.xx 指定ntp服务的IP或域名，默认使用UDP 123端口ntpq -p 或 ntpstat 可以查看ntp同步状态 备份svn库在迁移之前我们讨论了很多种方案，其中也包括一些备份方式，其实最简单的备份就是拷贝，所以我们可以使用 rsync 命令通过网络备份，把 svn 库的主目录 svnrepos 和 /opt/lampp 同步到备份机器，如果svn服务器出现了问题，可以立即用备份机提供服务。 主动生成密码文件/opt/lampp/bin/htpasswd -c /export/svnrepos/config/svn_passwd_file test 命令可以创建svn_passwd_file文件，并添加第一个用户test，执行命令后会提示输入两次新密码，切记不要在已经存在用户的svn仓库中使用，否则会被覆盖 其他问题如果搭建好的svnmanager可以创建用户和仓库，但是无法授予权限，网页错误中包含 libserf，可以通过 yum install libserf 命令解决。 如果添加好用户授予权限后，无法通过svn客户端更新，可以用检查 /export/svnrepos/config/svn_passwd_file 和 /export/svnrepos/config/svn_access_file 文件归属是否正确为 daemon，如果为文件归属 root，svnmanager 无法修改文件内容，也就会导致用户权限分配不正确 如果想限制访问svn的IP，可以启用 firewalld 服务或者安装使用 iptables 服务 总结 XMAPP 可以方便的提供WEB服务，可以把网站放到 /opt/lampp/htdocs 目录下 可以利用 XAMPP + subversion + svnmanager 的组合提供SVN服务，方便的创建仓库和管理各目录权限 CentOS 安装具体的环境前，可以安装一些通用的软件，yum install -y epel-release， EPEL全称Extra Packages for Enterprise Linux，由 Fedora 社区打造，为 RHEL 及衍生发行版 CentOS 等提供高质量软件包的项目 systemctl 是一个常用的服务命令，比如关闭 firewalld 服务 systemctl stop firewalld，查询状态 systemctl status iptables ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 岁月没有在某些人的脸上留下痕迹，但却对我特别照顾，好像在我这里过的特别匆忙，认真想想自己确实对保留青春阳光付出甚少，甚至连洗脸的时间都一再压缩，也难怪它还给我这些沧桑，今后多花点时间注意一些吧，你认真的注视着它，它便不会轻易溜走~ 2022-9-17 20:47:42]]></content>
      <categories>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>SVN</tag>
        <tag>XAMPP</tag>
        <tag>svnmanager</tag>
        <tag>ntp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于竞赛，CSDN还有很长的路要走]]></title>
    <url>%2Fblog%2F2022%2F09%2F09%2F%E5%85%B3%E4%BA%8E%E7%AB%9E%E8%B5%9B%EF%BC%8CCSDN%E8%BF%98%E6%9C%89%E5%BE%88%E9%95%BF%E7%9A%84%E8%B7%AF%E8%A6%81%E8%B5%B0%2F</url>
    <content type="text"><![CDATA[前言 CSDN 编程竞赛·第五期总结AlbertS 41 52.5 52m36s 竞赛勋章 虽说这篇总结写在第五期之后，但是我却是从第三期就开始参加了的，前两期没有注意到就错过了，好在最近这几期比赛成绩很喜人，第三期6分，第四期67.5，第五期52.5，我承认我有点水，可是我也是能在力扣竞赛里常年水两题的选手啊，这个6分（满分100）真的惊到我了，从最近几期的参赛体验来看，CSDN在竞赛这个方向上还有很长的路要走。 第三期其实我也不算竞赛小白啦，毕竟之前在力扣参加过很多次了，因为平时总在CSDN上写总结，所以看到这里也有竞赛就想来试试，结果第一次有点手足无措，开头居然是4道Java和前端的选择题，我可是C++后端搬运工，嗯，我承认确实不会，根据经验选一选总能对一道题吧，既然选择题是特定语言的，那我直接写大题吧，一段操作猛如虎，一看分数2.5，当然分数不是立马出来的，我感觉答得还可以，结果过了几天出成绩是6分~ 吐槽开始： 要求关闭聊天软件，应该是防止作弊，关就关吧，这倒也无所谓 不能切屏，必须一直保持在答题状态，这有点严格了吧，万一这会我有事要处理呢，其实通常就是处理一些聊天信息 不能复制粘贴，你说不能从其他地方复制答案我可以理解，但是复制自己的代码也不行是要闹哪样 我知道为啥不能粘贴自己刚写的代码了，因为判断不出来是从哪里复制的，可是已经限制切屏了呀 测试用例简陋，有时看不太懂题全靠用例，好嘛，这里看不懂题就算了，用例可有可无 验证流程麻烦，好几个关闭按钮切来切去的 主函数不用给我提供，我又不能改，写在那有啥用 头文件要自己加，这是考算法来还是考背诵呢？（我确实被惯得不想写包含头文件了） 第四期这期已经取消了选择题，直接是4道编程题，还是这样答起来顺利一点，各种语言都行，哪个顺手就用哪个了 吐槽开始： 因为取消了选择题，一时间竟找不到槽点了(#^.^#)，必须编一条 测试用例依旧简单的可怜 第五期我知道为啥第四期表现好了，因为在憋大招对付第五期，因为早饭吃的迟了，晚来了半小时参赛，结果发现进不去，正想报个bug发现帖子里都说进不去，这可有意思了，总共就一两千人参加，应该不是服务器性能不行吧，肯定是页面bug了 虽然大部分人都进不去，但还是有个别人进去比赛了的，已经出了成绩，20分暂居第一名，如果能保持到比赛结束就开心了，官方一直在回帖说解决问题，反正到比赛结束我也没进去。 一直到下午的1点之后，距离开始已经过去了5个小时，我终于进去了，告诉我比赛结束，分数是0，居然不能答题了，赶紧发帖找客服处理，好在工作人员及时赶到，帮我重置了一下，可以正常进入了。 吐槽开始： 千人左右的比赛服务器就炸了 出现问题后修复过程缓慢 发帖提示我违规，但是不说哪里有问题，我也不知道哪句话说错了 通过BUG交流，沟通效率低下 优势与不足前面吐槽了那么多，接下来说说优点吧： 答题参与时间相对自由，仅限制了答题时长2小时候，不严格限制开始时间 分数按照通过的用例比例来给，没有采用非1即0的赋分方式，照顾了很多缺乏竞赛经验的选手，但是这样做未必是正确的 参加基本上都有奖品可以拿，调动了选手们的积极性，礼不在重，有就行 出现问题后尽力解决，虽然解决的慢了点，但是让人看到了负责人的一面 缺点前面已经说了那么多，就不再赘述了，重点说一下答题环境需要改进，给出一些建议吧，其实编程竞赛已经不是新鲜事物了，有那么多好的平台可以学，比如力扣，每次的比赛都很多人参加的，把好的地方学过来就行： 删除无关代码，只保留必要的输入参数就行了，把main函数给我也没用，又不允许本地调试 限制切屏这条尽力优化，限制复制自己代码这条必须去掉，我调整一下逻辑还要重新敲一遍体验未免太差了 比赛之后可以查看别人提交的代码，用于学习提升 赛后有针对这些题的讨论渠道，可以了解到自己有哪些不足，知道什么样的用例通不过 写包含头文件的这件事不太重要，重点放在逻辑实现上，现在谁还不是面向搜索引擎编程了，只要找到方法，头文件算什么难事 总结 继续参加这个比赛，督促自己进步，同时也促进平台进步 如果自己不知道怎么做，就照着表现好的学，前面有那么多优秀的产品可以借鉴 产品上线还是要充分测试，像服务器炸了这种事最好不要出现了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 『雄关漫道真如铁 而今迈步从头越』豪气如虹，真的很有才~ 2022-9-10 11:50:37]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>竞赛</tag>
        <tag>CNDN</tag>
        <tag>活动</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单总结下近期遇到的网络概念gcp、anycast IP、vlan]]></title>
    <url>%2Fblog%2F2022%2F09%2F04%2F%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%E4%B8%8B%E8%BF%91%E6%9C%9F%E9%81%87%E5%88%B0%E7%9A%84%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5gcp%E3%80%81anycast-IP%E3%80%81vlan%2F</url>
    <content type="text"><![CDATA[前言 你知道得越多，就越会意识到自己知道得多么少 知识这个东西总是越学越越多，特别是你去接触一个新领域的时候，会忽然发现之前的自己是多么的狂妄，多么的无知，最近有一些网络相关的需求，这沟通过程中接触到了一些新的概念，比如gcp、anycast IP、vlan等等，初听这些词确实不太理解，所以查了一些资料，对了解到的内容进行一个简单的总结。 gcpGCP 全称 Google Cloud Platform，即谷歌云平台，再这样一个万物皆可云的发展趋势下，云服务发展突飞猛进，作为海外乃至全球的科技巨头，谷歌在云服务领域有着极强的优势，特别是在海外环境下被广泛使用，为计算、存储、网络、大数据、机器学习和物联网（IoT）以及云管理、安全和开发人员工具提供服务，云计算产品包括： Google Compute Engine，这是一种基础架构即服务 (IaaS) 产品，可为用户提供用于工作负载托管的虚拟机实例。 Google App Engine，这是一种平台即服务 (PaaS) 产品，可让软件开发人员访问 Google 的可扩展托管。开发人员还可以使用软件开发工具包 (SDK) 来开发在 App Engine 上运行的软件产品 Google Cloud Storage，这是一个云存储平台，旨在存储大型非结构化数据集。Google 还提供数据库存储选项，包括用于NoSQL 非关系存储的Cloud Datastore 、用于MySQL 完全关系存储的Cloud SQL 和 Google 的原生 Cloud Bigtable 数据库。 Google Container Engine，它是运行在 Google 公共云中的Docker容器的管理和编排系统，是基于 Google Kubernetes 容器编排引擎。 Google Cloud Pub/Sub 是一种托管的实时消息传递服务，允许在应用程序之间交换消息，是一种应用程序开发和集成服务。 Google Cloud Endpoints 允许开发人员创建基于RESTful API 的服务，然后让 Apple iOS、Android 和 JavaScript 客户端可以访问这些服务。 其他产品包括任播 DNS 服务器、直接网络互连、 负载平衡、监控和日志服务。 Anycast任播(Anycast)，又称为选播、泛播或任意播，是IPv6中定义的一种新型通信服务，是IPv6中三大通信方式之一，该定义最早由C Partridge在RFC 1546中首次提出，但是RFC1546对任播的描述仅仅是一个实验性的服务。 Anycast指IPV6协议中一个发送方同最近的一组接收方之间的通信，当一个单播地址被分配到多于一个的接口上时，发到该接口的报文被网络路由到由路由协议度量的“最近”的目标接口上。 与Unicast和Multicast类似，Anycast也是IP网络的一种通信模式。 Multicast指多播，它是指网络中一个节点发出的信息被多个节点收到。与此相对的有单播Unicast和广播Broadcast，前者是指一个节点发出的信息只被一个节点收到，后者是指一个节点发出的信息被子网内所有节点收到 Anycast 允许源结点向一组目标结点中的一个结点发送数据报，而这个结点由路由系统选择，对源结点透明；同时，路由系统就近选择结点为源结点提供服务，从而在一定程度上为源结点提供了更好的服务也减轻了网络负载。 实现上分布的服务共享相同的IP地址，同时在IP层进行透明的服务定位，这使得各种网络服务特别是应用层服务具有更强的透明性，而路由系统选择了“最近”的服务，缩短了服务响应的时间，同时减轻了网络负载，同时在网络上冗余分布相同的服务，路由系统可以提供机制选择负载相对轻的带宽相对高的路径来转发报文，这样就给用户带来了两个方面的好处： 减弱了分布式拒绝服务攻击（DDOS） 减弱了网络拥塞给用户带来的影响 Anycast IPAnycast IP 是集Multicast（多播IP）和Unicast（单播IP）特性于一身的特殊IP地址类型，在Anycast 这种通信模式下，同时存在多个有效的数据包接收端，但是就某一个特定IP数据包而言，仅有一个接收端主机收到了此数据包。 客户分散在多地又需要就近接入的服务，需要IP不同的多个地域部署机器、配置 DNS 实现负载均衡，十分繁琐。使用 Anycast 的 IP 后，无需每个地域都配置 IP，后端维护一套逻辑即可，各地域请求直接用专线加速到后端机器。 Anycast 的 IP 能起到游戏加速器的作用，游戏请求就近进入云服务，走云服务的内网专线到达游戏服务器，极大缩短经过的公网路径，减少了延时、抖动、丢包。此外跟传统加速比，IP 入口无需额外部署流量接收设备，且IP无需区分地域，简化了 DNS 部署。 vlanVLAN 全称 Virtual Local Area Network，即虚拟局域网。 VALN 是一组逻辑上的设备和用户，这些设备和用户并不受物理位置的限制，可以根据功能、部门及应用等因素将它们组织起来，相互之间的通信就好像它们在同一个网段中一样，由此得名虚拟局域网。 VLAN可以为信息业务和子业务、以及信息业务间提供一个相符合业务结构的虚拟网络拓扑架构并实现访问控制功能。与传统的局域网技术相比较，VLAN技术更加灵活，它具有以下优点： 网络设备的移动、添加和修改的管理开销减少；可以控制广播活动；可提高网络的安全性。 当一个交换机上的所有端口中有至少一个端口属于不同网段的时候，当路由器的一个物理端口要连接2个或者以上的网段的时候，就是VLAN发挥作用的时候，这就是VLAN的目的。 VLAN限制网络上的广播，将网络划分为多个VLAN可减少参与广播风暴的设备数量。VLAN分段可以防止广播风暴波及整个网络。VLAN可以提供建立防火墙的机制，防止交换网络的过量广播。 WAN：Wide Area Network，广域网 LAN：Local Area Network，局域网 WLAN：Wireless Local Area Network，无线局域网 总结 GCP 全称 Google Cloud Platform，指的是谷歌云平台 Anycast IP 可以实现多地用户就近接入服务，提升用户体验 VLAN 指虚拟局域网，可以更合理划分网络资源，控制广播活动，提高安全性 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 飞鸟尽，良弓藏；狡兔死，走狗烹。缘何众人都执着登顶？恐被藏与烹而已~ 2022-9-4 01:49:39]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>gcp</tag>
        <tag>anycast</tag>
        <tag>vlan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rm -rf 真是删库跑路的一把好手]]></title>
    <url>%2Fblog%2F2022%2F08%2F26%2Frm-rf-%E7%9C%9F%E6%98%AF%E5%88%A0%E5%BA%93%E8%B7%91%E8%B7%AF%E7%9A%84%E4%B8%80%E6%8A%8A%E5%A5%BD%E6%89%8B%2F</url>
    <content type="text"><![CDATA[前言上回书说到《一个月黑风高的夜晚紧急完成gitlab服务器数据迁移》，因为数据迁移后原数据还是存在的，该分区硬盘快满了，进而影响了原目录下的日志存储，既然数据已经迁移到新的路径了，那原来的库直接删掉就好了，往往就是这么不经意间做了一个令人十分后怕的决定。 删库说干就干，连上服务器就开始操作了，为了避免搞错了，我还打开了另一个ssh窗口，对照着正在使用的git库，来一步步查找原来路径下已经废弃的仓库，嗯，终于找到了，对比各种信息没啥问题，两个窗口相互对照，十分“保险”。 rm -rf xxx 走你，一切都安静了，好了退出当前路径检查一下空间大小，咦？路径怎么不对，好像删的是正在使用的那个库哎！服了，还真是受到了惊吓啊！背后发凉啊！gitlab网站访问一下，嗯，果然找不到了，拜拜！ 跑路既然库都删完了，要不跑路吧？ 算了，能跑到哪呢？先回去看看能不能找回来吧~ 恢复rm -rf 恢复硬盘数据是别想了，一般会让你卸载硬盘，断网，防止擦除，用第三方工具等，这我之前都演练过，几乎没什么用，这个时候需要冷静，先理智的分析一下： 既然是git库，我本地也是有的，要不我把我的库推上去试试？虽然没有那么新，但也差不了几个提交了，不过远程库都被我删了，我如果推上去一个新库，别人是不是直接访问不了，或者引发冲突呢？ 想起之前迁移的时候我还备份了数据目录呢，那这样，先把备份的数据恢复到误删除的目录下，然后我再找一个本地的拉取到了最新状态git库推上去，既然想清楚了，那就动手吧。 通知相关人员先不要拉取和推送数据 把一月前备份的git-data目录中对应数据通过 rsync 命令拷贝到误删除目录，这时通过gitlab网站已经能看到数据了，只是数据是一个月前的 跳到版本发布机，上面的Git库数据是最新的，按照分支把版本发布机上的git数据逐个推送到gitlab服务器 再次打开gitlab网站发现一切恢复如初，真是…… 感想rm -rf 命令真是删库跑路的一把好手，一点也不拖泥带水，更无回收站这个后悔药可以吃，所以在服务器上对文件使用了这个命令，基本上等于判了死刑，但是git库真是一个好东西，分布式的存储可以保证每个人那都有完整的仓库，只要能找到一个最新的就行。 为了保证我能有最新的库可以用，我赶紧在 jenkins 上新建了两个定时任务，每天定时把仓库拉取到最新，防止类似意外的发生。 后续其实这个后续和删库这件事没有任何关系，如果非得说有什么关系，就是它们都属于“灾难”，删库刚刚处理完，紧接着游戏玩家出现登录不上的问题，一开始以为是网络波动，因为我登录过程也不太顺畅，直到玩家发来了录屏，我才发现这个问题又有的查了。 玩家所说的无法登录并不是真的登不进去，而是登录之后加载完读条刚要进场景，直接退到登录界面，查询网络消息发现每次登录后几秒钟，网络连接自动断开，但是断开前的通讯流程日志显示的延迟信息，又说明网络状况良好，一头雾水。 最后耗时两天，在收集了各种线索以后，发现是升级Unity版本后，在法语、俄语、乌克兰语作为系统语言时，对c#的字符串处理逻辑要求更加严格，如果不做处理沿用之前的写法，很容易出现崩溃错误，因为有try-catch处理，表现出来就是直接断网会登录界面，统一设置语言处理函数时修复了此问题。 身心俱疲~ 总结 使用 rm -rf 命令还是要谨慎，谨慎，再谨慎 如果真的删库了，也不一定非得跑路，先冷静想想有没有补救的措施 语言、字符集、编码真的是相互纠结，至此我的bug库里又收录了系统语言运行时，神奇 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 北风卷地白草折，胡天八月即飞雪~ 2022-8-26 23:41:18]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>Git</tag>
        <tag>rm</tag>
        <tag>删除</tag>
        <tag>恢复</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git查看历史记录及修改内容]]></title>
    <url>%2Fblog%2F2022%2F08%2F21%2Fgit%E6%9F%A5%E7%9C%8B%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95%E5%8F%8A%E4%BF%AE%E6%94%B9%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[前言在 Git 中没有真正的方法来做任何事情，这就是它的妙处！比如查看修改内容这件事，有些人会想到 git log，有些人会想到 git show，最近我又学到一个 git whatchanged，实现目的方法多种多样，各种途径任君挑选。 刚开始步入软件开发行业时喜欢捣鼓各种软件，进行各种个性化配置，任意修改快捷键，这样在开发过程中确实会舒服一些，但是换了一个环境时（电脑重装或在别人的机器），就好像一个什么也不会的傻子一样，所以慢慢的我开始强迫自己熟悉软件自己的快捷键和各种命令，这样在重装系统时会免去很多麻烦，并且因为一些命令用习惯了，在编写部署脚本时也不会总是写出不能识别的简写命令了。 修改文件为了测试各种的查找修改记录的命令，我先进行一次包含增加、修改、删除的提交，然后对比来看各个命令的作用，实际修改如下： 在文件address.txt中增加两行数据 清空文件age.txt中4行数据 修改文件name.txt中一行数据 增加带有3行数据的phone.txt文件 删除带有2行数据的story.txt文件 在执行了 git add . 命令后，可以用 git diff --staged 查看即将提交的文件修改，展示如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849$ git diff --stageddiff --git a/address.txt b/address.txtindex e69de29..8f9d6e6 100644--- a/address.txt+++ b/address.txt@@ -0,0 +1,2 @@+beijing+shanghai\ No newline at end of filediff --git a/age.txt b/age.txtindex 58f78c9..e69de29 100644--- a/age.txt+++ b/age.txt@@ -1,4 +0,0 @@-12-16-17-15\ No newline at end of filediff --git a/name.txt b/name.txtindex ac37a53..0ed306a 100644--- a/name.txt+++ b/name.txt@@ -1,4 +1,4 @@ tom-alice+jerry bily andy\ No newline at end of filediff --git a/phone.txt b/phone.txtnew file mode 100644index 0000000..241bcdb--- /dev/null+++ b/phone.txt@@ -0,0 +1,3 @@+110+120+119\ No newline at end of filediff --git a/story.txt b/story.txtdeleted file mode 100644index 0d89902..0000000--- a/story.txt+++ /dev/null@@ -1,2 +0,0 @@-King-Wolf\ No newline at end of file 关于 git diff 对于很多使用 git 管理代码的小伙伴再熟悉不过了，但是其中有些细节还是需要学习的： diff --git a/name.txt b/name.txt 这一行是说以下展示 name.txt 文件修改前后的信息，a/name.txt 和 b/name.txt 分别表示修改前后的文件名 index ac37a53..0ed306a 100644 这一行表示文件修改前后的 object，100644 表示这是一个常规文件，文件权限为 644，使用 git cat-file -p可以查看文件内容： 1234567891011$ git cat-file -p ac37a53tomalicebilyandy$ git cat-file -p 0ed306atomjerrybilyandy a/name.txt``` 表示修改前的文件，```+++ b/name.txt``` 表示修改后的文件12345678910114. `@@ -1,4 +1,4 @@` 这一句应该是最有意思的，也是不用一看懂的一行，其中开头和结尾的 `@@` 为固定格式，`-1,4` 表示修改前的 `1~4`行，`+1~4`表示修改后的`1~4`行，这句话的意思就是，以下内容展示了修改前 `1~4`行到修改后 `1~4`行的文件变化，因为我们只修改了一行，所以修改前后行数不变，如果新增行数和删除行数不同，那么这个位置展示的行数也是不同的，例如 `phone.txt` 文件的变化 `@@ -0,0 +1,3 @@`5. 最后就是文件具体的变化了，新增内容前面是加号 `+`，删除内容前面是减号 `-`： ```bash tom -alice +jerry bily andy 查询文件修改当我们把修改的内容提交以后，这条修改属于存入了仓库的历史之中，git diff 就无法再查看文件的变化了，而需要使用 git log 或 git show 来查看文件修改的内容，接下来我们来看看这些命令都能用来干嘛。 git showgit show 默认展示最近一次提交的修改，与执行 git commit 命令之前的 git diff --staged 查看得到的绝大部分内容相同，只是在开头位置包含最新提交的信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455$ git showcommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example datadiff --git a/address.txt b/address.txtindex e69de29..8f9d6e6 100644--- a/address.txt+++ b/address.txt@@ -0,0 +1,2 @@+beijing+shanghai\ No newline at end of filediff --git a/age.txt b/age.txtindex 58f78c9..e69de29 100644--- a/age.txt+++ b/age.txt@@ -1,4 +0,0 @@-12-16-17-15\ No newline at end of filediff --git a/name.txt b/name.txtindex ac37a53..0ed306a 100644--- a/name.txt+++ b/name.txt@@ -1,4 +1,4 @@ tom-alice+jerry bily andy\ No newline at end of filediff --git a/phone.txt b/phone.txtnew file mode 100644index 0000000..241bcdb--- /dev/null+++ b/phone.txt@@ -0,0 +1,3 @@+110+120+119\ No newline at end of filediff --git a/story.txt b/story.txtdeleted file mode 100644index 0d89902..0000000--- a/story.txt+++ /dev/null@@ -1,2 +0,0 @@-King-Wolf\ No newline at end of file git show 还可以加数字，比如 git show -3 就是展示最近3次提交修改信息。 git show --stat 可以查看最新提交的修改文件，如果想查看指定提交的修改文件信息，可以在后面跟上commit-id，例如 git show --stat 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee 12345678910111213$ git show --statcommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data address.txt | 2 ++ age.txt | 4 ---- name.txt | 2 +- phone.txt | 3 +++ story.txt | 2 -- 5 files changed, 6 insertions(+), 7 deletions(-) 这个展示信息可以清楚的看出哪些文件新增了内容，哪些文件删除了内容，并且展示了文件增伤行数的比例，但是有一点它不够清晰，那就是无法看出哪些是新增的文件，哪些是删除的文件，比如 age.txt 和 story.txt 都显示删除了数据，但实际上 story.txt 整个文件都从仓库中删除了，要想看出文件增删状态可以使用接下来展示的这个命令 git whatchanged git whatchangedgit whatchanged 可以展示出文件的增删状态和权限修改，默认分页展示所有提交记录，可以后面加数字来展示最近几次的文件增删状态： 123456789101112$ git whatchanged -1commit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data:100644 100644 e69de29 8f9d6e6 M address.txt:100644 100644 58f78c9 e69de29 M age.txt:100644 100644 ac37a53 0ed306a M name.txt:000000 100644 0000000 241bcdb A phone.txt:100644 000000 0d89902 0000000 D story.txt 从这个文件中就可以看出 address.txt、age.txt、name.txt 三个文件被修改了，phone.txt 是新增加的，story.txt 文件被删除了，如果在命令后面加上选项 --stat 作用就和 git show 一样了，兜兜转转回到原点~ 12345678910111213$ git whatchanged -1 --statcommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data address.txt | 2 ++ age.txt | 4 ---- name.txt | 2 +- phone.txt | 3 +++ story.txt | 2 -- 5 files changed, 6 insertions(+), 7 deletions(-) git log根据我个人的理解，git show 注重查看一次提交中修改的内容，而 git log 主要用于查找历史提交的脉络，但这不是绝对的，因为git做一件事，没有绝对的一种方式，你也可以用 git log 来实现 git show。 展示最近一次提交 123456$ git log -1commit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data 展示其他分支最近一次提交 123456$ git log -1 devcommit 62cc52cbc7f9581fa825b443aba3481083459656 (dev)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 12:02:11 2022 +0800 init git repository 展示修改的文件列表及文件修改的统计 12345678910111213$ git log -1 --statcommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data address.txt | 2 ++ age.txt | 4 ---- name.txt | 2 +- phone.txt | 3 +++ story.txt | 2 -- 5 files changed, 6 insertions(+), 7 deletions(-) 展示每次修改的文件列表 123456789101112$ git log -1 --name-onlycommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example dataaddress.txtage.txtname.txtphone.txtstory.txt 展示修改的文件列表和显示状态 123456789101112$ git log -1 --name-statuscommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example dataM address.txtM age.txtM name.txtA phone.txtD story.txt 展示指定作者提交的记录 123456$ git log -1 --author="albert"commit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data 单行展示提交的记录 123$ git log --oneline2447e2b (HEAD -&gt; master) update example data62cc52c (dev) init git repository 展示指定日期之前的提交记录 123456789101112$ git log --before='2022-08-22'commit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example datacommit 62cc52cbc7f9581fa825b443aba3481083459656 (dev)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 12:02:11 2022 +0800 init git repository 展示一天之内的提交记录 123456789101112$ git log --since=1.day.agocommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example datacommit 62cc52cbc7f9581fa825b443aba3481083459656 (dev)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 12:02:11 2022 +0800 init git repository 展示指定包含指定内容的提交记录 123456$ git log --grep=updatecommit 2447e2b9c15472f2ead7bf451aa5fc9c3f34f5ee (HEAD -&gt; master)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 15:22:08 2022 +0800 update example data 展示指定不包含指定内容的提交记录 123456$ git log --grep=update --invert-grepcommit 62cc52cbc7f9581fa825b443aba3481083459656 (dev)Author: albert &lt;albert101@163.com&gt;Date: Sun Aug 21 12:02:11 2022 +0800 init git repository 终极大招，图形化展示，其实用的并不多 12345678$ git log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --date=relative --all* 2447e2b - (HEAD -&gt; master) update example data (77 minutes ago) &lt;albert&gt;* 62cc52c - (dev) init git repository (2 hours ago) &lt;albert&gt;* 719ec7a - (refs/stash) WIP on master: 83f00c5 init git repository (2 hours ago) &lt;albert&gt;|\| * 9c87e06 - index on master: 83f00c5 init git repository (2 hours ago) &lt;albert&gt;|/* 83f00c5 - init git repository (5 hours ago) &lt;albert&gt; 总结 查看最近一次修改的内容使用 git show 查看最近一次修改的文件使用 git show --stat 或 git log -1 --name-only 查看最近一次修改的文件状态使用 git whatchanged -1 或 git log -1 --name-status 图形化显示git提交记录使用 git log --graph ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 轻生的人到底是勇敢还是懦弱呢？虽说我未经历过他的人生不该轻易评判，但就我主观来思考这类事，大抵是因为对生活失去了信心。人固有一死，或重于泰山，或轻于鸿毛。而遇到困难就选择轻生的人是自私且不负责任的，若你无牵无挂走了也就走了，倘若还有家人岂不是还要拿出额外一笔钱给你办个葬礼？连死都不怕了，还有什么是过不去的呢？ 2022-8-21 21:19:13 &lt;!–我曾走过山，走过水，其实只是借助它们走过我的生命；我看着天，看着地，其实只是借助它们确定我的位置；我爱着他，爱着你，其实只是借助别人实现了我的爱欲。 ——史铁生《务虚笔记》– &gt;]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>log</tag>
        <tag>版本控制</tag>
        <tag>Git</tag>
        <tag>历史记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参考开源项目实现一个简易的C++枚举转字符串的函数]]></title>
    <url>%2Fblog%2F2022%2F08%2F07%2F%E5%8F%82%E8%80%83%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%9A%84C-%E6%9E%9A%E4%B8%BE%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言前段时间接触了 magic_enum 这个开源库，代码量不算太多，是一个但头文件的枚举操作库，关于如何使用还写了一篇总结 《推荐一个C++枚举转字符串的开源项目magic_enum》，当时觉得这个库很棒，但是对于我当前枚举转化字符串的需求还说还是太臃肿了，所以决定改造一下，这不今天过来填坑了。 改造一开始还没太理解开源库的原理，认为原来的实现限制太大，为了实现后面字符串转枚举，获取所有枚举名等需求，不得不限定一个枚举的范围，这个范围在 magic_enum 这个开源库中是 [-128, 128]，所以当我开始改造时打算把这个范围去掉，但是当我真正弄懂它的原理后，才发现这个范围是必须指定的，不然无法在编译期预处理，从而达到枚举值转换成字符串的目的。 认识到这一点以后，我也不再纠结范围的限制，设定了一个 [0, 31] 的常用枚举范围，相比于原来 [-128, 128] 的范围缩小了不少，这样能加快编译的速度，参考这个开源库和一些网络上关于这个库的讲解，我也实现了一个功能单一的简洁的枚举转字符串的函数 Enum2String，大约70行代码，使用起来还是比较方便的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;array&gt;#include &lt;string&gt;#include &lt;utility&gt;#include &lt;string_view&gt;template &lt;typename E, E V&gt;constexpr auto PrettyName()&#123; std::string_view name&#123;__PRETTY_FUNCTION__, sizeof(__PRETTY_FUNCTION__) - 2&#125;; name.remove_prefix(name.find_last_of(" ") + 1); if (name.front() == '(') name.remove_prefix(name.size()); return name;&#125;template &lt;typename E, E V&gt;constexpr bool IsValidEnum()&#123; return !PrettyName&lt;E, V&gt;().empty();&#125;template &lt;int... Seq&gt;constexpr auto MakeIntegerSequence(std::integer_sequence&lt;int, Seq...&gt;)&#123; return std::integer_sequence&lt;int, (Seq)...&gt;();&#125;constexpr auto NormalIntegerSequence = MakeIntegerSequence(std::make_integer_sequence&lt;int, 32&gt;());template &lt;typename E, int... Seq&gt;constexpr size_t GetEnumSize(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::array&lt;bool, sizeof...(Seq)&gt; valid&#123;IsValidEnum&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; constexpr std::size_t count = [](decltype((valid)) v) constexpr noexcept-&gt;std::size_t &#123; auto cnt = std::size_t&#123;0&#125;; for (auto b : v) if (b) ++cnt; return cnt; &#125;(valid); return count;&#125;template &lt;typename E, int... Seq&gt;constexpr auto GetAllValidValues(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::size_t count = sizeof...(Seq); constexpr std::array&lt;bool, count&gt; valid&#123;IsValidEnum&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; constexpr std::array&lt;int, count&gt; seq&#123;Seq...&#125;; std::array&lt;int, GetEnumSize&lt;E&gt;(NormalIntegerSequence)&gt; values&#123;&#125;; for (std::size_t i = 0, v = 0; i &lt; count; ++i) if (valid[i]) values[v++] = seq[i]; return values;&#125;template &lt;typename E, int... Seq&gt;constexpr auto GetAllValidNames(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::array&lt;std::string_view, sizeof...(Seq)&gt; names&#123;PrettyName&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; std::array&lt;std::string_view, GetEnumSize&lt;E&gt;(NormalIntegerSequence)&gt; validNames&#123;&#125;; for (std::size_t i = 0, v = 0; i &lt; names.size(); ++i) if (!names[i].empty()) validNames[v++] = names[i]; return validNames;&#125;template &lt;typename E&gt;constexpr std::string_view Enum2String(E V)&#123; constexpr auto names = GetAllValidNames&lt;E&gt;(NormalIntegerSequence); constexpr auto values = GetAllValidValues&lt;E&gt;(NormalIntegerSequence); constexpr auto size = GetEnumSize&lt;E&gt;(NormalIntegerSequence); for (size_t i = 0; i &lt; size; ++i) if (static_cast&lt;int&gt;(V) == values[i]) return names[i]; return std::to_string(static_cast&lt;int&gt;(V));&#125; 函数使用这个Enum2String函数使用也非常方便，直接把枚举变量作为参数传进去就可以了: 1234567891011121314151617#include "myenum.h"#include &lt;iostream&gt;enum class Color&#123; RED, GREEN, BLUE,&#125;;int main()&#123; Color c = Color::BLUE; std::cout &lt;&lt; Enum2String(c) &lt;&lt; std::endl; return 0;&#125; 编译运行后的结果为： 12$ g++ enumtest.cpp -std=c++17 &amp;&amp; ./a.outColor::BLUE 各函数的作用前面提到过，我这个库还是参考 magic_enum 这个开源库的源码及网上对它的讲解来实现的，只不过精简了大部分我用不到的内容，仅实现了我想要的枚举转字符串的功能，并且大部分都在编译器求值，仅 Enum2String 函数中遍历的部分只能在运行时才能计算求得，所以效率还算不错，各个模板函数作用明确，下面简单描述下： 12345678template &lt;typename E, E V&gt;constexpr auto PrettyName()&#123; std::string_view name&#123;__PRETTY_FUNCTION__, sizeof(__PRETTY_FUNCTION__) - 2&#125;; name.remove_prefix(name.find_last_of(" ") + 1); if (name.front() == '(') name.remove_prefix(name.size()); return name;&#125; PrettyName() 函数是利用 __PRETTY_FUNCTION__ 这个宏来截取最终我们想要的字符串，如果不做处理，__PRETTY_FUNCTION__ 的值会是这样： constexpr auto PrettyName() [with E = Color; E V = Color::BLUE] 靠近结尾的 Color::BLUE 正是我们想要得到的字符串，所以我们可以按照自己的需要把它截取出来。 12345template &lt;typename E, E V&gt;constexpr bool IsValidEnum()&#123; return !PrettyName&lt;E, V&gt;().empty();&#125; IsValidEnum() 函数是用于判断一个枚举名字是否有效，如果截取的最终名字为空，则认为此枚举无效。 1234567template &lt;int... Seq&gt;constexpr auto MakeIntegerSequence(std::integer_sequence&lt;int, Seq...&gt;)&#123; return std::integer_sequence&lt;int, (Seq)...&gt;();&#125;constexpr auto NormalIntegerSequence = MakeIntegerSequence(std::make_integer_sequence&lt;int, 32&gt;()); MakeIntegerSequence() 用于生成一个范围是 [0, 32) 的整数数列。 123456789101112template &lt;typename E, int... Seq&gt;constexpr size_t GetEnumSize(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::array&lt;bool, sizeof...(Seq)&gt; valid&#123;IsValidEnum&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; constexpr std::size_t count = [](decltype((valid)) v) constexpr noexcept-&gt;std::size_t &#123; auto cnt = std::size_t&#123;0&#125;; for (auto b : v) if (b) ++cnt; return cnt; &#125;(valid); return count;&#125; GetEnumSize() 用于遍历数列范围内的各个整数，找出有效的枚举有多少个。 1234567891011template &lt;typename E, int... Seq&gt;constexpr auto GetAllValidValues(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::size_t count = sizeof...(Seq); constexpr std::array&lt;bool, count&gt; valid&#123;IsValidEnum&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; constexpr std::array&lt;int, count&gt; seq&#123;Seq...&#125;; std::array&lt;int, GetEnumSize&lt;E&gt;(NormalIntegerSequence)&gt; values&#123;&#125;; for (std::size_t i = 0, v = 0; i &lt; count; ++i) if (valid[i]) values[v++] = seq[i]; return values;&#125; GetAllValidValues() 用于遍历数列范围内各个整数，找出全部有效枚举值，返回包含有效值的数组。 123456789template &lt;typename E, int... Seq&gt;constexpr auto GetAllValidNames(std::integer_sequence&lt;int, Seq...&gt;)&#123; constexpr std::array&lt;std::string_view, sizeof...(Seq)&gt; names&#123;PrettyName&lt;E, static_cast&lt;E&gt;(Seq)&gt;()...&#125;; std::array&lt;std::string_view, GetEnumSize&lt;E&gt;(NormalIntegerSequence)&gt; validNames&#123;&#125;; for (std::size_t i = 0, v = 0; i &lt; names.size(); ++i) if (!names[i].empty()) validNames[v++] = names[i]; return validNames;&#125; GetAllValidNames() 用于遍历数列范围内各个整数，找出全部有效枚举值的名字，返回包含这些名字的数组。 12345678910template &lt;typename E&gt;constexpr std::string_view Enum2String(E V)&#123; constexpr auto names = GetAllValidNames&lt;E&gt;(NormalIntegerSequence); constexpr auto values = GetAllValidValues&lt;E&gt;(NormalIntegerSequence); constexpr auto size = GetEnumSize&lt;E&gt;(NormalIntegerSequence); for (size_t i = 0; i &lt; size; ++i) if (static_cast&lt;int&gt;(V) == values[i]) return names[i]; return std::to_string(static_cast&lt;int&gt;(V));&#125; Enum2String() 用于从编译期生成的数组中遍历寻找枚举值等于参数的枚举值名字，如果枚举值无效或者超出范围就范围对应的整数字符串。 总结 magic_enum 是个很不错的库，但他相对于我的需求来说显得太大了 根据自己的需求改造开源库，一方面巩固了知识，另一方面也更适合自己的要求 constexpr 这个东东可以在编译期求值，后面可以多花点时间研究一下 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 原始财富的积累的真的是太难了，那些趁着各种东风各种红利的人们是幸运的，运气也是人生的一部分，而大部分没有这运气的人们想要积累财富，必然要付出十倍甚至上百倍的努力，这些不可选择也无需抱怨，只要踏踏实实往前走就好了~ 2022-8-7 16:19:20]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>字符串</tag>
        <tag>enum</tag>
        <tag>开源项目</tag>
        <tag>枚举</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[std::uniform_real_distribution的一个bug引发的服务器崩溃]]></title>
    <url>%2Fblog%2F2022%2F08%2F06%2Fstd-uniform-real-distribution%E7%9A%84%E4%B8%80%E4%B8%AAbug%E5%BC%95%E5%8F%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B4%A9%E6%BA%83%2F</url>
    <content type="text"><![CDATA[前言近日发生一次线上游戏服务器宕机问题，通过日志和core文件信息定位到崩溃的函数，但是崩溃的位置却是一段很长时间都没有改动过的代码，起初怀疑是配置数据的问题，但仔细查看之后均正常，然后又怀疑是玩家旧数据异常导致，但是分析代码逻辑后也没发现问题，最后是一个同事发现生成随机数的代码有bug，导致数组越界了，还真是个意想不到的原因。 崩溃问题崩溃出现在从数组中随机一个数的逻辑中，其中用到了 std::uniform_real_distribution 这个模板类，示例代码如下： 123456789vector&lt;int&gt; v&#123;1, 3, 5, 6&#125;;std::random_device rd;std::mt19937 gen(rd());std::uniform_real_distribution&lt;float&gt; dis(0, 1.0f);int n = static_cast&lt;int&gt;(dis(gen) * v.size());return v[n]; 之前也了解过 std::uniform_real_distribution&lt;float&gt; dis(0, 1.0f); 这个用法，他应该返回的范围是 [0, 1.0) 内左闭右开的浮点数，所以最终计算出的 n 的值应该为 [0, n-1] 范围内的整数，所以这段代码不应该有问题，但是问题却恰恰出现在 std::uniform_real_distribution 的身上。 std::uniform_real_distribution&lt;&gt; 的bugstd::uniform_real_distribution 这个模板类定义在头文件 &lt;random&gt; 中，是C++11新加的内容，定义如下： 12template&lt; class RealType = double &gt;class uniform_real_distribution; 可产生均匀分布在区间 [a, b) 上的随机浮点值 x。 但是这个函数有个bug，它有时候会返回边界值b，也就是说实际范围变成了 [a, b]。 可以通过 cppreference.com - uniform_real_distribution查到，具体描如下： It is difficult to create a distribution over the closed interval [a,b] from this distribution. Using std::nextafter(b, std::numeric_limits::max()) as the second parameter does not always work due to rounding error. Most existing implementations have a bug where they may occasionally return b (GCC #63176 LLVM #18767 MSVC STL #1074). This was originally only thought to happen when RealType is float and when LWG issue 2524 is present, but it has since been shown that neither is required to trigger the bug. 关于这个bug还可以看一下这个帖子的讨论： Is 1.0 a valid output from std::generate_canonical? 看得时候注意一下这段描述 This problem can also occur with std::uniform_real_distribution; the solution is the same, to specialize the distribution on double and round the result towards negative infinity in float. bug 重现方法这个bug有多种变体，其中一个就是说它和 generate_canonical 产生随机数有关 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;limits&gt;#include &lt;random&gt;int main()&#123; std::mt19937 rng; std::seed_seq sequence&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; rng.seed(sequence); rng.discard(12 * 629143 + 6); float random = std::generate_canonical&lt;float, std::numeric_limits&lt;float&gt;::digits&gt;(rng); if (random == 1.0f) &#123; std::cout &lt;&lt; "Bug!\n"; &#125; return 0;&#125; 此段代码在编译器 g++ 5.4.0 上编译执行时能重现，但是在 g++ 10.0.3 上已经被修复无法重现了，再看下面这段代码： 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;random&gt;int main()&#123; std::random_device rd; std::mt19937 gen(rd()); std::uniform_real_distribution&lt;&gt; dis(0, 1.0f); while (true) &#123; float f = dis(gen); if (f &gt;= 1.0) &#123; std::cout &lt;&lt; "BUG\n"; break; &#125; &#125; return 0;&#125; 这段代码无论是 g++ 5.4.0 版本还是 g++ 10.0.3 都能重现打印出 BUG，这个问题在于模板默认是 double 类型，最后转化成 float 来使用，我按照建议之前帖子中的建议，都改成 double 来使用，之后一直运行了10来天再没出现过随机到边界值的问题。 总结 标准库中的内容很权威，但是不保证一定是正确的，可以持有怀疑态度 std::uniform_real_distribution的历史版本是有bug，几乎各个编译器都出现过随机到边界值的情况 这个bug其实在文档中已经指出了，所以大家看文档时还是要仔细一点，往往使用不规范也容易造成bug ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 适当的放松是生活的调味剂，有时候真的需要肆意挥霍一下，一张一弛，生活之道~ 2022-8-7 01:30:30]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>随机数</tag>
        <tag>uniform_real_distribution</tag>
        <tag>bug</tag>
        <tag>random</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVN如何删除文件名包含空格的文件]]></title>
    <url>%2Fblog%2F2022%2F07%2F31%2FSVN%E5%A6%82%E4%BD%95%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E5%90%8D%E5%8C%85%E5%90%AB%E7%A9%BA%E6%A0%BC%E7%9A%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言SVN 和 Git 两个常用的版本管理工具都有 add 和 commit 命令，但是这两个命令的含义是不同的，git add 命令添加的是变化的文件，不管是增加的文件、删除的文件还是更新的文件，使用 git add 命令之后都可以放到暂存区，而 svn add 命令只能影响新增和变化的文件，对原在库中现已删除的文件没有影响，想要真正删除文件需要在执行 svn commit 命令之前先执行 svn delete 命令。 SVN删除文件SVN 删除文件需要指定文件名字，而使用在一些自动化脚本中时，被删除的文件都是其他脚本自动删除的，要想获得这些文件的名字，可以利用 svn st 命令来查询，然后搭配管道再使用 svn del 命令来删除这些文件。 12345678$ svn st! test/A.txt! test/B.txt! test/C Blank.txt! test/DD.txtM test/E.txt? test/F.txtA test/G.txt svn st 命令的结果对每个变化文件显示一行，第一列是文件状态，第二列及后面的部分是文件名 !：表示已经删除的文件 M：表示文件内容发生变化的文件 ?：表示一个新文件，不在原版本库中 A：表示本次新增的文件 要想删除文件只需要把 ! 开头的文件删除就可以了，常规命令如下： 1svn st | grep ^! | awk '&#123;print " --force "$2&#125;' | xargs -r svn del 这个命令就是要生成 svn del --force test/A.txt 命令然后执行，这样就能达到删除文件的目的了 基于这个目的，一般自动化部署的机器上的svn提交命令如下： 123svn st | grep ^! | awk '&#123;print " --force "$2&#125;' | xargs -r svn delsvn add . --no-ignore --forcesvn commit -m "build message" 被删除的文件名带空格上述命令能处理绝大部分的情况，但是如果被删除的文件中包含空格，那么执行命令时就会报错: 12$ svn st | grep ^! | awk '&#123;print " --force "$2&#125;' | xargs -r svn delsvn: E125001: 'test/C' does not exist 为什么会报错的呢？就是说因为文件 test/C Blank.txt 的名字中包含了空格，使用 awk &#39;{print &quot; --force &quot;$2}&#39; 把空格后半部分的文件名丢掉了，所以只要处理一下这种情况就可以了，处理命令如下： 1svn st | grep ^! | awk '&#123;$1="";print $0 &#125;' | awk '$1=$1' | awk '&#123;print " --force ""\""$0"\""&#125;' | xargs -r svn del svn st 显示个文件状态 grep ^! 过滤出原在版本库中但现已删除的文件 awk &#39;{$1=&quot;&quot;;print $0 }&#39; 去掉每行信息中第一列的状态值 awk &#39;$1=$1&#39; 去掉数据开头和结尾的空格 awk &#39;{print &quot; --force &quot;&quot;\&quot;&quot;$0&quot;\&quot;&quot;}&#39; 拼接文件名，并在前后添加引号，--force &quot;test/C Blank.txt&quot; xargs -r svn del 利用管道传递文件名将文件删除 总结 svn st 命令可以查询 SVN 当前目录下各个文件的状态 svn del 命令用于删除原版本库中的文件 如果待删除的文件名包含空格可使用 svn st | grep ^! | awk &#39;{$1=&quot;&quot;;print $0}&#39; | awk &#39;$1=$1&#39; | awk &#39;{print &quot; --force &quot;&quot;\&quot;&quot;$0&quot;\&quot;&quot;}&#39; | xargs -r svn del ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 互发声明并非只是打打嘴架，应该是在相互试探着对方的底线，各自盘算着多方的利益，双方都是极其聪明的人，背后也都有强大的团队提供支持，博弈啊博弈，每一方都会选择对自己最有利的行动~ 2022-7-31 20:13:21]]></content>
      <categories>
        <category>SVN</category>
      </categories>
      <tags>
        <tag>SVN</tag>
        <tag>del</tag>
        <tag>add</tag>
        <tag>commit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个月黑风高的夜晚紧急完成gitlab服务器数据迁移]]></title>
    <url>%2Fblog%2F2022%2F07%2F30%2F%E4%B8%80%E4%B8%AA%E6%9C%88%E9%BB%91%E9%A3%8E%E9%AB%98%E7%9A%84%E5%A4%9C%E6%99%9A%E7%B4%A7%E6%80%A5%E5%AE%8C%E6%88%90gitlab%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[前言真是无巧不成书，白天刚刚讨论了一天SVN服务器迁移事情，晚上突然被告知 gitlab 服务无法访问了，赶紧连接到服务器上查看，好家伙，磁盘还剩下可怜的 98K，SVN 不管了，先把 gitlab 搞定吧，毕竟第二天的游戏更新包都打不出来了，确认是磁盘不足问题后已经是9点半了，月亮都升起老高了，赶紧开始修复吧。 修复过程关于修复过程，网上有很多文章写了具体的步骤，都非常的相似，基本来自于官方文档的翻译，不过抄来抄去的内容产生了一些错误，甚至是在关键的步骤上出错，所以在看这些文章是要特别注意，如果你的英文还不错的话，建议看一下官方文档 为了记录过程我也不免俗套的把这些步骤写一写，方便日后拿来即用，关于每步的作用我会进行简述，并且提一下注意事项。 总体步骤：在配置文件中把存放数据的路径改一个新目录，把原数据移动到新的目录，重启gitlab服务即可 备份数据虽然只是做数据迁移，不涉及切换物理机，也不会更换服务版本，但是为了保险起见，还是先把数据备份一份，现在遇到的问题是 gitlab 是默认安装的，数据存放在 /var/opt/gitlab/git-data 目录下，仓库存放在子目录 repositories 里面，这个目录一般空间都不大，很容易出现磁盘满的情况，目录结构如下： 123456789101112131415git-data└── repositories ├── +gitaly │ ├── cache │ ├── state │ └── tmp └── @hashed ├── 19 ├── 35 ├── 3f ├── 45 ├── 4a ├── 4b ├── ... └── f5 真正的仓库数据就在 @hashed 目录下众多子目录下，再往下走就是一个个 .git目录了，结构与我们本地的 .git 目录一致。 备份的命令需要记录一下: 1gitlab-rake gitlab:backup:create 备份的路径可以在配置文件 /etc/gitlab/gitlab.rb 中修改： 12345[root@code-server ~]# vim /etc/gitlab/gitlab.rbgitlab_rails['manage_backup_path'] = truegitlab_rails['backup_path'] = "/data/gitlab/backups" // gitlab备份目录gitlab_rails['backup_archive_permissions'] = 0644 // 生成的备份文件权限gitlab_rails['backup_keep_time'] = 7776000 // 备份保留天数为90天 备份从21:30开始一直到凌晨1点左右结束，200G 数据用时3个半小时，真是太慢了，大部分的时间都卡在备份了 Dumping lfs objects 了 1234567891011121314151617181920212223242526272829303132333435363738394041[root@gitlab-server ~]# gitlab-rake gitlab:backup:create2022-07-25 21:27:34 +0800 -- Dumping database ...Dumping PostgreSQL database gitlabhq_production ... [DONE]2022-07-25 21:27:42 +0800 -- done2022-07-25 21:27:42 +0800 -- Dumping repositories ... * xxxx/project (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x) ... * xxxx/project (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x) ... [DONE] * xxxx/project.wiki (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x.wiki) ... * xxxx/project.wiki (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x.wiki) ... [EMPTY] [SKIPPED] * xxxx/project.design (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x.design) ... * xxxx/project.design (@hashed/d4/73/d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab3x.design) ... [EMPTY] [SKIPPED] ... ...2022-07-25 22:16:14 +0800 -- done2022-07-25 22:16:14 +0800 -- Dumping uploads ...2022-07-25 22:16:15 +0800 -- done2022-07-25 22:16:15 +0800 -- Dumping builds ...2022-07-25 22:16:15 +0800 -- done2022-07-25 22:16:15 +0800 -- Dumping artifacts ...2022-07-25 22:16:15 +0800 -- done2022-07-25 22:16:15 +0800 -- Dumping pages ...2022-07-25 22:16:15 +0800 -- done2022-07-25 22:16:15 +0800 -- Dumping lfs objects ...2022-07-26 00:41:39 +0800 -- done2022-07-26 00:41:39 +0800 -- Dumping container registry images ...2022-07-26 00:41:39 +0800 -- [DISABLED]Creating backup archive: 1658767299_2022_07_26_xx.xx.xx-ee_gitlab_backup.tar ... doneUploading backup archive to remote storage ... skippedDeleting tmp directories ... donedonedonedonedonedonedonedoneDeleting old backups ... skippingWarning: Your gitlab.rb and gitlab-secrets.json files contain sensitive dataand are not included in this backup. You will need these files to restore a backup.Please back them up manually.Backup task is done. 修改存放目录这一步注意创建新目录后修改权限，并修改配置文件 12345[root@gitlab ~]# mkdir -p /data/gitlab-data[root@gitlab ~]# chown -R git:root /data/gitlab-data[root@gitlab ~]# vim /etc/gitlab/gitlab.rb# 把注释取消然后指定新的仓库存储位置，原配置是# git_data_dirs(&#123; "default" =&gt; &#123; "path" =&gt; "/mnt/nas/git-data" &#125; &#125;)git_data_dirs(&#123; "default" =&gt; &#123; "path" =&gt; "/data/gitlab-data" &#125; &#125;) /data/gitlab-data 是新挂载的目录，目标路径和其子路径必须不能为软链接。 关闭服务迁移数据前关闭服务主要是怕别人再使用，其实已经凌晨应该没有人用了，为了保险起见还是先关了吧 1gitlab-ctl stop 关闭时会自动关闭多个服务项 12345678910111213141516[root@gitlab-server gitlab]# gitlab-ctl stopok: down: alertmanager: 0s, normally upok: down: gitaly: 0s, normally upok: down: gitlab-exporter: 0s, normally upok: down: gitlab-workhorse: 0s, normally upok: down: grafana: 0s, normally upok: down: logrotate: 0s, normally upok: down: nginx: 1s, normally upok: down: node-exporter: 0s, normally upok: down: postgres-exporter: 1s, normally upok: down: postgresql: 0s, normally upok: down: prometheus: 0s, normally upok: down: puma: 0s, normally upok: down: redis: 0s, normally upok: down: redis-exporter: 1s, normally upok: down: sidekiq: 0s, normally up 迁移数据1rsync -av /var/opt/gitlab/git-data/repositories /data/gitlab-data/ 末尾的/要有，这一点很多文章都提到了，但是它们给的例子中有没有，很容易造成迷惑，这一步其实就是把 repositories 完整的拷贝到 /data/gitlab-data/ 目录下，并保留原文件和目录的属性，不加 / 就少了一级目录，不过也有文章让使用 cp 或者 mv 命令迁移数据，虽然也能达到目的，但是不如 rsync 合适，他们无法保留原属性。 200G 数据拷贝了半小时，比备份操作快多了 刷新配置1gitlab-ctl reconfigure 这一步我在执行前以为瞬间就能完成，运行后才发现这个命令检查了很多数据，在控制台输出了一大堆内容，可以注意一下有没有报错信息。 启动服务1gitlab-ctl start 启动之后需要等大概一分钟才能访问，否则会报服务器内部错误，应该是有些服务还没完全开始工作吧 12345678910111213141516[root@gitlab-server gitlab]# gitlab-ctl startok: run: alertmanager: (pid 17961) 1sok: run: gitaly: (pid 17984) 0sok: run: gitlab-exporter: (pid 18023) 0sok: run: gitlab-workhorse: (pid 18025) 1sok: run: grafana: (pid 18054) 0sok: run: logrotate: (pid 18063) 1sok: run: nginx: (pid 18082) 0sok: run: node-exporter: (pid 18111) 1sok: run: postgres-exporter: (pid 18120) 0sok: run: postgresql: (pid 18129) 0sok: run: prometheus: (pid 18136) 1sok: run: puma: (pid 18151) 0sok: run: redis: (pid 18157) 1sok: run: redis-exporter: (pid 18173) 0sok: run: sidekiq: (pid 18183) 1s 总结 gitlab 服务器配置文件是 /etc/gitlab/gitlab.rb gitlab 配置文中设置备份目录 gitlab_rails[&#39;manage_backup_path&#39;]，存储目录 git_data_dirs gitlab-rake gitlab:backup:create 用于备份 gitlab 关闭组件 gitlab-ctl stop，开启组件 gitlab-ctl start，重启组件 gitlab-ctl restart gitlab-ctl reconfigure 用于刷新配置 gitlab 迁移数据命令 rsync -av /var/opt/gitlab/git-data/repositories /data/gitlab-data/ ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 曾经幅员辽阔，东征西讨的元朝占据了欧亚大陆，但持续时间不过百年，在历史长河中只是星星点点，若要在这长河中留下痕迹，必然要做出影响全人类的重大成就，而你我绝大多数都是普通人，承认自己是普通人的那一刻你已经成长了~ 2022-7-30 16:59:18]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>服务器</tag>
        <tag>gitlab</tag>
        <tag>数据迁移</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐一个C++枚举转字符串的开源项目magic_enum]]></title>
    <url>%2Fblog%2F2022%2F07%2F23%2F%E6%8E%A8%E8%8D%90%E4%B8%80%E4%B8%AAC-%E6%9E%9A%E4%B8%BE%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AEmagic-enum%2F</url>
    <content type="text"><![CDATA[前言C++11引入了强类型的枚举类 enum class 用来代替旧风格枚举enum，新引入的 enum class 具有诸多优点：防止命名空间污染，不能隐式的转换为整型，防止不同类型的枚举相互赋值，支持前置声明。当然它也不是只有优点，因为类型不能隐式转换成int，所以在使用或者输出时需要使用 static_cast 进行转换，不过即便使用 static_cast 可以转换后输出，也不便于我们辨识枚举的值，如果想输出枚举定义时的名字就需要使用一些魔法了。 magic_enum因为C++本身不支持反射，或者说反射能力极弱，所以想反射我们必须自己实现一些东西，比如 UE 引擎就为C++写了一套自己的反射标签，而我们想获得枚举定义时的名字就需要自己记录了，因为编译后的枚举一般都转化成了整数，一个简单粗暴的想法是在定义时为每个枚举值同时指定一个同名字符串，构成map存储下来，不过我们不想每次都自己来做这件事，要是有人能帮忙就好了，这不它来了， magic_enum 就可以帮你实现这个愿望。 简单介绍magic_enum 是一个单头文件的开源库，使用方便，可以轻松帮你实现打印枚举值定义时名字的需求，另外除了可以实现这个功能，还可以根据字符串生成枚举值，根据整数生成枚举值，获取枚举值数组，获取枚举值和名字对应的数字组等等，简直是一个封装了枚举操作的宝库。 具体使用直接引用头文件 magic_enum.hpp，然后调用函数 magic_enum::enum_name(enum_xxx) 即可： 1234567891011121314151617181920212223#include &lt;iostream&gt;#include "magic_enum.hpp"enum class WeekDay&#123; WD_SUNDAY = 0, WD_MONDAY, WD_TUESDAY, WD_WEDNESDAY, WD_THURSDAY, WD_FRIDAY, WD_SATURDAY,&#125;;int main()&#123; WeekDay day = WeekDay::WD_MONDAY; std::cout &lt;&lt; "enum value: " &lt;&lt; static_cast&lt;std::underlying_type&lt;WeekDay&gt;::type&gt;(day) &lt;&lt; "\n"; std::cout &lt;&lt; "enum name: " &lt;&lt; magic_enum::enum_name(day) &lt;&lt; "\n"; return 0;&#125; 编译运行如下： 123$ g++ testenum.cpp -std=c++17 &amp;&amp; ./a.outenum value: 1enum name: WD_MONDAY 原理简述很神奇对不对，其实枚举值转换成字符串这一步，是是利用了函数模板和 __PRETTY_FUNCTION__ 组合使用获得到的，也就是对 __PRETTY_FUNCTION__ 进行截取得到的字符串。 __PRETTY_FUNCTION__ 在预编译阶段会替换成带有参数的函数名，比如 constexpr auto magic_enum::detail::n() [with E = WeekDay; E e = WD_MONDAY] 从中截取出 WD_MONDAY 就可以了。 局限性为了实现从字符串到枚举值的转换，这个库的内部定义了一个整数范围，默认从-128到128，用于遍历查找字符串对应的枚举值是多少，并且在代码中加了 static_assert 来判断范围，如果超过了这个范围就会报编译错误，这个范围可以通过修改源码中的 MAGIC_ENUM_RANGE_MIN 和 MAGIC_ENUM_RANGE_MAX 重新编译来修改，在这个范围之外还有个最大值 std::numeric_limits&lt;std::uint16_t&gt;::max 的限制。 这个最大限制也是可以改的，不过我尝试把 MAGIC_ENUM_RANGE_MAX 改到上限值 32767 之后编译时间明显变成，编译过程变得异常的慢，单个文件编译30秒，所以不建议把这个值调太大。 对比一下不使用 magic_enum 和使用它之后生成的汇编代码，从100多行扩充到1000多行，利用 type_traits 生成了大量的函数： 1234567891011121314151617181920... .type _ZN10magic_enum6detail7names_vI7WeekDayEE, @gnu_unique_object .size _ZN10magic_enum6detail7names_vI7WeekDayEE, 112_ZN10magic_enum6detail7names_vI7WeekDayEE: .quad 9 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_0EEE .quad 9 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_1EEE .quad 10 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_2EEE .quad 12 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_3EEE .quad 11 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_4EEE .quad 9 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_5EEE .quad 11 .quad _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_6EEE .weak _ZN10magic_enum6detail11enum_name_vI7WeekDayLS2_0EEE... 简化与改进其实我最想要的还是通过枚举值转化转化成字符串名称的功能，可以将这个开源库简化一下，仅保留这个功能，这样也不会有范围限制了，感觉这个库为了实现从字符串到枚举值转换背负了太多，去掉它会很清爽。 总结 magic_enum 是一个开源的、单头文件的、枚举操作工具箱 magic_enum 可以实现枚举值到字符串、字符串到枚举值、获取所有枚举名等多种操作 magic_enum 本身对枚举值有范围限制，默认是 [-128, 128], 可通过 MAGIC_ENUM_RANGE_MIN 和 MAGIC_ENUM_RANGE_MAX 修改 不建议将 magic_enum 默认枚举范围改的太大，这会明显拖慢编译时间 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 世界上有那么多美好，不要跟自己过不去，总是揪着那些角落里的肮脏不放。我们无法选择抓到什么牌，但可以决定怎么把已经抓到手的牌打出去，摆烂是一天，奋斗也是一天，究竟要怎么做，取决于你自己~ 2022-7-24 02:30:58]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>enum</tag>
        <tag>枚举类</tag>
        <tag>magic_enum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++17使用std::optional表示一个可能存在的值]]></title>
    <url>%2Fblog%2F2022%2F07%2F17%2FC-17%E4%BD%BF%E7%94%A8std-optional%E8%A1%A8%E7%A4%BA%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%83%BD%E5%AD%98%E5%9C%A8%E7%9A%84%E5%80%BC%2F</url>
    <content type="text"><![CDATA[前言平时写代码会遇到一种传递参数特殊值标记特殊流程，或者函数返回值存在魔法数的情况，很需要一种标记参数或返回值状态的结构，那么在 C++17 标准下提供了 std::optional 这个模板类，可以表示一个值不存在的状态，一起来看看用法吧。 返回一个bool值以下例子纯属虚构，只为说明问题，无实际意义 1234567891011121314151617181920212223bool getBoolVal(int a, int b)&#123; int* n = new int; if (!n) return false; *n = 1; if (a + *n &gt; b) return true; else return false;&#125;int main()&#123; if (getBoolVal(10, 9)) std::cout &lt;&lt; 1 &lt;&lt; std::endl; else std::cout &lt;&lt; 1 &lt;&lt; std::endl; return 0;&#125; 这个例子中的函数 getBoolVal 本意是想返回一个 bool 类型的判断结果，但是函数中有一些异常情况时，比如申请内存异常时，也会返回一个bool值，这是与原判断结果语义不同的，所以需要单独返回这种情况，如果也放到同一个返回值中会导致含义模糊，这时可以考虑使用引用变量参数来返回实际比较结果。 12345678910111213141516171819202122232425262728293031bool getBoolVal(int a, int b, bool&amp; ret)&#123; int* n = new int; if (!n) return false; *n = 1; if (a + *n &gt; b) ret = true; else ret = false; return true;&#125;int main()&#123; bool ret = false; if (getBoolVal(10, 9, ret)) std::cout &lt;&lt; "error" &lt;&lt; std::endl; else &#123; if (ret) std::cout &lt;&lt; 1 &lt;&lt; std::endl; else std::cout &lt;&lt; 0 &lt;&lt; std::endl; &#125; return 0;&#125; 这个引用参数 ret 使用起来有点不方便，那把两个值都返回怎么样，虽然C++不允许有多个返回值，但可以把它们包装成 std::pair 或者 std::tuple 来返回，再来改写一下： 1234567891011121314151617181920212223242526272829std::pair&lt;bool, bool&gt; getBoolVal3(int a, int b)&#123; int* n = new int; if (!n) return &#123;false, false&#125;; *n = 1; if (a + *n &gt; b) return &#123;true, true&#125;; else return &#123;true, false&#125;;&#125;int main()&#123; auto [err, ret] = getBoolVal(10, 9); if (err) std::cout &lt;&lt; "error" &lt;&lt; std::endl; else &#123; if (ret) std::cout &lt;&lt; 1 &lt;&lt; std::endl; else std::cout &lt;&lt; 0 &lt;&lt; std::endl; &#125; return 0;&#125; 这种方法把实际的返回值，搭配一个表示状态的 bool 变量，组成 std::pair 进行返回，基本上得到而来语义明确的目的，但是看起来还是不太优雅，而 std::optional 可以帮助我们实现类似的需求，并且代码看起来能更简洁一点。 使用 std::optional 改写std::optional 本身是一个模板类：会有一个 std::nullopt 12template &lt;class T&gt;class optional; 它内部有两种状态，要么有一个T类型的值，要么用 std::nullopt 表示没有值，查看一个 std::optional 对象是否有值，可以用 has_value() 进行判断，当一个 std::optional 有值时，可以通过用指针的方式(*号和-&gt;号)来使用它，或者用 value()函数取它的值，下面我们用它来改写一下之前的实现： 123456789101112131415161718192021222324252627282930std::optional&lt;bool&gt; getBoolVal4(int a, int b)&#123; int* n = new int; if (!n) return std::nullopt; *n = 1; if (a + *n &gt; b) return true; else return false;&#125;int main()&#123; std::optional&lt;bool&gt; ret = getBoolVal(10, 9); if (ret.has_value()) std::cout &lt;&lt; "error" &lt;&lt; std::endl; else &#123; if (ret.value()) std::cout &lt;&lt; 1 &lt;&lt; std::endl; else std::cout &lt;&lt; 0 &lt;&lt; std::endl; &#125; return 0;&#125; 使用了 std::optional 之后就把 bool 类型之前的两态变成了三态，很多类似的逻辑也被封装成了函数，使用它之后代码更清晰了，从此可以告别一些烦人的魔法数了，一些函数参数也可以使用 std::optional 来包装，用法类似，在此就不展开说了。 总结 std::optional 是一个模板类，可以表示一个可能存在的值 std::optional 的内部有两种状态，要么表示一个T类型的值，要么用 std::nullopt 表示没有值 可以用 has_value() 判断一个 std::optional 是否有值，然后用 value() 函数取它表示的值 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 子未经历过，安知此文是鸡汤，子非我，安知我不知此文是鸡汤。意见向左的人往往在内心互道傻X，而现实生活中哪有什么绝对的对错，只是出发点和眼界不同罢了，即使是真理也有适用的环境，“两点之间线段最短”，这一定是对的吗？ 2022-7-17 23:26:03]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>optional</tag>
        <tag>可选值</tag>
        <tag>魔法数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偶然在github开源项目中发现了.travis.yml这货]]></title>
    <url>%2Fblog%2F2022%2F07%2F02%2F%E5%81%B6%E7%84%B6%E5%9C%A8github%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E4%B8%AD%E5%8F%91%E7%8E%B0%E4%BA%86-travis-yml%E8%BF%99%E8%B4%A7%2F</url>
    <content type="text"><![CDATA[前言偶然在一个github开源项目中发现了.travis.yml这货，然后一发不可收拾，翻了翻之前看的几个开源库都有这个文件，并且最近经常看到它，这被称为“巴德尔-迈因霍夫现象”，是一种认知偏见，即在第一次注意到某一事物后，有一种更频繁地注意到它的倾向，导致某人相信它有很高的频率，既然这样索性就深入研究了一下这个文件，发现它原来是用于持续集成的。 持续集成持续集成是一种 DevOps（Development和Operations的组合词）软件开发实践。采用持续集成时，开发人员会定期将代码变更合并到一个中央存储库中，之后系统会自动运行构建和测试操作。每次集成都通过自动化的构建（包括编译，发布，自动化测试）来进行验证，从而尽早地发现集成错误，提高软件质量，并减少验证和发布新软件更新所需的时间。 持续集成是将构建并测试的过程自动化，在你提交代码时，持续集成服务能够自动触发构建与测试过程，并反馈结果，加快开发周期，同时减少脏代码的引入，而持续集成工具有很多，常见的包括 Jenkins、Gitlab-CI、Travis CI 和 AppVeyor，github上项目的持续集成可以选择使用 Travis CI，也有项目使用 AppVeyor，它们都是开源持续集成云服务。 YAML.travis.yml 是 github 用于说明持续集成步骤配置文件，使用的语言是 YAML。它是一种可读性非常高，与程序语言数据结构非常接近，同时具备丰富的表达能力和可扩展性，并且易于使用的数据标记语言。经常会拿它和 XML 和 JSON 进行对比，YAML 比 XML 语法简洁得多，但是没有 XML 的标签概念，而 JSON 语法是 YAML 1.2 的子集，非常接近 YAML1.0 与 YAML1.1 的子集。 YAML 可以简单表达清单、散列表，标量等数据结构。它使用空白符号缩进，适合用来表达或编辑数据结构、各种配置文件、倾印调试内容、文件大纲等，配置文件后缀为通常为 .yml，比如：.travis.yml。 关于具体的语法本文就不展开说了，网上自行搜索一下，不同类型的项目的配置通常有自己的规范，可以参照travis官方配置说明，下面展示一个 .travis.yml 文件 12345678910111213141516171819language: cppsudo: falseos: - linux - osxcompiler: - gcc - clangscript: - $CC ccronexpr.c ccronexpr_test.c -I. -Wall -Wextra -std=c89 -DCRON_TEST_MALLOC -o a.out &amp;&amp; ./a.out - $CXX ccronexpr.c ccronexpr_test.c -I. -Wall -Wextra -std=c++11 -DCRON_TEST_MALLOC -o a.out &amp;&amp; ./a.outnotifications: email: on_success: always 第一次使用 .travis.yml要想学会一件事必须反复强化记忆，所以我决定自己写个.travis.yml来使用一次，刚开始语法还不太熟悉，所以我打算在一些开源项目的文件基础上来修改，需求也比较简单，只要能实现我上传到github的代码能自动编译就可以了。 注册登录travis登陆 travis 官网，直接用github账号登陆即可，这样 travis 可以直接关联登录的github账号，自动获取你的仓库信息。 登陆之后，点击settings，然后激活 Travis CI 勾选需要持续集成的仓库。 编写代码为了方便测试，我们只编写一个简单的 HelloWolrd.cpp 测试文件好了 1234567#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; "Hello World!" &lt;&lt; std::endl; return 0;&#125; 编写.travis.yml我只写了一个文件，要求只要编译 gcc 通过就行了，暂时也不需要邮件通知 123456789101112language: cppsudo: falseos: - linuxcompiler: - gccscript: - $CXX wolrd/code/c++/HelloWorld.cpp -o a.out &amp;&amp; ./a.out 推送代码启动Travis CI123456789101112131415161718Albert@home-pc MINGW64 /d/data/maingit/HelloWorld (master)$ git add .Albert@home-pc MINGW64 /d/data/maingit/HelloWorld (master)$ git commit -m&quot;test travis ci&quot;[master 8e92384] test travis ci 2 files changed, 2 insertions(+), 2 deletions(-)Albert@home-pc MINGW64 /d/data/maingit/HelloWorld (master)$ git pushEnumerating objects: 13, done.Counting objects: 100% (13/13), done.Delta compression using up to 4 threadsCompressing objects: 100% (6/6), done.Writing objects: 100% (7/7), 604 bytes | 302.00 KiB/s, done.Total 7 (delta 3), reused 0 (delta 0)remote: Resolving deltas: 100% (3/3), completed with 3 local objects.To github.com:AlbertGithubHome/HelloWorld.git af45c67..8e92384 master -&gt; master 推送之后travis-ci网站会自动启动，构建过程和结果如下： 第一次尝试失败，检查发现编译文件的路径写错了，修改后再次推送，成功构建的界面如下： 然后就可以在编译状态按钮后面领取这样一个标签，它可以根据项目构建状态实时变化，快把它加到项目的README文件里吧。 [![Build Status](https://app.travis-ci.com/AlbertGithubHome/HelloWorld.svg?branch=master)](https://app.travis-ci.com/AlbertGithubHome/HelloWorld) 总结 .travis.yml 是使用 Travis CI 持续集成服务的配置文件，使用 YAML 语言编写 YAML 比 XML 语法简洁得多，但是没有 XML 的标签概念，而 JSON 语法是 YAML 1.2 的子集 GitHub 和 Travis CI 是一对好基友，几乎不用额外的配置，只要按照官方语法写好 .travis.yml 文件即可 可以把Travis CI 看成一个机器人，每当我们 push 代码时，这个机器人会按照既定流程帮我们自动构建和检测 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 卅是一个阶段，更是一个开始~ 2022-7-3 01:08:33]]></content>
      <categories>
        <category>CI/CD</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>CI</tag>
        <tag>CD</tag>
        <tag>YAML</tag>
        <tag>travis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11中的noexcept说明符和操作符]]></title>
    <url>%2Fblog%2F2022%2F06%2F26%2FC-11%E4%B8%AD%E7%9A%84noexceptno%E7%9A%84%E8%AF%B4%E6%98%8E%E7%AC%A6%E5%92%8C%E6%93%8D%E4%BD%9C%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[前言noexcept 这个说明符看起来很厉害的样子，给人一种函数加上它就可以不产生异常的感觉，但事实真的如此吗？它真的能消除一切异常吗？答案是不能！它只是函数的一种说明，作为开发者我们需要自己保证标记了 noexcept 的函数不产生异常。啥？这不是没事找事吗？ 为什么要用 noexcept既然给函数添加了 noexcept 说明符并不能杜绝异常的产生，那我们为什么还要用它呢？我个人的理解是为了优化，因为异常存在的意义就是用来干脏活累活的，为了实现异常的处理，我们编写逻辑的每一次函数调用，编译器都会生成一堆外围的处理代码，而当我们把一个函数标记为 noexcept 时，表示开发者向运行时保证调用这个函数不会抛出异常，这意味着这些脏活累活都不用干了，如果声明了 noexcept 的函数还试图抛出异常，C++会调用std::terminate函数终止程序运行。 添加 noexcept 说明符后编译器是否会进行优化，我们可以通过生成的汇编代码来看一下，编译器是 gcc 12.1，先看下面这段代码： 123456789101112struct Obj&#123; ~Obj();&#125;void inner();void entrance()&#123; Obj instance; inner();&#125; 函数 entrance() 先定义一个Obj类型的对象，然后调用一个 inner() 函数，假如 inner() 在执行过程中抛出了异常，我们需要在保证对象实例 instance 被正确析构后才能退出 entrance() 函数，为了实现了这个保证，编译器真的是操碎了心，可以看一下生成汇编代码： 123456789101112131415161718192021entrance(): push rbp mov rbp, rsp push rbx sub rsp, 24 call inner() lea rax, [rbp-17] mov rdi, rax call Obj::~Obj() [complete object destructor] jmp .L4 mov rbx, rax lea rax, [rbp-17] mov rdi, rax call Obj::~Obj() [complete object destructor] mov rax, rbx mov rdi, rax call _Unwind_Resume.L4: mov rbx, QWORD PTR [rbp-8] leave ret 从这段汇编代码中可以发现，有两句是调用了析构函数 Obj::~Obj()，其中第二次就是为了异常发生时准备的，接下来我们加上 noexcept 后代码变成了下面这样： 123456789101112struct Obj&#123; ~Obj();&#125;;void inner() noexcept;void entrance()&#123; Obj o; inner();&#125; 源代码几乎没有变化，而生成的汇编代码清爽了太多： 12345678910entrance(): push rbp mov rbp, rsp sub rsp, 16 call inner() lea rax, [rbp-1] mov rdi, rax call Obj::~Obj() [complete object destructor] leave ret 对比来看，以下这部分代码一直都是编译器在默默付出，现在你可以通过 noexcept 帮它减负了 12345678910 jmp .L4 mov rbx, rax lea rax, [rbp-17] mov rdi, rax call Obj::~Obj() [complete object destructor] mov rax, rbx mov rdi, rax call _Unwind_Resume.L4: mov rbx, QWORD PTR [rbp-8] 实际测试 noexcept不处理异常首先实验一下不处理异常时的情景： 12345678910111213141516171819202122#include &lt;iostream&gt;struct Obj&#123; ~Obj() &#123; std::cout &lt;&lt; "~Obj" &lt;&lt; std::endl; &#125;;&#125;;void inner()&#123; throw 1;&#125;int main()&#123; Obj o; inner(); return 0;&#125; 编译运行结果如下： 1234albert@home-pc:/mnt/d/data/cpp/testExcept$ g++ testexcept.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testExcept$ ./a.outterminate called after throwing an instance of 'int'Aborted (core dumped) 可以看到程序直接崩溃了，在抛出一个异常之后被终止了。 正常捕获异常再实验一次不加 noexcept 时常规处理异常的方式，代码如下： 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;struct Obj&#123; ~Obj() &#123; std::cout &lt;&lt; "~Obj" &lt;&lt; std::endl; &#125;;&#125;;void inner()&#123; throw 1;&#125;int main()&#123; Obj o; try &#123; inner(); &#125; catch(int) &#123; std::cout &lt;&lt; "catch exception." &lt;&lt; std::endl; &#125; return 0;&#125; 编译运行结果如下： 1234albert@home-pc:/mnt/d/data/cpp/testExcept$ g++ testexcept.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testExcept$ ./a.outcatch exception.~Obj 异常被捕获，Obj对象被正常析构，程序正常退出了 添加noexcept标记给函数加上noexcept标记，再测试一次： 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;struct Obj&#123; ~Obj() &#123; std::cout &lt;&lt; "~Obj" &lt;&lt; std::endl; &#125;;&#125;;void inner() noexcept&#123; throw 1;&#125;int main()&#123; Obj o; try &#123; inner(); &#125; catch(int) &#123; std::cout &lt;&lt; "catch exception." &lt;&lt; std::endl; &#125; return 0;&#125; 编译运行结果如下： 1234albert@home-pc:/mnt/d/data/cpp/testExcept$ g++ testexcept.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testExcept$ ./a.outterminate called after throwing an instance of 'int'Aborted (core dumped) 我们看到加了noexcept如果抛出异常，程序会被直接终止，try...catch 语句就不起作用了，所以 noexcept 不能随便加，一定得保证确实没有异常才可以应用此优化。 noexcept操作符noexcept本身后面可以加一个表达式，返回一个bool值，用来判定一个函数是否会抛出异常，可以看一下这个例子： 123456789101112131415161718#include &lt;iostream&gt;void inner() noexcept&#123; throw 1;&#125;void test();void test2() noexcept;int main()&#123; std::cout &lt;&lt; std::boolalpha &lt;&lt; "Is inner() noexcept? " &lt;&lt; noexcept(inner()) &lt;&lt; std::endl &lt;&lt; "Is test() noexcept? " &lt;&lt; noexcept(test()) &lt;&lt; std::endl &lt;&lt; "Is tes2() noexcept? " &lt;&lt; noexcept(test2()) &lt;&lt; std::endl;&#125; 编译运行后的结果如下： 12345albert@home-pc:/mnt/d/data/cpp/testExcept$ g++ testexcept.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testExcept$ ./a.outIs inner() noexcept? trueIs test() noexcept? falseIs tes2() noexcept? true test() 和 test2() 函数的判断应该没有什么疑问，而 inner()明明会抛出异常，而 noexcept操作符却返回了false，这是因为我们在函数后面加上了 noexcept 说明符，这时就需要我们自己保证函数不会抛出异常了，如果你硬要抛出异常那编译器也没有办法了。 noexcept的优缺点优点调用标记为 noexcept 的函数时不需要额外记录exception handler，所以编译器拥有更高的自由度，便于生成更加高效的执行代码。 缺点noexcept关键字啊会影响接口的灵活性，如果基类某个虚函数设置为noexcept，派生类重写虚函数时也必须遵守，派生类中只要有一个函数遗漏了noexcept约定，就可能会导致整个程序在发生异常时被终止。 如果标记为noexcept的函数调用了第三方库的函数，就需要对这些第三方库都做一层封装，保证处理了第三方库所有可能抛出的异常，增大了工作量。 总结 noexcept 是一个说明符同时也是一个操作符 noexcept 作为说明符放在函数名后面，表明次函数不会抛出异常，等同于 noexcept(true) noexcept 作为操作符时，可以用来判断一个函数是否会抛出异常，用法为 noexcept(funcName) 当使用 noexcept 标记函数时，我们需要自己保证函数不会抛出异常，这样可以生成更高效的代码 如果标记了 noexcept函数还是抛出了异常，那么程序会直接调用 std::abort() 终止程序，try...catch都没用 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 曾经接受教育努力学习是打破阶级固化的长矛，如今教育更多表现为阻碍阶级跨越的护盾，不能否认好好学习可以让我们的生活变得更好，但寒门再难出贵子确实是摆在我们面前的现实，马太效应，强者愈强、弱者愈弱。条条大路通罗马，而有些人就生在罗马，我们只能靠自己微弱的锋利，去尝试突破那一层层禁锢。 2022-6-26 17:23:42]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>noexcept</tag>
        <tag>terminate</tag>
        <tag>try-catch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++17新语法之if和switch语句中可以初始化变量啦]]></title>
    <url>%2Fblog%2F2022%2F06%2F18%2FC-17%E6%96%B0%E8%AF%AD%E6%B3%95%E4%B9%8Bif%E5%92%8Cswitch%E8%AF%AD%E5%8F%A5%E4%B8%AD%E5%8F%AF%E4%BB%A5%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%98%E9%87%8F%E5%95%A6%2F</url>
    <content type="text"><![CDATA[前言今天在看一些c++代码时偶然间发现一些示例，在if语句的小括号内居然出现了分号，难道这一段是伪代码吗？之前从来没见过这种写法，然后写了一个例子，用最新的编译器编译后发现真的是正常的代码，并且可以成功运行，然后脑袋有个声音一闪而过，是不是一直就支持这种写法，只是我不知道而已，后来几经周折，终于发现真相，原来这其实是C++17新的语法。 if 语句本来的样子C++17之前 if 语句的小括号内只能放判断逻辑，if (condition) { /* ... */ } 也就是这样，所以如果想使用一个变量进行判断，那么这个变量必须在if语句之前进行定义，比如我们常常使用的查找map元素的逻辑： 1234567&#123; auto it = mp.find("key"); if (it != mp.end()) return it-&gt;second; else return defaultValue;&#125; if 语句新能力从C++17开始，if 语句的小括号之中也可以初始化变量了，语法为 if (initializer; condition) { /* ... */ }，这样做的好处就是可以更加精细的控制变量的作用域，使代码看起来更加紧凑，比如上面的例子可以改写为： 123456&#123; if (auto it = mp.find("key"); it != mp.end()) return it-&gt;second; else return defaultValue;&#125; 最直观的感受就是少了一行代码，但是更重要的特点还是他把变量 it 的作用域限制在了 if 语句之内。 另外就是能一定程度上解决代码缩进层数太多的问题，这也是实际开发过程中遇到的问题，比如下面这段代码： 12345678910111213&#123; auto it = mp.find("key"); if (it != mp.end()) return it-&gt;second; else &#123; int cfg = GetConfig(); if (cfg) return defaultValue1; else return defaultValue2; &#125;&#125; 在C++17之前的代码中，因为 if 语句中不能包含初始化变量的逻辑，所以会造成 if 嵌套层层递进，当这种条件太多时，缩进层数太多导致代码可读性变差，使用C++17的语法再改进一下就变成了下面这样，逻辑更加清晰了： 12345678&#123; if (auto it = mp.find("key"); it != mp.end()) return it-&gt;second; else if (int cfg = GetConfig(); cfg) return defaultValue1; else return defaultValue2;&#125; switch 语句也是相同的能力扩展，就不再展开说了，这里贴一个 cppreference - switch 上的例子： 12345678910111213141516171819202122232425&#123; struct Device &#123; enum State &#123; SLEEP, READY, BAD &#125;; auto state() const &#123; return m_state; &#125; /*...*/ private: State m_state&#123;&#125;; &#125;; switch (auto dev = Device&#123;&#125;; dev.state()) &#123; case Device::SLEEP: /*...*/ break; case Device::READY: /*...*/ break; case Device::BAD: /*...*/ break; &#125;&#125; 似曾相识在go中这个在if中既初始化又进行判断的语法，之前在go中也用过，类似的需求在go中的写法是： 1234567func xxx() &#123; if value := mp["key"]; value != nil &#123; return value &#125; else &#123; return defaultValue &#125;&#125; 是不是很像？其实在golang中更规范的用法是使用map查找的第二个返回值来判断元素是否存在，就像下面这样： 1234567func xxx() &#123; if value, ok := mp["key"]; ok &#123; return value &#125; else &#123; return defaultValue &#125;&#125; 关于语言的思考看到上面这种golang和c++17的对比，会发现语言之间都在相互促进，这些所谓的高级语言，总能在其中一种语言中发现另一种语言的影子，也就是说在某些方面上他们是“趋同”的，可能在不久的将来会出现一种“超高级语言”，类似于C++模板机制，C++、Golang、Python 这些语言作为一种类型，传递到超高级语言之中，编写或者描述完功能逻辑后，自动生成对应语言的代码： 12345SuperLanguageTemplate&lt;C++/Golang/Python&gt;[AutoGenerateByFollowingDescription]&#123; / * ... * /&#125; 总结 if 和 switch 语句都可以在逻辑判断前初始化变量，变量的定义域可以更精细的控制 编程语言之间相互“借鉴”，部分语法长得越来越像，最后不知道会不会归于统一 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生并不是不断失去，反过来看，其实人生应该是不断拥有，1秒，2秒，3秒，这些本就不一定属于你的时间，作为馈赠出现在了你的生命里~ 2022-6-19 01:44:55]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>if</tag>
        <tag>switch</tag>
        <tag>initializer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解git裸仓库并利用post-receive自动化部署]]></title>
    <url>%2Fblog%2F2022%2F06%2F12%2F%E4%BA%86%E8%A7%A3git%E8%A3%B8%E4%BB%93%E5%BA%93%E5%B9%B6%E5%88%A9%E7%94%A8post-receive%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前言【裸仓库】指的是使用 git init --bare 命令得到的仓库，是对这种操作结果的一种直译，这个词对于刚接触 git 软件的小伙伴来说可能是第一次听说，而我也是最近实际操作了几次才渐渐理解，下面解释一下什么是裸仓库，以及为什么要使用它，有理解不对的地方还请大家指正。 普通库和裸仓库普通库在解释裸仓库之前，还是先来看看 git init命令创建一个普通仓库的目录结构: 123456789101112131415161718192021222324252627282930313233[root@VM-0-3-centos data]# git init simpleInitialized empty Git repository in /data/simple/.git/[root@VM-0-3-centos data]# cd simple/[root@VM-0-3-centos simple]# touch README.md[root@VM-0-3-centos simple]# cd ..[root@VM-0-3-centos data]# tree -a simple/simple/|-- .git| |-- branches| |-- config| |-- description| |-- HEAD| |-- hooks| | |-- applypatch-msg.sample| | |-- commit-msg.sample| | |-- post-update.sample| | |-- pre-applypatch.sample| | |-- pre-commit.sample| | |-- prepare-commit-msg.sample| | |-- pre-push.sample| | |-- pre-rebase.sample| | `-- update.sample| |-- info| | `-- exclude| |-- objects| | |-- info| | `-- pack| `-- refs| |-- heads| `-- tags`-- README.md10 directories, 14 files 通过上述命令操作后可以看到，git init simple 操作之后，创建了一个名为 simple 的库，simple 目录下还有一个 .git 子目录，其中包含了git系统常用的文件，在 .git 目录外是我们的工作区，可以存放我们库中待更新的文件，修改之后可以通过 git add，git commit 等命令更新 .git 中的内容，简单来说普通库就是在工作目录 simple 中还包括一个 .git 目录，下面添加一个文件试试。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@VM-0-3-centos simple]# git add README.md[root@VM-0-3-centos simple]# git commit -m"add readme file"[master (root-commit) 9a9b255] add readme file 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README.md[root@VM-0-3-centos simple]# tree . -a.|-- .git| |-- branches| |-- COMMIT_EDITMSG| |-- config| |-- description| |-- HEAD| |-- hooks| | |-- applypatch-msg.sample| | |-- commit-msg.sample| | |-- post-update.sample| | |-- pre-applypatch.sample| | |-- pre-commit.sample| | |-- prepare-commit-msg.sample| | |-- pre-push.sample| | |-- pre-rebase.sample| | `-- update.sample| |-- index| |-- info| | `-- exclude| |-- logs| | |-- HEAD| | `-- refs| | `-- heads| | `-- master| |-- objects| | |-- 9a| | | `-- 9b255b81e994fa9af2b9c7ecbd852eb716ad6c| | |-- e6| | | `-- 9de29bb2d1d6434b8b29ae775ad8c2e48c5391| | |-- f9| | | `-- 3e3a1a1525fb5b91020da86e44810c87a2d7bc| | |-- info| | `-- pack| `-- refs| |-- heads| | `-- master| `-- tags`-- README.md16 directories, 22 files[root@VM-0-3-centos simple]# 添加文件之后，.git 目录中的内容发生了变化，多了3个新的object。 裸仓库还是先从目录结构入手，我们使用 git init --bare 命令创建一个裸仓库，目录结构如下： 1234567891011121314151617181920212223242526272829[root@VM-0-3-centos data]# git init --bare bare.gitInitialized empty Git repository in /data/bare.git/[root@VM-0-3-centos data]# tree bare.git/ -abare.git/|-- branches|-- config|-- description|-- HEAD|-- hooks| |-- applypatch-msg.sample| |-- commit-msg.sample| |-- post-update.sample| |-- pre-applypatch.sample| |-- pre-commit.sample| |-- prepare-commit-msg.sample| |-- pre-push.sample| |-- pre-rebase.sample| `-- update.sample|-- info| `-- exclude|-- objects| |-- info| `-- pack`-- refs |-- heads `-- tags9 directories, 13 files[root@VM-0-3-centos simple]# 从目录结构来看裸仓库和普通库很像，但是仔细对比你会发现，这个裸仓库相比普通库少了一层目录，库目录 bare.git 内直接就是之前普通库 .git 目录下的内容，也就是说在 git 目录外层没有了工作目录来进行文件的增删改操作，那么我们仿照普通库操作在这个目录下提交一个文件会怎样呢？ 1234567[root@VM-0-3-centos data]# cd bare.git/[root@VM-0-3-centos bare.git]# touch README.md[root@VM-0-3-centos bare.git]# git add README.mdfatal: This operation must be run in a work tree[root@VM-0-3-centos bare.git]# git statusfatal: This operation must be run in a work tree[root@VM-0-3-centos bare.git]# 通过操作发现这个裸仓库不允许增删改库内的文件，甚至连 git status 这种命令都无法使用，统一提示了 fatal: This operation must be run in a work tree 这句话，告诉用户这些命令都必须在工作区内操作，既然不能修改，那么这个裸仓库就是“只读”的，那么它还有什么用呢？ 虽然裸仓库不允许直接修改，但是可以作为服务端远程仓库，在本地克隆这个远程仓库之后再进行修改，这也是最常见的应用方式，总结来说，普通库和裸仓库的区别就是：普通库拥有工作目录，并且工作目录中可以存放正常编辑和提交的文件，而裸库只存放这些文件的commit记录，不允许用户直接在上面进行各种git操作。 使用裸仓库前面提到裸仓库不能直接修改，但是我们可以采取修改克隆后库文件的方式达到更新的目的，下面列举两种常见的方式： 使用 git remote add 方式关联这种方式需要我们先在本地初始化一个普通库，再使用 git remote add 命令建立关联（PowerShell命令行操作，git命令是相同的）： 12345678910111213141516171819202122232425262728293031323334PS-Win D:\data\maingit\test&gt; git init barebyremoteInitialized empty Git repository in D:/data/maingit/test/barebyremote/.git/PS-Win D:\data\maingit\test&gt; cd .\barebyremote\PS-Win D:\data\maingit\test\barebyremote&gt; git remote add origin root@82.156.125.196:/data/bare.gitPS-Win D:\data\maingit\test\barebyremote&gt; new-item README.md 目录: D:\data\maingit\test\barebyremoteMode LastWriteTime Length Name---- ------------- ------ -----a---- 2022/6/12 16:51 0 README.mdPS-Win D:\data\maingit\test\barebyremote&gt; git add .\README.mdPS-Win D:\data\maingit\test\barebyremote&gt; git commit -m"add readme file"[master (root-commit) f1c41db] add readme file 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README.mdPS-Win D:\data\maingit\test\barebyremote&gt; git push -u origin masterEnumerating objects: 3, done.Counting objects: 100% (3/3), done.Writing objects: 100% (3/3), 223 bytes | 223.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)To 82.156.125.196:/data/bare.git * [new branch] master -&gt; masterBranch 'master' set up to track remote branch 'master' from 'origin'.PS-Win D:\data\maingit\test\barebyremote&gt; git log -3commit f1c41db4699f71e9750d8d6aa2c01875ac6d4a14 (HEAD -&gt; master, origin/master)Author: albert &lt;albert@163.com&gt;Date: Sun Jun 12 16:51:34 2022 +0800 add readme filePS-Win D:\data\maingit\test\barebyremote&gt; 使用 git clone 直接克隆使用克隆方式时，按照普通库来操作就可以（PowerShell命令行操作，git命令是相同的）： 123456789101112131415161718192021PS-Win D:\data\maingit\test&gt; git clone root@82.156.125.196:/data/bare.git barebycloneCloning into 'barebyclone'...remote: Counting objects: 3, done.remote: Total 3 (delta 0), reused 0 (delta 0)Receiving objects: 100% (3/3), done.PS-Win D:\data\maingit\test&gt; cd .\barebyclone\PS-Win D:\data\maingit\test\barebyclone&gt; git log -3commit f1c41db4699f71e9750d8d6aa2c01875ac6d4a14 (HEAD -&gt; master, origin/master, origin/HEAD)Author: albert &lt;albert@163.com&gt;Date: Sun Jun 12 16:51:34 2022 +0800 add readme filePS-Win D:\data\maingit\test\barebyclone&gt; ls 目录: D:\data\maingit\test\barebycloneMode LastWriteTime Length Name---- ------------- ------ -----a---- 2022/6/12 16:57 0 README.md 为什么要使用裸仓库既然裸仓库相比于普通库只是少了工作目录，那么我们直接用普通库作为远程仓库可不可以呢？结论是可以，但是不建议，我们来实际操作一下，利用刚刚的建立的 simple 作为远端库，我们在本地clone后修改，再上传看看会遇到什么问题。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152PS-Win D:\data\maingit\test&gt; git clone root@82.156.125.196:/data/simple simpleremote: Counting objects: 3, done.remote: Total 3 (delta 0), reused 0 (delta 0)Receiving objects: 100% (3/3), done.PS-Win D:\data\maingit\test&gt; cd .\simple\PS-Win D:\data\maingit\test\simple&gt; git log -3commit 9a9b255b81e994fa9af2b9c7ecbd852eb716ad6c (HEAD -&gt; master, origin/master, origin/HEAD)Author: albert &lt;albert@example.com&gt;Date: Sun Jun 12 15:53:30 2022 +0800 add readme filePS-Win D:\data\maingit\test\simple&gt; new-item .gitignore 目录: D:\data\maingit\test\simpleMode LastWriteTime Length Name---- ------------- ------ -----a---- 2022/6/12 17:20 0 .gitignorePS-Win D:\data\maingit\test\simple&gt; git add .\.gitignorePS-Win D:\data\maingit\test\simple&gt; git commit -m"add gitignore file"[master b5a679f] add gitignore file 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 .gitignorePS-Win D:\data\maingit\test\simple&gt; git pushEnumerating objects: 3, done.Counting objects: 100% (3/3), done.Delta compression using up to 4 threadsCompressing objects: 100% (2/2), done.Writing objects: 100% (2/2), 263 bytes | 263.00 KiB/s, done.Total 2 (delta 0), reused 0 (delta 0)remote: error: refusing to update checked out branch: refs/heads/masterremote: error: By default, updating the current branch in a non-bare repositoryremote: error: is denied, because it will make the index and work tree inconsistentremote: error: with what you pushed, and will require 'git reset --hard' to matchremote: error: the work tree to HEAD.remote: error:remote: error: You can set 'receive.denyCurrentBranch' configuration variable toremote: error: 'ignore' or 'warn' in the remote repository to allow pushing intoremote: error: its current branch; however, this is not recommended unless youremote: error: arranged to update its work tree to match what you pushed in someremote: error: other way.remote: error:remote: error: To squelch this message and still keep the default behaviour, setremote: error: 'receive.denyCurrentBranch' configuration variable to 'refuse'.To 82.156.125.196:/data/simple ! [remote rejected] master -&gt; master (branch is currently checked out)error: failed to push some refs to 'root@82.156.125.196:/data/simple'PS-Win D:\data\maingit\test\simple&gt; 克隆之后正常的修改和提交都没有问题，但是 git push的时候报错，原因提示 ! [remote rejected] master -&gt; master (branch is currently checked out)，提示当前的 master 分支是检出状态，不允许直接推送。 仔细想想就会有些思路，普通库实际上包含两份数据的，一份在 .git 目录中以object形式存在，一份在工作目录中以源文件形式存在，我们每次使用 git 命令，可以保证工作目录内文件和 .git 目录数据是一致的，但是如果将普通库作为远端时，在下游提交数据时，远端库中的 .git 目录会直接更新，但是工作区却不知道此时谁在用，不能直接更新覆盖，这就造成了数据不一致的情况。 如果非得使用普通库作为服务端仓库，那么可以参照上面报错的建议，在采用额外方式保证一致性的同时，修改服务端库的 receive.denyCurrentBranch 这个git配置项，或者将服务端分支切换到一个无人使用的分支上，这样下游端就可以直接推送了。 1234567[root@VM-0-3-centos data]# cd simple/[root@VM-0-3-centos simple]# git checkout -b unlessSwitched to a new branch 'unless'[root@VM-0-3-centos simple]# git branch -a master* unless[root@VM-0-3-centos simple]# 1234567891011121314151617PS-Win D:\data\maingit\test\simple&gt; pwdPath----D:\data\maingit\test\simplePS-Win D:\data\maingit\test\simple&gt; git pushEnumerating objects: 3, done.Counting objects: 100% (3/3), done.Delta compression using up to 4 threadsCompressing objects: 100% (2/2), done.Writing objects: 100% (2/2), 263 bytes | 263.00 KiB/s, done.Total 2 (delta 0), reused 0 (delta 0)To 82.156.125.196:/data/simple 9a9b255..b5a679f master -&gt; masterPS-Win D:\data\maingit\test\simple&gt; 自动化部署利用 post-receive 进行自动化部署的原理就是，git 本身提供了一些脚本接口，在某些 git 操作发生时，会调用预定脚本执行命令，相当于给 git 用户开放了接口，我们可以修改 post-receive 脚本，在修改提交后自动部署最新内容，进一步实现自动化集成。 因为前面已经介绍了很多有关裸仓库的知识，接下来我只叙述操作步骤，看了之前的介绍，这部分内容应该没什么难度了。 需求服务端建立裸仓库，在接收到新的提交时，自动将项目部署到/data/publish/game 目录下 操作步骤服务端远端操作建立裸仓库 /data/repo/game.git，对应部署目录是 /data/publish/game 123456[root@VM-0-3-centos data]# mkdir -p /data/repo[root@VM-0-3-centos data]# mkdir -p /data/publish/game[root@VM-0-3-centos data]# cd repo/[root@VM-0-3-centos repo]# git init --bare game.gitInitialized empty Git repository in /data/repo/game.git/[root@VM-0-3-centos repo]# 新建 /data/repo/game.git/hooks/post-receive 脚本，可以拷贝 post-receive.sample 进行修改，脚本内编写内容如下： 123456789# 指定部署目录DIR=/data/publish/gamegit --work-tree=$&#123;DIR&#125; clean -fd# 强制检出git --work-tree=$&#123;DIR&#125; checkout --force# 运行启动脚本cd $&#123;DIR&#125;chmod 755 start.sh./start.sh 客户端本地操作本地项目普通库目录结构如下，启动脚本为 start.sh： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051albert@home-pc MINGW64 /d/data/maingit/test/game (master)$ tree game/ -agame/├── .git│ ├── COMMIT_EDITMSG│ ├── config│ ├── description│ ├── HEAD│ ├── hooks│ │ ├── applypatch-msg.sample│ │ ├── commit-msg.sample│ │ ├── fsmonitor-watchman.sample│ │ ├── post-update.sample│ │ ├── pre-applypatch.sample│ │ ├── pre-commit.sample│ │ ├── pre-merge-commit.sample│ │ ├── prepare-commit-msg.sample│ │ ├── pre-push.sample│ │ ├── pre-rebase.sample│ │ ├── pre-receive.sample│ │ └── update.sample│ ├── index│ ├── info│ │ └── exclude│ ├── logs│ │ ├── HEAD│ │ └── refs│ │ └── heads│ │ └── master│ ├── objects│ │ ├── 53│ │ │ └── dd8b65afe02329eb73cbe142b9359ffd2c4c70│ │ ├── 68│ │ │ └── 31f81503989c192a10b47ecf48bc6bfe7c2cf4│ │ ├── 81│ │ │ └── aaa9093e1d32996c53766fa5f943e3ea6c79b0│ │ ├── e6│ │ │ └── 9de29bb2d1d6434b8b29ae775ad8c2e48c5391│ │ ├── info│ │ └── pack│ └── refs│ ├── heads│ │ └── master│ └── tags├── README.md└── start.sh16 directories, 27 filesalbert@home-pc MINGW64 /d/data/maingit/test/game (master)$ cat start.shcp README.md test.txt 与远端裸仓库建立关联1234567891011121314albert@home-pc MINGW64 /d/data/maingit/test/game (master)$ git remote add origin root@82.156.125.196:/data/repo/game.gitalbert@home-pc MINGW64 /d/data/maingit/test/game (master)$ git push -u origin masterEnumerating objects: 4, done.Counting objects: 100% (4/4), done.Delta compression using up to 4 threadsCompressing objects: 100% (2/2), done.Writing objects: 100% (4/4), 286 bytes | 286.00 KiB/s, done.Total 4 (delta 0), reused 0 (delta 0)To 82.156.125.196:/data/repo/game.git * [new branch] master -&gt; masterBranch 'master' set up to track remote branch 'master' from 'origin'. 至此自动化部署环境已建立，当本地 game 仓库推送更新时，远端服务器会自动更新部署 快速回顾文中主要命令收于此节，方便自己后期快速查找操作 服务端远程新建裸仓库 12cd /data/repogit init --bare game.git 本地库与远端库建立关联 1git remote add origin root@82.156.125.196:/data/repo/game.git 新建或修改 hooks 目录下 post-receive 脚本 123456DIR=/data/publish/gamegit --work-tree=$&#123;DIR&#125; clean -fdgit --work-tree=$&#123;DIR&#125; checkout --forcecd $&#123;DIR&#125;chmod 755 start.sh./start.sh 总结 裸仓库是一个只包含提交记录，没有工作目录的仓库，适合用来做服务端远程仓库 裸仓库不能直接在仓库中执行修改文件的git命令，可以在客户端克隆之后修改之后再进行提交 自动化部署利用了git服务器提供的脚本接口，当新的推送达到时会调用 post-receive 脚本 配置自动化部署环境时需要注意，如果没有配置ssh免密码登陆，需要在push代码的时候输入密码 另外自动化部署时要注意各个文件及目录的权限，因为要运行脚本，要保证推送用户有足够的运行权限 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 每个人都有自己的选择，很多看似突如其来的决定，往往都是深思熟虑后的结果，每个人在自己的旅途中不断的分类、选择、分类、选择，无法逃离的坚持到最后一刻~ 2022-6-12 20:19:30]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>bare</tag>
        <tag>clone</tag>
        <tag>post_receive</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下常用的查找命令find、which、grep]]></title>
    <url>%2Fblog%2F2022%2F06%2F05%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%B8%B8%E7%94%A8%E7%9A%84%E6%9F%A5%E6%89%BE%E5%91%BD%E4%BB%A4find%E3%80%81which%E3%80%81grep%2F</url>
    <content type="text"><![CDATA[前言查找是运维工作的很重要的一部分，不管是文件查找，还是内容查找，在日常开发维护过程中都常常用到，本文把一些日常用到的查找命令总结到一起，通过对比来学习异同点，进而达到 增强记忆的目的。 本文只是想对常用命令进行一个罗列，并不会对每个命令进行详细的解释，如果想看更详细的用法，直接查询 man 手册是一个不错的选择，我们接下来会说到通用文件查找的 find 命令，快速定位文件的 locate 命令，仅用于搜索程序和文档的 whereis 命令，用于查找系统命令的 which 命令，最后是用于文件内容查找的 grep 命令。 find命令格式1find [指定目录] 搜索条件 [指定动作] 具体示例 全局查找tendis文件所在目录 12[root@VM-0-3-centos ~]# find / -name tendis/root/tendis 当前目录按指定名找到tendis并打印文件信息 12[root@VM-0-3-centos ~]# find . -name tendis -ls918146 4 drwxr-xr-x 4 root root 4096 May 1 2021 ./tendis 全局查找test开头的文件 12345678910[root@VM-0-3-centos ~]# find / -name 'test*'/boot/grub2/i386-pc/testspeed.mod/boot/grub2/i386-pc/test.mod/boot/grub2/i386-pc/test_blockarg.mod/boot/grub2/i386-pc/testload.mod/usr/lib/modules/3.10.0-1127.19.1.el7.x86_64/kernel/drivers/ntb/test/usr/lib/python2.7/site-packages/jinja2/tests.pyc/usr/lib/python2.7/site-packages/jinja2/tests.py/usr/lib/python2.7/site-packages/jinja2/testsuite... 当前目录下查找所有的目录 12345678[root@VM-0-3-centos ~]# find . -type d../tendis./tendis/scripts./tendis/bin./tendis/bin/deps./extundelete-0.2.4... 查找大于1M的文件 123456789[root@VM-0-3-centos ~]# find . -size +1M -ls918152 164712 -rwxr-xr-x 1 root root 168663910 Dec 17 2020 ./tendis/bin/tendisplus_static918151 18036 -rwxr-xr-x 1 root root 18464898 Dec 17 2020 ./tendis/bin/binlog_tool918148 2576 -rwxr-xr-x 1 root root 2635759 Dec 17 2020 ./tendis/bin/redis-cli918150 10896 -rwxr-xr-x 1 root root 11154937 Dec 17 2020 ./tendis/bin/deps/libstdc++.so.6918145 165076 -rwxr-xr-x 1 root root 169036319 Dec 17 2020 ./tendis/bin/tendisplus1311915 1860 -rw-r--r-- 1 root root 1904320 Nov 28 2021 ./extundelete-0.2.4/src/extundelete-extundelete.o1311926 1296 -rwxr-xr-x 1 root root 1323360 Nov 28 2021 ./extundelete-0.2.4/src/extundelete... 查找10分钟内修改的普通文件 123[root@VM-0-3-centos ~]# find . -type f -mmin -10./b.txt./.bash_history locatelocate 也是用来查找文件的，只不过它不是通过文件系统来找，而是通过自己的数据库来找，默认在 /var/lib/mlocate/mlocate.db，每天自动更新一次，所以查不到最新变动的文件，可以手动通过 updatedb 来更新数据库（我查了一下才2M很小的）。 命令格式1locate [选项] [匹配串] 具体示例 查找家目录下包含te的文件 123456789101112131415[root@VM-0-3-centos ~]# locate ~/te/root/tendis/root/test.iso/root/tendis/bin/root/tendis/file.xml/root/tendis/scripts/root/tendis/bin/binlog_tool/root/tendis/bin/deps/root/tendis/bin/redis-cli/root/tendis/bin/tendisplus/root/tendis/bin/tendisplus_static/root/tendis/bin/deps/libstdc++.so.6/root/tendis/scripts/start.sh/root/tendis/scripts/stop.sh/root/tendis/scripts/tendisplus.conf 不区分大小写查找 12345678910111213141516[root@VM-0-3-centos ~]# locate -i ~/tE/root/TE.txt/root/tendis/root/test.iso/root/tendis/bin/root/tendis/file.xml/root/tendis/scripts/root/tendis/bin/binlog_tool/root/tendis/bin/deps/root/tendis/bin/redis-cli/root/tendis/bin/tendisplus/root/tendis/bin/tendisplus_static/root/tendis/bin/deps/libstdc++.so.6/root/tendis/scripts/start.sh/root/tendis/scripts/stop.sh/root/tendis/scripts/tendisplus.conf whereiswhereis 只能用于二进制文件、man手册和源代码文件的搜索，默认返回所有信息。 命令格式1whereis [-bmsBMS] 匹配串 具体示例 查找二进制程序 ls 12[root@VM-0-3-centos ~]# whereis -b lsls: /usr/bin/ls 查找 grep 所有信息 12[root@VM-0-3-centos ~]# whereis grepgrep: /usr/bin/grep /usr/share/man/man1/grep.1.gz whichwhich 是在 PATH 变量中找到第一个匹配的命令并返回，这能帮助我们确认多个相同命令时用的是哪一个。 命令格式1which [选项] 匹配串 具体示例 打印当前使用的gcc程序，打印所有可加 -a 参数 1234[root@VM-0-3-centos ~]# which gcc/usr/bin/gcc[root@VM-0-3-centos ~]# which gcc -a/usr/bin/gcc grepgrep 不算是单纯查找文件的命令，更多的是用于从文件中过滤指定内容。 命令格式1grep [选项] 匹配串 [指定文件] 具体示例 过滤包含指定字符串的行 1234[root@VM-0-3-centos ~]# grep "which" w.txt which - shows the full path of (shell) commands. which [options] [--] programname [...] This man page is generated from the file which.texinfo. 显示匹配行之后的2行 123456789101112[root@VM-0-3-centos ~]# grep "which" w.txt -A 2 which - shows the full path of (shell) commands.SYNOPSIS which [options] [--] programname [...]DESCRIPTION-- This man page is generated from the file which.texinfo.OPTIONS-- 当前目录下查找包含 wonderful 的文件 123[root@VM-0-3-centos ~]# grep -r "wonderful" ../.rediscli_history:hset life family wonderful./.bash_history:grep -r "wonderful" . | head 总结 find命令查找文件最全面 find . -name tendis -ls locate 命令查找最快，locate -i /etc/redis，可用 updatedb 命令更新数据库 whereis 命令可以查找二进制、man手册、源码，whereis -b grep which 可以从PATH路径下找到第一个匹配的二进制程序 grep 一个强大的过滤命令，也可用于找文件 grep -r &quot;wonderful&quot; . ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 幸福感从比较中诞生，亦从比较中消亡，并且与比较双方的关系紧密程度高度相关。我有一块糖，而你没有，我就很幸福，转身发现他有10块糖，然后嘴里的糖瞬间就不甜了~ 2022-6-5 23:21:58]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>find</tag>
        <tag>查找</tag>
        <tag>locate</tag>
        <tag>which</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（十一）：替换字符串中包含百分号%的子串]]></title>
    <url>%2Fblog%2F2022%2F05%2F15%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%EF%BC%9A%E6%9B%BF%E6%8D%A2%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E5%8C%85%E5%90%AB%E7%99%BE%E5%88%86%E5%8F%B7-%E7%9A%84%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[前言今天这篇总结是之前批处理替换字符串的延伸问题，同样来源于读者的提问，要处理的问题是被替换的子串中如果有百分号 % 要怎样替换，因为 % 在批处理脚本中也比较特殊，如果要想表示一个 % 字符，那么在给变量赋值时需要写成 %% 的样子，用两个表示一个，类似于进行转义，因为在批处理中， %开头的内容通常表示一个变量。 之前也处理过一些替换问题，列举如下，不过今天的问题需要新的解法。 《.bat批处理（六）：替换字符串中匹配的子串》 《.bat批处理（九）：替换带有等号=的字符串的子串》 问题示例 将字符串 https://blog.csdn.net/alb%3crtsh/articl%3c/d%3ctails/124760925 中的 %3c 替换成字母 e 问题比较明确，就是因为被替换的子串中包含了 % 导致常规的替换写法 %a:b=c% 的写法失效了。 解决方法既然子串中包含 % 会影响变量替换字符串的写法，我们就可以考虑换一种变量写法，用 ! 代替 %，看到这很多人应该反应过来了，那就是启用延迟变量扩展，这个我就不展开说了，之前总结过，可以看一下这篇文章《.bat批处理（八）：各种形式的变量%0、%i、%%i、var、%var%、!var!的含义和区别》。 示例代码1234567891011@echo offrem 将输入字符串中的%3c替换成字母eSET INPUT_PARAM=%1setlocal EnableDelayedExpansionecho -echo replace result is !INPUT_PARAM:%%3c=e!echo -pause 运行结果12345D:\data\bat&gt;replace%.bat https://blog.csdn.net/alb%3crtsh/articl%3c/d%3ctails/124760925-replace result is https://blog.csdn.net/albertsh/article/details/124760925-请按任意键继续. . . 总结 批处理脚本中的替换语法不仅可以写成 %a:b=c%，还可以写成 !a:b=c!的形式 批处理脚本执行机制是会按行执行，在执行之前会先预处理 开启延迟环境变量扩展setlocal enabledelayedexpansion，变量会在用到时再估值，不会预处理了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 公平不一定平等，平等也不代表公平。究竟什么是秩序，往往强者指定规则，弱者小心遵守，达到一个稳态，那就是秩序~ 2022-5-15 23:02:49]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下常用的网络命令ping、telnet、traceroute、tcpdump]]></title>
    <url>%2Fblog%2F2022%2F05%2F10%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%B8%B8%E7%94%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E5%91%BD%E4%BB%A4ping%E3%80%81telnet%E3%80%81traceroute%E3%80%81tcpdump%2F</url>
    <content type="text"><![CDATA[前言因特网(Internet)的前身是美国国防部高级研究计划局(ARPA)用于军事目的的通信网络，真的就是一个内部的工具出圈成了改变世界的事物。网这个词用的很形象也很贴切，如今的世界任何事情都离不开这个大网了，特别是之前炒的非常火爆的万物互联概念，更是把所有事物都挂在了“网”上，虽然这张大网某些节点偶尔会不太通畅，但这并不能阻断消息的往来。之前都在讲全球化，谁能想到短短几年“逆全球化”居然越来越热。 1983年1月1日被认为是互联网的官方生日，在此之前，各种计算机网络没有相互通信的标准方式。但从这一天开始，用于军事目的的阿帕网和国防数据网络正式定义为TCP/IP标准，建立了一种统一的通信协议，它允许不同网络上不同类型的计算机相互”交谈”，互联网就此诞生了。 网络如今人们已经离不开网络了，不管是每天工作，还是日常娱乐，网络无处不在，连两岁的宝宝看到动画片转圈圈都知道是网卡了，而进行应用开发和游戏开发的搬砖小哥儿们更是无人能逃脱网络问题，遇到网络问题不可怕，重启电脑、重启路由器，重启光猫，问题很可能就解决了。 但，总有那么几个问题是重启解决不了的，所以我们还是得掌握一些正常的工具来排查问题，那么接下来就列举几个常用的命令，真的很常见，大神请绕路~ 网络命令ping这应该是用的最多的一个网络命令了吧，“ping一下通不通”，这句话经常在日常开发调试中被提及，ping 命令常常用来测试，指定的两台机器之间的网络是否可以连通，命令格式如下： 1ping [OPTIONS] 域名或IP地址 一些常用的选项： -4：只使用 IPv4 -6：只使用 IPv6 -c count：发送多少个测试包之后停止，linux环境下如果不加这个参数会一直发包 -i interval：指定发包的的间隔时间 用常用网站测试一下： 1234567891011[root@VM-0-3-centos ~]# ping -c 5 -i 2 www.baidu.comPING www.a.shifen.com (110.242.68.3) 56(84) bytes of data.64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=1 ttl=251 time=10.3 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=2 ttl=251 time=10.2 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=3 ttl=251 time=10.2 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=4 ttl=251 time=10.2 ms64 bytes from 110.242.68.3 (110.242.68.3): icmp_seq=5 ttl=251 time=10.2 ms--- www.a.shifen.com ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 8007msrtt min/avg/max/mdev = 10.217/10.243/10.311/0.132 ms 测试信息的含义： 数据 含义 -c 5 发送5个测试包 -i 2 每个测试包发送间隔2s www.baidu.com 指定的目标地址 www.a.shifen.com (110.242.68.3) 实际的目标主机的主机名和IP地址 56(84) bytes ICMP数据部分的大小56字节，加上8字节的ICMP头，则ICMP包大小为64字节，再加上20字节的IP头，IP包大小为84字节 64 bytes ICMP数据包大小 icmp_seq=1 ICMP包序号 ttl=251 剩余生存时间，生存时间是指数据包被路由器丢弃之前允许通过的网段数量，由发送主机设置的，以防止数据包在互联网络上永不终止的循环，每经过一个路由器至少将TTL减 1 time=10.3 ms 响应时间，这个时间越小，连接速度越快 — www.a.shifen.com ping statistics — 统计信息分割线，以下为统计信息 5 packets transmitted 发送数据包的数量 5 received 接收到的数据包的数量 0% packet loss 数据包的丢失率 time 8007ms 整个过程消耗的总时间 rtt min/avg/max/mdev = 10.217/10.243/10.311/0.132 ms 最小响应时间/平均响应时间/最大响应时间/响应时间的平均差 ping 命令全称 Packe InterNet Groper， 翻译为因特网包探索器，是一个用于测试网络连接状况的程序。该令会使用ICMP（Internet Control Message Protocol）传输协议，向特定的目标主机发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 ICMP协议是IP层的附属协议，是介于IP层和TCP层之间的协议，一般认为属于IP层协议，也就是网络层协议。ICMP用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。 注意：在Windows命令行中执行ping命令，默认只发送4个数据包就停止，这个行为和Linux环境下是不同的 telnettelnet是电信(telecommunications)和网络(networks)的联合缩写，是Internet远程登陆服务的标准协议，为用户提供了在本地计算机上完成远程主机工作的能力。在终端使用者的电脑上使用telnet程序，可以连接到服务器，直接在telnet程序中输入命令，这些命令会在服务器上运行，就像直接在服务器的控制台上输入一样，进而可以在本地就能控制远端的服务器。 telnet位于OSI模型的第7层，属于应用层上的一种协议，使用端口23，底层基于TCP协议。传输的数据和口令是明文形式，相对来说不安全，存在很大的安全隐患，不再用于通过公共网络访问网络设备和服务器。 现在使用更加安全的ssh代替telnet进行远程管理终端，ssh传输方式是以加密形式传输，并且功能比telnet更齐全，而telnet主要作用变成了查看某个端口是否可访问，常用命令格式如下： 1telnet IP或域名 端口 测试一下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@VM-0-3-centos ~]# telnet www.baidu.com 80Trying 110.242.68.4...Connected to www.baidu.com.Escape character is '^]'.^]telnet&gt; send ?ao Send Telnet Abort outputayt Send Telnet 'Are You There'brk Send Telnet Breakec Send Telnet Erase Characterel Send Telnet Erase Lineescape Send current escape characterga Send Telnet 'Go Ahead' sequenceip Send Telnet Interrupt Processnop Send Telnet 'No operation'eor Send Telnet 'End of Record'abort Send Telnet 'Abort Process'susp Send Telnet 'Suspend Process'eof Send Telnet End of File Charactersynch Perform Telnet 'Synch operation'getstatus Send request for STATUS? Display send optionstelnet&gt; ?Commands may be abbreviated. Commands are:close close current connectionlogout forcibly logout remote user and close the connectiondisplay display operating parametersmode try to enter line or character mode ('mode ?' for more)open connect to a sitequit exit telnetsend transmit special characters ('send ?' for more)set set operating parameters ('set ?' for more)unset unset operating parameters ('unset ?' for more)status print status informationtoggle toggle operating parameters ('toggle ?' for more)slc change state of special charaters ('slc ?' for more)z suspend telnet! invoke a subshellenviron change environment variables ('environ ?' for more)? print help informationtelnet&gt; qConnection closed.[root@VM-0-3-centos ~]# 一般情况下测试80端口是否可用，看到第3行的 Connected to www.baidu.com. 就够了，如果想使用telnet工具发送数据，可以按组合键 Ctrl+]进入输入命令的模式，按字母 q 可以退出。 注意：在Windows环境下如果端口可以连通会显示一个无任何信息的黑框，按组合键 Ctrl+]才会有反应，如果端口不通则会提示超时 nc上一小节说过，telnet是基于TCP的应用层协议，所以只能检测TCP端口是否正常，如果想检测一个UDP端口是否可用，使用telnet命令是办不到的，这时候就可以使用 nc 命令来实现，详细的用法可以参考之前的文章《网络工具nc的常见功能和用法》，下面只列举一下检查UDP端口的命令参数： 12albert@home-pc:~$ nc -nvuz 82.156.125.169 666Connection to 82.156.125.169 666 port [udp/*] succeeded! traceroutetraceroute是一个路由跟踪命令，用于追踪数据包在网络上的传输时的全部路径，IPv4是发送的探测包大小是60字节，而IPv6默认是80字节，这个可以通过查询 man 手册来查询。 通过traceroute我们可以探测出数据从当前计算机到另一台主机是走了什么样网络路径，不过相同的数据包每次由从相同的出发点到相同目的地走的路径可能并相同，但大部分时候所走的路由是一样的，所以可以帮助我们了解网络状况。 traceroute的原理是利用逐步设置 ttl 参数进行参数，这个参数全称time-to-live, 指当前数据包在网络中存在的是时间，而时间不是我们平常所说的时间，指的是数据包在网络环境中最多可以被中转的次数，每经过一个路由设备就要减1，减到0则说明数据包超时，要给原地址一个包含自身信息的回应。 既然这样就可以利用 ttl 的特点来探测出路径，先将第一个数据包的 ttl 设置为 1，到达第一个路由设备后 ttl 减为0，返回给发送数据的起始设备，这样我们就找到了网络路径中的第一个点。然后将下一个数据包的 ttl 设置为2，就可以探测出路径上的第二个路由设备，直到找到目的地址，整个探测过程结束。 1traceroute [OPTIONS] IP或域名 一些常用的选项： -4：强制使用 IPv4 追踪，默认自动选择 -6：强制使用 IPv6 追踪，默认自动选择 -I：使用 ICMP 回应进行探测 -T：使用 TCP 同步进行探测 -m max_ttl：指定 time-to-live 的最大值，默认是30 测试如下： 12345678910111213141516[root@VM-0-3-centos ~]# traceroute www.baidu.com -Ttraceroute to www.baidu.com (110.242.68.4), 30 hops max, 60 byte packets 1 9.61.26.129 (9.61.26.129) 0.464 ms 0.626 ms 0.788 ms 2 9.61.119.152 (9.61.119.152) 0.843 ms 1.085 ms 1.282 ms 3 * * * 4 10.200.44.221 (10.200.44.221) 1.130 ms 10.162.5.252 (10.162.5.252) 1.089 ms 10.200.44.205 (10.200.44.205) 1.385 ms 5 61.49.142.157 (61.49.142.157) 2.842 ms 61.49.142.153 (61.49.142.153) 0.923 ms 61.49.142.149 (61.49.142.149) 1.467 ms 6 123.126.0.217 (123.126.0.217) 2.031 ms 61.148.7.157 (61.148.7.157) 2.057 ms 202.96.13.5 (202.96.13.5) 5.015 ms 7 125.33.186.17 (125.33.186.17) 2.499 ms 124.65.194.161 (124.65.194.161) 6.422 ms 124.65.194.157 (124.65.194.157) 3.125 ms 8 * * * 9 110.242.66.190 (110.242.66.190) 10.030 ms 110.242.66.166 (110.242.66.166) 10.261 ms 110.242.66.162 (110.242.66.162) 10.751 ms10 * * *11 * * *12 * * *13 * * *14 * * 110.242.68.4 (110.242.68.4) 9.245 ms 从上面的测试结果来看，每一行都是一个记录，每个纪录表示一跳，从我的机器到 www.baidu.com 一共经过了14个设备才到达，不过并不是每次探测的结果都一样，这个是会变化的，可以看到每行有3个以ms为单位时间，是因为该命令每次默认发送3个探测数据包，这3个时间就是网关响应后返回的时间。 另外在输出信息中部分节点显示 * * *，关于这种现象，我找到几种说法，遇到了需要针对具体情况进行分析： 设备防火墙封掉了ICMP的返回信息，我们得不到什么相关的数据包返回数据 由于回送TTL超时信息的时候，CPU生成这个返回包必须被打断，为保证其它工作的正常进行，每隔一秒才会处理traceroute，所以可能会看到中间一路 * * *，但却看得到最后的destination. 这时往往是路由设备CPU太忙或者中间路由器不回送TTL超时包的原因（感觉不太靠谱）。 虚拟机nat路由器，默认丢弃port&gt;32767的包，导致看不到中间路由的信息 注意：在Windows环境下，相同功能的命令为Tracert，而不是traceroute tcpdumptcpdump命令是一个网络嗅探，它可以打印所有经过该设备网卡的数据包的信息，也可以使用-w选项将数据包保存到文件中，方便以后分析，功能与Windows平台上的Wiresh相同，生成的数据文件也可以传送到Windows平台，使用 Wireshark 软件进行分析。 命令格式非常简单，必要时加一些参数就可以： 1tcpdump [OPTIONS] 一些常用的选项： -c count：接受count个数据包之后退出 -i：指定网卡 -v：打印较详细的信息用于分析 -w：结果写到文件之中 tcp：过滤出tcp数据 port xxxx：仅打印指定端口的数据 测试一下： 1234567891011121314151617[root@VM-0-3-centos ~]# tcpdump -c 6 -vtcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes10:30:45.344349 IP (tos 0x10, ttl 64, id 47149, offset 0, flags [DF], proto TCP (6), length 188) VM-0-3-centos.ssh &gt; 68.128.126.124.broad.bjtelecom.net.6183: Flags [P.], cksum 0x9127 (correct), seq 14744777:14744925, ack 1170924064, win 340, length 14810:30:45.344782 IP (tos 0x0, ttl 64, id 39263, offset 0, flags [DF], proto UDP (17), length 73) VM-0-3-centos.35281 &gt; 183.60.83.19.domain: 48669+ PTR? 68.128.126.124.in-addr.arpa. (45)10:30:45.344826 IP (tos 0x10, ttl 64, id 47150, offset 0, flags [DF], proto TCP (6), length 188) VM-0-3-centos.ssh &gt; 68.128.126.124.broad.bjtelecom.net.6183: Flags [P.], cksum 0x5e8b (correct), seq 148:296, ack 1, win 340, length 14810:30:45.353844 IP (tos 0xa0, ttl 251, id 62658, offset 0, flags [DF], proto TCP (6), length 40) 68.128.126.124.broad.bjtelecom.net.6183 &gt; VM-0-3-centos.ssh: Flags [.], cksum 0x6909 (correct), ack 296, win 258, length 010:30:45.424218 IP (tos 0x0, ttl 56, id 9062, offset 0, flags [DF], proto UDP (17), length 121) 183.60.83.19.domain &gt; VM-0-3-centos.35281: 48669 1/0/0 68.128.126.124.in-addr.arpa. PTR 68.128.126.124.broad.bjtelecom.net. (93)10:30:45.424327 IP (tos 0x0, ttl 64, id 17697, offset 0, flags [DF], proto UDP (17), length 68) VM-0-3-centos.40741 &gt; 183.60.82.98.domain: 15078+ PTR? 3.0.10.10.in-addr.arpa. (40)6 packets captured13 packets received by filter0 packets dropped by kernel 因为不能一直盯着某个设备的网络情况，所以会将数据打印到文件中便于日后分析，常常将tcpdump命令后台运行，比如可以写成下面这样： 1nohup tcpdump -i eth0 tcp port 8080 -v -w dm8080.cap &amp; 在Windows平台一般就直接使用Wiresh软件啦，非常方便 总结 设备一旦接入网络很难“独善其身” ping 命令可以测试目的IP是否可达 ping www.baidu.com telnet 命令可以测试指定的tcp端口是否可达 telnet 110.242.68.4 80 nc 命令可以测试指定udp端口是否可达 nc -nvuz 110.242.68.4 666 tracerout 命令可以探测网络路径 traceroute www.baidu.com tcpdump 命令可以收集所有经过网卡的数据包 tcpdump -i eth0 tcp port 8080 -v ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== “王侯将相，宁有种乎？”讲的是权利；“天下兴亡，匹夫有责！”说的是义务；权力和义务的辩证关系体现于此，二者统一，不可分离~ 2022-5-13 20:59:38]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>ping</tag>
        <tag>telnet</tag>
        <tag>traceroute</tag>
        <tag>tcpdump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11新式洗牌std::shuffle与老式洗牌函数std::random_shuffle的区别]]></title>
    <url>%2Fblog%2F2022%2F05%2F03%2FC-11%E6%96%B0%E5%BC%8F%E6%B4%97%E7%89%8Cstd-shuffle%E4%B8%8E%E8%80%81%E5%BC%8F%E6%B4%97%E7%89%8C%E5%87%BD%E6%95%B0std-random-shuffle%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言洗牌算法是项目开发中常用的一种算法，它和随机数有着密不可分的关系，比如我们从报名参与活动的前10个用户中选取一个人发放幸运奖，这时可以从[1, 10] 范围内随机一个数来确定幸运儿；如果是抽取两个人呢？那就随机两次！是的，确实可以这样做，但是随着随机次数的增多，后面随机的数字很可能和前面一样，这时就要重复随机才能解决。 想想现实生活中我们可以怎么做，取红桃A-红桃10一共10张扑克牌，然后把它们随机洗一洗牌，如果需要取3个幸运儿，那么只需取扑克牌的前三张就可以了，这样很容易就取到了不重复的3个数字，当然你从中间随机抽取也是可以的，这样操作在逻辑实现上要复杂一点点。 洗牌算法其实洗牌利用的是一个排列的概念，学过排列组合的知识以后我们很清楚，n个元素排列的种类数一共是: $$A^n_n$$ 也就是n!，这是个恐怖的数字，比如打印N个数字的全排列，它的是时间复杂度就是O(N!)，这个谁也没办法优化，因为打印这些排列情况就需要O(N!)这么多的时间，而洗牌就是保证从这些元素组成的全排列中等概率的选取一种排列。 把所有的排列情况列举出来，然后从中选择一个所需时间是O(N!)，这显然是不现实的，所以很多大神们进行了优化，出现了多种洗牌算法，下面我只列举一种比较好理解的 Knuth-Durstenfeld Shuffle 算法。 改洗牌算法可简单表述为：一个拥有n个元素的初始序列，将最后一个数和该序列的前 n 个数中的随机一个数进行交换（如果随机结果是和第n个数交换，相当于没换），然后倒数第二个数和该序列的前 n - 1 个数中的随机一个数进行交换，以此类推，直到将该序列第一个数操作完，就完成了洗牌，该算法保证了每个元素在每个位置的概率都是相等的，时间复杂度为O(N)。 举个例子就像下面这样： 初始序列是 A、B、C、D、E、F，为了便于和刚才的算法思路对应描述，索引从1开始 第一轮从1-6个位置中随机一个和最后的 F 交换，假如随机到位置3，也就是和 C 交换，结果为： A B F D E C 概率是P=1/6，也就是随机一个数字的概率 第二轮从1-5个位置中随机一个和倒数第二个元素 E 交换，假如随机到的是位置2，也就是和 B 交换，结果为： A E F D B C 概率是P=(5/6)*(1/5)=1/6，为什么这么算呢？要想和 B 交换必须第一轮随机不到B才可以，所以要在前面乘以 5/6 第三轮从1-2个位置中随机一个和倒数第二个元素 D 交换，假如随机到的还是位置2，也就是和 E 交换，结果为 A D F E B C 概率是P=(5/6)*(4/5)*(1/4)=1/6，有了第二轮这个就应该明白了吧 依次类推，直到操作完第五次随机交换，整个洗牌算法也就完成了，伪代码也就几行 1234for (int i = vec.size() - 1; i &gt; 0; i--)&#123; std::swap(vec[i], vec[std::rand() % (i + 1)]);&#125; std::random_shuffle使用这个函数需要引用头文件 &lt;algorithm&gt;，共有以下几个重载函数： 12345678910template&lt; class RandomIt &gt;void random_shuffle( RandomIt first, RandomIt last ); //(deprecated in C++14)(removed in C++17)template&lt; class RandomIt, class RandomFunc &gt;void random_shuffle( RandomIt first, RandomIt last, RandomFunc&amp; r ); //(until C++11) //(deprecated in C++14)(removed in C++17)template&lt; class RandomIt, class RandomFunc &gt;void random_shuffle( RandomIt first, RandomIt last, RandomFunc&amp;&amp; r ); //(since C++11) //(deprecated in C++14)(removed in C++17) 从文档来看 std::random_shuffle 这个函数的实现在C++14标准中已经不推荐使用，在C++17中已经被移除了，函数定义如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * @brief Randomly shuffle the elements of a sequence. * @ingroup mutating_algorithms * @param __first A forward iterator. * @param __last A forward iterator. * @return Nothing. * * Reorder the elements in the range @p [__first,__last) using a random * distribution, so that every possible ordering of the sequence is * equally likely. */ template&lt;typename _RandomAccessIterator&gt; inline void random_shuffle(_RandomAccessIterator __first, _RandomAccessIterator __last) &#123; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt; _RandomAccessIterator&gt;) __glibcxx_requires_valid_range(__first, __last); if (__first != __last) for (_RandomAccessIterator __i = __first + 1; __i != __last; ++__i) &#123; // XXX rand() % N is not uniformly distributed _RandomAccessIterator __j = __first + std::rand() % ((__i - __first) + 1); if (__i != __j) std::iter_swap(__i, __j); &#125; &#125; /** * @brief Shuffle the elements of a sequence using a random number * generator. * @ingroup mutating_algorithms * @param __first A forward iterator. * @param __last A forward iterator. * @param __rand The RNG functor or function. * @return Nothing. * * Reorders the elements in the range @p [__first,__last) using @p __rand to * provide a random distribution. Calling @p __rand(N) for a positive * integer @p N should return a randomly chosen integer from the * range [0,N). */ template&lt;typename _RandomAccessIterator, typename _RandomNumberGenerator&gt; void random_shuffle(_RandomAccessIterator __first, _RandomAccessIterator __last,#if __cplusplus &gt;= 201103L _RandomNumberGenerator&amp;&amp; __rand)#else _RandomNumberGenerator&amp; __rand)#endif &#123; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt; _RandomAccessIterator&gt;) __glibcxx_requires_valid_range(__first, __last); if (__first == __last) return; for (_RandomAccessIterator __i = __first + 1; __i != __last; ++__i) &#123; _RandomAccessIterator __j = __first + __rand((__i - __first) + 1); if (__i != __j) std::iter_swap(__i, __j); &#125; &#125; 以上函数实现来源于文件 /usr/include/c++/5/bits/stl_algo.h，看源码时发现一个问题，原来标准库中的代码也是空格和Tab混用，复制过来的时候我还专门整理了一下。 第一个仅两个参数的函数中首先验证了迭代器的类型和范围的有效性，同时使用了 std::rand() 函数来随机选择了一个需要交换的元素，而拥有三个参数的函数逻辑几乎一样，只是使用了自定义传入的随机函数来选择需要交换的元素，所以洗牌算法的核心逻辑就是这个随机函数。 rand 和 srand这两个是C标准函数，在C++中被放在头文件 &lt;cstdlib&gt; 之中，搜索到的函数声明如下： 123456__BEGIN_NAMESPACE_STD/* Return a random integer between 0 and RAND_MAX inclusive. */extern int rand (void) __THROW;/* Seed the random number generator with the given number. */extern void srand (unsigned int __seed) __THROW;__END_NAMESPACE_STD 其中 std::rand() 是用于返回一个介于[0, RAND_MAX] 范围的伪随机整型值，RAND_MAX 的值最小为 32767，也就是有符号short的最大值，我查到的版本库中的值是2147483647，即有符号int的最大值。 std::srand() 的作用是为 st::rand() 这个伪随机数生成器设置种子，如果在调用 std::srand() 之前使用了 std::rand()，种子默认为1，相当于调用了 std::srand(1)，rand通常不是线程安全的函数，依赖于具体的实现。 另外你可能还见过 random 和 srandom 等函数，他们通常是另一个标准（BSD）的随机函数，比如下面这段描述： 123456789/* These are the functions that actually do things. The `random', `srandom', `initstate' and `setstate' functions are those from BSD Unices. The `rand' and `srand' functions are required by the ANSI standard. We provide both interfaces to the same random number generator. *//* Return a random long integer between 0 and RAND_MAX inclusive. */extern long int random (void) __THROW;/* Seed the random number generator with the given number. */extern void srandom (unsigned int __seed) __THROW; 如果是在 POSIX 平台你可能还会遇到 rand_r(int *seed) 函数。 需要注意的是， std::rand() 生成的是一个伪随机序列，如果随机种子相同，则得到的序列也是相同的，这也是 std::rand 不建议使用的原因，建议是使用C++11随机数生成工具来替换它。 伪随机序列也并不是“一无是处”，两个进程可以通过设置相同的随机数种子来产生相同的序列，比如可以用于服务器和客户端做帧同步时产生随机数，这样的随机数产生是同步可控的。 下面举个 std::rand() 使用的例子 12345678910111213141516#include &lt;iostream&gt;#include &lt;cstdlib&gt;int main()&#123; std::srand(1); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; std::srand(1); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; std::srand(1); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 12345albert@home-pc:/mnt/d/data/cpp/testrandom$ g++ testrandom.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testrandom$ ./a.out180428938318042893831804289383 我们可以看到因为随机种子相同，生成的随机数都是同一个，为了使的生成的序列更随机，通常使用当前时间戳 std::time(nullptr) 作为随机种子，然后再生成随机序列： 1234567891011121314151617#include &lt;iostream&gt;#include &lt;cstdlib&gt;#include &lt;ctime&gt;int main()&#123; std::srand(std::time(nullptr)); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; std::srand(std::time(nullptr)); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; std::srand(std::time(nullptr)); std::cout &lt;&lt; std::rand() &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 12345albert@home-pc:/mnt/d/data/cpp/testrandom$ g++ testrandom.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testrandom$ ./a.out177757254117775725411777572541 怎么还是相同的呢？那是因为 std::time(nullptr) 函数返回的时间戳单位是秒，在一秒中内的时间种子是相同的，所以返回的序列也是相同的，通常的使用方法是在程序启动时设置一次时间种子就可以了，并不需要每次都进行设置，而 random_shuffle 中使用了 std::rand() 函数，如果不手动设置时间种子，每次同一时间洗同一副牌，得到的结果也是相同的，所以这也是random_shuffle被后续版本移除的一个原因。 随机数生成器和分布器random是C++11提供的一个头文件，其中包含多个随机数生成工具，可以使用生成器和分布器的组合产生随机数，其中包含随机数生成器和分布器的多个类实现，分为以下两种： Uniform random bit generators (URBGs)：均匀随机位生成器，也就是生成均匀分布随机数的对象，可以生成伪随机序列，也可生成真正的随机数序列Random number distributions：随机数分布器，用于将URBGs产生的随机数转换为某种特定数学概率分布的序列，如均匀分布、正态分布、泊松分布等 常见的生成器： linear_congruential_engine: 线性同余生成算法，是最常用也是速度最快的，随机效果一般 mersenne_twister_engine: 梅森旋转算法，随机效果最好 subtract_with_carry_engine: 滞后Fibonacci算法 常见的适配器，我理解的它的作用是生成器的二次加工厂，对生成器结果进行特定操作 discard_block_engine: 丢弃一些数 independent_bits_engine: 将序列打包成指定位数的块 shuffle_order_engine: 调整序列顺序 预定义的随机数生成器，利用通用生成器和适配器组合出的流行特定生成器： minstd_rand minstd_rand0 mt19937: mt是因为这个伪随机数产生器基于Mersenne Twister算法，19937来源于产生随的机数的周期长可达到2^19937-1。 mt19937_64 ranlux24_base ranlux48_base ranlux24 ranlux48 knuth_b default_random_engine: 编译器可以自行实现 以上随机数引擎需要一个整型参数作为种子，对于给定的随机数种子，伪随机数生成器总会生成相同的序列，这在测试的时候是相当有用的。而在实际使用时，需要设置随机树作为种子来产出不同的随机数，推荐使用 std::random_device 的值作为随机数种子。 std::random_device 是一个使用硬件熵源的非确定性随机数发生器，不可预测。 常见的分布器： uniform_int_distribution: 均匀离散分布 uniform_real_distribution: 均匀实数分布 bernoulli_distribution: 伯努利分布 binomial_distribution: 二项式分布 geometric_distribution: 几何分布 negative_binomial_distribution: 负二项式分布 poisson_distribution: 泊松分布 exponential_distribution: 指数分布 gamma_distribution: 伽玛分布 weibull_distribution: 威布尔分布 extreme_value_distribution: 极值分配 normal_distribution: 正态分布 lognormal_distribution: 对数正态分布 chi_squared_distribution: 卡方分布 cauchy_distribution: 柯西分布 fisher_f_distribution: Fisher F分布 student_t_distribution: 学生T分布 discrete_distribution: 离散分布 piecewise_constant_distribution: 分段常数分布 piecewise_linear_distribution: 分段线性分布 下面举个生成器和分布器组合生成随机常用例子，以下为模拟掷骰子生成点数的实现： 12345678910111213#include &lt;iostream&gt;#include &lt;random&gt;int main()&#123; std::mt19937 gen(std::random_device&#123;&#125;()); std::uniform_int_distribution&lt;&gt; dist(1, 6); for (int i = 0; i &lt; 10; ++i) std::cout &lt;&lt; dist(gen) &lt;&lt; std::endl; return 0;&#125; 编译运行结果如下： 123456789101112albert@home-pc:/mnt/d/data/cpp/testrandom$ g++ testrandom.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testrandom$ ./a.out3241541134 std::shuffle终于又转回来了，去随机数那一块儿溜了半天，终于回到了洗牌函数，这个函数是C++11版本才加入的，函数定义如下： 12345678910111213141516171819202122232425262728293031323334353637/** * @brief Shuffle the elements of a sequence using a uniform random * number generator. * @ingroup mutating_algorithms * @param __first A forward iterator. * @param __last A forward iterator. * @param __g A UniformRandomNumberGenerator (26.5.1.3). * @return Nothing. * * Reorders the elements in the range @p [__first,__last) using @p __g to * provide random numbers.*/template&lt;typename _RandomAccessIterator, typename _UniformRandomNumberGenerator&gt; void shuffle(_RandomAccessIterator __first, _RandomAccessIterator __last, _UniformRandomNumberGenerator&amp;&amp; __g) &#123; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt; _RandomAccessIterator&gt;) __glibcxx_requires_valid_range(__first, __last); if (__first == __last) return; typedef typename iterator_traits&lt;_RandomAccessIterator&gt;::difference_type _DistanceType; typedef typename std::make_unsigned&lt;_DistanceType&gt;::type __ud_type; typedef typename std::uniform_int_distribution&lt;__ud_type&gt; __distr_type; typedef typename __distr_type::param_type __p_type; __distr_type __d; for (_RandomAccessIterator __i = __first + 1; __i != __last; ++__i) std::iter_swap(__i, __first + __d(__g, __p_type(0, __i - __first))); &#125; 这种实现和之前 std::random_shuffle 函数实现很类似，只是随机数部分有些不同，它的第3个参数需要的是一个均匀随机数生成器URBGs，一个常见的使用方法如下： 1234567891011121314151617#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;iterator&gt;#include &lt;random&gt;#include &lt;vector&gt;int main()&#123; std::vector&lt;int&gt; vec&#123;1, 2, 3, 4, 5, 6&#125;; std::mt19937 gen(std::random_device&#123;&#125;()); std::shuffle(vec.begin(), vec.end(), gen); std::copy(vec.begin(), vec.end(), std::ostream_iterator&lt;int&gt;(std::cout, " ")); return 0;&#125; 编译后运行结果如下： 123albert@home-pc:/mnt/d/data/cpp/testrandom$ g++ testrandom.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testrandom$ ./a.out3 5 1 2 6 4 randint结尾了顺便说一下偶然看到的一个实验性函数 std::experimental::randint，用于生成指定范围内的一个随机数，目前还没有进入标准，不过看起来使用很方便了，后续有可能被纳入标准吧，贴一下 cppreference 上的例子 std::experimental::randint 如下： 12345678#include &lt;iostream&gt;#include &lt;experimental/random&gt;int main()&#123; int random_number = std::experimental::randint(100, 999); std::cout &lt;&lt; "random 3-digit number: " &lt;&lt; random_number &lt;&lt; '\n';&#125; 总结 std::random_shuffle 可以只传递一个待洗牌的区间，函数内会使用默认的 std::rand 函数来完成随机元素的选择，依赖全局状态 std::random_shuffle 也可以传入自定义的随机函数，不过这个函数在C++14表中已经不建议时使用了，在C++17标准中已经被移除 std::shuffle 是C++11标准添加的，也是推荐使用的洗牌函数，它的第三个参数需要传递一个均匀随机数生成器对象 C++11中的&lt;random&gt;头文件中提供了很多生成随机数的工具，需要搭配生成器和分布器来使用 mt19937 名字看起来有点怪，但它是常用的生成器，mt表示它基于Mersenne Twister算法，19937源于产生随的机数的周期长可达到2^19937-1 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当被误解时，解释或者争论都是没有用的，有些事情就解释不清楚，或者根本无法解释，甚至没有人会听你解释，想一想，真的什么也做不了，就像一句名言说的，你永远叫不醒一个装睡的人，那个故意误解你的人又怎会听你解释~ 2022-5-3 21:02:56]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>shuffle</tag>
        <tag>洗牌</tag>
        <tag>随机数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[恶搞一下std::forward函数]]></title>
    <url>%2Fblog%2F2022%2F04%2F20%2F%E6%81%B6%E6%90%9E%E4%B8%80%E4%B8%8Bstd-forward%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言关于 std::forward 的用法在之前的文章 《C++11中std::move和std::forward到底干了啥》已经总结过了，它被称为完美转发函数，用于函数模板中完成参数转发任务，当形参为左值引用时把它转发成左值，而在形参成为右值引用时把它转发成右值，依靠了引用折叠规则和 std::remove_reference 模板。 前段时间看到std::forward的源代码时突然有发现有些疑问，后来弄明白了决定换个花样试一试，不过在“恶搞”这个函数之前，先来看一看使用模板的规则，我们以模板函数为例，看看模板是怎么用的。 函数模板12345template&lt;class T&gt;T Add(T a, T b)&#123; return a + b;&#125; 这是一个非常简单的模板函数，直接传入参数就可以调用这个函数做加法运算，就像下面这样： 12345678910111213141516#include &lt;iostream&gt;template&lt;class T&gt;T Add(T a, T b)&#123; return a + b;&#125;int main(void)&#123; std::cout &lt;&lt; Add(2020, 2) &lt;&lt; std::endl; std::cout &lt;&lt; Add(3.0, 2.1) &lt;&lt; std::endl; std::cout &lt;&lt; Add(std::string("happy"), std::string(" holiday")) &lt;&lt; std::endl; return 0;&#125; 函数的运行结果如下： 20225.1happyholiday 我们在调用模板函数时虽然没有指定模板 T 的类型，但是编译器会自动推导，分别生成以下三个函数： 1234567891011121314int Add(int a, int b)&#123; return a + b;&#125;double Add(double a, double b)&#123; return a + b;&#125;std::string Add(std::string a, std::string b)&#123; return a + b;&#125; 但是当我们采用以下的方式调用函数的时候就会出现编译错误 1std::cout &lt;&lt; Add(3, 2.1) &lt;&lt; std::endl; 123456789101112albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testt.cpp --std=c++11testt.cpp: In function ‘int main()’:testt.cpp:13:28: error: no matching function for call to ‘Add(int, double)’ std::cout &lt;&lt; Add(3, 2.1) &lt;&lt; std::endl; ^testt.cpp:5:3: note: candidate: template&lt;class T&gt; T Add(T, T) T Add(T a, T b) ^testt.cpp:5:3: note: template argument deduction/substitution failed:testt.cpp:13:28: note: deduced conflicting types for parameter ‘T’ (‘int’ and ‘double’) std::cout &lt;&lt; Add(3, 2.1) &lt;&lt; std::endl; ^ 编译器给出的错误很明显，那就是没有匹配 Add(int, double) 的函数生成，这个模板只提供了一个类型参数，遇到这种情况应该怎么办呢？我们知道 int 可以隐式转换成 double 类型，那就让它默认生成一个类型为 double 的模板函数 Add(int, double) 就可以了，所以这种情况下把调用函数写成下面这样就可以成功编译了： 1std::cout &lt;&lt; Add&lt;double&gt;(3, 2.1) &lt;&lt; std::endl; 通过这个例子我们发现有些情况下，这个模板函数的参数类型必须显式传递，接下来我们再来熟悉一下 std::forward 函数。 forwawrd 函数定义先来复习一下函数的定义： 12345678910111213141516171819202122232425/** * @brief Forward an lvalue. * @return The parameter cast to the specified type. * * This function is used to implement "perfect forwarding". */template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp; __t) noexcept &#123; return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125;/** * @brief Forward an rvalue. * @return The parameter cast to the specified type. * * This function is used to implement "perfect forwarding". */template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept &#123; static_assert(!std::is_lvalue_reference&lt;_Tp&gt;::value, "template argument" " substituting _Tp is an lvalue reference type"); return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125; 以上两个模板函数就是用来实现完美转发左值引用和右值引用的，那么你可以试试，当调用第一个函数的时候能不能推导出 _Tp 是什么类型，我之前的疑惑也在这里，这个函数的参数 typename std::remove_reference&lt;_Tp&gt;::type&amp; __t 和模板参数类型 _Tp 看起来关系很密切，但好像又没关系，因为虽然知道参数类型 typename std::remove_reference&lt;_Tp&gt;::type&amp; __t 是个左值引用，但是你并不知道 _Tp 是什么类型，它还是需要显式来指定的，我们接下来试一试和我们想的一不一样。 forwawrd 完美转发直接拿一个之前写过的完美转发例子吧，代码如下： 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;utility&gt;void Print(int&amp; val)&#123; std::cout &lt;&lt; "lvalue refrence: val=" &lt;&lt; val &lt;&lt; std::endl;&#125;void Print(int&amp;&amp; val)&#123; std::cout &lt;&lt; "rvalue refrence: val=" &lt;&lt; val &lt;&lt; std::endl;&#125;template&lt;typename T&gt;void TPrint(T&amp;&amp; t)&#123; return Print(std::forward&lt;T&gt;(t));&#125;int main()&#123; int date = 2022; TPrint(date); TPrint(501); return 0;&#125; 编译运行之后的结果如下： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testf.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.outlvalue refrence: val=2022rvalue refrence: val=501 通过结果我们发现 std::forward 函数在拥有万能引用参数的模板函数中实现了完美转发，左值转发后调用了参数为左值引用的函数，右值转发后调用了参数为右值引用的函数，这时如果我们把调用 std::forward 的地方改一下，去掉指定的参数类型 T，写成 return Print(std::forward(t));，然后编译看看会发生什么 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testf.cpp --std=c++11testf.cpp: In instantiation of ‘void TPrint(T&amp;&amp;) [with T = int&amp;]’:testf.cpp:23:16: required from heretestf.cpp:17:30: error: no matching function for call to ‘forward(int&amp;)’ return Print(std::forward(t)); ^In file included from /usr/include/c++/5/bits/stl_pair.h:59:0, from /usr/include/c++/5/bits/stl_algobase.h:64, from /usr/include/c++/5/bits/char_traits.h:39, from /usr/include/c++/5/ios:40, from /usr/include/c++/5/ostream:38, from /usr/include/c++/5/iostream:39, from testf.cpp:1:/usr/include/c++/5/bits/move.h:76:5: note: candidate: template&lt;class _Tp&gt; constexpr _Tp&amp;&amp; std::forward(typename std::remove_reference&lt;_From&gt;::type&amp;) forward(typename std::remove_reference&lt;_Tp&gt;::type&amp; __t) noexcept ^/usr/include/c++/5/bits/move.h:76:5: note: template argument deduction/substitution failed:testf.cpp:17:30: note: couldn't deduce template parameter ‘_Tp’ return Print(std::forward(t)); ^In file included from /usr/include/c++/5/bits/stl_pair.h:59:0, from /usr/include/c++/5/bits/stl_algobase.h:64, from /usr/include/c++/5/bits/char_traits.h:39, from /usr/include/c++/5/ios:40, from /usr/include/c++/5/ostream:38, from /usr/include/c++/5/iostream:39, from testf.cpp:1:/usr/include/c++/5/bits/move.h:87:5: note: candidate: template&lt;class _Tp&gt; constexpr _Tp&amp;&amp; std::forward(typename std::remove_reference&lt;_From&gt;::type&amp;&amp;) forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept ^/usr/include/c++/5/bits/move.h:87:5: note: template argument deduction/substitution failed:testf.cpp:17:30: note: couldn't deduce template parameter ‘_Tp’ return Print(std::forward(t)); ^testf.cpp:17:33: error: return-statement with a value, in function returning 'void' [-fpermissive] return Print(std::forward(t)); ^testf.cpp: In instantiation of ‘void TPrint(T&amp;&amp;) [with T = int]’:testf.cpp:24:15: required from heretestf.cpp:17:30: error: no matching function for call to ‘forward(int&amp;)’ return Print(std::forward(t)); ^In file included from /usr/include/c++/5/bits/stl_pair.h:59:0, from /usr/include/c++/5/bits/stl_algobase.h:64, from /usr/include/c++/5/bits/char_traits.h:39, from /usr/include/c++/5/ios:40, from /usr/include/c++/5/ostream:38, from /usr/include/c++/5/iostream:39, from testf.cpp:1:/usr/include/c++/5/bits/move.h:76:5: note: candidate: template&lt;class _Tp&gt; constexpr _Tp&amp;&amp; std::forward(typename std::remove_reference&lt;_From&gt;::type&amp;) forward(typename std::remove_reference&lt;_Tp&gt;::type&amp; __t) noexcept ^/usr/include/c++/5/bits/move.h:76:5: note: template argument deduction/substitution failed:testf.cpp:17:30: note: couldn't deduce template parameter ‘_Tp’ return Print(std::forward(t)); ^In file included from /usr/include/c++/5/bits/stl_pair.h:59:0, from /usr/include/c++/5/bits/stl_algobase.h:64, from /usr/include/c++/5/bits/char_traits.h:39, from /usr/include/c++/5/ios:40, from /usr/include/c++/5/ostream:38, from /usr/include/c++/5/iostream:39, from testf.cpp:1:/usr/include/c++/5/bits/move.h:87:5: note: candidate: template&lt;class _Tp&gt; constexpr _Tp&amp;&amp; std::forward(typename std::remove_reference&lt;_From&gt;::type&amp;&amp;) forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept ^/usr/include/c++/5/bits/move.h:87:5: note: template argument deduction/substitution failed:testf.cpp:17:30: note: couldn't deduce template parameter ‘_Tp’ return Print(std::forward(t)); ^testf.cpp:17:33: error: return-statement with a value, in function returning 'void' [-fpermissive] return Print(std::forward(t)); 这次出了一个很长的编译错误，看来还是需要指定类型的，既然是需要指定的，那我们指定成其他的有没有问题呢？比如写成下面这样： 1return Print(std::forward&lt;float&gt;(t)); 编译运行结果如下，都变成了右值引用： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testf.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.outrvalue refrence: val=2022rvalue refrence: val=501 再改成下面这样： 1return Print(std::forward&lt;T&amp;&gt;(t)); 编译运行结果如下，都变成了左值引用： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testf.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.outlvalue refrence: val=2022lvalue refrence: val=501 完美转发失效上面的这两个例子能说明完美转发失效了吗？这倒也不能说明，第一个例子全都转发成了右值引用，第二个例子全部转发成了左值引用，和我们指定的类型是一致的，也算实现了完美转发，只不过通过这些例子更加深入的理解了完美转发的含义，就是能保证转化成指定的类型，如果指定的类型是个万能引用，就会根据原始类型来完成转发，本次探索之旅到此也就结束了，解答疑惑是个有趣的事情。 总结 std::forward 的本质还是进行强制类型转换，它会把传入的参数转发成指定的类型 完美转发其实是可以脱离左值右值概念的，这也是对完美转发更加深入的理解吧 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 每一点付出终有回报，每一滴汗水从不会白流，可能你看不见也摸不着，但其实它已经悄然声息的改变了你，改变了你周围的点点滴滴~ 2022-5-1 22:58:21]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>forward</tag>
        <tag>完美转发</tag>
        <tag>funny</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++可变参数模板的展开方式]]></title>
    <url>%2Fblog%2F2022%2F04%2F04%2FC-%E5%8F%AF%E5%8F%98%E5%8F%82%E6%95%B0%E6%A8%A1%E6%9D%BF%E7%9A%84%E5%B1%95%E5%BC%80%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言可变参数模板（variadic templates）是C++11新增的强大的特性之一，它对模板参数进行了高度泛化，能表示0到任意个数、任意类型的参数。相比C++98/03这些类模版和函数模版中只能含固定数量模版参数的“老古董”，可变模版参数无疑是一个巨大的进步。 如果是刚接触可变参数模板可能会觉得比较抽象，使用起来会不太顺手，使用可变参数模板时通常离不开模板参数的展开，所以本文来列举一些常用的模板展开方式，帮助我们来对可变参数模板有一个初步的了解。 可变参数模板的定义可变参数模板和普通模板的定义类似，在写法上需要在 typename 或 class 后面带上省略号...，以下为一个常见的可变参数函数模板： 12345template &lt;class... T&gt;void func(T... args)&#123; //...&#125; 上面这个函数模板的参数 args 前面有省略号，所以它就是一个被称为模板参数包（template parameter pack）的可变模版参数，它里面包含了0到N个模版参数，而我们是无法直接获取 args 中的每个参数的，只能通过展开参数包的方式来获取参数包中的每个参数，这也是本文要重点总结的内容。 参数包的展开参数包展开的方式随着c++语言的发展也在与时俱进，我们以实现一个可变参格式化打印函数为例，列举一些常用的方式： 递归函数方式展开1234567891011121314151617181920#include &lt;iostream&gt;void FormatPrint()&#123; std::cout &lt;&lt; std::endl;&#125;template &lt;class T, class ...Args&gt;void FormatPrint(T first, Args... args)&#123; std::cout &lt;&lt; "[" &lt;&lt; first &lt;&lt; "]"; FormatPrint(args...);&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 这种递归展开的方式与递归函数的定义是一样的，需要递归出口和不断调用自身，仔细看看这个函数模板是不是都满足啦？递归出口就是这个无模板参数的 FormatPrint，并且在有参模板中一直在调用自身，递归调用的过程时这样的 FormatPrint(4,3,2,1) -&gt; FormatPrint(3,2,1) -&gt; FormatPrint(2,1) -&gt; FormatPrint(1) -&gt; FormatPrint()，输出内容如下： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out[1][2][3][4][good][2][hello][4][110] 逗号表达式展开123456789101112131415#include &lt;iostream&gt;template &lt;class ...Args&gt;void FormatPrint(Args... args)&#123; (void)std::initializer_list&lt;int&gt;&#123; (std::cout &lt;&lt; "[" &lt;&lt; args &lt;&lt; "]", 0)... &#125;; std::cout &lt;&lt; std::endl;&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 这种方式用到了C++11的新特性初始化列表（Initializer lists）以及很传统的逗号表达式，我们知道逗号表达式的优先级最低，(a, b) 这个表达式的值就是 b，那么上述代码中(std::cout &lt;&lt; &quot;[&quot; &lt;&lt; args &lt;&lt; &quot;]&quot;, 0)这个表达式的值就是0，初始化列表保证其中的内容从左往右执行，args参数包会被逐步展开，表达式前的(void)是为了防止变量未使用的警告，运行过后我们就得到了一个N个元素为0的初始化列表，内容也被格式化输出了： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out[1][2][3][4][good][2][hello][4][110] 说到这顺便提一下，可以使用sizeof...(args)得到参数包中参数个数。 enable_if方式展开12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;tuple&gt;#include &lt;type_traits&gt;template&lt;std::size_t k = 0, typename tup&gt;typename std::enable_if&lt;k == std::tuple_size&lt;tup&gt;::value&gt;::type FormatTuple(const tup&amp; t)&#123; std::cout &lt;&lt; std::endl;&#125;template&lt;std::size_t k = 0, typename tup&gt;typename std::enable_if&lt;k &lt; std::tuple_size&lt;tup&gt;::value&gt;::type FormatTuple(const tup&amp; t)&#123; std::cout &lt;&lt; "[" &lt;&lt; std::get&lt;k&gt;(t) &lt;&lt; "]"; FormatTuple&lt;k + 1&gt;(t);&#125;template&lt;typename... Args&gt;void FormatPrint(Args... args)&#123; FormatTuple(std::make_tuple(args...));&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; C++11的enable_if常用于构建需要根据不同的类型的条件实例化不同模板的时候。顾名思义，当满足条件时类型有效。可作为选择类型的小工具，其广泛的应用在 C++ 的模板元编程（meta programming）之中，利用的就是SFINAE原则，英文全称为Substitution failure is not an error，意思就是匹配失败不是错误，假如有一个特化会导致编译时错误，只要还有别的选择，那么就无视这个特化错误而去选择另外的实现，这里的特化概念不再展开，感兴趣可以自行了解，后续可以单独总结一下。 在上面的代码实现中，基本思路是先将可变模版参数转换为std::tuple，然后通过递增参数的索引来选择恰当的FormatTuple函数，当参数的索引小于tuple元素个数时，会不断取出当前索引位置的参数并输出，当参数索引等于总的参数个数时调用另一个模板重载函数终止递归，编译运行输入以下内容： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++11albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out[1][2][3][4][good][2][hello][4][110] 折叠表达式展开（c++17）1234567891011121314#include &lt;iostream&gt;template&lt;typename... Args&gt;void FormatPrint(Args... args)&#123; (std::cout &lt;&lt; ... &lt;&lt; args) &lt;&lt; std::endl;&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 折叠表达式（Fold Expressions）是C++17新引进的语法特性，使用折叠表达式可以简化对C++11中引入的参数包的处理，可以在某些情况下避免使用递归，更加方便的展开参数，如上述代码中展示的这样可以方便的展开参数包，不过输出的内容和之前的有些不一样： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++17albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out1234good2hello4110 对比结果发现缺少了格式化的信息，需要以辅助函数的方式来格式化： 123456789101112131415161718192021#include &lt;iostream&gt;template&lt;typename T&gt;string format(T t) &#123; std::stringstream ss; ss &lt;&lt; "[" &lt;&lt; t &lt;&lt; "]"; return ss.str();&#125;template&lt;typename... Args&gt;void FormatPrint(Args... args)&#123; (std::cout &lt;&lt; ... &lt;&lt; format(args)) &lt;&lt; std::endl;&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 这次格式化内容就被加进来了： 1234albert@home-pc:/mnt/d/data/cpp/testtemplate$ g++ testtemplate.cpp --std=c++17albert@home-pc:/mnt/d/data/cpp/testtemplate$ ./a.out[1][2][3][4][good][2][hello][4][110] 这样好像还是有点麻烦，我们可以把折叠表达式和逗号表达式组合使用，这样得到的代码就简单多啦，也能完成格式化输出的任务： 1234567891011121314#include &lt;iostream&gt;template&lt;typename... Args&gt;void FormatPrint(Args... args)&#123; (std::cout &lt;&lt; ... &lt;&lt; (std::cout &lt;&lt; "[" &lt;&lt; args, "]")) &lt;&lt; std::endl;&#125;int main(void)&#123; FormatPrint(1, 2, 3, 4); FormatPrint("good", 2, "hello", 4, 110); return 0;&#125; 总结 Variadic templates 是C++11新增的强大的特性之一，它对模板参数进行了高度泛化 Initializer lists 是C++11新加的特性，可以作为函数参数和返回值，长度不受限制比较方便 Fold Expressions 是C++17新引进的语法特性，可以方便的展开可变参数模板的参数包 可变参数模板的参数包在C++11的环境下，可以利用递归、逗号表达式、enable_if等方式进行展开 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有些人苦中作乐，而有些人却是身在福中不知福。人性本贪婪，只是度不同。我虽知福，奈何要想一家安稳还差的太多~ 2022-4-5 22:02:27]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>可变参数</tag>
        <tag>template</tag>
        <tag>模板</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[epoll的LT模式（水平触发）和ET模式（边沿触发）]]></title>
    <url>%2Fblog%2F2022%2F03%2F27%2Fepoll%E7%9A%84LT%E6%A8%A1%E5%BC%8F%EF%BC%88%E6%B0%B4%E5%B9%B3%E8%A7%A6%E5%8F%91%EF%BC%89%E5%92%8CET%E6%A8%A1%E5%BC%8F%EF%BC%88%E8%BE%B9%E6%B2%BF%E8%A7%A6%E5%8F%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言epoll的触发模式是个引发讨论非常多的话题，网络上这方面总结的文章也很多，首先从名字上就不是很统一，LT模式常被称为水平触发、电平触发、条件触发，而ET模式常被称为边缘触发、边沿触发等，这些都是从英文翻译过来的，只不过翻译的时候有些差异，LT全称 level-triggered，ET全称 edge-triggered。 虽然这个知识点热度很高，但很多人对于它的理解总是差那么一点，特别是在面试的时候，很多面试者总是处于一种回忆和背诵的状态，其实这两种模式真的不需要去死记硬背，下面说说我个人对这两种模式的理解和记忆方法。 名称的记忆每次提到ET（边沿触发）首先映入我脑海的是大学里《数字逻辑电路》这门课程，里面会提到低电平、高电平，当电平从低到高时会有一个上升沿，而电平从高到低时会有一个下降沿，这个“沿”就是边沿触发时提到的“边沿”，跟马路边的马路牙子是同一种概念，也就是指状态变化的时候。提起上升沿和下降沿我还是印象很深的，当时我可是占用了好几节课的时间用Verilog语言写了一个显示“HELLO WORLD”的仿真波形，依靠的就是电平变化中的“沿”。 状态变化LT模式和ET模式可以类比电平变化来学习，但是在实际应用中概念却不是完全一样的，在epoll的应用中涉及到关于IO的读写，而读写的状态变化有哪些呢？可读、不可读、可写、不可写，其实就是这四种状态而已，以socket为例。 可读：socket上有数据不可读：socket上没有数据了可写：socket上有空间可写不可写：socket上无空间可写 对于水平触发模式，一个事件只要有，就会一直触发。对于边缘触发模式，只有一个事件从无到有才会触发。 LT模式对于读事件 EPOLLIN，只要socket上有未读完的数据，EPOLLIN 就会一直触发；对于写事件 EPOLLOUT，只要socket可写（一说指的是 TCP 窗口一直不饱和，我觉得是TCP缓冲区未满时，这一点还需验证），EPOLLOUT 就会一直触发。 在这种模式下，大家会认为读数据会简单一些，因为即使数据没有读完，那么下次调用epoll_wait()时，它还会通知你在上没读完的文件描述符上继续读，也就是人们常说的这种模式不用担心会丢失数据。 而写数据时，因为使用 LT 模式会一直触发 EPOLLOUT 事件，那么如果代码实现依赖于可写事件触发去发送数据，一定要在数据发送完之后移除检测可写事件，避免没有数据发送时无意义的触发。 ET模式对于读事件 EPOLLIN，只有socket上的数据从无到有，EPOLLIN 才会触发；对于写事件 EPOLLOUT，只有在socket写缓冲区从不可写变为可写，EPOLLOUT 才会触发（刚刚添加事件完成调用epoll_wait时或者缓冲区从满到不满） 这种模式听起来清爽了很多，只有状态变化时才会通知，通知的次数少了自然也会引发一些问题，比如触发读事件后必须把数据收取干净，因为你不一定有下一次机会再收取数据了，即使不采用一次读取干净的方式，也要把这个激活状态记下来，后续接着处理，否则如果数据残留到下一次消息来到时就会造成延迟现象。 这种模式下写事件触发后，后续就不会再触发了，如果还需要下一次的写事件触发来驱动发送数据，就需要再次注册一次检测可写事件。 数据的读取和发送关于数据的读比较好理解，无论是LT模式还是ET模式，监听到读事件从socket开始读数据就好了，只不过读的逻辑有些差异，LT模式下，读事件触发后，可以按需收取想要的字节数，不用把本次接收到的数据收取干净，ET模式下，读事件触发后通常需要数据一次性收取干净。 而数据的写不太容易理解，因为数据的读是对端发来数据导致的，而数据的写其实是自己的逻辑层触发的，所以在通过网络发数据时通常都不会去注册监可写事件，一般都是调用 send 或者 write 函数直接发送，如果发送过程中， 函数返回 -1，并且错误码是 EWOULDBLOCK 表明发送失败，此时才会注册监听可写事件，并将剩余的服务存入自定义的发送缓冲区中，等可写事件触发后再接着将发送缓冲区中剩余的数据发送出去。 代码实践基础代码以下为一个epoll触发模式测试的基础代码，也不算太长，直接拿来就可以测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158#include &lt;sys/socket.h&gt; //for socket#include &lt;arpa/inet.h&gt; //for htonl htons#include &lt;sys/epoll.h&gt; //for epoll_ctl#include &lt;unistd.h&gt; //for close#include &lt;fcntl.h&gt; //for fcntl#include &lt;errno.h&gt; //for errno#include &lt;iostream&gt; //for coutclass fd_object&#123;public: fd_object(int fd) &#123; listen_fd = fd; &#125; ~fd_object() &#123; close(listen_fd); &#125;private: int listen_fd;&#125;;/*./epoll for lt modeand./epoll 1 for et mode*/int main(int argc, char* argv[])&#123; //create a socket fd int listen_fd = socket(AF_INET, SOCK_STREAM, 0); if (listen_fd == -1) &#123; std::cout &lt;&lt; "create listen socket fd error." &lt;&lt; std::endl; return -1; &#125; fd_object obj(listen_fd); //set socket to non-block int socket_flag = fcntl(listen_fd, F_GETFL, 0); socket_flag |= O_NONBLOCK; if (fcntl(listen_fd, F_SETFL, socket_flag) == -1) &#123; std::cout &lt;&lt; "set listen fd to nonblock error." &lt;&lt; std::endl; return -1; &#125; //init server bind info int port = 51741; struct sockaddr_in bind_addr; bind_addr.sin_family = AF_INET; bind_addr.sin_addr.s_addr = htonl(INADDR_ANY); bind_addr.sin_port = htons(port); if (bind(listen_fd, (struct sockaddr *)&amp;bind_addr, sizeof(bind_addr)) == -1) &#123; std::cout &lt;&lt; "bind listen socket fd error." &lt;&lt; std::endl; return -1; &#125; //start listen if (listen(listen_fd, SOMAXCONN) == -1) &#123; std::cout &lt;&lt; "listen error." &lt;&lt; std::endl; return -1; &#125; else std::cout &lt;&lt; "start server at port [" &lt;&lt; port &lt;&lt; "] with [" &lt;&lt; (argc &lt;= 1 ? "LT" : "ET") &lt;&lt; "] mode." &lt;&lt; std::endl; //create a epoll fd int epoll_fd = epoll_create(88); if (epoll_fd == -1) &#123; std::cout &lt;&lt; "create a epoll fd error." &lt;&lt; std::endl; return -1; &#125; epoll_event listen_fd_event; listen_fd_event.data.fd = listen_fd; listen_fd_event.events = EPOLLIN; if (argc &gt; 1) listen_fd_event.events |= EPOLLET; //add epoll event for listen fd if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_fd, &amp;listen_fd_event) == -1) &#123; std::cout &lt;&lt; "epoll ctl error." &lt;&lt; std::endl; return -1; &#125; while (true) &#123; epoll_event epoll_events[1024]; int n = epoll_wait(epoll_fd, epoll_events, 1024, 1000); if (n &lt; 0) break; else if (n == 0) //timeout continue; for (int i = 0; i &lt; n; ++i) &#123; if (epoll_events[i].events &amp; EPOLLIN)//trigger read event &#123; if (epoll_events[i].data.fd == listen_fd) &#123; //accept a new connection struct sockaddr_in client_addr; socklen_t client_addr_len = sizeof(client_addr); int client_fd = accept(listen_fd, (struct sockaddr*)&amp;client_addr, &amp;client_addr_len); if (client_fd == -1) continue; socket_flag = fcntl(client_fd, F_GETFL, 0); socket_flag |= O_NONBLOCK; if (fcntl(client_fd, F_SETFL, socket_flag) == -1) &#123; close(client_fd); std::cout &lt;&lt; "set client fd to non-block error." &lt;&lt; std::endl; continue; &#125; epoll_event client_fd_event; client_fd_event.data.fd = client_fd; client_fd_event.events = EPOLLIN | EPOLLOUT; if (argc &gt; 1) client_fd_event.events |= EPOLLET; if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &amp;client_fd_event) == -1) &#123; std::cout &lt;&lt; "add client fd to epoll fd error." &lt;&lt; std::endl; close(client_fd); continue; &#125; std::cout &lt;&lt; "accept a new client fd [" &lt;&lt; client_fd &lt;&lt; "]." &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; "EPOLLIN event triggered for client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "]." &lt;&lt; std::endl; char recvbuf[1024] = &#123; 0 &#125;; int m = recv(epoll_events[i].data.fd, recvbuf, 1, 0); // only read 1 bytes when read event triggered if (m == 0 || (m &lt; 0 &amp;&amp; errno != EWOULDBLOCK &amp;&amp; errno != EINTR)) &#123; if (epoll_ctl(epoll_fd, EPOLL_CTL_DEL, epoll_events[i].data.fd, NULL) != -1) std::cout &lt;&lt; "the client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] disconnected." &lt;&lt; std::endl; close(epoll_events[i].data.fd); &#125; std::cout &lt;&lt; "recv data from client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] and data is [" &lt;&lt; recvbuf &lt;&lt; "]." &lt;&lt; std::endl; &#125; &#125; else if (epoll_events[i].events &amp; EPOLLOUT) &#123; if (epoll_events[i].data.fd == listen_fd) //trigger write event continue; std::cout &lt;&lt; "EPOLLOUT event triggered for client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "]." &lt;&lt; std::endl; &#125; &#125; &#125; return 0;&#125; 简单说下这段代码的测试方法，可以使用 g++ testepoll.cpp -o epoll 进行编译，编译后通过 ./epoll 运行为LT模式，通过 ./epoll et模式运行为ET模式，我们用编译好的epoll程序作为服务器，使用nc命令来模拟一个客户端。 测试分类 编译后直接./epoll，然后在另一个命令行窗口用 nc -v 127.0.0.1 51741 命令模拟一次连接，此时 ./epoll 会产生大量的 EPOLLOUT event triggered for client fd ...，那是因为在LT模式下，EPOLLOUT会被一直触发。 123456789101112albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epollstart server at port [51741] with [LT] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLOUT event triggered for client fd [5].... 注释包含 EPOLLOUT event triggered for client fd 输出内容的第152行代码，编译后 ./epoll运行，然后在另一个命令行窗口用 nc -v 127.0.0.1 51741 模拟一次连接后，输入abcd回车，可以看到服务器./epoll输出内容，EPOLLIN被触发多次，每次读取一个字节。 1234567891011121314albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epollstart server at port [51741] with [LT] mode.accept a new client fd [5].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [a].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [b].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [c].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [d].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is []. 还原刚才注释的那行代码，编译后执行 ./epoll et 启动服务器，然后在另一个命令行窗口用 nc -v 127.0.0.1 51741 模拟一次连接后，然后在另一个命令行窗口用 nc -v 127.0.0.1 51741 模拟一次连接，服务器窗口显示触发了EPOLLOUT事件 1234albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epoll etstart server at port [51741] with [ET] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5]. 在此基础上，从刚刚运行 nc命令的窗口中输入回车、输入回车、输出回车，那么epoll服务器窗口看到的是触发了三次EPOLLIN事件，每次收到一个回车: 12345678910111213albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epoll etstart server at port [51741] with [ET] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is []. 但是如果在nc模拟的客户端里输出abcd回车，那么在epoll服务器窗口触发一次EPOLLIN事件接收到一个a之后便再也不会触发EPOLLIN了，即使你在nc客户端在此输入也没有用，那是因为在接受的缓冲区中一直还有数据，新数据来时没有出现缓冲区从空到有数据的情况，所以在ET模式下也注意这种情况。 123456789101112131415albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epoll etstart server at port [51741] with [ET] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [a]. 怎么解决ET触发了一次就不再触发了改代码呗，ET模式在连接后触发一次EPOLLOUT，接收到数据时触发一次EPOLLIN，如果数据没收完，以后这两个事件就再也不会被触发了，要想改变这种情况可以再次注册一下这两个事件，时机可以选择接收到数据的时候，所以可以修改这部分代码： 12345678910111213141516else&#123; std::cout &lt;&lt; "EPOLLIN event triggered for client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "]." &lt;&lt; std::endl; char recvbuf[1024] = &#123; 0 &#125;; int m = recv(epoll_events[i].data.fd, recvbuf, 1, 0); // only read 1 bytes when read event triggered if (m == 0 || (m &lt; 0 &amp;&amp; errno != EWOULDBLOCK &amp;&amp; errno != EINTR)) &#123; if (epoll_ctl(epoll_fd, EPOLL_CTL_DEL, epoll_events[i].data.fd, NULL) != -1) std::cout &lt;&lt; "the client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] disconnected." &lt;&lt; std::endl; close(epoll_events[i].data.fd); &#125; std::cout &lt;&lt; "recv data from client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] and data is [" &lt;&lt; recvbuf &lt;&lt; "]." &lt;&lt; std::endl;&#125; 添加再次注册的逻辑： 1234567891011121314151617181920212223else&#123; std::cout &lt;&lt; "EPOLLIN event triggered for client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "]." &lt;&lt; std::endl; char recvbuf[1024] = &#123; 0 &#125;; int m = recv(epoll_events[i].data.fd, recvbuf, 1, 0); // only read 1 bytes when read event triggered if (m == 0 || (m &lt; 0 &amp;&amp; errno != EWOULDBLOCK &amp;&amp; errno != EINTR)) &#123; if (epoll_ctl(epoll_fd, EPOLL_CTL_DEL, epoll_events[i].data.fd, NULL) != -1) std::cout &lt;&lt; "the client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] disconnected." &lt;&lt; std::endl; close(epoll_events[i].data.fd); &#125; epoll_event client_fd_event; client_fd_event.data.fd = epoll_events[i].data.fd; client_fd_event.events = EPOLLIN | EPOLLOUT; if (argc &gt; 1) client_fd_event.events |= EPOLLET; epoll_ctl(epoll_fd, EPOLL_CTL_MOD, epoll_events[i].data.fd, &amp;client_fd_event); std::cout &lt;&lt; "recv data from client fd [" &lt;&lt; epoll_events[i].data.fd &lt;&lt; "] and data is [" &lt;&lt; recvbuf &lt;&lt; "]." &lt;&lt; std::endl;&#125; 这次以 ./epoll et 方式启动服务器，使用 nc -v 127.0.0.1 51741 模拟客户端，输入abc回车发现，epoll服务器输出显示触发的事件变了： 1234567891011121314albert@home-pc:/mnt/d/data/cpp/testepoll$ ./epoll etstart server at port [51741] with [ET] mode.accept a new client fd [5].EPOLLOUT event triggered for client fd [5].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [a].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [b].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [c].EPOLLIN event triggered for client fd [5].recv data from client fd [5] and data is [].EPOLLOUT event triggered for client fd [5]. 总结 LT模式会一直触发EPOLLOUT，当缓冲区有数据时会一直触发EPOLLIN ET模式会在连接建立后触发一次EPOLLOUT，当收到数据时会触发一次EPOLLIN LT模式触发EPOLLIN时可以按需读取数据，残留了数据还会再次通知读取 ET模式触发EPOLLIN时必须把数据读取完，否则即使来了新的数据也不会再次通知了 LT模式的EPOLLOUT会一直触发，所以发送完数据记得删除，否则会产生大量不必要的通知 ET模式的EPOLLOUT事件若数据未发送完需再次注册，否则不会再有发送的机会 通常发送网络数据时不会依赖EPOLLOUT事件，只有在缓冲区满发送失败时会注册这个事件，期待被通知后再次发送 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 即使是在灿烂的阳光下也会有黑暗的角落，不能因为角落的阴暗就忽略阳光下的美好，我们要做的不是把黑暗面放大，而是要做阳光的传递者，哪怕是一面面镜子，通过反射来照亮那星星点点的黑暗，认清自己，不与黑暗为伍，那绝不是你自甘堕落的借口。 两千光束已然出发~ 2022-4-4 18:25:50]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>epoll</tag>
        <tag>LT</tag>
        <tag>ET</tag>
        <tag>水平触发</tag>
        <tag>边沿触发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结下各种常见树形结构的定义及特点（二叉树、AVL树、红黑树、Trie树、B树、B+树）]]></title>
    <url>%2Fblog%2F2022%2F03%2F13%2F%E6%80%BB%E7%BB%93%E4%B8%8B%E5%90%84%E7%A7%8D%E5%B8%B8%E8%A7%81%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E7%9A%84%E5%AE%9A%E4%B9%89%E5%8F%8A%E7%89%B9%E7%82%B9%2F</url>
    <content type="text"><![CDATA[前言树形结构相比于数组、链表、队列和栈等线性结构要复杂的多，因为树本身的概念就比较多，通过设定一些条件和限制就可以定义出一种新类型的树，结果造成了树的“变化多端”，所以要学习一种树要从树的定义入手，然后根据定义和特点来熟悉各种树适合的场景，这样就可以做到“树尽其用”目的了。 一棵普通的树树形结构和现实中的树很像，只不过现实中的树根长在地上，而树形结构再展示的时候一般把树根画在“天上”，树形结构中数据元素之间存在着“一对多”的关系，具有以下特点： 没有父节点的节点称为根节点 除空树外每棵树只有一个根节点 每个节点都只有有限个子节点或无子节点 每个非根节点有且只有一个父节点 树里面没有环路，如果从一个节点出发，除非往返，否则无法回到起点 相关术语 根节点：最顶层的节点就是根结点，它是整棵树的源头 叶子节点：在树下端的节点，就是其子节点个数为0的节点 节点的度：指定节点有几个分叉就说这个节点的度是几 树的度：只看根结点，树的度等价于根节点的度 节点高度：指从这个节点到叶子节点的距离（一共经历了几个节点） 节点深度：指从这个节点到根节点的距离（一共经历了几个节点） 树的高度：指所有节点高度的最大值 树的深度：指所有节点深度的最大值 节点的层：从根节点开始，假设根节点为第1层，根节点的子节点为第2层，依此类推 二叉树二叉树是对普通树形结构进行限定得到的一种特殊的树，规定树中节点的度不大于2，当节点有两个子节点，也就是有两颗子树时，它们有左右之分，分别被称为左子树和右子树，左子树和右子树又同样都是二叉树。 二叉树性质 二叉树的第i层上至多有2^(i-1)（i≥1）个节点 深度为h的二叉树中至多含有2^h-1个节点 若在任意一棵二叉树中，有n个叶子节点，有m个度为2的节点，则必有n=m+1 具有n个节点的满二叉树深为log(2n+1) 若对一棵有n个节点的完全二叉树进行顺序编号（1≤i≤n），那么，对于编号为i（i≥1）的节点 当i=1时，该节点为根，它无双亲节点 当i&gt;1时，该节点的双亲节点的编号为i/2 若2i≤n，则有编号为2i的左节点，否则没有左节点 若2i+1≤n，则有编号为2i+1的右节点，否则没有右节点 二叉树特例完美二叉树（Perfect Binary Tree）：除了叶子结点之外的每一个结点都有两个孩子，每一层都被完全填充 完全二叉树（Complete Binary Tree）：除了最后一层之外的其他每一层都被完全填充，并且所有结点都保持向左对齐 完满二叉树（Full Binary Tree）： 除了叶子结点之外的每一个结点都有两个孩子结点 二叉查找树二叉查找树是一种特殊的二叉树，又称为排序二叉树、二叉搜索树、二叉排序树等等，它实际上是数据域有序的二叉树，即对树上的每个结点，都满足其左子树上所有结点的数据域均小于或等于根结点的数据域，右子树上所有结点的数据域均大于根结点的数据域。 AVL树平衡二叉树是由前苏联的两位数学家G.M.Adelse-Velskil和E.M.Landis联合提出，因此一般也称作AVL树，AVL树本质还是一棵二叉查找树，只是在其基础上增加了“平衡”的要求，需保证其左子树与右子树的高度之差的绝对值不超过1，其中左子树与右子树的高度因子之差称为平衡因子。 对于AVL树，不管我们是执行插入还是删除操作，只要不满足上面的条件，就要通过旋转来保持平衡。由于旋转比较耗时，所以AVL树适合用于插入与删除次数比较少，但查找多的情况。 特点及应用所有节点的左右子树高度差不超过1，广泛用于Windows NT内核中 红黑树红黑树也是一颗二叉查找树，需要为每个节点存储节点的颜色，可以是红或黑。通过对任何一条从根到叶子的路径上各个节点着色的方式的限制，来确保没有一条路径会比其它路径长出两倍，因此，红黑树是一种弱平衡二叉树。 由于是弱平衡二叉树，那么在相同的节点情况下，AVL树的高度小于等于红黑树的高度，相对于要求严格的AVL树来说，它的旋转次数少，所以对于插入，删除操作较多的情况下，用红黑树的查找效率会更高一些。 特点 每个节点非红即黑 根节点是黑的 每个叶子节点（叶子节点即树尾端NULL节点）都是黑的 每条路径都包含相同的黑节点 如果一个节点是红的，那么它的两儿子都是黑的 对于任意节点而言，其到叶子点的每条路径都包含相同数目的黑节点 应用 广泛用于C++的STL中，如 map 和 set 是用红黑树实现的 Linux的进程调度用红黑树管理进程控制块，进程的虚拟内存空间都存储在一颗红黑树上，每个虚拟内存空间都对应红黑树的一个节点 IO多路复用的 epoll 采用红黑树组织管理socket fd，以支持快速的增删改查 Nginx中用红黑树管理定时器，可以快速得到距离当前最小的定时器 Java的TreeMap的用红黑树实现 Trie树Trie树又被称为前缀树、字典树是一种用于快速检索的多叉树结构。字典树把字符串看成字符序列，根据字符串中字符序列的先后顺序构造从上到下的树结构，树结构中的每一条边都对应着一个字符。字典树上存储的字符串被视为从根节点到某个节点之间的一条路径，并在终点节点上做个标记”该节点对应词语的结尾”，正因为有终点节点的存在，字典树不仅可以实现简单的存储字符串，还可以实现字符串的映射，只需要将相对应的值悬挂在终点节点上即可。 特点及应用Trie的核心思想是空间换时间，有如下基本性质： 根节点不包含字符，除根节点外每一个节点都只包含一个字符 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串 每个节点的所有子节点包含的字符都不相同 字典树能够利用字符串中的公共前缀，这样可能会节省内存，利用字符串的公共前缀可以减少查询字符串的时间，能够最大限度的减少无谓的字符串比较，同时在查询的过程中不需要预知待查询字符串的长度，沿着字典树的边进行匹配，查询效率比较高，但是如果系统中存在大量字符串并且这些字符串基本没有前缀，相应的字典树内存消耗也会很大。正是由于字典树的这些特点，字典树被用于统计、排序和保存大量的字符串（不仅限于字符串），还可用于搜索引擎的关键词提示功能。 B树B树是一个多路平衡查找树，B树的出现是为了弥合不同的存储级别之间的访问速度上的巨大差异，实现高效的I/O。平衡二叉树的查找效率是非常高的，并可以通过降低树的深度来提高查找的效率。但是当数据量非常大，树的存储的元素数量是有限的，这样会导致二叉查找树结构由于树的深度过大而造成磁盘I/O读写过于频繁，进而导致查询效率低下，同时数据量过大会导致内存空间不够容纳平衡二叉树所有结点的情况，而B树是解决这个问题的很好的结构。 要想了解B树需要了解一个很重要的概念，B树中所有节点的度的最大值称为B树的阶，记为m，这是一个跟重要值，也就是说m阶B树指的是节点度最大为m的B树。 定义及特点 每个节点最多只有m个子节点 根结点的儿子数为[2, m] 除根结点以外的非叶子结点的儿子数为[m/2, m]，向上取整 非叶子结点的关键字个数=子节点数-1 所有叶子都出现在同一层 k个关键字把节点拆成k+1段，分别指向k+1个儿子，同时满足查找树的大小关系 非叶子节点中不仅包含索引，也会包含数据 应用B树是一种平衡的多路查找树，主要用作文件的索引。其优势是当你要查找的值恰好处在一个非叶子节点时，查找到该节点就会成功并结束查询，有很多基于频率的搜索是选用B树，越频繁查询的结点越往根上走，前提是需要对查询做统计，而且要对key做一些变化。 B+树B+树是b树的一种变体，查询性能更好，m阶的b+树具有以下特征： 有n棵子树的非叶子结点中含有n个关键字（b树是n-1个），这些关键字不保存数据，只用来索引，所有数据都保存在叶子节点（b树是每个关键字都保存数据） 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接 所有的非叶子结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字 通常在b+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点 同一个数字会在不同节点中重复出现，根节点的最大元素就是b+树的最大元素 B+树的优势及应用 B+tree的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多，相对来说IO读写次数也就降低了。 由于非叶子结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 B+树支持范围遍历，只要遍历叶子节点就可以实现整棵树的遍历，而在数据库中基于范围的查询是非常频繁的，这一点要明显由于B树。 由于拥有以上特点，B+广泛应用于文件存储系统以及数据库系统中。 总结 树是一种常见的非线性结构，拥有众多变种 二叉树是树形结构的一大类，每个节点最多拥有两个子节点树，左右子树顺序固定 AVL树是平衡二叉树，任意节点的左右子树高度差最大为1 红黑树是弱平衡二叉树，每个节点记录的自己的颜色，用来控制左右子树高度不大于2倍 Trie树又叫字典树，是一种用于快速检索的多叉树结构 B树是一种多路平衡树，用于提高了磁盘IO性能，多用于文件系统的索引 B+树是对B树的改进，仅在叶子节点存储数据，相比于B树更加矮胖，支持范围遍历 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 世间从来没有什么『感同身受』，每个人面对相同的事件和意外都会因为家庭背景、个人经历的差异而有不同的反应，更不要说那些没经历过的人，即使你曾经真的经历过类似的事情，那么在被漫长的时间洗礼之后，一切都会淡化许多，所以“未经他人苦，莫劝他人善。你若经我苦，未必有我善”~ 2022-3-13 22:47:31]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>树的特点</tag>
        <tag>红黑树</tag>
        <tag>字典树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单梳理下git的使用感受，思考git中最重要的是什么]]></title>
    <url>%2Fblog%2F2022%2F02%2F13%2F%E7%AE%80%E5%8D%95%E6%A2%B3%E7%90%86%E4%B8%8Bgit%E7%9A%84%E4%BD%BF%E7%94%A8%E6%84%9F%E5%8F%97%EF%BC%8C%E6%80%9D%E8%80%83git%E4%B8%AD%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[前言工作中使用git是从两年前开始的，之前一直add -&gt; commit -&gt;push常规操作，真正在工作中使用之后才渐渐理解了git的强大，这种理解是建立在不断解决问题的基础上的，不断的处理遇到的问题，就像升级打怪一样，对git的理解也越来越全面。因为在使用git之前一直用svn作为版本控制工具，所以对git和svn的区别也有了自己的认识，关于两者的区别网上的文章一搜一大把，我就不重复了，我仅仅从自己的理解来描述下两者的不同。 git和svn关于git和svn的区别，网上的文章确实很多，大多数会提到分布式、存储方式、版本号、完整性等方面，而我今天要写的区别是两者提交记录的结构。 既然作为版本控制工具，那么每次历史提交都必须可以追溯和回退，在svn中提交记录时线性的，以时间轴为参考基准，所有提交按照时间先后排列，因为svn记录必须提交到服务器才能生效，所有服务器相当于各个svn客户端的总控，各个svn提交到服务器时线性排列，且必须将本地文件状态更新成和服务器相同时才能修改提交。 正因为在svn中有服务器负责总控操作，所以能保证时间最新的提交记录就是整个svn最新的状态，提交记录不依赖客户端时间，完全由服务器时间进行排序。 在git中没有这样的总控服务器，虽然一般情况下每个代码库都会有统一的托管服务器，但是它的作用任何一个git客户端都能代替，因为git是可以离线提交的，托管服务器只是我们用来存储代码的地方，与svn服务器按时间排序的做法大不相同。 git的提交记录通常是一个树形结构，个别时候会变成有“起点”和“终点”的网状结构，在git中时间只具备参考意义，并不能决定提交记录的先后，如果你对这一点还心存怀疑，可能你是个svn的重度用户，一时还没理解git操作原理。 对于这个问题可以举个例子，操作同一个文件，在svn中2月13日修改一次，2月14日修改一次，那么2月15日看这个文件一定是2月14日修改后的状态；而在git中，同样是那个文件分别在2月13日和2月14日修改一次，2月15日文件的状态取决了两次修改是否在同一分支，以及合并时是怎样处理的，这种错位随着时间的延长和多分支的合并，往往对时间的依赖“微乎其微”，此时再也不能用时间来衡量提交的先后了。 如果一开始就是git，上面提到的这个问题还不太明显，但是用惯了svn再使用git，处理历史回溯问题时往往容易找错方向，经常通过时间过滤出来的内容并不是自己想要的，这一点在实际操作中需要注意。 git最重要的是什么相信这个问题每个人都有自己的答案，有人认为是分布式，有人认为是切换分支很方便，而我的答案是 commit 的设计哲学，我觉得这是git中的精髓，git中的commit就像一个链表中的元素，用来将自身和其他的commit串联到一起，形成branch、tag、HEAD 等等。 我们可以通过 git log 命令来看一条 commit： 123456$ git log -1commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6. 这条commit id 为 7bf665f125a4771db095c83a7ad6ed46692cd314，这在整个库是惟一的，通过 git log 可以看到这次提交的时间、作者、简要说明等信息，那么这次提交和库是什么关系呢？ 通过括号中的内容可以知道当前提交是这个库的6.0分支，同时为标签6.0.6，也与远端的6.0分支同步。 使用 git cat-file 命令可以进一步查询这个commit的组织形式： 1234567$ git cat-file -p HEADtree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dauthor Oran Agra &lt;oran@redislabs.com&gt; 1595156420 +0300committer Oran Agra &lt;oran@redislabs.com&gt; 1595268506 +0300Redis 6.0.6. 可以发现这次提交包含了 tree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3，同时它的父提交就是 parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957d，有了这两个id就可以递推出当前版本内容和这个历史记录。 通过 tree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3 可以递归找出当前版本中的所有文件： 12345678910111213141516171819202122$ git cat-file -p c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3040000 tree 6608d88fe6a7a25b137b869040103ab261310da4 .github100644 blob e445fd2017bb0c13af2f40cd7f24afefdb603ade .gitignore100644 blob 484aeb62186033d32e9a4bdf12434cb6b8c56fb5 00-RELEASENOTES100644 blob 7af2593407805c308cc25739ac9c6520031de60f BUGS100644 blob 000edbeaf0270bf3b9e457274ab092b02b176b84 CONTRIBUTING100644 blob a381681a1c2524ed586c6a87dfeb9ccdf1e86ded COPYING100644 blob 3083f1afd50c34e1139ab1577510a17e968b0ed4 INSTALL100644 blob 3727894624fdabf72995e6f94998a2cad359f760 MANIFESTO100644 blob e614ede891f2dd183a3ae41ea1ac3b63fe2e7634 Makefile100644 blob 55537e01fe862dd200ebe1078033122facfc854e README.md100644 blob 2d020d0ceb0ddc7fd0bb2a6185e57a9afd5aef79 TLS.md040000 tree 43ccdd93a80b35e03160d9db34f1e844a62a74b4 deps100644 blob 8c53f015a20934bdb41c77152fd32a557d719fae redis.conf100755 blob ade1bd09a539ecd8dcdd09e59a658539dab9bce6 runtest100755 blob 27829a5fe8afacf893fe9bafc4245971ce375d6c runtest-cluster100755 blob f6cc0a2589dea0f95b77b226e54200a29b8237ae runtest-moduleapi100755 blob 3fb1ef61561289b2bf8622e49645f66dab83eeea runtest-sentinel100644 blob 4ca5e5f8fc5abe2938c66a6851bba0c90058620f sentinel.conf040000 tree e3b3338a7c60eafb3d9c19d3784e2482beea1d4b src040000 tree af5de133fa0a0da30fe487be40783ef9644fba6d tests040000 tree 5a82556097d23f0c16a8e5432d464f2ab434fd2a utils 通过 parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957d 可以找出上一次提交，进而递归找出所有的提交，要注意有些commit的parent不止一个： 1234567$ git cat-file -p a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dtree 1adcf548620c6134f7d5fd072c05b981d0f36118parent e15528bf1da1f1232fd08801ad382c915be94662author Oran Agra &lt;oran@redislabs.com&gt; 1595162001 +0300committer Oran Agra &lt;oran@redislabs.com&gt; 1595268506 +0300Run daily CI on PRs to release a branch 这个commit的设计真的很神奇，一个个commit串起来就是一个branch，本质来讲branch只是commit的一个别名，包括HEAD也是，而 tag 也是对commit的一个描述，在不加描述信息时和commit也是一样的。 123456789101112131415$ git cat-file -p 6.0.6tree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dauthor Oran Agra &lt;oran@redislabs.com&gt; 1595156420 +0300committer Oran Agra &lt;oran@redislabs.com&gt; 1595268506 +0300Redis 6.0.6.$ git cat-file -p HEADtree c3d4b2bcd934be7e4ed98edac5aa7e9c054503c3parent a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dauthor Oran Agra &lt;oran@redislabs.com&gt; 1595156420 +0300committer Oran Agra &lt;oran@redislabs.com&gt; 1595268506 +0300Redis 6.0.6. 所以理解了commit的定位以后，所有切换分支、切换tag、操作HEAD，本质上都是在对commit进行操作，这些操作的参数完全可以用commit id来替换HEAD、branch name、tag name等等。 总结 svn的提交记录是一个按时间排序的线性结构，git的提交记录是一个参考时间的树状结构 git记录中时间先后不能代表commit修改的先后，回溯查找时要注意这一点才能解释很多疑惑 git中的commit我认为是它的精髓，通过commit的串联和别名，形成分支、标签、HEAD等多种元素，隐藏了细节，方便了操作 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 什么才是精彩的人生？扬在脸上的自信、长在心底的善良、融进血里的骨气、刻进生命里的坚强~ 2022-2-13 23:19:05]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>commit</tag>
        <tag>branch</tag>
        <tag>stash</tag>
        <tag>tag</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下随时照看服务器进程的ps和top命令]]></title>
    <url>%2Fblog%2F2022%2F01%2F23%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E9%9A%8F%E6%97%B6%E7%85%A7%E7%9C%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E7%A8%8B%E7%9A%84ps%E5%92%8Ctop%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言在linux环境服务器下通常是不会安装图形界面的，这时如果想看程序是否在正常运行着，就不能像在windows上一样去查看任务管理器，而是需要通过命令来查询程序的进程运行情况，ps 就是一个查询linux环境下进程运行状态的命令，请注意这个可不是我们P图用的那个工具哦。 ps命令ps 命令在man手册中描述是 “report a snapshot of the current processes”，其实它的作用就是上报程序运行时的一个快照。 ps displays information about a selection of the active processes. If you want a repetitive update of the selection and the displayed information, use top(1) instead. man手册中的这句话也很好理解，ps 命令执行一次就会显示一次所选进程的信息，若想重复刷新显示所选进程的信息，可以使用 top 命令，从作用上来看，可以简单理解为 top 命令是一个反复执行的 ps 命令。 语法格式1ps [options] ps 命令的格式很简单，但是为什么感觉同一个查询进程的需求，每个人写出来的命令都不太一样呢？其实这是因为有好几种语法规范，无论怎么写都可以，甚至交叉混合的写命令都是能识别的，比如最常见的查询所有进程有些人会写成 ps -ef，而有的人会写成 ps aux，这两种都是可以的。 语法规范ps -ef 是使用了UNIX选项，是一种标准语法，而 ps aux使用了BSD选项，是一种BSD语法，另外还有一种加两个短横线的写法，例如 ps --pid 3678，这是一种GNU长选项。 常用的ps选项 显示所有的进程，一般搭配grep使用 1ps aux | grep processname 显示指定的列，可以使用 -eo 来指列名，我最常用的 cmd 是启动时的命令，lstart 是启动时的时间 12[root@VM-0-3-centos ~]# ps -eo cmd,lstart | grep sshdusr/sbin/sshd -D Tue Feb 9 11:13:55 2021 以树形结构显示进程 1ps -axjf 查询指定用户的进程 1ps -fu root 按cpu使用率降序排序 1ps -aux --sort -pcpu 按内存使用率降序排序 1ps -aux --sort +pmem ps命令的表头1234567891011[root@VM-0-3-centos ~]# ps -aux --sort -pcpuUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 17444 0.4 3.7 994796 71276 ? Sl 2021 181:54 /usr/local/qcloud/YunJing/YDEyes/YDServiceroot 8572 0.3 0.7 614824 14416 ? Sl Jan20 21:35 barad_agentroot 1 0.0 0.1 191084 3336 ? Ss 2021 76:42 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root 2 0.0 0.0 0 0 ? S 2021 0:09 [kthreadd]root 4 0.0 0.0 0 0 ? S&lt; 2021 0:00 [kworker/0:0H]root 1063 0.0 0.0 110208 792 tty1 Ss+ 2021 0:00 /sbin/agetty --noclear tty1 linuxroot 1064 0.0 0.0 110208 772 ttyS0 Ss+ 2021 0:00 /sbin/agetty --keep-baud 115200,38400,9600 ttyS0 vt220root 2388 0.0 0.1 157696 1952 pts/0 R+ 00:27 0:00 ps -aux --sort -pcpuroot 6 0.0 0.0 0 0 ? S 2021 13:09 [ksoftirqd/0] 执行 ps 命令通常有以上几列：USER、PID、%CPU、%MEM、VSZ、RSS、TTY、STAT、START、TIME、COMMAND USER：进程所属用户名PID：进程ID%CPU：进程CPU使用率，注意所有CPU使用率加起来可以超过100%，如果有4个CPU，在不考虑多核的情况下，最大应该为400%%MEM：进程内存使用率，可以配合 free -h 命令使用VSZ：进程使用的虚拟内存量RSS：进程占用的固定内存量TTY：进程在哪个终端上运行，若与终端无关，则显示 ?STAT：进程启动的时间START：进程状态，是一个组合值，每个符号有单独含义，见下文TIME：进程使用CPU运行的时间COMMAND：启动进程时执行的命令的名称和参数 进程状态这一部分是要是对man手册的翻译内容 1234567891011121314151617181920Here are the different values that the s, stat and state output specifiers (header "STAT" or "S") will display to describe the state of a process: D uninterruptible sleep (usually IO) R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) T stopped by job control signal t stopped by debugger during the tracing W paging (not valid since the 2.6.xx kernel) X dead (should never be seen) Z defunct ("zombie") process, terminated but not reaped by its parent For BSD formats and when the stat keyword is used, additional characters may be displayed: &lt; high-priority (not nice to other users) N low-priority (nice to other users) L has pages locked into memory (for real-time and custom IO) s is a session leader l is multi-threaded (using CLONE_THREAD, like NPTL pthreads do) + is in the foreground process group 常规状态码D: 无法中断的休眠状态（通常 IO 的进程）R: 正在运行可中在队列中可过行的；S: 处于休眠状态T: 停止或被追踪W: 进入内存交换（从内核2.6开始无效）X: 死掉的进程（基本很少见）Z: 僵尸进程，想杀死通常需要重启系统 BSD格式状态码&lt;: 优先级高的进程N: 优先级较低的进程L: 有些页被锁进内存s: 一个会话进程的领导者（在它之下有子进程）l: 多进程的（使用 CLONE_THREAD, 类似 NPTL pthreads）+: 位于后台的进程组 进程相关的后续命令 查询进程的可执行文件所在位置 1cat /proc/进程id 按进程名查询进程id 1pidof 进程名 top命令语法格式1top [options] top 命令的man手册巨长，它的作用是动态显示进程状态，实际上就是一份份静态数据间隔显示而已，虽然他也有很多参数，但是通常仅使用一个 top 命令即可。 常用的top选项和快捷键 显示top帮助 1top -hV 按照CPU使用率排序 运行top命令后，输入大写字母P，也就是按快捷键 ctrl+p 按照内存使用率排序 运行top命令后，输入大写字母M，也就是按快捷键 ctrl+m 展开多核cpu显示 运行top命令后，输入数字1 切换显示命令名称和完整命令行 运行top命令后，输入小写字母c 切换显示平均负载和启动时间信息 运行top命令后，输入小写字母l top命令的表头12345678910111213[root@VM-0-3-centos ~]# toptop - 00:07:35 up 0 min, 0 users, load average: 0.52, 0.58, 0.59Tasks: 4 total, 1 running, 3 sleeping, 0 stopped, 0 zombie%Cpu(s): 3.4 us, 8.0 sy, 0.0 ni, 88.4 id, 0.0 wa, 0.2 hi, 0.0 si, 0.0 stKiB Mem : 8248540 total, 3284196 free, 4727868 used, 236476 buff/cacheKiB Swap: 13107196 total, 13080444 free, 26752 used. 3379816 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 8940 316 272 S 0.0 0.0 0:00.15 init 10 root 20 0 8940 224 176 S 0.0 0.0 0:00.00 init 11 albert 20 0 15024 3568 3468 S 0.0 0.0 0:00.24 bash 38 albert 20 0 15900 1960 1420 R 0.0 0.0 0:00.04 top 执行 top 命令后默认有这些列 PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND，它们的含义分表是： PID：进程idUSER：进程所有者用户名PR：优先级NI：NICE值.负值表示高优先级，正值表示低优先级VIRT：进程使用的虚拟内存总量，单位KB，VIRT=SWAP+RESRES：进程使用的未被换出的物理内存大小，单位KB，RES=CODE+DATASHR：共享内存大小，单位KB，S：进程状态，D=不可中断的睡眠状态，R=运行，S=睡眠，T=跟踪/停止，Z=僵尸进程%CPU：CPU使用百分比%MEM、内存使用百分比TIME+：进程使用的CPU时间总计,单位1/100秒COMMAND：启动命令 总结 显示所有进程可以使用 ps -ef 或者 ps aux，他们通常是后续过滤的第一步 按cpu使用率降序排序使用 ps aux --sort -pcpu，按内存使用率降序排序使用 ps aux --sort +pmem top 命令结果的第一行是整体信息，包括了当前时间，系统已运行时间，登录用户数，系统负载等信息 top 命令中按大写字母 P 会按照CPU排序，按大写字母 M 会按照内存排序 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 小孩子才有对错，长大了都是利益。做选择的本质是分类，权衡利弊的过程是分类，趋利避害的行事生活更是分类，所以遇到选择我们才要考虑方方面面，因为这样我们才有足够的理由来把它归为有利的一类，还是有害的一类~ 2022-1-28 00:41:05]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ps</tag>
        <tag>命令</tag>
        <tag>top</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过WindowsStore安装QuickLook小工具方便文件预览]]></title>
    <url>%2Fblog%2F2022%2F01%2F09%2F%E9%80%9A%E8%BF%87WindowsStore%E5%AE%89%E8%A3%85QuickLook%E5%B0%8F%E5%B7%A5%E5%85%B7%E6%96%B9%E4%BE%BF%E6%96%87%E4%BB%B6%E9%A2%84%E8%A7%88%2F</url>
    <content type="text"><![CDATA[前言话说Windows商店一直没有什么存在感，普通大众从互联网野蛮生长开始就一直被各大流氓软件控制着连接互联网的门户，下载软件去官网只是少数人拿来伸张正义的说辞，有些官网的访问速度确实…不过随着手机的普及，应用商店的下载安装方式逐渐被人们所接受，而Windows商店也渐渐进入了人们的视野，毕竟又这么个官方的东西，总比满大街搜索捆绑的软件要方便的多。 今天主要是想记录一下QuickLook这个软件，毕竟小而美的工具可以极大的提高生产力，或者是摸鱼的能力，这个软件可以在你想要预览某个文档或者图片时，轻轻敲一个空格就搞定，比如想看一个压缩包的内容，只需敲个空格就可以。 Windows商店这个小工具在windows商店里就有，搜索安装比较方便，但是我发现我电脑里的WindowsStore不见了，还真是神奇，前一段时间还用过的，不知不觉就消失了，所以先安装Windows商店吧。 首先按组合键Win+X，在弹出菜单中左键单击 Windows PowerShell(管理员)选项，输入命令 Get-AppxPackage -allusers | Select Name, PackageFullName，之后会在命令窗口中显示可以安装的软件包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202# Albert at HOME-PC in C:\Windows\system32 [19:55:50]$ Get-AppxPackage -allusers | Select Name, PackageFullNameName PackageFullName---- ---------------1527c705-839a-4832-9118-54d4Bd6a0c89 1527c705-839a-4832-9118-54d4Bd6a0c89_10.0.18362.387_neutral_neutral_cw5...c5e2524a-ea46-4f67-841f-6a9465d9d515 c5e2524a-ea46-4f67-841f-6a9465d9d515_10.0.18362.387_neutral_neutral_cw5...E2A4F912-2574-4A75-9BB0-0D023378592B E2A4F912-2574-4A75-9BB0-0D023378592B_10.0.18362.387_neutral_neutral_cw5...F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE_10.0.18362.387_neutral_neutral_cw5...InputApp InputApp_1000.18362.387.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AAD.BrokerPlugin Microsoft.AAD.BrokerPlugin_1000.18362.387.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AccountsControl Microsoft.AccountsControl_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.AsyncTextService Microsoft.AsyncTextService_10.0.18362.387_neutral__8wekyb3d8bbweMicrosoft.BioEnrollment Microsoft.BioEnrollment_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.CredDialogHost Microsoft.CredDialogHost_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.ECApp Microsoft.ECApp_10.0.18362.387_neutral__8wekyb3d8bbweMicrosoft.LockApp Microsoft.LockApp_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.MicrosoftEdgeDevToolsClient Microsoft.MicrosoftEdgeDevToolsClient_1000.18362.387.0_neutral_neutral_...Microsoft.MicrosoftEdge Microsoft.MicrosoftEdge_44.18362.387.0_neutral__8wekyb3d8bbweMicrosoft.PPIProjection Microsoft.PPIProjection_10.0.18362.387_neutral_neutral_cw5n1h2txyewyMicrosoft.Win32WebViewHost Microsoft.Win32WebViewHost_10.0.18362.387_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.Apprep.ChxApp Microsoft.Windows.Apprep.ChxApp_1000.18362.387.0_neutral_neutral_cw5n1h...Microsoft.Windows.AssignedAccessLockApp Microsoft.Windows.AssignedAccessLockApp_1000.18362.387.0_neutral_neutra...Microsoft.Windows.CallingShellApp Microsoft.Windows.CallingShellApp_1000.18362.387.0_neutral_neutral_cw5n...Microsoft.Windows.CapturePicker Microsoft.Windows.CapturePicker_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.Windows.CloudExperienceHost Microsoft.Windows.CloudExperienceHost_10.0.18362.387_neutral_neutral_cw...Microsoft.Windows.ContentDeliveryManager Microsoft.Windows.ContentDeliveryManager_10.0.18362.387_neutral_neutral...Microsoft.Windows.Cortana Microsoft.Windows.Cortana_1.13.0.18362_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.NarratorQuickStart Microsoft.Windows.NarratorQuickStart_10.0.18362.387_neutral_neutral_8we...Microsoft.Windows.OOBENetworkCaptivePortal Microsoft.Windows.OOBENetworkCaptivePortal_10.0.18362.387_neutral__cw5n...Microsoft.Windows.OOBENetworkConnectionFlow Microsoft.Windows.OOBENetworkConnectionFlow_10.0.18362.387_neutral__cw5...Microsoft.Windows.ParentalControls Microsoft.Windows.ParentalControls_1000.18362.387.0_neutral_neutral_cw5...Microsoft.Windows.PeopleExperienceHost Microsoft.Windows.PeopleExperienceHost_10.0.18362.387_neutral_neutral_c...Microsoft.Windows.PinningConfirmationDialog Microsoft.Windows.PinningConfirmationDialog_1000.18362.387.0_neutral__c...Microsoft.Windows.SecHealthUI Microsoft.Windows.SecHealthUI_10.0.18362.387_neutral__cw5n1h2txyewyMicrosoft.Windows.SecureAssessmentBrowser Microsoft.Windows.SecureAssessmentBrowser_10.0.18362.387_neutral_neutra...Microsoft.Windows.ShellExperienceHost Microsoft.Windows.ShellExperienceHost_10.0.18362.387_neutral_neutral_cw...Microsoft.Windows.StartMenuExperienceHost Microsoft.Windows.StartMenuExperienceHost_10.0.18362.387_neutral_neutra...Microsoft.Windows.XGpuEjectDialog Microsoft.Windows.XGpuEjectDialog_10.0.18362.387_neutral_neutral_cw5n1h...Microsoft.XboxGameCallableUI Microsoft.XboxGameCallableUI_1000.18362.387.0_neutral_neutral_cw5n1h2tx...Windows.CBSPreview Windows.CBSPreview_10.0.18362.387_neutral_neutral_cw5n1h2txyewywindows.immersivecontrolpanel windows.immersivecontrolpanel_10.0.2.1000_neutral_neutral_cw5n1h2txyewyWindows.PrintDialog Windows.PrintDialog_6.2.1.0_neutral_neutral_cw5n1h2txyewyMicrosoft.Advertising.Xaml Microsoft.Advertising.Xaml_10.1808.3.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.1.7 Microsoft.NET.Native.Framework.1.7_1.7.25531.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.1.7 Microsoft.NET.Native.Framework.1.7_1.7.25531.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.27011.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.1.7 Microsoft.NET.Native.Runtime.1.7_1.7.25531.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.1.7 Microsoft.NET.Native.Runtime.1.7_1.7.25531.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.2.2 Microsoft.NET.Native.Runtime.2.2_2.2.27011.0_x64__8wekyb3d8bbweMicrosoft.Services.Store.Engagement Microsoft.Services.Store.Engagement_10.0.18101.0_x64__8wekyb3d8bbweMicrosoft.Services.Store.Engagement Microsoft.Services.Store.Engagement_10.0.18101.0_x86__8wekyb3d8bbweMicrosoft.UI.Xaml.2.0 Microsoft.UI.Xaml.2.0_2.1810.18004.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.26905.0_x64__8wekyb3d8bbweMicrosoft.Wallet Microsoft.Wallet_2.4.18324.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.27323.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.27323.0_x86__8wekyb3d8bbweRealtekSemiconductorCorp.RealtekAudioControl RealtekSemiconductorCorp.RealtekAudioControl_1.3.179.0_x64__dt26b99r8h8gjMicrosoft.Print3D Microsoft.Print3D_3.3.791.0_x64__8wekyb3d8bbweMicrosoft.XboxSpeechToTextOverlay Microsoft.XboxSpeechToTextOverlay_1.21.13002.0_x64__8wekyb3d8bbweMicrosoft.Xbox.TCUI Microsoft.Xbox.TCUI_1.24.10001.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.27810.0_x86__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.27810.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.0 Microsoft.UI.Xaml.2.0_2.1810.18004.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.27912.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.1.7 Microsoft.NET.Native.Framework.1.7_1.7.27413.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.27912.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Framework.1.7 Microsoft.NET.Native.Framework.1.7_1.7.27413.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.2.1 Microsoft.NET.Native.Runtime.2.1_2.1.26424.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.2.1 Microsoft.NET.Native.Runtime.2.1_2.1.26424.0_x64__8wekyb3d8bbweMicrosoft.Messaging Microsoft.Messaging_4.1901.10241.1000_x64__8wekyb3d8bbweMicrosoft.Advertising.Xaml Microsoft.Advertising.Xaml_10.1811.1.0_x86__8wekyb3d8bbweMicrosoft.Advertising.Xaml Microsoft.Advertising.Xaml_10.1811.1.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.1 Microsoft.NET.Native.Framework.2.1_2.1.27427.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.1 Microsoft.NET.Native.Framework.2.1_2.1.27427.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.2 Microsoft.UI.Xaml.2.2_2.21909.17002.0_x86__8wekyb3d8bbweMicrosoft.UI.Xaml.2.2 Microsoft.UI.Xaml.2.2_2.21909.17002.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.27810.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.27810.0_x86__8wekyb3d8bbweMicrosoft.UI.Xaml.2.1 Microsoft.UI.Xaml.2.1_2.11906.6001.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.1 Microsoft.UI.Xaml.2.1_2.11906.6001.0_x86__8wekyb3d8bbweMicrosoft.Services.Store.Engagement Microsoft.Services.Store.Engagement_10.0.19011.0_x86__8wekyb3d8bbweMicrosoft.Services.Store.Engagement Microsoft.Services.Store.Engagement_10.0.19011.0_x64__8wekyb3d8bbweInputApp InputApp_1000.18362.449.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AAD.BrokerPlugin Microsoft.AAD.BrokerPlugin_1000.18362.449.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AccountsControl Microsoft.AccountsControl_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.AsyncTextService Microsoft.AsyncTextService_10.0.18362.449_neutral__8wekyb3d8bbweMicrosoft.BioEnrollment Microsoft.BioEnrollment_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.CredDialogHost Microsoft.CredDialogHost_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.ECApp Microsoft.ECApp_10.0.18362.449_neutral__8wekyb3d8bbweMicrosoft.LockApp Microsoft.LockApp_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.MicrosoftEdgeDevToolsClient Microsoft.MicrosoftEdgeDevToolsClient_1000.18362.449.0_neutral_neutral_...Microsoft.Windows.Apprep.ChxApp Microsoft.Windows.Apprep.ChxApp_1000.18362.449.0_neutral_neutral_cw5n1h...Microsoft.Windows.AssignedAccessLockApp Microsoft.Windows.AssignedAccessLockApp_1000.18362.449.0_neutral_neutra...Microsoft.Windows.CallingShellApp Microsoft.Windows.CallingShellApp_1000.18362.449.0_neutral_neutral_cw5n...Microsoft.Windows.CapturePicker Microsoft.Windows.CapturePicker_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.Windows.OOBENetworkCaptivePortal Microsoft.Windows.OOBENetworkCaptivePortal_10.0.18362.449_neutral__cw5n...Microsoft.Windows.OOBENetworkConnectionFlow Microsoft.Windows.OOBENetworkConnectionFlow_10.0.18362.449_neutral__cw5...Microsoft.Windows.ParentalControls Microsoft.Windows.ParentalControls_1000.18362.449.0_neutral_neutral_cw5...Microsoft.Windows.PinningConfirmationDialog Microsoft.Windows.PinningConfirmationDialog_1000.18362.449.0_neutral__c...Microsoft.XboxGameCallableUI Microsoft.XboxGameCallableUI_1000.18362.449.0_neutral_neutral_cw5n1h2tx...1527c705-839a-4832-9118-54d4Bd6a0c89 1527c705-839a-4832-9118-54d4Bd6a0c89_10.0.18362.449_neutral_neutral_cw5...c5e2524a-ea46-4f67-841f-6a9465d9d515 c5e2524a-ea46-4f67-841f-6a9465d9d515_10.0.18362.449_neutral_neutral_cw5...E2A4F912-2574-4A75-9BB0-0D023378592B E2A4F912-2574-4A75-9BB0-0D023378592B_10.0.18362.449_neutral_neutral_cw5...F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE_10.0.18362.449_neutral_neutral_cw5...Microsoft.MicrosoftEdge Microsoft.MicrosoftEdge_44.18362.449.0_neutral__8wekyb3d8bbweMicrosoft.PPIProjection Microsoft.PPIProjection_10.0.18362.449_neutral_neutral_cw5n1h2txyewyMicrosoft.Win32WebViewHost Microsoft.Win32WebViewHost_10.0.18362.449_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.CloudExperienceHost Microsoft.Windows.CloudExperienceHost_10.0.18362.449_neutral_neutral_cw...Microsoft.Windows.ContentDeliveryManager Microsoft.Windows.ContentDeliveryManager_10.0.18362.449_neutral_neutral...Microsoft.Windows.NarratorQuickStart Microsoft.Windows.NarratorQuickStart_10.0.18362.449_neutral_neutral_8we...Microsoft.Windows.PeopleExperienceHost Microsoft.Windows.PeopleExperienceHost_10.0.18362.449_neutral_neutral_c...Microsoft.Windows.SecHealthUI Microsoft.Windows.SecHealthUI_10.0.18362.449_neutral__cw5n1h2txyewyMicrosoft.Windows.SecureAssessmentBrowser Microsoft.Windows.SecureAssessmentBrowser_10.0.18362.449_neutral_neutra...Microsoft.Windows.ShellExperienceHost Microsoft.Windows.ShellExperienceHost_10.0.18362.449_neutral_neutral_cw...Microsoft.Windows.StartMenuExperienceHost Microsoft.Windows.StartMenuExperienceHost_10.0.18362.449_neutral_neutra...Microsoft.Windows.XGpuEjectDialog Microsoft.Windows.XGpuEjectDialog_10.0.18362.449_neutral_neutral_cw5n1h...Windows.CBSPreview Windows.CBSPreview_10.0.18362.449_neutral_neutral_cw5n1h2txyewyMicrosoft.UI.Xaml.2.3 Microsoft.UI.Xaml.2.3_2.32002.13001.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.3 Microsoft.UI.Xaml.2.3_2.32002.13001.0_x86__8wekyb3d8bbweAppUp.IntelGraphicsControlPanel AppUp.IntelGraphicsControlPanel_3.3.0.0_x64__8j3eq9eme6cttMicrosoft.NET.Native.Runtime.2.2 Microsoft.NET.Native.Runtime.2.2_2.2.28604.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.2.2 Microsoft.NET.Native.Runtime.2.2_2.2.28604.0_x64__8wekyb3d8bbweMicrosoft.EdgeDevtoolsPlugin Microsoft.EdgeDevtoolsPlugin_10.0.18362.449_neutral_neutral_cw5n1h2txyewyCanonicalGroupLimited.Ubuntu16.04onWindows CanonicalGroupLimited.Ubuntu16.04onWindows_2020.1604.14.0_x64__79rhkp1f...Microsoft.NET.Native.Runtime.1.7 Microsoft.NET.Native.Runtime.1.7_1.7.27422.0_x86__8wekyb3d8bbweMicrosoft.NET.Native.Runtime.1.7 Microsoft.NET.Native.Runtime.1.7_1.7.27422.0_x64__8wekyb3d8bbweMicrosoft.XboxGameOverlay Microsoft.XboxGameOverlay_1.54.4001.0_x64__8wekyb3d8bbweMicrosoft.XboxIdentityProvider Microsoft.XboxIdentityProvider_12.67.21001.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.4 Microsoft.UI.Xaml.2.4_2.42007.9001.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.4 Microsoft.UI.Xaml.2.4_2.42007.9001.0_x86__8wekyb3d8bbweMicrosoft.ScreenSketch Microsoft.ScreenSketch_10.2008.2277.0_x64__8wekyb3d8bbweMicrosoft.OneConnect Microsoft.OneConnect_5.2011.3081.0_x64__8wekyb3d8bbweMicrosoft.DesktopAppInstaller Microsoft.DesktopAppInstaller_1.4.3161.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.29512.0_x64__8wekyb3d8bbweMicrosoft.NET.Native.Framework.2.2 Microsoft.NET.Native.Framework.2.2_2.2.29512.0_x86__8wekyb3d8bbweMicrosoft.Office.OneNote Microsoft.Office.OneNote_16001.13328.20478.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.5 Microsoft.UI.Xaml.2.5_2.52012.2002.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.5 Microsoft.UI.Xaml.2.5_2.52012.2002.0_x86__8wekyb3d8bbweMicrosoft.WindowsAlarms Microsoft.WindowsAlarms_10.2101.28.0_x64__8wekyb3d8bbweMicrosoft.WindowsCamera Microsoft.WindowsCamera_2021.105.10.0_x64__8wekyb3d8bbweMicrosoft.WebMediaExtensions Microsoft.WebMediaExtensions_1.0.40831.0_x64__8wekyb3d8bbweMicrosoft.WebpImageExtension Microsoft.WebpImageExtension_1.0.41203.0_x64__8wekyb3d8bbweMicrosoft.WindowsStore Microsoft.WindowsStore_12104.1001.1.0_x64__8wekyb3d8bbweMicrosoft.BingWeather Microsoft.BingWeather_4.46.31121.0_x64__8wekyb3d8bbweMicrosoft.WindowsCalculator Microsoft.WindowsCalculator_10.2103.8.0_x64__8wekyb3d8bbweMicrosoft.WindowsSoundRecorder Microsoft.WindowsSoundRecorder_10.2103.28.0_x64__8wekyb3d8bbweMicrosoft.MSPaint Microsoft.MSPaint_6.2105.4017.0_x64__8wekyb3d8bbweMicrosoft.Microsoft3DViewer Microsoft.Microsoft3DViewer_7.2105.4012.0_x64__8wekyb3d8bbweMicrosoft.HEIFImageExtension Microsoft.HEIFImageExtension_1.0.40978.0_x64__8wekyb3d8bbwe1527c705-839a-4832-9118-54d4Bd6a0c89 1527c705-839a-4832-9118-54d4Bd6a0c89_10.0.18362.1533_neutral_neutral_cw...c5e2524a-ea46-4f67-841f-6a9465d9d515 c5e2524a-ea46-4f67-841f-6a9465d9d515_10.0.18362.1533_neutral_neutral_cw...E2A4F912-2574-4A75-9BB0-0D023378592B E2A4F912-2574-4A75-9BB0-0D023378592B_10.0.18362.1533_neutral_neutral_cw...F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE F46D4000-FD22-4DB4-AC8E-4E1DDDE828FE_10.0.18362.1533_neutral_neutral_cw...InputApp InputApp_1000.18362.1533.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AAD.BrokerPlugin Microsoft.AAD.BrokerPlugin_1000.18362.1533.0_neutral_neutral_cw5n1h2txyewyMicrosoft.AccountsControl Microsoft.AccountsControl_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.AsyncTextService Microsoft.AsyncTextService_10.0.18362.1533_neutral__8wekyb3d8bbweMicrosoft.BioEnrollment Microsoft.BioEnrollment_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.CredDialogHost Microsoft.CredDialogHost_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.ECApp Microsoft.ECApp_10.0.18362.1533_neutral__8wekyb3d8bbweMicrosoft.EdgeDevtoolsPlugin Microsoft.EdgeDevtoolsPlugin_10.0.18362.1533_neutral_neutral_cw5n1h2txyewyMicrosoft.LockApp Microsoft.LockApp_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.MicrosoftEdgeDevToolsClient Microsoft.MicrosoftEdgeDevToolsClient_1000.18362.1533.0_neutral_neutral...Microsoft.MicrosoftEdge Microsoft.MicrosoftEdge_44.18362.1533.0_neutral__8wekyb3d8bbweMicrosoft.PPIProjection Microsoft.PPIProjection_10.0.18362.1533_neutral_neutral_cw5n1h2txyewyMicrosoft.Win32WebViewHost Microsoft.Win32WebViewHost_10.0.18362.1533_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.Apprep.ChxApp Microsoft.Windows.Apprep.ChxApp_1000.18362.1533.0_neutral_neutral_cw5n1...Microsoft.Windows.AssignedAccessLockApp Microsoft.Windows.AssignedAccessLockApp_1000.18362.1533.0_neutral_neutr...Microsoft.Windows.CallingShellApp Microsoft.Windows.CallingShellApp_1000.18362.1533.0_neutral_neutral_cw5...Microsoft.Windows.CapturePicker Microsoft.Windows.CapturePicker_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.Windows.CloudExperienceHost Microsoft.Windows.CloudExperienceHost_10.0.18362.1533_neutral_neutral_c...Microsoft.Windows.ContentDeliveryManager Microsoft.Windows.ContentDeliveryManager_10.0.18362.1533_neutral_neutra...Microsoft.Windows.Cortana Microsoft.Windows.Cortana_1.13.1.18362_neutral_neutral_cw5n1h2txyewyMicrosoft.Windows.NarratorQuickStart Microsoft.Windows.NarratorQuickStart_10.0.18362.1533_neutral_neutral_8w...Microsoft.Windows.OOBENetworkCaptivePortal Microsoft.Windows.OOBENetworkCaptivePortal_10.0.18362.1533_neutral__cw5...Microsoft.Windows.OOBENetworkConnectionFlow Microsoft.Windows.OOBENetworkConnectionFlow_10.0.18362.1533_neutral__cw...Microsoft.Windows.ParentalControls Microsoft.Windows.ParentalControls_1000.18362.1533.0_neutral_neutral_cw...Microsoft.Windows.PeopleExperienceHost Microsoft.Windows.PeopleExperienceHost_10.0.18362.1533_neutral_neutral_...Microsoft.Windows.PinningConfirmationDialog Microsoft.Windows.PinningConfirmationDialog_1000.18362.1533.0_neutral__...Microsoft.Windows.SecHealthUI Microsoft.Windows.SecHealthUI_10.0.18362.1533_neutral__cw5n1h2txyewyMicrosoft.Windows.SecureAssessmentBrowser Microsoft.Windows.SecureAssessmentBrowser_10.0.18362.1533_neutral_neutr...Microsoft.Windows.ShellExperienceHost Microsoft.Windows.ShellExperienceHost_10.0.18362.1533_neutral_neutral_c...Microsoft.Windows.StartMenuExperienceHost Microsoft.Windows.StartMenuExperienceHost_10.0.18362.1533_neutral_neutr...Microsoft.Windows.XGpuEjectDialog Microsoft.Windows.XGpuEjectDialog_10.0.18362.1533_neutral_neutral_cw5n1...Microsoft.XboxGameCallableUI Microsoft.XboxGameCallableUI_1000.18362.1533.0_neutral_neutral_cw5n1h2t...Windows.CBSPreview Windows.CBSPreview_10.0.18362.1533_neutral_neutral_cw5n1h2txyewyMicrosoft.VP9VideoExtensions Microsoft.VP9VideoExtensions_1.0.41182.0_x64__8wekyb3d8bbweMicrosoft.WindowsMaps Microsoft.WindowsMaps_10.2104.2.0_x64__8wekyb3d8bbweMicrosoft.SkypeApp Microsoft.SkypeApp_15.72.94.0_x86__kzf8qxf38zg5cMicrosoft.StorePurchaseApp Microsoft.StorePurchaseApp_12105.5555.19.0_x64__8wekyb3d8bbweMicrosoft.GetHelp Microsoft.GetHelp_10.2105.41472.0_x64__8wekyb3d8bbweMicrosoft.XboxGamingOverlay Microsoft.XboxGamingOverlay_5.721.5282.0_x64__8wekyb3d8bbweMicrosoft.YourPhone Microsoft.YourPhone_1.21042.143.0_x64__8wekyb3d8bbweMicrosoft.ZuneVideo Microsoft.ZuneVideo_10.21061.10121.0_x64__8wekyb3d8bbweMicrosoft.ZuneMusic Microsoft.ZuneMusic_10.21061.10121.0_x64__8wekyb3d8bbweMicrosoft.MicrosoftStickyNotes Microsoft.MicrosoftStickyNotes_4.1.6.0_x64__8wekyb3d8bbwe Microsoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.30035.0_x64__8wekyb3d8bbwe Microsoft.XboxApp Microsoft.XboxApp_48.78.15001.0_x64__8wekyb3d8bbwe Microsoft.MixedReality.Portal Microsoft.MixedReality.Portal_2000.21051.1282.0_x64__8wekyb3d8bbwe Microsoft.People Microsoft.People_10.2105.4.0_x64__8wekyb3d8bbwe Microsoft.MicrosoftOfficeHub Microsoft.MicrosoftOfficeHub_18.2106.12410.0_x64__8wekyb3d8bbwe Microsoft.WindowsTerminal Microsoft.WindowsTerminal_1.9.1942.0_x64__8wekyb3d8bbwe AppUp.IntelGraphicsExperience AppUp.IntelGraphicsExperience_1.100.3370.0_x64__8j3eq9eme6ctt Microsoft.Getstarted Microsoft.Getstarted_10.4.41811.0_x64__8wekyb3d8bbwe Microsoft.Windows.Photos Microsoft.Windows.Photos_2021.21070.22007.0_x64__8wekyb3d8bbwe Microsoft.MicrosoftSolitaireCollection Microsoft.MicrosoftSolitaireCollection_4.10.7290.0_x64__8wekyb3d8bbwe Microsoft.LanguageExperiencePackzh-CN Microsoft.LanguageExperiencePackzh-CN_18362.41.126.0_neutral__8wekyb3d8... microsoft.windowscommunicationsapps microsoft.windowscommunicationsapps_16005.14326.20090.0_x64__8wekyb3d8bbwe Microsoft.WindowsFeedbackHub Microsoft.WindowsFeedbackHub_1.2106.1801.0_x64__8wekyb3d8bbwe Microsoft.UI.Xaml.2.7 Microsoft.UI.Xaml.2.7_7.2109.13004.0_x86__8wekyb3d8bbwe Microsoft.UI.Xaml.2.7 Microsoft.UI.Xaml.2.7_7.2109.13004.0_x64__8wekyb3d8bbwe Microsoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.30704.0_x86__8wekyb3d8bbweMicrosoft.VCLibs.140.00 Microsoft.VCLibs.140.00_14.0.30704.0_x64__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.30704.0_x86__8wekyb3d8bbweMicrosoft.VCLibs.140.00.UWPDesktop Microsoft.VCLibs.140.00.UWPDesktop_14.0.30704.0_x64__8wekyb3d8bbweMicrosoft.UI.Xaml.2.6 Microsoft.UI.Xaml.2.6_2.62112.3002.0_x86__8wekyb3d8bbweMicrosoft.UI.Xaml.2.6 Microsoft.UI.Xaml.2.6_2.62112.3002.0_x64__8wekyb3d8bbwe 搜索 Store 可以找到 Microsoft.WindowsStore Microsoft.WindowsStore_12104.1001.1.0_x64__8wekyb3d8bbwe 这一项，具体包名根据不同电脑搜索结果而定。 然后运行安装命令就可以了，注意把其中的 Microsoft.WindowsStore_12104.1001.1.0_x64__8wekyb3d8bbwe内容替换成自己搜索出的结果，Add-appxpackage... 命令运行完之后Windows商店就出现了。 12? # Albert at HOME-PC in C:\Windows\system32 [20:00:00]$ Add-appxpackage -register "C:\Program Files\WindowsApps\Microsoft.WindowsStore_12104.1001.1.0_x64__8wekyb3d8bbwe\appxmanifest.xml" -disabledevelopmentmode QuickLook双击打开Windows商店，搜索QuickLook直接安装就可以，一个很不错的小工具，这是来自商店的介绍 官网传送门 总结 Win+x 快速打开管理菜单，Win+i快速打开设置界面 PowerShell中运行 Get-AppxPackage -allusers | Select Name, PackageFullName 列举可安装的软件包 Add-appxpackage -register &quot;C:\Program Files\WindowsApps\****\appxmanifest.xml&quot; -disabledevelopmentmode 安装指定软件** QuickLook快速预览小程序，值得一试 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 元宇宙，一个人人都想分一杯羹的概念，理想很美好，但是现实往往很残酷，当一个人很上头的时候，任何语言都会显得苍白无力，不要只是头脑发热的一直去想，看看别人都是怎么做的，有多少人已经退了出来？人类往往很诚实，但是这不表现在言语上~ 2022-1-9 23:24:47]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>windows</tag>
        <tag>商店</tag>
        <tag>软件</tag>
        <tag>QuickLook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2021年终总结——脚踏实地，为下一次腾飞积蓄力量]]></title>
    <url>%2Fblog%2F2022%2F01%2F09%2F2021%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94%E8%84%9A%E8%B8%8F%E5%AE%9E%E5%9C%B0%EF%BC%8C%E4%B8%BA%E4%B8%8B%E4%B8%80%E6%AC%A1%E8%85%BE%E9%A3%9E%E7%A7%AF%E8%93%84%E5%8A%9B%E9%87%8F%2F</url>
    <content type="text"><![CDATA[有一种幸福叫照片上的人都还在~ 前言又到了一年一度的交卷时刻，去年的flag可以改个年份继续提上日程了。2021年，平平淡淡的一年，平淡到蜗居在这个寸土寸金的帝都，都没回过几次家。不过平淡并非不好，这份平淡恰恰是一种幸福，感恩我们所在的这个和平的国度，感恩所有为了这份和平付出努力的人们。 人是会慢慢成长的，很多曾经不懂的事情一瞬间全都明白了，有些事情已经晚了，可是还有很多事情可以补救，人生就是不断拿一些东西去换另一些东西的旅程，这种交换无时无刻不在进行着，随着时间这个魔法师渐渐推移，我们突然想换回来了，才发现做了半生的无用功。 太年轻的人 他总是不满足固执地不愿停下 远行的脚步望着高高的天走了长长的路忘了回头看 她有没有哭 生活还要继续，把握当下才能赢得未来，踏踏实实迈出每一步，不需要鲜花和掌声，其实踩出的每一个脚印都是实实在在的生活。 回顾2021既然是年终总结，总要回忆一下去年过的怎么样，最简单的还是来看看去年flag的大脸盛况，拿出我的八股文框架，开整~ 工作上 FLAG 脚踏实地做好本职工作 额外挤出时间去尝试技术提升（优化、解决痛点） 在熟悉业务的同时更多参与设计的工作，拓宽自己的认知范围 完成度：85% 一直觉得“靠谱”这个词对程序员来说是一个很高的评价，而我也在向这个方向默默努力着，事事有反馈是一个人的态度，同时也是一种良好的品质，在过去的一年中没有出大问题，也没有出彩，总体上算是中规中矩，也实现了自己“脚踏实地做好本职工作”的目标。 在技术提升方面，也在进一步努力，优化和解决痛点的工作一直在做，但相比较来说有些欠缺，技术提升和补充这方面主要体现在两部分，一部分来源于日常工作，需求推动进步，为了实现需求而提升自己，虽然进步不大，但贵在学以致用。 另一部分来源于面试，今年发生了角色的转换，我从一个求职者变成了面试官，这是一个可以锻炼自己的宝贵的机会，阅人然后识己，通过准备面试，翻看笔试题，同时观察候选人的表现，来发掘出自己需要提升的部分，循序渐进，总结提升，比如《树的带权路径长度和哈夫曼树》、《完全图与强连通图的那些坑》 都是在面试之后总结得来的。 至于更多的参与设计工作，今年也做了不少努力，得益于项目组的调整以及和老大的热心支持，今年抽出了一些时间来做偏向底层的设计工作，将自己的想法融入项目之中，静等花开。 学习上 FLAG 博客总结继续，基本保持在1周一篇，可以适当偷懒，一年懒10次可以产出40篇 开源代码还是要继续学习，libevent需要详细看一下，今年的出镜率太高 读2本技术类书籍，可以是开阔眼界的，也可以是现有技能提升的 读2本经济学、金融理财相关的书籍 完成度：70% 关于博客总结这个flag真的是把偷懒发挥到了极致，到年底正好产出40篇总结，内容比较分散，基本上都是日常工作学习中遇到的问题，弄明白之后总结到一起，方便后续翻看查找。 开源代码看的相对较少，这部分还需继续努力，libevent因为懒没有看进去，只在年初看了几个数据结构，不靠问题驱动着看代码还是挺难的，特别是这种代码量比较多的库，所以后面转变思路，从一些代码量少的小工具入手，比如这个神奇的grep命令 读书方面，并没有完整看完两本技术书籍，但是其他类型的书籍确实看了不少，如果加上各种故事书都堆成小山了： 小狗钱钱 富爸爸穷爸爸 半小时漫画中国史 半小时漫画中国史2 半小时漫画中国史3 半小时漫画中国史4 半小时漫画中国史5 半小时漫画世界史 硬笔行书字帖 ——《樂墨書院》 不吼不叫培养好孩子 领导力的本质 ——向松下幸之助和稻盛和夫学习 素直之心，以人为本，换位思考，尊重人性，经营哲学 深度思考——不断逼近问题的本质 一本书读懂财报 每一个企业都有它所处行业的烙印 闭环思维——让靠谱成为习惯 我会为你的过程鼓掌，但我只会给你的结果买单 底层逻辑——半秒钟看透问题本质 一切以结果为导向，没有功劳，苦劳也是无用的 刷题方面，今天有点虎头蛇尾了，前几个月还在努力紧跟大佬的脚步，后几个月因为工作、生活中的各种事情吧，刷题速度直线下降，人的精力是有限的，作为一个已经步入社会的人，像学生一样每天拿出大把时间来高强度刷题是不现实的，只能自己来平衡时间了，一段时间刷几道，避免太手生就好了。 生活上 FLAG 陪娃娃，陪家人，工作内容适应后可以多拿出一点时间和家人在一起（需要比2020多一些） 投资理财还是要多花一点时间研究下，目标7%（靠工资是不可能财富自由的，必须开源才行） 注重身体的保养，身体是本钱，可不能把身体搞垮了，愿丢掉体检时的小毛病~ 完成度：65% 陪伴是最长情的告白，今年调整了工作学习方式，尽可能把工作学习时间放到工作日，然后周末节假日就是一心一意陪家人。工作日疯狂输出，周末节假日休养生息，这种方式也不错。 利用放假时间和小娃娃一起玩耍，比如买几盆多肉，一起体会栽种花草的乐趣，一起做丢手绢、木头人的游戏，宝宝欢喜的不得了，只要有人陪着疯，什么电视、手机都抛到脑后，所以当自家娃娃沉迷于某些东西不能自拔时，需要反思一下自己有没有认真的陪过她。 我抱起砖头就没法抱你，放下砖头就没法养你，在这样高速发展的社会，社会分工相当明确，再想守着自己家的一亩三分地来自给自足是不可能了，我们都想争做人上人，但是吃的苦中苦只有自己清楚。 谈到投资理财，我人没了，之前作为小韭菜在股市摸爬滚打、追涨杀跌赚了一些银子，今年转变思路，拥抱大白马，等待低估值修复，结果成了一个被人割的老韭菜，不过暂时不打算调仓，重仓银地保、中丐互怜，投资是反人性的，只要握紧头寸不撒手，你就别想割到我。 人的情绪是波动的，想想一个没有任何消息的公司股价为什么会起起伏伏，是因为每个人对后市的股价走向做了预测，然后根据预测进行买入卖出操作，这种短期预测毫无意义，但是价格在价值周围波动这是市场规律，顺大势，勿逆行。 今年的股票成绩略微惨淡，其实和去年也差不多，坐了几波小板车，加上ETF和基金，浮亏一点点，在今天价值投资被埋在谷底的年头，也还可以吧，继续出清垃圾股。 展望2022工作 继续踏实做好本职工作，做好工作内容的总结，落实到纸上 学习和了解常见中间件的使用，更多的参与设计的工作 学习 博客总结不能落，继续保持一周一更，完成40篇基础目标 刷题不用太频繁，每周总得有贡献，不可抗拒因素除外 建立自己的技能树，搭配工作总结，统计出自己到底会什么 选取经典开源代码学习，代码量要少一点，毕竟精力有限了 读书、读书、读书，书都买好了 投资 基金和ETF继续定投，适当配置债券固收 股票池里把曾经“瞎选”的股票逐渐出清，依旧拥抱大白马，少折腾 目标收益不太高，8个点，希望不要打脸 生活 疫情结束了多回几次老家吧，去看看那些想我又不愿说出口的亲人 注重身体的保养，锻炼提上日程，降低亮红灯的指标 总结 身体有些吃不消了，适当缩减睁着眼睛的时间吧 工作学习分不开，所以两者会共同产出一颗技能树，为后续选择提供腾飞的力量 我觉得是时候把投资这项单独拿出来了，靠搬砖是不能实现自由梦的，需要学会借力搬砖 噢，忘了一点，去年出现在年终总结中的那颗茉莉花挂了，我补两张今年养的多肉吧，希望他能抗住今年的flag~ 梦想 每个人都有自己梦想，而我的梦很简单，那里有一个大大的书房，中间摆一个简单的书桌，一把带轮子的小椅，后面是占满整面墙的书柜，书柜上摆放着我喜欢的技术、财经、历史方面的书籍。书桌上斜放着两台电脑，看书累了就写写自己喜欢的代码，我并不喜欢庞大的程序，更热衷于一些实用的小工具，比如像grep这种锋利的小匕首。偶尔小宝宝会跑进来要我陪她玩，我会给它讲讲正在写哪些有趣的东西，玩一玩自己开发的游戏，是的，她很喜欢，一个简单的小球都玩得不亦乐乎~ 当然了大大的玻璃窗前必须摆满喜欢的花草，他们尽情的吮吸窗外的阳光，同时为书屋提供了生命的颜色，花架旁边是喷壶和铲子，虽然看起来不像是书房里的东西，但是我想把它们摆在那里，这里仿佛是另一番天地。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 空有悲天悯人之心，却无普度众生之力，不愿最后伸出那无助的手，要在可以选择时努力做一个施粥人。 2021-12-31 23:43:27]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows设置防火墙允许指定应用正常使用网络]]></title>
    <url>%2Fblog%2F2021%2F12%2F26%2FWindows%E8%AE%BE%E7%BD%AE%E9%98%B2%E7%81%AB%E5%A2%99%E5%85%81%E8%AE%B8%E6%8C%87%E5%AE%9A%E5%BA%94%E7%94%A8%E6%AD%A3%E5%B8%B8%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[前言很长一段时间以来，防火墙这个神奇的“玩意”在我的电脑上基本属于名存死亡，因为做网络软件开发的缘故，经常需要别人连接我的电脑来进行测试，打开Windows的防火墙常常出现网络无法联通的情况，所以在我的电脑上防火墙基本处于关闭状态，就像下面这样： 但这确实不是一个好的习惯，最近学习了一些网络安全相关的课程，发现打开防火墙还是很有必要的，它可以避免我们遭受一些攻击，最起码可以降低受到攻击的概率，所以我还是决定打开防火墙，同时设置防火墙对自己的开发的应用放行，也就是把应用放到防火墙白名单里，这样就既安全又方便了。 什么是Windows防火墙 防火墙是一项协助确保信息安全的设备，会依照特定的规则，允许或限制传输的数据通过。防火墙可以是一台专属的硬件也可以是架设在一般硬件上的一套软件。总而言之，防火墙就是帮助计算机网络于其内、外网之间构建一道相对隔绝的保护屏障。 这段文字来自于百科，翻译的直白一点就是说：你的电脑就相当于一个城池，然后网络上的病毒、木马等恶意程序就相当于敌对势力派来的间谍，想混进城去，而防火墙就相当于城门口设置的关卡哨兵用来盘查筛选，放合适的人进城，同时放合法的人出去，既要满足城内人与外界的交流，又要防止恶意的坏人蒙混过关。 设置允许通过防火墙的应用要想设置指定应用允许通过防火墙，需要先打开防火墙功能再设置才会生效，下面以Windows10为例操作一次。 打开控制面板 按键盘上的 Win 键，然后输入控制面板，在筛选结果中点击“控制面板”选项： 点击系统和安全选项 点击Windows Defender防火墙选项 其实前面这三步可以简化为，按Win+R快捷键，输入firewall.cpl回车即可 点击启用或关闭Windows Defender防火墙选项 勾选启用 Windows Defender 防火墙 选项，点击确定按钮 点击允许应用或功能通过 Windows Defender 防火墙选项 找到目标应用，把后面两个选项勾选上，然后点击确定即可 这里以 WeChat 为例，在专用 和 公用两个复选框上打钩，然后单击下面的确定按钮就可以了，如果列表中没有想要添加白名单的应用，还可以点击确定按钮上面的 允许其他应用(R)按钮来自己添加。 防火墙中的专用和公用防火墙中的专用 和 公用是针对网络而言的，Windows中给出的说明：专用网络，例如家庭或工作网络；公用网络，比如机场和咖啡店中的网络。那么是不是当我在家里连网时就是专用网络，而在咖啡厅连接网络时就自动识别为公用网络呢？ Windows当然没有这么智能，连接新的网络时属于什么类型的网络需要你自己选择，也就是说你自己要做出判断，如果是自己家的网络，基本是上独享的安全的，那就可以设置为专用网络，如果是连接的公共Wifi那必须要选择公用网络。 而防火墙中对专用网络和公用网络有不同的设置，这个比较好理解，还是以前面城池和哨兵的比喻为例，防火墙作为哨兵在不同的环境下有不同的标准，比如在和平时期，对非常可疑的人才禁止入城，而在战时，除了必要的粮草供应，其他的普通商人可能都不允许入内了。 所以当一个应用无法联网时，需要检查一下应用设置白名单中是在哪个网络环境下，如果是可以信赖的应用，通常在专用网络和公用网络都会设置允许通过防火墙。 总结 快捷键 Win+R 输入 firewall.cpl 后回车，可以快速打开防火墙设置 Windows中的专用网络和公用网络需要自己在连接网络时设置，系统本身没有准确分辨的能力 防火墙中对专用网络和公用网络有不同的设置，相当于在不同的网络环境下设置了不同的安全级别 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 默默的付出是否值得？一查问题就消耗几个小时，修复个BUG前前后后花了几天，有时会假设站在高处，望着此时的自己，这样做是否值得？成年人的世界往往只看性价比，如果说花了几天时间却一无所获，那之前的付出确实不值得，也就是做了一件性价比很低的事情。但是这个结果是做完才知道的，在做之前并不能准确判断是否值得，无法进行“挑选”，也不应该抱着我不做总有人会做的态度，既然做就全力以赴，若无果也仅仅是我们“赌”输了而已。但要记住，现实的世界只记得功劳，没有结论、没有反馈、毫无借鉴意义的苦劳一文不值~ 2021-12-26 23:54:19]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>Win10</tag>
        <tag>windows</tag>
        <tag>防火墙</tag>
        <tag>firewall</tag>
        <tag>白名单</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git常用配置——git show/diff tab 显示宽度]]></title>
    <url>%2Fblog%2F2021%2F12%2F19%2Fgit%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94git-show-diff-tab-%E6%98%BE%E7%A4%BA%E5%AE%BD%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[前言平时在代码开发过程中使用编辑器时一般会使用空格来代替Tab符，而Tab的宽度通常设置为4个空格的宽度。在使用git diff 命令时发现有些老代码的格式混乱，原因就是使用了8个字符宽度的Tab显示格式，经过一番查找发现可以修改 core.pager 来进行调整，特此记录方便日后查找。 常用配置 设置Tab显示为4个字符的宽度 git config --global core.pager &#39;less -x1,5&#39; 设置Tab显示为8个字符的宽度（默认） git config --global core.pager &#39;less&#39; 设置用户名 git config --global user.name &quot;albert&quot; 设置邮箱 git config --global user.email &quot;albert52190@gmail.com&quot; 设置别名 git config --global alias.st status 配置自动更新子模块的命令 git config --global alias.pullall &#39;!f(){ git pull &quot;$@&quot; &amp;&amp; git submodule update --init --recursive; }; f&#39; 设置自动处理换行符 git config --global core.autocrlf true true 在提交时将CRLF转换为LF，当签出代码时，LF会被转换成CRLFinput 在提交是将CRLF转换为LF，签出时不转换false 不进行转换 总结 git config --global core.pager &#39;less -x1,5&#39; 可将 git show/diff 命令的tab按照4字符宽度显示 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 时间不多了，还有很多事情没有做呢，虽说欲速则不达，但是有些事如果还不做可能就真的没机会了《一荤一素》 2021-12-20 00:09:54]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
        <tag>Git</tag>
        <tag>config</tag>
        <tag>pager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络工具nc的常见使用功能方法]]></title>
    <url>%2Fblog%2F2021%2F12%2F12%2F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7nc%E7%9A%84%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%8A%9F%E8%83%BD%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言nc 是一个Linux环境下常用的工具命令，可以用来帮助开发者查询和解决网路问题，通常被认为是 NetCat 工具的缩写，在网络工具中有“瑞士军刀”的美誉。 nc 在Linux环境下常常是自带的，如果你使用的Linux发行版本没有这个工具也可以自行安装，比如在CentOS上的安装命令就是 yum install nc -y，并且这个工具在Windows上也可以直接下载使用，下载页面地址为 netcat，简约而不简单。 nc netcat ncat这三个名字常常出现在一起，也常常混用，可以简单的认为它们都一样，但是如果要追究细节它们还是有些差异的。原始netcat（也就是nc），在2007年发布1.10稳定版本之后，就不再更新了，原作者是Hobbit。而ncat是nmap项目的作者Fyodor，在原始nc之上进行二次开发的另一款强大工具。另外socat、cryptcat等也属于是原始nc的升级，而原始nc在windows上有时会被杀毒软件查杀，这时可以考虑使用ncat、socat。 关于nc的不同，可以看下我本地的查看情况，第一条是在Ubuntu中的man手册说明，第二条是在CentOS中的man手册说明： 12345678910111213NC(1) BSD General Commands Manual NC(1)NAME nc — arbitrary TCP and UDP connections and listensSYNOPSIS nc [-46bCDdhklnrStUuvZz] [-I length] [-i interval] [-O length] [-P proxy_username] [-p source_port] [-q seconds] [-s source] [-T toskeyword] [-V rtable] [-w timeout] [-X proxy_protocol] [-x proxy_address[:port]] [destination] [port]DESCRIPTION The nc (or netcat) utility is used for just about anything under the sun involving TCP, UDP, or UNIX-domain sockets. It can open TCP connections, send UDP packets, listen on arbitrary TCP and UDP ports, do port scanning, and deal with both IPv4 and IPv6. Unlike telnet(1), nc scripts nicely, and separates error messages onto standard error instead of sending them to standard output, as telnet(1) does with some 123456789101112NCAT(1) Ncat Reference Guide NCAT(1)NAME ncat - Concatenate and redirect socketsSYNOPSIS ncat [OPTIONS...] [hostname] [port]DESCRIPTION Ncat is a feature-packed networking utility which reads and writes data across networks from the command line. Ncat was written for the Nmap Project and is the culmination of the currently splintered family of Netcat incarnations. It is designed to be a reliable back-end tool to instantly provide network connectivity to other applications and users. Ncat will not only work with IPv4 and IPv6 but provides the user with a virtually limitless number of potential uses. ncat 是 nmap 套件的一部分，关于nmap的介绍可以参考下来自百科的以下引用内容： nmap是linux最早的网络扫描工具和嗅探工具包，它可以用来扫描网络上电脑开放的网络连接端，确定哪些服务运行在那些连接端，并且推断出计算机运行的是哪个操作系统（这是亦称 fingerprinting）。它是网络管理员必用的软件之一，用以评估网络系统安全。 正如大多数被用于网络安全的工具，nmap 也是不少黑客及骇客爱用的工具 。系统管理员可以利用nmap来探测工作环境中未经批准使用的服务器，但是黑客会利用nmap来搜集目标电脑的网络设定，从而计划攻击的方法。 nc的用法nc是一个强大的网络工具，下面列举几个常见的用法。 测试udp端口是否可用如果是测试某个IP地址地址是否可以访问，通常会使用 ping 命令，执行之后如果可以到达就会得到数据反馈： 12345678910albert@home-pc:~$ ping 82.156.125.169PING 82.156.125.169 (82.156.125.169) 56(84) bytes of data.64 bytes from 82.156.125.169: icmp_seq=1 ttl=54 time=17.8 ms64 bytes from 82.156.125.169: icmp_seq=2 ttl=54 time=39.9 ms64 bytes from 82.156.125.169: icmp_seq=3 ttl=54 time=12.9 ms64 bytes from 82.156.125.169: icmp_seq=4 ttl=54 time=6.81 ms^C--- 82.156.125.169 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3008msrtt min/avg/max/mdev = 6.816/19.397/39.961/12.502 ms 想要测试一个tcp端口是否可以访问，映入脑海的第一个命令应该是telnet，直接在IP后面加空格和端口就可以了： 12345albert@home-pc:~$ telnet 82.156.125.169 22Trying 82.156.125.169...Connected to 82.156.125.169.Escape character is '^]'.SSH-2.0-OpenSSH_7.4 如果想测试UDP端口是否可达使用telnet就不行了，因为它是一种基于TCP的应用层协议，用来测试UDP端口会因为长时间没有反应而超时： 123albert@home-pc:~$ telnet 82.156.125.169 666Trying 82.156.125.169...telnet: Unable to connect to remote host: Connection refused 这时用 nc 命令就可以解决了，命令格式为 nc -nvuz ip port： 12albert@home-pc:~$ nc -nvuz 82.156.125.169 666Connection to 82.156.125.169 666 port [udp/*] succeeded! 端口扫描这实际实际上是对上一个应用的扩展，使用 nc 命令可以指定一个端口范围，用来扫描多个端口是否可用： 123456albert@home-pc:~$ nc -nvz 82.156.125.169 20-24nc: connect to 82.156.125.169 port 20 (tcp) failed: Connection refusednc: connect to 82.156.125.169 port 21 (tcp) failed: Connection refusedConnection to 82.156.125.169 22 port [tcp/*] succeeded!nc: connect to 82.156.125.169 port 23 (tcp) failed: Connection refusednc: connect to 82.156.125.169 port 24 (tcp) failed: Connection refused 一对一聊天使用 nc 命令可以监听一个端口作为服务器，然后在另一台机器上启动 nc 作为客户端发数据： 12345# 启动服务器[root@VM-0-3-centos ~]# nc -l 1314# 以下为接收到客户端发来的数据12client send msg 12345# 启动客户端albert@home-pc:~$ nc 82.156.125.169 1314# 以下为输入的数据12client send msg 传输文件这个用法是对上一种用法的扩展，通过重定向将文件内容通过网络传输： 12# 服务端将socket内容保存到w文件中[root@VM-0-3-centos ~]# nc -l 1314 &gt; w.txt 123456789albert@home-pc:~$ cat w.txtw-rxd# 客户端将w.txt文件内容发送给服务器albert@home-pc:~$ nc 82.156.125.169 1314 &lt; w.txtalbert@home-pc:~$ 客户端运行完 nc 命令之后就会退出，服务器端的 nc 命令也会结束，w.txt 文件的内容就传送过去了。 端口转发这其实也是传输数据的命令的一个变种，使用 -c 参数可以完成数据转发： 1[root@VM-0-3-centos ~]# nc -l 520 -c "nc 82.156.125.169 1314" 执行命令之后，发送到本机 520 端口的数据就会被转发到 IP 为82.156.125.169的1314端口上了。 总结 nc -nvuz ip port 可以检测指定IP的UDP端口能否访问，如果是TCP端口去掉 -u 选项就可以了 nc -l port 可以启动一个本地服务器，接受发往指定端口的数据，并打印到控制台 nc -l port &gt; filename 可以启动一个本地服务器，接受发往指定端口的数据，并保存到名为 filename 的文件中 nc -l port -c &quot;nc ip new_port&quot; 可以启动一个本地服务器，接受发往指定端口的数据，并转发到ip:new_port的机器上 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生每时每刻都在积攒素材，能否拼凑出高光的时刻，决定了某些事迹有无机会诉说。 2021-12-12 22:11:51]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>nc</tag>
        <tag>NetCat</tag>
        <tag>ncat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次使用Valgrind查找解决内存问题的玄幻旅程]]></title>
    <url>%2Fblog%2F2021%2F12%2F02%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8Valgrind%E6%9F%A5%E6%89%BE%E8%A7%A3%E5%86%B3%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E7%9A%84%E7%8E%84%E5%B9%BB%E6%97%85%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言看题目来说这应该是一篇教程式文章，但为了突出“玄幻”二字，我们不讲细节只讲过程，在过程中体会解决问题的方式和方法，以及避免一些我在这个过程中绕的弯路，如果想找工具的详细使用方法可以去参考文章中翻一翻，有几篇文章写的真不错，下面我们开始扯淡啦。 玄幻旅途 本故事并非虚构，如有雷同，纯属命苦~ 故事背景作为本文主人公的我——小Z，是一个后端C/C++搬运工（这不废话吗，不是C系列谁老倒腾指针和内存？），在一个阳（yue）光（hei）明（feng）媚（gao）的下（wan）午（shang）接到一个完善游戏战斗系统的需求，然后便开始了紧张开发、积极调试的每一天，事情比较顺利，一切都在计划之中，不过平凡的日子总是无趣，没有一点点意外总让人感到有点意外。 初入泥潭好吧，到目前为止一切都很顺利，服务器各个功能模块分批完成，终于完成了最后拼装，启动调试看结果，出现了一点点逻辑问题，这个战斗过程根本停不下来，整个程序一直在递归，最终导致函数调用栈溢出崩溃。不过这都是小问题，简单梳理逻辑后增加必要的出口判断条件，问题很快被解决。 再次启动调试，程序正常运行，符合预期结果。啥？这就完了，幸福来的有点突然啊，整个流程基本符合需求，只是缺少一些细节逻辑需要补充，感觉胜利就在前方了啊！ 一片混沌补充细节的过程中，也需要不断调试来验证结果，咦？怎么连不上服务器了？查看一下进程，果然服务器进程已经不在了，难道是我不小心关掉了，先记一下，解决掉手头上更重要的问题后再来看它。 上次的问题好几天都没有出现了，可能真的是我不小心把服务器进程关掉了，今天还有个小BUG需要修复一下，先搭建好调试环境准备定位一下问题。整个过程比较顺利，没过几分钟BUG就找到了，修复后调试看看结果，Duang！进程挂了，好在这次是在调试状态，能看到是哪里引发的崩溃，查看函数调用栈来看看是谁捣的鬼。 什么玩意，智能指针出作用域时自动析构挂了？这是什么鬼，从上到下看了一遍近百层的函数调用关系，感觉没什么问题啊，真是奇怪。 重新启动进程，开始了疯狂测试，跑了20几次相同的逻辑，没有任何问题啊，那刚刚发生了什么，转过头来继续看刚刚出现崩溃的位置，完全找不到问题。这个问题先放一放，继续补充细节，调试解决发现的BUG，在多次调试之后，Duang！进程又挂了，这次更离谱，在定义lambda表达式的时候崩了，看着函数调用栈依旧一头雾水，看不出是什么问题。 退出调试状态，重启进程，继续跑了10多次相同的逻辑，这次进程真的崩溃了，看来程序真的是有隐藏的BUG。再次重启，继续跑，这次又不崩溃了，这种状况让人有点头大啊。启动调试状态开始测试，跑了几次就崩溃了，原来和调试有关系呀！经过多次测试发现，如果在调试状态下测试几次就会出现崩溃的情况，如果在非调试状态下大概需要跑10多次才会崩。 为了查出问题便开始在调试状态下更加疯狂的测试，这次真的开了眼了，每次崩溃的位置都不太一样，有的在析构函数中，有的设置变量值时，有的在发送函数中，有的在申请内存时，总体来看基本都是围绕着内存出现的问题，但是问题原因未知。 追根溯源虽然经过大量测试仍不能准确给出问题原因，但是几十次的崩溃结果中还是能看到一些规律的，其中有50%左右出现在第二场战斗释放之前战斗对象的时候，40%出现在玩家重新登录释放之前战斗对象的时候，这两种情况加在一起就占了绝大多数，所以要从这里开始入手，查看释放战斗对象的函数是不是存在问题。 因为程序中很少直接使用简单的指针，基本都会用智能指针来代替，所以在战斗对象析构时会有很多小对象自动析构，花了不少时间来看这些代码，结果一无所获，这就怪了，那么多次崩溃都是在这，居然找不到任何问题。 抽丝剥茧因为之前测试时需要完成跑完整个战斗流程，严重影响了测试效率，既然感觉释放战斗对象这部分代码有问题，那就单独跑这一段逻辑呗，单独建个分支，改代码！！！另外还发现一个事情，本来在我机器上需要在调试状态下跑好几次才能重现出的问题，在另一台发布机上两三次就能重现，干脆用它来验证结果。 说干就干，从原来的逻辑中，剥离出创建、释放战斗对象的代码，每次测试重复创建和释放过程几百次，这样就应该很容易就能重现问题了，修改完本地先测试，结果跑了十几次也没出现，部署到发布机上测试多次也没出现问题，和预想的完全不一样，实验失败，这个结果基本说明我的方向错误，并不是这段释放战斗对象的逻辑代码问题，又得重新寻找线索了。 大海捞针上面的验证虽说失败了，但也给我提了醒，既然释放战斗对象的逻辑代码没问题，但是绝大多数奔溃还发生在这里，那肯定是别人把它影响了，结合之前看到的内存问题，应该是有其他的逻辑写错了内存数据，导致释放战斗对象的内存时出现了问题。 这个崩溃在主分支是没有出现过的，在我开发完这个新需求之后才出现了这个问题，那么需要查新加了哪些代码，但是这个版本单单是新的文件就增加了几十个，要想从中找到一个内存问题犹如大海捞针一样。 祭出法宝在大量代码中直接寻找内存问题，非寻常人所能企及，这时可以考虑借助第三方力量——比如检测工具，根据以往经验，我用的最多的内存检测工具是 Valgrind 和 AddressSanitizer，起初 Valgrind 用的比较多的，后来认识了 AddressSanitizer 之后发现使用 Valgrind 后程序运行太慢了，而使用 AddressSanitizer 虽然需要重新编译一次，但是基本不影响原有程序的运行速度，所以渐渐偏向了 ASAN。 但是，这次我先用了 Valgrind，还原代码，重新编译，调整参数后启动服务器程序，果然是半天没反应，测试多次之后居然没崩溃，查看了它的检测报告也没发现什么问题，决定换 ASAN 试试，因为每次用 Valgrind 启动和运行真的太慢了。 修改Makefile重新编译，使用 AddressSanitizer 来进行检测，这次更奇怪，添加了 ASAN 选项的程序编译后，貌似代码逻辑感觉到了它（ASAN）的存在，程序运行逻辑直接变了，原来能完整跑完的战斗逻辑，总是跑到一半因为条件不满足停下了，不过有几次跑到了最后，也出现了崩溃的情况，但是从检测报告中未查到问题的原因，仅仅找到一处内存泄漏问题，修改完崩溃问题依旧存在。 屏蔽无关既然上面的工具没能提供帮助，那么还得依靠我硬啃代码了，还是先来分析之前各种崩溃结果，发现每次析构对象前都给客户端发了消息，而这些消息使用了 protobuf 中 oneof 结构，这个结构之前没用过，会不会因为使用不当，把内存写坏了。 这次我没有直接去看代码的细节，而是采用了屏蔽的方式，将一些不影响战斗逻辑的消息数据精简，不断注释代码，不断发布测试，结果依旧崩溃，最后仅剩一处同步技能的协议，其中也用了 oneof 结构，这时我更加感觉它有问题，但是它不能被注释掉，需要通过它发消息给客户端，然后客户端请求放技能才能将战斗进行下去，测试暂时卡在这了。 移形换位必须想一种办法把这仅剩的一条消息同步去掉，如果不给客户端的同步消息，客户端就不能通知服务器放技能，那只好服务器自己把这些事都做了，修改服务器代码，采用延迟触发的方法，来驱动整个战斗进程能进行下去，最终把仅剩的那一条消息屏蔽掉了，同时把所有的try-catch也屏蔽了。 打包部署发布服，启动测试，问题依旧存在，唉，我麻了！ 再请法宝因为 ASAN 这个工具我一直在观察着输出的报告，并没有发现什么值得注意的问题，所以我打算换为 Valgrind，因为它们两个有点冲突，所以得把Makefile还原回去，重新编译再使用 Valgrind 来测试。 启动程序，依旧卡的像时间静止了一样，启动客户端开始了常规的疯狂测试，Duang！进程挂了，赶紧打开 Valgrind 的输出报告看看，亲人呐，我在里面找到了 Invalid write 的字样。 赶紧去查看这段报告对应的代码问题，其中包含了 std::sort 函数的使用，但是自定义的排序函数不满足严格弱排序规则，感觉这逻辑确实有问题，把它先注释掉来试一下。 风平浪静注释掉 std::sort 之后，在本地机器测试半小时未发生崩溃，重新编译打包发布，几十次测试之后也没有发生崩溃的情况，一切又恢复了平静。 若有所思如果在第一次使用工具时，我给予Valgrind 多一点点宽容就好了。 其实事后看来好像没有多磨曲折，但是真实情况却是，前面的步骤交叉进行，经常会出现反复的情况，前前后后调试了近3天。 为什么如此执着？因为如果类似的问题不再早期发现时解决，后面要想再解决所付出的成本会更大，所以早发现早解决。 参考文章 AddressSanitizerLeakSanitizer 内存错误检测工具-AddressSanitizer（ASAN） 查找内存错误 c++中智能指针使用小结 静态或者全局智能指针使用的注意几点 谈谈如何利用 valgrind 排查内存错误 几个C++内存泄漏和越界检测工具简介 内存泄漏检测工具valgrind神器 使用valgrind检查内存问题 Valgrind学习笔记(一) 关于C#：valgrind-地址是在分配大小为16的块之前的8个字节 c++ seg fault issue: gnu_cxx::exchange_and_add 记一次 TCMalloc Debug 经历 Segmentation fault in gnu_cxx::exchange_and_add () from /usr/lib64/libstdc++.so.6 C++中使用std::sort自定义排序规则时要注意的崩溃问题 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 靠想象打开未来一扇扇大门，靠理性选择其中正确的一扇~ 2021-12-5 01:03:26]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>linux</tag>
        <tag>Valgrind</tag>
        <tag>内存问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下恢复rm误删的文件]]></title>
    <url>%2Fblog%2F2021%2F11%2F25%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%81%A2%E5%A4%8Drm%E8%AF%AF%E5%88%A0%E7%9A%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言一提到在 linux 环境下删除文件，那绝对离不开 sudo rm -rf /* 这个梗，每次看到这个命令，我都想到一幅恶搞的图片： 这个『清理垃圾』的说明真是解释的“恰到好处”，据说有小白在论坛问问题，被人开玩笑的回复了 sudo rm -rf /* 这个命令，结果问题就解决了，人也拜拜了~ 从删库到跑路，一天一个入狱小技巧，所以我们一定要谨慎使用 rm -rf 命令，这相当于我们在 Windows 上使用 Shift+Delete 组合，文件不会放到回收站中，而是直接永久删除了，在 Linux 中执行 rm 命令就相当于永久删除。 虽说 sudo rm -rf /* 危险无比，但是我们很少会直接这样写，但是 rm 命令还是经常用的，这不就在前几天，辛辛苦苦写的Shell脚本就被我直接 rm -rf 删掉了，幸亏我之前将内容打印到了控制台，否则整个脚本就白写了。 rm之后还有救吗尽管 rm 命令表示永久删除，但是不代表文件就一定找不回来，只是找回的几率有大有小。其实删除命令只是在文件节点中作删除标记，并不真正清除文件内容，如果删除后马上进行恢复，那么成功的概率还是很大的，但是如果其他用户一直在用这台机器，或者有一些写盘操作的进程一直在执行，那么这部分数据可能很快就会被覆盖。这时基本上就无法恢复该文件了。 使用foremost找回文件foremost 是一个基于文件头和尾部信息以及文件的内建数据结构恢复文件的命令行工具，接下来是安装和恢复的步骤： 安装 foremost CentOS系统直接运行 sudo yum install https://forensics.cert.org/centos/cert/7/x86_64//foremost-1.5.7-13.1.el7.x86_64.rpm -y 命令就可以完成 如果是Ubuntu系统请尝试命令：sudo apt install foremost 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@VM-0-3-centos ~]# sudo yum install https://forensics.cert.org/centos/cert/7/x86_64//foremost-1.5.7-13.1.el7.x86_64.rpm -yLoaded plugins: fastestmirror, langpacksRepository epel is listed more than once in the configurationforemost-1.5.7-13.1.el7.x86_64.rpm | 46 kB 00:00:01Examining /var/tmp/yum-root-XJIfxM/foremost-1.5.7-13.1.el7.x86_64.rpm: foremost-1.5.7-13.1.el7.x86_64Marking /var/tmp/yum-root-XJIfxM/foremost-1.5.7-13.1.el7.x86_64.rpm to be installedResolving Dependencies--&gt; Running transaction check---&gt; Package foremost.x86_64 0:1.5.7-13.1.el7 will be installed--&gt; Finished Dependency Resolutionepel/7/x86_64 | 4.7 kB 00:00:00epel/7/x86_64/group_gz | 96 kB 00:00:00epel/7/x86_64/updateinfo | 1.0 MB 00:00:00epel/7/x86_64/primary_db | 7.0 MB 00:00:01extras/7/x86_64 | 2.9 kB 00:00:00extras/7/x86_64/primary_db | 243 kB 00:00:00os/7/x86_64 | 3.6 kB 00:00:00updates/7/x86_64 | 2.9 kB 00:00:00updates/7/x86_64/primary_db | 12 MB 00:00:01Dependencies Resolved===================================================================================================================== Package Arch Version Repository Size=====================================================================================================================Installing: foremost x86_64 1.5.7-13.1.el7 /foremost-1.5.7-13.1.el7.x86_64 85 kTransaction Summary=====================================================================================================================Install 1 PackageTotal size: 85 kInstalled size: 85 kDownloading packages:Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : foremost-1.5.7-13.1.el7.x86_64 1/1 Verifying : foremost-1.5.7-13.1.el7.x86_64 1/1Installed: foremost.x86_64 0:1.5.7-13.1.el7Complete![root@VM-0-3-centos ~]# 创建一个测试文件 123456[root@VM-0-3-centos ~]# echo "this is a important file"&gt;important.txt[root@VM-0-3-centos ~]# pwd/root[root@VM-0-3-centos ~]# lsconnecttendis.sh important.txt restore tarlist tendis test.iso[root@VM-0-3-centos ~]# mkdir -p /tmp/restore 删除文件后尝试还原 1234[root@VM-0-3-centos ~]# rm important.txt[root@VM-0-3-centos ~]# foremost -i /dev/vda1 -o /tmp/restore/Processing: /dev/vda1|***********Segmentation fault 执行几分钟之后崩溃，恢复失败，打开目录查看发现： 123[root@VM-0-3-centos ~]# ls /tmp/restore/audit.txt bmp doc exe htm jpg mov mpg pdf ppt rar sdw sxc sxw wav xls zipavi dll docx gif jar mbd mp4 ole png pptx rif sx sxi vis wmv xlsx 看来与需要恢复的文件类型有关，换台机器再换一个png文件试试，先找一个showball.png测试文件，然后确认分区 /dev/vda1 12345678910111213[root@VM-0-3-centos ~]# dfFilesystem 1K-blocks Used Available Use% Mounted ondevtmpfs 930496 0 930496 0% /devtmpfs 941004 24 940980 1% /dev/shmtmpfs 941004 508 940496 1% /runtmpfs 941004 0 941004 0% /sys/fs/cgroup/dev/vda1 51473868 6458344 42692404 14% //dev/loop0 361650 361650 0 100% /mnt/isotmpfs 188204 0 188204 0% /run/user/0[root@VM-0-3-centos ~]# pwd/root[root@VM-0-3-centos ~]# lsrestore showball.png 删除png文件后尝试恢复，进度条结束后即可进入指定的目录 /root/restore/ 查看 1234567[root@VM-0-3-centos ~]# rm showball.pngrm: remove regular file ‘showball.png’? y[root@VM-0-3-centos ~]# pwd/root[root@VM-0-3-centos ~]# foremost -t png -i /dev/vda1 -o /root/restore/Processing: /dev/vda1|*********************************************************************************************| 在指定目录下会有一个 audit.txt 统计文件和一个类型文件夹 png 123456789101112131415[root@VM-0-3-centos ~]# ll restore/total 40-rw-r--r-- 1 root root 24548 Nov 27 22:57 audit.txtdrwxr-xr-- 2 root root 16384 Nov 27 22:56 png[root@VM-0-3-centos ~]# cd restore/[root@VM-0-3-centos restore]# ll pngtotal 43764-rw-r--r-- 1 root root 3500 Nov 27 22:53 00367400.png-rw-r--r-- 1 root root 3578 Nov 27 22:53 00367408.png-rw-r--r-- 1 root root 3445 Nov 27 22:53 00367416.png-rw-r--r-- 1 root root 368 Nov 27 22:53 00367432.png-rw-r--r-- 1 root root 363 Nov 27 22:53 00367456.png-rw-r--r-- 1 root root 392 Nov 27 22:53 00367464.png-rw-r--r-- 1 root root 199 Nov 27 22:53 00367616.png... png目录下的文件名都是一些编号，与原来删除的文件完全不一样了，需要根据 audit.txt 文件确认，打开文件确认一下： 123456789101112131415161718192021222324252627282930313233[root@VM-0-3-centos restore]# head audit.txtForemost version 1.5.7 by Jesse Kornblum, Kris Kendall, and Nick MikusAudit FileForemost started at Sat Nov 27 22:53:48 2021Invocation: foremost -t png -i /dev/vda1 -o /root/restore/Output directory: /root/restoreConfiguration file: /etc/foremost.conf------------------------------------------------------------------File: /dev/vda1Start: Sat Nov 27 22:53:48 2021[root@VM-0-3-centos restore]# head -n 20 audit.txtForemost version 1.5.7 by Jesse Kornblum, Kris Kendall, and Nick MikusAudit FileForemost started at Sat Nov 27 22:53:48 2021Invocation: foremost -t png -i /dev/vda1 -o /root/restore/Output directory: /root/restoreConfiguration file: /etc/foremost.conf------------------------------------------------------------------File: /dev/vda1Start: Sat Nov 27 22:53:48 2021Length: 49 GB (53686025728 bytes)Num Name (bs=512) Size File Offset Comment0: 00367400.png 3 KB 188108800 (16 x 16)1: 00367408.png 3 KB 188112896 (16 x 16)2: 00367416.png 3 KB 188116992 (16 x 16)3: 00367432.png 368 B 188125184 (16 x 16)4: 00367456.png 363 B 188137472 (16 x 16)5: 00367464.png 392 B 188141568 (16 x 16)... audit.txt 文件中记录着恢复文件的简要信息，这需要你知道原来删除文件的相关信息，不然就只能一个个打开查看了，我是通过分辨率查找的 123[root@VM-0-3-centos restore]# grep "1217" audit.txt116: 12888200.png 40 KB 6598758400 (1217 x 690)360: 38088960.png 40 KB 19501547520 (1217 x 690) 根据过滤出的信息把 12888200.png 打开发现就是自己“误删”的文件这就恢复好了 使用extundelete找回文件extundelete 支持ext3、ext4文件系统下的文件恢复，使用 cat /etc/fstab 可以在linux环境下查看文件系统类型 1234567[root@VM-0-3-centos ~]# cat /etc/fstabUUID=21dbe030-aa71-4b3a-8610-3b942dd447fa / ext4 noatime,acl,user_xattr 1 1proc /proc proc defaults 0 0sysfs /sys sysfs noauto 0 0debugfs /sys/kernel/debug debugfs noauto 0 0devpts /dev/pts devpts mode=0620,gid=5 0 0[root@VM-0-3-centos ~]# 安装依赖文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@VM-0-3-centos ~]# yum install e2fsprogs-develLoaded plugins: fastestmirror, langpacksRepository epel is listed more than once in the configurationDetermining fastest mirrorsepel | 4.7 kB 00:00:00extras | 2.9 kB 00:00:00os | 3.6 kB 00:00:00updates | 2.9 kB 00:00:00(1/2): epel/7/x86_64/updateinfo | 1.0 MB 00:00:00(2/2): epel/7/x86_64/primary_db | 7.0 MB 00:00:01Resolving Dependencies--&gt; Running transaction check---&gt; Package e2fsprogs-devel.x86_64 0:1.42.9-19.el7 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved=============================================================================================================== Package Arch Version Repository Size===============================================================================================================Installing: e2fsprogs-devel x86_64 1.42.9-19.el7 os 73 kTransaction Summary===============================================================================================================Install 1 PackageTotal download size: 73 kInstalled size: 162 kIs this ok [y/d/N]: yDownloading packages:e2fsprogs-devel-1.42.9-19.el7.x86_64.rpm | 73 kB 00:00:00Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : e2fsprogs-devel-1.42.9-19.el7.x86_64 1/1 Verifying : e2fsprogs-devel-1.42.9-19.el7.x86_64 1/1Installed: e2fsprogs-devel.x86_64 0:1.42.9-19.el7Complete![root@VM-0-3-centos ~]# 下载extundelete源码 1234567891011121314[root@VM-0-3-centos ~]# wget https://src.fedoraproject.org/repo/pkgs/extundelete/extundelete-0.2.4.tar.bz2/77e626ad31433680c0a222069295d2ca/extundelete-0.2.4.tar.bz2--2021-11-28 18:36:15-- https://src.fedoraproject.org/repo/pkgs/extundelete/extundelete-0.2.4.tar.bz2/77e626ad31433680c0a222069295d2ca/extundelete-0.2.4.tar.bz2Resolving src.fedoraproject.org (src.fedoraproject.org)... 38.145.60.20, 38.145.60.21Connecting to src.fedoraproject.org (src.fedoraproject.org)|38.145.60.20|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 108472 (106K) [application/x-bzip2]Saving to: ‘extundelete-0.2.4.tar.bz2’100%[============================================================================&gt;] 108,472 33.5KB/s in 3.2s2021-11-28 18:36:20 (33.5 KB/s) - ‘extundelete-0.2.4.tar.bz2’ saved [108472/108472][root@VM-0-3-centos ~]# lsextundelete-0.2.4.tar.bz2 解压extundelete源码 1234567891011121314151617181920212223242526272829303132[root@VM-0-3-centos ~]# tar -jxvf extundelete-0.2.4.tar.bz2extundelete-0.2.4/extundelete-0.2.4/acinclude.m4extundelete-0.2.4/missingextundelete-0.2.4/autogen.shextundelete-0.2.4/aclocal.m4extundelete-0.2.4/configureextundelete-0.2.4/LICENSEextundelete-0.2.4/READMEextundelete-0.2.4/install-shextundelete-0.2.4/config.h.inextundelete-0.2.4/src/extundelete-0.2.4/src/extundelete.ccextundelete-0.2.4/src/block.hextundelete-0.2.4/src/kernel-jbd.hextundelete-0.2.4/src/insertionops.ccextundelete-0.2.4/src/block.cextundelete-0.2.4/src/cli.ccextundelete-0.2.4/src/extundelete-priv.hextundelete-0.2.4/src/extundelete.hextundelete-0.2.4/src/jfs_compat.hextundelete-0.2.4/src/Makefile.inextundelete-0.2.4/src/Makefile.amextundelete-0.2.4/configure.acextundelete-0.2.4/depcompextundelete-0.2.4/Makefile.inextundelete-0.2.4/Makefile.am[root@VM-0-3-centos ~]# cd extundelete-0.2.4/[root@VM-0-3-centos extundelete-0.2.4]# lsacinclude.m4 aclocal.m4 autogen.sh config.h.in configure configure.ac depcompinstall-sh LICENSE Makefile.am Makefile.in missing README src[root@VM-0-3-centos extundelete-0.2.4]# 编译xtundelete源码安装 12345678910111213141516[root@VM-0-3-centos extundelete-0.2.4]# ./configure --prefix=/usr/local/extundelete &amp;&amp; make &amp;&amp; make installConfiguring extundelete 0.2.4Writing generated files to diskmake -s all-recursiveMaking all in srcextundelete.cc: In function ‘ext2_ino_t find_inode(ext2_filsys, ext2_filsys, ext2_inode*, std::string, int)’:extundelete.cc:1272:29: warning: narrowing conversion of ‘search_flags’ from ‘int’ to ‘ext2_ino_t &#123;aka unsigned int&#125;’ inside &#123; &#125; [-Wnarrowing] buf, match_name2, priv, 0&#125;; ^Making install in src /usr/bin/install -c extundelete '/usr/local/extundelete/bin'[root@VM-0-3-centos extundelete-0.2.4]# which extundelete/usr/bin/which: no extundelete in (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin)[root@VM-0-3-centos extundelete-0.2.4]# ll /usr/local/extundelete/bin/total 1296-rwxr-xr-x 1 root root 1323360 Nov 28 18:45 extundelete 如果在这一步报错 configure: error: C++ compiler cannot create executables，可以运行 yum -y install gcc-c++ 命令安装编译环境 准备测试文件 123456789101112[root@VM-0-3-centos examples]# df -TFilesystem Type 1K-blocks Used Available Use% Mounted ondevtmpfs devtmpfs 930496 0 930496 0% /devtmpfs tmpfs 941004 24 940980 1% /dev/shmtmpfs tmpfs 941004 508 940496 1% /runtmpfs tmpfs 941004 0 941004 0% /sys/fs/cgroup/dev/vda1 ext4 51473868 6465732 42685016 14% //dev/loop0 iso9660 361650 361650 0 100% /mnt/isotmpfs tmpfs 188204 0 188204 0% /run/user/0[root@VM-0-3-centos examples]# cp ../extundelete-0.2.4.tar.bz2 .[root@VM-0-3-centos examples]# lsextundelete-0.2.4.tar.bz2 查询文件的inode信息 我们选择刚刚下载的extundelete源码包作为“误删”的文件，先查看一下信息，-li 可以在第一列查看文件的inode信息，examples文件夹的inode值为1311798： 12345678[root@VM-0-3-centos ~]# ls examples/extundelete-0.2.4.tar.bz2[root@VM-0-3-centos ~]# ls -litotal 3616761311798 drwxr-xr-x 2 root root 4096 Nov 28 20:28 examples1310761 drwxr-xr-x 3 1000 1000 4096 Nov 28 18:45 extundelete-0.2.4 918157 drwxr-xr-x 2 root root 4096 Feb 28 2021 tarlist 396057 -rw-r--r-- 1 root root 370329600 Feb 27 2021 test.iso 删除测试文件，并用查询信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@VM-0-3-centos ~]# cd examples/[root@VM-0-3-centos examples]# lsextundelete-0.2.4.tar.bz2[root@VM-0-3-centos examples]# rm extundelete-0.2.4.tar.bz2 -f[root@VM-0-3-centos examples]# ls[root@VM-0-3-centos examples]# /usr/local/extundelete/bin/extundelete /dev/vda1 --inode 1311798NOTICE: Extended attributes are not restored.WARNING: EXT3_FEATURE_INCOMPAT_RECOVER is set.The partition should be unmounted to undelete any files without further data loss.If the partition is not currently mounted, this message indicatesit was improperly unmounted, and you should run fsck before continuing.If you decide to continue, extundelete may overwrite some of the deletedfiles and make recovering those files impossible. You should unmount thefile system and check it with fsck before using extundelete.Would you like to continue? (y/n)yLoading filesystem metadata ... 400 groups loaded.Group: 160Contents of inode 1311798:0000 | ed 41 00 00 00 10 00 00 7a 62 a3 61 1b 7a a3 61 | .A......zb.a.z.a0010 | 1b 7a a3 61 00 00 00 00 00 00 02 00 08 00 00 00 | .z.a............0020 | 00 00 08 00 0b 00 00 00 0a f3 01 00 04 00 00 00 | ................0030 | 00 00 00 00 00 00 00 00 01 00 00 00 79 20 50 00 | ............y P.0040 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0050 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0060 | 00 00 00 00 7c 63 ab ad 00 00 00 00 00 00 00 00 | ....|c..........0070 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................0080 | 1c 00 00 00 80 da 0d a3 80 da 0d a3 94 24 04 08 | .............$..0090 | 7a 62 a3 61 94 24 04 08 00 00 00 00 00 00 00 00 | zb.a.$..........00a0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00b0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00c0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00d0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00e0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................00f0 | 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 | ................Inode is AllocatedFile mode: 16877Low 16 bits of Owner Uid: 0Size in bytes: 4096Access time: 1638097530Creation time: 1638103579Modification time: 1638103579Deletion Time: 0Low 16 bits of Group Id: 0Links count: 2Blocks count: 8File flags: 524288File version (for NFS): 2913690492File ACL: 0Directory ACL: 0Fragment address: 0Direct blocks: 127754, 4, 0, 0, 1, 5251193, 0, 0, 0, 0, 0, 0Indirect block: 0Double indirect block: 0Triple indirect block: 0File name | Inode number | Deleted status. 1311798.. 393219extundelete-0.2.4.tar.bz2 396764 Deletedconftest.err 1311833 Deleted[root@VM-0-3-centos examples]# 我们发现 extundelete-0.2.4.tar.bz2 文件的状态为 `Deleted` 使用extundelete恢复文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@VM-0-3-centos ~]# /usr/local/extundelete/bin/extundelete /dev/vda1 --restore-directory /tmpNOTICE: Extended attributes are not restored.WARNING: EXT3_FEATURE_INCOMPAT_RECOVER is set.The partition should be unmounted to undelete any files without further data loss.If the partition is not currently mounted, this message indicatesit was improperly unmounted, and you should run fsck before continuing.If you decide to continue, extundelete may overwrite some of the deletedfiles and make recovering those files impossible. You should unmount thefile system and check it with fsck before using extundelete.Would you like to continue? (y/n)yLoading filesystem metadata ... 400 groups loaded.Loading journal descriptors ... 31842 descriptors loaded.*** Error in `/usr/local/extundelete/bin/extundelete': double free or corruption (!prev): 0x00000000014d6020 ***======= Backtrace: =========/lib64/libc.so.6(+0x81299)[0x7f5c08190299]/usr/local/extundelete/bin/extundelete[0x40cdcb]/usr/local/extundelete/bin/extundelete[0x40fee6]/usr/local/extundelete/bin/extundelete[0x4045b4]/lib64/libc.so.6(__libc_start_main+0xf5)[0x7f5c08131555]/usr/local/extundelete/bin/extundelete[0x404aef]======= Memory map: ========00400000-0041c000 r-xp 00000000 fd:01 1311942 /usr/local/extundelete/bin/extundelete0061c000-0061d000 r--p 0001c000 fd:01 1311942 /usr/local/extundelete/bin/extundelete0061d000-0061e000 rw-p 0001d000 fd:01 1311942 /usr/local/extundelete/bin/extundelete0061e000-0061f000 rw-p 00000000 00:00 0014c6000-0176e000 rw-p 00000000 00:00 0 [heap]7f5c00000000-7f5c00021000 rw-p 00000000 00:00 07f5c00021000-7f5c04000000 ---p 00000000 00:00 07f5c07ca1000-7f5c07ef3000 rw-p 00000000 00:00 07f5c07ef3000-7f5c07f0a000 r-xp 00000000 fd:01 265649 /usr/lib64/libpthread-2.17.so7f5c07f0a000-7f5c08109000 ---p 00017000 fd:01 265649 /usr/lib64/libpthread-2.17.so7f5c08109000-7f5c0810a000 r--p 00016000 fd:01 265649 /usr/lib64/libpthread-2.17.so7f5c0810a000-7f5c0810b000 rw-p 00017000 fd:01 265649 /usr/lib64/libpthread-2.17.so7f5c0810b000-7f5c0810f000 rw-p 00000000 00:00 07f5c0810f000-7f5c082d2000 r-xp 00000000 fd:01 265623 /usr/lib64/libc-2.17.so7f5c082d2000-7f5c084d2000 ---p 001c3000 fd:01 265623 /usr/lib64/libc-2.17.so7f5c084d2000-7f5c084d6000 r--p 001c3000 fd:01 265623 /usr/lib64/libc-2.17.so7f5c084d6000-7f5c084d8000 rw-p 001c7000 fd:01 265623 /usr/lib64/libc-2.17.so7f5c084d8000-7f5c084dd000 rw-p 00000000 00:00 07f5c084dd000-7f5c084f2000 r-xp 00000000 fd:01 291206 /usr/lib64/libgcc_s-4.8.5-20150702.so.17f5c084f2000-7f5c086f1000 ---p 00015000 fd:01 291206 /usr/lib64/libgcc_s-4.8.5-20150702.so.17f5c086f1000-7f5c086f2000 r--p 00014000 fd:01 291206 /usr/lib64/libgcc_s-4.8.5-20150702.so.17f5c086f2000-7f5c086f3000 rw-p 00015000 fd:01 291206 /usr/lib64/libgcc_s-4.8.5-20150702.so.17f5c086f3000-7f5c087f4000 r-xp 00000000 fd:01 287349 /usr/lib64/libm-2.17.so7f5c087f4000-7f5c089f3000 ---p 00101000 fd:01 287349 /usr/lib64/libm-2.17.so7f5c089f3000-7f5c089f4000 r--p 00100000 fd:01 287349 /usr/lib64/libm-2.17.so7f5c089f4000-7f5c089f5000 rw-p 00101000 fd:01 287349 /usr/lib64/libm-2.17.so7f5c089f5000-7f5c08ade000 r-xp 00000000 fd:01 266798 /usr/lib64/libstdc++.so.6.0.197f5c08ade000-7f5c08cde000 ---p 000e9000 fd:01 266798 /usr/lib64/libstdc++.so.6.0.197f5c08cde000-7f5c08ce6000 r--p 000e9000 fd:01 266798 /usr/lib64/libstdc++.so.6.0.197f5c08ce6000-7f5c08ce8000 rw-p 000f1000 fd:01 266798 /usr/lib64/libstdc++.so.6.0.197f5c08ce8000-7f5c08cfd000 rw-p 00000000 00:00 07f5c08cfd000-7f5c08d3f000 r-xp 00000000 fd:01 267873 /usr/lib64/libext2fs.so.2.47f5c08d3f000-7f5c08f3f000 ---p 00042000 fd:01 267873 /usr/lib64/libext2fs.so.2.47f5c08f3f000-7f5c08f40000 r--p 00042000 fd:01 267873 /usr/lib64/libext2fs.so.2.47f5c08f40000-7f5c08f42000 rw-p 00043000 fd:01 267873 /usr/lib64/libext2fs.so.2.47f5c08f42000-7f5c08f45000 r-xp 00000000 fd:01 265948 /usr/lib64/libcom_err.so.2.17f5c08f45000-7f5c09144000 ---p 00003000 fd:01 265948 /usr/lib64/libcom_err.so.2.17f5c09144000-7f5c09145000 r--p 00002000 fd:01 265948 /usr/lib64/libcom_err.so.2.17f5c09145000-7f5c09146000 rw-p 00003000 fd:01 265948 /usr/lib64/libcom_err.so.2.17f5c09146000-7f5c09168000 r-xp 00000000 fd:01 265614 /usr/lib64/ld-2.17.so7f5c092b1000-7f5c0935d000 rw-p 00000000 00:00 07f5c09363000-7f5c09367000 rw-p 00000000 00:00 07f5c09367000-7f5c09368000 r--p 00021000 fd:01 265614 /usr/lib64/ld-2.17.so7f5c09368000-7f5c09369000 rw-p 00022000 fd:01 265614 /usr/lib64/ld-2.17.so7f5c09369000-7f5c0936a000 rw-p 00000000 00:00 07ffe581db000-7ffe581fc000 rw-p 00000000 00:00 0 [stack]7ffe581fc000-7ffe581fe000 r-xp 00000000 00:00 0 [vdso]ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall]Aborted[root@VM-0-3-centos ~]# 恢复失败，此路不通！！！ 有其他人成功了，但是我测试失败，释放内存崩溃，有大神给说一下怎么改源码吗？此处存疑，后续再测，先记录一下常用参数 查询inode文件状态：/usr/local/extundelete/bin/extundelete /dev/vda1 --inode 1311798 恢复指定节点数据：/usr/local/extundelete/bin/extundelete /dev/vda1 --restore-inode 1311798 恢复单个文件：/usr/local/extundelete/bin/extundelete /dev/vda1 --restore-file root/examples/extundelete-0.2.4.tar.bz2 恢复一个目录：/usr/local/extundelete/bin/extundelete /dev/vda1 --restore-files root/examples 恢复所有文件：/usr/local/extundelete/bin/extundelete /dev/vda1 --restore-all 预防误删引发的事故 定义别名，提示删除 定义别名 alias rm=&#39;rm -i&#39;， 在删除文件前会出现一个提示，使用 -i 选项来需要逐个确认要删除的文件，只有用户输入 y 才会将文件删除，但是这种做法在加上 -f 选项之后会失效。 禁用rm，使用mv代替 在系统中不允许直接使用rm命令直接删除文件，需要mv文件到指定的回收目录 ~/.delete，然后配合一个定时任务，每周清空~/.delete下文件，相当于手动创建了一个回收站。 总结 使用foremost恢复时的目标目录最好是另外一个磁盘中的目录，把文件恢复到被删除文件所在的磁盘中很可能会在恢复前覆盖被误删的文件 sodu 的全称目前有 substitute user do 和 super user do 两种说法，使用sudo通常是行驶超级用户的权限，但有时也可以其他普通用户，所以翻译成 substitute user do 代替其他用户来做更准确一点 foremost 支持的文件系统比较多，其中包括 ext2、 ext3 、vfat、NTFS、ufs、jfs 等，但是只能恢复特定格式的文件，而 extundelete 只支持ext3、ext4文件系统，不过可恢复的文件类型很多。 除了本文中总结的这两款不怎么好用的恢复软件，还有 testdisk 和 photorec 可以用来恢复，后续可以尝试一下 数据无价，请谨慎删除，可参考别名方法或禁用 rm -rf 来减少事故的发生 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 成年人，结果导向！没有功劳的苦劳是那么的虚弱无力~ 2021-11-28 22:14:54]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>rm</tag>
        <tag>rf</tag>
        <tag>回收站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[完全图与强连通图的那些坑]]></title>
    <url>%2Fblog%2F2021%2F11%2F17%2F%E5%AE%8C%E5%85%A8%E5%9B%BE%E4%B8%8E%E5%BC%BA%E8%BF%9E%E9%80%9A%E5%9B%BE%E7%9A%84%E9%82%A3%E4%BA%9B%E5%9D%91%2F</url>
    <content type="text"><![CDATA[前言图这个数据结构相比队列、栈、树来说算是复杂多了，关于图的问题也多如牛毛，先来看一下常见的问题： 若无向图 G 中含7个顶点，要想保证图 G 在任何情况下都是连通的，则需要的边数最少是几条？ 回答这种问题一定要注意细节，找到关键的点，不然一定会掉到坑里的。这个题关键点有以下几个： 7个顶点 任何条件下连通 最少几条边 其中第1点和第3点不容易出错，比较容易出现问题的是第2点，要想保证任何条件下连通，意思给定边数以后无论怎么连都能通？ 先说下答案是16，至于为什么，我们后面先复习一下图相关的概念再慢慢解释，因为此刻的我连什么是强联通图都忘了~ 一些概念 图：是由顶点V集和边E集构成，边表示了与之相连的两点间的关系，因此图可以表示成G = (V, E) 有向图：是指图中的两个顶点从A到B和从B到A的含义是不同的，我们认为两点的关系是有方向的，则称其为有向图 无向图：是指两点间的连接线无方向无关，这种图叫做无向图 连通性：从图中一个顶点到达另一顶点，若存在至少一条路径，则称这两个顶点是连通着的 连通图：在无向图中，如果任意两个顶点之间都能够连通，则称此无向图为连通图 完全图：在无向图中，如果任意两个顶点之间都边直接相连，则称此无向图为完全图 连通分量：若无向图不是连通图，但图中存在某个子图符合连通图的性质，则称该子图为连通分量 强连通图：在有向图中，若任意两个顶点之间包含至少来回两条通路，则称此有向图为强连通图 有向完全图：在有向图中，如果任意两个顶点之间都有相反的两条弧直接相连，则称此有向图为有向完全图 强连通分量：若有向图不是强连通图，但图中存在某个子图符合强连通图的性质，则称该子图为强连通分量 关于题目的解释这是一个无向图，要想在任何情况下都连通，那考虑极端情况就是孤立一个顶点，让尽可能多的边连接剩余的顶点，那会构成一个 n-1 个顶点的完全图，然后再考虑加一条边把剩下的孤立顶点连起来，这样得到的边数是 N = 5+4+3+2+1 + 1 = 16，用组合数表示就是 $$C^2_{n-1} + 1= (n-1) * (n-2) / 2 + 1$$ 题目变型 若无向图 G 中含7个顶点，要想保证图 G 在是连通的，至少需要几条边？ 答案6条，即 (n-1) 一个包含7个顶点的无向图 G 为完全图，那么它共有几条边？ 答案21条，即 n * (n-1) / 2 若有向图 G 中含7个顶点，要想保证图 G 在是强连通的，至少需要几条弧？ 答案7条，即 n，也就是形成一个环 一个包含7个顶点的有向图 G 为完全图，那么它共有几条弧？ 答案42条，即 n * (n-1) 补充两个图例 完全图，特点是任何两个顶点都有直接的边相连 1234567891011121314graph TB A((A))---B((B)); A((A))---C((C)); A((A))---D((D)); A((A))---E((E)); B((B))---C((C)); B((B))---D((D)); B((B))---E((E)); C((C))---D((D)); C((C))---E((E)); D((D))---E((E)); 强连通图，任意两点间都有路径可达 12345678graph TB A((A))--&gt;B((B)); B((B))--&gt;C((C)); C((C))--&gt;D((D)); D((D))--&gt;E((E)); E((E))--&gt;A((A)); A((A))--&gt;F((F)); F((F))--&gt;A((A));]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>图</tag>
        <tag>完全图</tag>
        <tag>强连通图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树的带权路径长度和哈夫曼树]]></title>
    <url>%2Fblog%2F2021%2F11%2F07%2F%E6%A0%91%E7%9A%84%E5%B8%A6%E6%9D%83%E8%B7%AF%E5%BE%84%E9%95%BF%E5%BA%A6%E5%92%8C%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91%2F</url>
    <content type="text"><![CDATA[前言树的所有叶子结点的带权路径长度之和，称为树的带权路径长度，英文缩写为 WPL，从百度百科中得到的信息为 “树的带权路径长度（weighted path length of tree）是2018年公布的计算机科学技术名词”，这就有点奇怪了，这个词印象中在大学课本里学过啊，怎么会是2018年的名词呢？难道我穿越了？ 我赶紧找来严蔚敏、吴伟民老师编著的《数据结构》翻开来看，在2009年9月第30次印刷的图书的144页中，明确的用加粗字体描述了这样个概念： 树的带权路径长度为树中所有叶子结点的带权路径之和，通常记作 WPL = … 看来我没记错，不是这个百科弄错了，就是2018年重新公布了一次，并不是新的概念，它确实是一个古老的名词了，接下来可以复习一下了。 树的带权路径长度前面虽然已经给出了定义，可什么是路径，为什么要带权，还要一步步来解释。 路径是指从树的一个结点到另一个结点所走过的部分，路径长度也可以理解为两个结点之间的距离，可以简单理解为路过的结点数，那为什么要带权呢？这和我们生活中的路径一样，并不是距离短的路所花费的时间就少，还要考虑路况、成本等多种因素，而权值就是在特定场景下我们赋予每条路的选择概率。 解释了这几个概念之后我们就可以理解文章开头的定义了，把树的每个叶子结点到根结点的带权路径长度加在一起，就是树的带权路径长度。 哈夫曼树当使用已知结点作为叶子结点，用其构成的所有树中，带全路径长度最小的树被称为最优二叉树，也就是哈夫曼树。 我们先来计算一下一颗二叉树的带权路径长度，二叉树形态如下： 1234567graph TB A((ROOT))--&gt;B((P)); A((ROOT))--&gt;C[[2]]; B((P))--&gt;D[[4]]; B((P))--&gt;F((R)); F((P))--&gt;G[[7]]; F((P))--&gt;H[[5]]; 计算二叉树的带权路径长度涉及到树的层数和权值，以上面这个图为例，ROOT 结点所在的层数为0层，往下数字2结点为1层，数字4结点为2层，数字7结点和5结点为3层，方块中的数字代表了该叶子结点的权值，那个这颗树的带权路径长度为： 7 3 + 5 3 + 4 2 + 2 1 = 46 那么这是一颗最优树吗？显然不是，因为它的带权路径长度不是最短，其实从计算公式也可以看出一点门道，计算带权路径长度时会用层数乘以权值，因为权值不会变，那么唯一能减小结果的就是调整层数，一个很直观的贪心思路就是把权值大的放在低层，权值小的放在高层，这样就可以减小最后的值，比如调整成这样： 1234567graph TB A((ROOT))--&gt;B((P)); A((ROOT))--&gt;C((P)); B((P))--&gt;D[[4]]; B((P))--&gt;F[[2]]; C((P))--&gt;G[[7]]; C((P))--&gt;H[[5]]; 这颗树的带权路径长度计算结果为36，比之前的值小了很多： 4 2 + 2 2 + 7 2 + 5 2 = 36 其实这还不是一颗最优的树，最优的结构应该是这样： 1234567graph TB A((ROOT))--&gt;B[[7]]; A((ROOT))--&gt;C((P)); C((P))--&gt;D[[5]]; C((P))--&gt;F((P)); F((P))--&gt;G[[2]]; F((P))--&gt;H[[4]]; 它的带权路径长度计算结果为5，从这可以看出，树的层数高的不一定计算成的带权路径长度就大。 作用前面说了这么多，那么哈夫曼树有什么作用呢？你应该听说过哈夫曼编码吧，这其实就是哈夫曼树的一个应用，用来找到存放一串字符所需的最少的二进制编码。存放二进制还要单独编码吗？也许你想说什么英文字母不都是编好的吗？ 单纯用字母来传递信息有一个问题，那就是会造成浪费，因为每个字母在日常交流中出现的次数并不一样，比如字母 e 是英文中出现频率最高的字母，而字母 z 却出现的很少，所以可以用较短的编码来表示 e 用较长的编码来表示字母 z，这样很直观的就能感觉到同样的信息采取这种方式处理之后会占用更小的空间。 构建哈夫曼树假设有一段英文文件，我们先统计这个文件中每个字母的出现得到次数，统计如下（别问我这个文件写的什么，我胡诌的(#^.^#)）： a:19b:6c:7d:3e:32f:10g:21h:2 因为哈夫曼树使用叶子结点来推导最终的编码，所有我们先用这些数字作为叶子结点： 123456789graph TB A[[19]] B[[6]] C[[7]] D[[3]] E[[32]] F[[10]] G[[21]] H[[2]] 接下来记住一个原则，那就是找当前树的根结点和剩余叶子结点的最小的两个值，然后组成新的树杈。 首先，从19、6、7、3、32、10、21、2 中选择频数最小的两个叶子结点，分别为2和3，计算两个结点的和5作为根： 123456789101112graph TB A[[19]] B[[6]] C[[7]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] F[[10]] G[[21]] 接着，从19、6、7、5、32、10、21 中选择两个最小的结点，分别是根结点5和叶子结点6，计算两个结点的和11作为新的树根： 12345678910111213graph TB A[[19]] C[[7]] R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] F[[10]] G[[21]] 然后，从19、7、11、32、10、21 中选择两个最小的结点，这次都是叶子结点，分别为7和10，计算两个结点的和17形成一颗新的树： 123456789101112131415graph TB A[[19]] R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] G[[21]] 继续，从 19、11、32、17、21 中选择最小的 11 和 17 这两个树的根结点，计算两个结点的和 28 作为组合树的根结点： 123456789101112131415161718graph TB A[[19]] R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] R4((28))--&gt;R2((11)) R4((28))--&gt;R3((17)) G[[21]] 然后，从 19、32、28、21 中选择最小的 19 和 21 这两个叶子结点，计算两个结点的和 40 形成一棵新的树： 123456789101112131415161718graph TB R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] E[[32]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] R4((28))--&gt;R2((11)) R4((28))--&gt;R3((17)) R5((40))--&gt;A[[19]] R5((40))--&gt;G[[21]] 接下来，从 32、28、 40 中选择最小的 32 和 28 这两个结点，求和 60 构成一棵树，根结点为60： 12345678910111213141516171819graph TB R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] R4((28))--&gt;R2((11)) R4((28))--&gt;R3((17)) R6((60))--&gt;E[[32]] R6((60))--&gt;R4((28)) R5((40))--&gt;A[[19]] R5((40))--&gt;G[[21]] 最后把剩下的 40 和 60 两个结点连在一起，和为100就得到了一颗哈夫曼树： 12345678910111213141516171819202122graph TB R2((11))--&gt;R((5)) R2((11))--&gt;B[[6]] R((5))--&gt;D[[3]] R((5))--&gt;H[[2]] R3((17))--&gt;C[[7]] R3((17))--&gt;F[[10]] R4((28))--&gt;R2((11)) R4((28))--&gt;R3((17)) R6((60))--&gt;E[[32]] R6((60))--&gt;R4((28)) R5((40))--&gt;A[[19]] R5((40))--&gt;G[[21]] R7((100))--&gt;R6((60)) R7((100))--&gt;R5((40)) 按照上面的定义来算，这颗二叉树的带权路径长度为： WPL = 2 (32 + 19 + 21) + 4 (6 + 7 + 10) + 5 * (3 + 2) = 261 其实还有另一种计算带权路径长度的方法，那就是把除根结点以外的所有数字都加起来： WPL = 60 + 40 + 28 + 32 + 19 + 21 + 11 + 17 + 5 + 6 + 7 + 10 + 3 + 2 = 261 编码我们用统计数量的字母来替换频数，然后在树的左右指针上分别标上数字就可以得到： 123456789101112131415161718192021graph TB R2((11))--0--&gt;R((5)) R2((11))--1--&gt;B[[b]] R((5))--0--&gt;D[[d]] R((5))--1--&gt;H[[h]] R3((17))--0--&gt;C[[c]] R3((17))--1--&gt;F[[f]] R4((28))--0--&gt;R2((11)) R4((28))--1--&gt;R3((17)) R6((60))--1--&gt;E[[e]] R6((60))--0--&gt;R4((28)) R5((40))--0--&gt;A[[a]] R5((40))--1--&gt;G[[g]] R7((100))--0--&gt;R6((60)) R7((100))--1--&gt;R5((40)) 至此我们就可以给出编码了呀，从根结点走到每个叶子结点路径上经过的0和1就是编码内容，编码表如下： a–&gt;10b–&gt;0001c–&gt;0010d–&gt;00000e–&gt;01f–&gt;0011g–&gt;11h–&gt;00001 要想等长编码这8个字母最少需要4个bit，采用哈夫曼编码以后最少用2bit，最多用5bit，这是考虑了出现频率以后的结果，在传输大量数据的时候，采用哈夫曼编码会是一个更优的解决方案。 总结 树的带权路径长度是指树的所有叶子结点的带权路径长度之和，简称WPL 当使用已知结点作为叶子结点，用其构造的所有树中，带全路径长度最小的树被称为最优二叉树，也就是哈夫曼树 哈夫曼树可以用来编码，采用哈夫曼编码后的信息可以可以使空间利用更加高效 哈夫曼树的构造并不是唯一的，相同的权值结点完全可以构造出不同形态的哈夫曼树，甚至连高度都不同 哈夫曼编码还保证了长编码不与短编码冲突的的特点，这个后续有时间我们再聊 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 埋下高昂的头颅，为一飞冲天的壮举积蓄力量，我就在这静静的等，期待你的绽放~ 2021-11-12 00:42:33]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>树</tag>
        <tag>WPL</tag>
        <tag>哈夫曼树</tag>
        <tag>带权路径长度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MD5是用来加密的吗？BCrypt又是什么呢？]]></title>
    <url>%2Fblog%2F2021%2F10%2F31%2FMD5%E6%98%AF%E7%94%A8%E6%9D%A5%E5%8A%A0%E5%AF%86%E7%9A%84%E5%90%97%EF%BC%9FBCrypt%E5%8F%88%E6%98%AF%E4%BB%80%E4%B9%88%E5%91%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[前言最近经常看到一种关于 MD5 的说法，比如某某系统的登录模块使用了 MD5 加密算法，保证了用户密码的安全。那么 MD5 究竟是不是一种加密算法呢？从概念上来说『加密』对应的是『解密』，我们把数据采用某种方式加密之后，可以在之后的某一时刻进行解密来获得原始数据，照此观点来说 MD5 并不是一种加密算法，它只能把原文加密成密文，而不能将密文解密成原文。但是 MD5 确实把明文变成了不容易被破解的密文，达到了 “加密” 的目的，好像说它可以用来加密勉强也可以。 更准确的说法，MD5 是一种哈希算法，又叫散列算法或者摘要算法，是一类把任意长度数据转换为定长数据的算法统称，它广泛应用于错误检查，经常通过计算 MD5 来检验下载到的文件的完整性，优秀的哈希算法通常需要具有低碰撞概率，MD5 就是其中的一种。 MD5的八卦 可用于处理密码，是不可逆的 刚刚前面已经提到它可以把原始数据变成定长的摘要信息，而不能把摘要信息再还原成原始数据。就比如 110+119=229，通过原始信息 110 和 119 可以转化成摘要信息229，而已知229却无法知道它是由那两个数相加得到的，当然这个例子只是象征性的，它的碰撞率太高了。 既然不可逆，那么怎样才能判断密码信息呢？这可以利用比较hash值来判断，我们在注册时计算密码的 MD5 值入库，当玩家登录时再次使用玩家输入的明文密码再次计算 MD5 值，如果一致就验证成功，这就是为什么哈希算法要有低碰撞率了。 MD5现在不太安全了 因为MD5算法是确定，用一个字符串计算出来的哈希值也是固定的，所以出现了一些针对该算法的破解方法。 暴力枚举法：因为可以不断尝试，并且随着计算机硬件能力的快速提升，使得这种方法来破解短密码称为了可能 字典法：也就是撞库，黑客通过收集互联网已泄露的用户和密码信息，生成对应的字典表，通过撞库来完成破解 彩虹表：在字典法的基础上改进，以时间换空间，使用预计算的哈希链集来降低存储空间，是目前最常用的方法 MD5可以为自己代言（带盐） 对于固定的哈希算法，相同的输入会得到相同的输出，那么针对MD5算法只需要准备一个字典或者一个彩虹表就可以一直沿用，如果在原有的密码上加点料，那么即使两个用户使用相同的密码，因为盐不同，得到的输出值也就不同，那么破解难度大大提高了。 BCrypt加密上面说过单独使用MD5加密不太安全，但是加盐以后可以大大提高破解的难度，为什么BCrypt加密火了起来，大有代替MD5的趋势~ BCrypt 是 Niels Provos 和 DavidMazières 基于 Blowfish 密码设计的，是 OpenBSD 的默认密码哈希算法。 目前有针对 C、C++、Python 、C# 、Java、JS、PHP 等多种编程语言的实现，使用起来非常方便。 它相对于MD5有哪些优势呢？ 自己加盐首先他不用自己来管理用户的“盐”，如果所有的用户使用相同的盐不太安全，每个人生成不同的盐，需要自己单独来存储使用，而BCrypt内部自己实现了随机加盐处理，可以实现每次加密后的密文是不一样的。 对于同一个密码，Bcrypt每次生成的哈希结果都不一样，那么它是如何进行校验的？ 其实BCrypt算法将盐随机生成并混入最终加密后的密码之中，验证时会自动提取，无需单独提供“盐”信息，生成的Hash值通常格式如下： 1$2b$12$ABJPtagiuqTVhnIPvOLoB.hbIlZ3joRkpck3joDsX6xe3O2KShuty 其中 $ 为分隔符，2b是bcrypt加密版本号，12是工作负载，紧接着22位是盐，剩下的字符串就是密码的密文了。 看到这个密码仿佛就是明牌了跟对手打呀，如果你真的获得了加密后的密码，那你就知道了加密版本、工作负载，盐的信息，这样会不会很危险呢？是挺危险的，但是即使你使用MD5加密，那个盐也是要存储的，也会面临同样的问题，另外BCrypt还有其他的法宝。 工作负载BCrypt的工作负载有时也称为加密轮数、成本因子等等（一提到工作负载就想到比特币，数字游戏而已），目的就是提高破解难度，带来的缺点就是速度慢。MD5的Hash值生成通常是微妙级别的，但是Bcrypt一个密码出来的时间比较长，Python环境使用默认12轮负载需要0.25秒生成一个密码（C++环境需要进一步测试）。 所以如果使用Bcrypt，需要考虑它的成本，负责做认证的服务器，可能在原基础上扩容几十倍或者几百倍，它是靠把计算成本提高多个数量级来换取安全的。 使用方便目前在各大主流编程语言中都可以方便的使用BCrypt相关函数，下面以Python为例： 12345678910111213141516#!/usr/bin/env python3# -*- coding: utf-8 -*-import timeimport bcryptpasswd = b'123456nx'start = time.time()salt = bcrypt.gensalt(12)pwsd = bcrypt.hashpw(passwd, salt)cost = time.time() - startprint("[salt]", salt)print("[pwsd]", pwsd)print("[cost]", cost) 运行结果如下： 123[salt] b&apos;$2b$12$lEsQ9dGnRe2vKfFDRUZYAO&apos;[pwsd] b&apos;$2b$12$lEsQ9dGnRe2vKfFDRUZYAOmmmdlgWfHfNO94C/UqCKGGRioruF77u&apos;[cost] 0.24636435508728027 总结 MD5从严格意义上来说并不是一个加密算法，更准确的说法应该是单向散列算法，因为无法逆向进行解密 通过 MD5 计算后的密码，可以使用的破解方法有暴力枚举、字典表、彩虹表等，其中《彩虹表》最常用 BCrypt算法可以随机生成盐，并将盐信息混入最终加密后的密码之中，验证时会自动提取 BCrypt算法引入了工作负载机制，生成Hash值的时间大大延长，相应的破解难度也随之增加 BCrypt算法在Python环境下使用默认参数需0.25秒生成一个密码，选用这种方式需考虑时间成本 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 以史为鉴，理解今天，展望未来~时刻准备着，这次的大团圆无需担心，可能某个清晨的早间新闻，你们已经回来了！ 2021-11-1 00:21:44]]></content>
      <categories>
        <category>加密与认证</category>
      </categories>
      <tags>
        <tag>MD5</tag>
        <tag>BCrypt</tag>
        <tag>Hash</tag>
        <tag>单项散列</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下从路径字符串中截取目录和文件名信息]]></title>
    <url>%2Fblog%2F2021%2F10%2F24%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BB%8E%E8%B7%AF%E5%BE%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E6%88%AA%E5%8F%96%E7%9B%AE%E5%BD%95%E5%92%8C%E6%96%87%E4%BB%B6%E5%90%8D%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言之前在文章《.bat批处理（十）：从路径字符串中截取盘符、文件名、后缀名等信息》中总结了在批处理文件中，也就是Windows环境下怎样从路径字符串中截取文件名、盘符等信息，利用的是Windows的扩展变量，而这种截取路径的需求在linux脚本中也很常见，实现方法相比批处理脚本而言要简单一些，下面列举一下常见的方法。 使用系统命令完成路径截取针对于路径的操作很基础，所以在linux环境下提供了专门的命令来完成路径字符串的截取工作，分别是 basename 和 dirname，从名字就可以很容易知道他们的作用，具体的使用方法如下： 使用 basename 命令获取文件名获取文件名时直接在命令后面添加待截取的路径即可，表现如下： 12albert@home-pc:/mnt/d/data/shell$ basename /mnt/d/data/shell/test.txttest.txt 在shell脚本的写法通常是这样： 12mypath=/mnt/d/data/shell/test.txtecho $(basename $mypath) 运行之后得到的带有后缀的文件名，如果想去掉后缀使用 -s 参数指定要去掉的后缀即可： 12albert@home-pc:/mnt/d/data/shell$ basename -s .txt /mnt/d/data/shell/test.txttest -s 选项也可省略，将后缀名直接放到完整路径后面也可以： 12albert@home-pc:/mnt/d/data/shell$ basename /mnt/d/data/shell/test.txt .txttest 其实 -s 参数后面不一定非得加后缀名，它就是在结果的末尾去掉匹配的字符串，所以可以指定任何结尾字符： 12albert@home-pc:/mnt/d/data/shell$ basename /mnt/d/data/shell/test.txt xttest.t 使用 dirname 命令获取目录名顾名思义 dirname 就是获取目录名的命令，直接在命令后面跟上待截取的路径即可： 12albert@home-pc:/mnt/d/data/shell$ dirname /mnt/d/data/shell/test.txt/mnt/d/data/shell 这个命令不仅可以获取文件所在目录，还可以获取目录的上一级目录，实际上它处理的仅仅是字符串，截取的目标字符也是 /，并不要求目录是有效的 12albert@home-pc:/mnt/d/data/shell$ dirname /mnt/d/data//mnt/d 利用变量提取操作完成截取通过对 ${var} 表达式进行变形可以完成对原变量的部分提取和替换，下面先列举一些主要的截取操作： 表达式 含义 示例 结果 ${var} var字符串原始值 ${var} ^Can you can a can as a canner can can a can$ ${\#var} var字符串的长度 ${\#var} ^43$ ${var:pos} 在字符串var中从位置pos提取子串 ${var:30} ^can can a can$ ${var:pos:len} 在var中从位置pos提取长度为len的子串 ${var:30:5} ^can c$ ${var#substr} 在var左侧删除最短匹配$substr的子串 ${var#*can a} ^ can as a canner can can a can$ ${var##substr} 在var左侧删除最长匹配$substr的子串 ${var##*can a} ^ can$ ${var%substr} 在var右侧删除最短匹配$substr的子串 ${var%can a*} ^Can you can a can as a canner can $ ${var%%substr} 在var右侧删除最长匹配$substr的子串 ${var%%can a*} ^Can you $ 好了，知道了这些规则我们就可以利用这些截取规则，来完成截取目录的需求了，下面假设 var 的值为 ./d/data/shell/test.txt 开始具体操作。 截取根目录要想截取根目录就是要把第一个 / 后面的内容删除就可以了，使用 ${var%%/*} 在var右侧删除最长匹配/*的子串，结果就只剩下 . 了: 123albert@home-pc:/mnt/d/data/shell$ var="./d/data/shell/test.txt"albert@home-pc:/mnt/d/data/shell$ echo $&#123;var%%/*&#125;. 截取文件名截取文件名需要把最后一个 /前面的内容删除，使用 ${var##*/} 在var左侧删除最长匹配 */ 的子串，结果为包含后缀的文件名： 123albert@home-pc:/mnt/d/data/shell$ var="./d/data/shell/test.txt"albert@home-pc:/mnt/d/data/shell$ echo $&#123;var##*/&#125;test.txt 截取文件后缀截取文件名后缀与截取文件名类似，需要包最后一个 . 前面的内容删除，使用 ${var##*.} 在var左侧删除最长匹配 *. 的子串，只保留后缀内容： 123albert@home-pc:/mnt/d/data/shell$ var="./d/data/shell/test.txt"albert@home-pc:/mnt/d/data/shell$ echo $&#123;var##*.&#125;txt 截取文件所在的目录截取文件坐在目录就是只删除文件名，把文件名前面的 / 包括之前的内容进行保留，使用 ${var%/*} 在var右侧删除最短匹配 /* 的子串： 123albert@home-pc:/mnt/d/data/shell$ var="./d/data/shell/test.txt"albert@home-pc:/mnt/d/data/shell$ echo $&#123;var%/*&#125;./d/data/shell 样例假设 var 的值为 ./d/data/shell/test.txt，具体实现的表格总结如下： 需求 表达式 结果 根目录 ${var%%/*} . 文件名 ${var##*/} test.txt 文件后缀 ${var##*.} txt 文件所在目录 ${var%/*} ./d/data/shell 总结 basename 和 dirname 是linux环境下专门截取文件名和目录名的命令工具 处理文件路径截取的通常使用 ${var} 形式的变量提取方法，这种方法很方便，但不仅限于相关目录的处理 常用截取表达式：文件名 ${var##*/}、文件后缀 ${var##*.}、文件所在目录 ${var%/*} ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 今年的1024和去年一样又是周末，凌晨登陆冰封了4、5年的对战平台账号，看着全部灰色的dota好友列表，再想凑齐所有人一起开黑几乎不可能了。打了半宿输多赢少，还是那个手残的我。出了新的英雄，添了新的装备，面对这个曾经挚爱的游戏感觉有些陌生，历史的车轮不会因为你不关注就会停止，相反，这些“平行世界”总是在向前奔跑着，唯一不变的就是变化~ 2021-10-24 19:46:47]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>dirname</tag>
        <tag>basename</tag>
        <tag>路径截取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（十）：从路径字符串中截取盘符、文件名、后缀名等信息]]></title>
    <url>%2Fblog%2F2021%2F10%2F16%2F%E6%89%B9%E5%A4%84%E7%90%86%E4%BB%8E%E8%B7%AF%E5%BE%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E6%88%AA%E5%8F%96%E7%9B%98%E7%AC%A6%E3%80%81%E6%96%87%E4%BB%B6%E5%90%8D%E3%80%81%E5%90%8E%E7%BC%80%E5%90%8D%E7%AD%89%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言又是实际开发中的问题，想要截取一个文件路径中的盘符、文件名等信息，第一反应是正则表达式？或者是 split 函数？这些往往都是“高级”语言中才会有的实现方法，对于批处理来说有点“带不动”啊，那么在bat批处理中要怎样处理类似的请求呢？最近找到了两种方法，接下来会逐一展示一下，不过在展示具体的写法前，我们先来看一下 %~dp0的含义。 %~dp0的含义关于 %~dp0 的作用在之前的总结中 《.bat批处理（四）：路径相关%cd%和%~dp0的区别》 有提到过，它表示当前运行的批处理文件所在的目录，那么它是一个特殊的变量吗？ 可以说算是吧，这个变量特殊在它是从参数变量 %0 扩展而来的，提到 %0 很多人都会想到它是批处理脚本的第一个参数，表示当前运行的脚本全路径，可以写个脚本试一下： 1234567891011# Albert at home-pc in D:\data\bat [0:03:25]% Get-Content showparams.bat@echo offecho %0echo %1# Albert at home-pc in D:\data\bat [0:03:31]% ./showparams.bat good"D:\data\bat\showparams.bat"good 类似的变量还有 1%、2%、3%…. 一直到9%，都依次表示运行批处理脚本时传入的参数，这些变量还有一个本领，那就是支持扩展，写起来花里胡哨的。 扩展字符串扩展字符串是批处理自带的功能，可以实现对表示文件路径的字符串进行特殊的处理，以%0 参数为例，具体功能列举如下： %~0 - 删除路径中的引号 %~f0 - 将 %0 扩展到一个完全合格的路径名 %~d0 - 将 %0 扩展到一个驱动器号 %~p0 - 将 %0 扩展到一个路径 %~n0 - 将 %0 扩展到一个文件名 %~x0 - 将 %0 扩展到一个文件扩展名 %~s0 - 将 %0 扩展的路径只含有短名 %~a0 - 将 %0 扩展到文件的文件属性 %~t0 - 将 %0 扩展到文件的日期/时间 %~z0 - 将 %0 扩展到文件的大小 %~$PATH:0 查找变量0%在路径环境变量$PATH的目录，并将 %0 扩展到找到的第一个完全合格的名称，$PATH未被定义或没找到文件，则结果为空字符串 当然这个写法也可以进行组合，比如 %~d0 和 %~p0 组合后变成 %~dp0 也就是我们常见的那个变量啦~ 可以将这些变量打印出来看一下具体的值： 1234567891011121314151617181920212223242526272829303132# Albert at home-pc in D:\data\bat [0:26:17]% Get-Content showparams.bat@echo offecho %0echo %~0echo %~f0echo %~d0echo %~p0echo %~n0echo %~x0echo %~s0echo %~a0echo %~t0echo %~z0echo %~dp0echo %~nx0# Albert at home-pc in D:\data\bat [0:26:28]% .\showparams.bat"D:\data\bat\showparams.bat"D:\data\bat\showparams.batD:\data\bat\showparams.batD:\data\bat\showparams.batD:\data\bat\showparams.bat--a--------2021/10/17 00:26156D:\data\bat\showparams.bat 从字符串中截取路径、文件名 上面的部分解释了%~dp0，同时也知道了这些脚本参数指出扩展语法，如果是普通变量的话就不能使用扩展语法了，那么对于一个普通的包含字符串怎么才能使用扩展语法，截取到想要的部分呢？目前我知道的有两种方法：一种是传参使其变成脚本参数，也就是 %n的形式，另一种方法就是使用 for 语句，接下来分别看一下。 脚本传参普通的字符串无法进行扩展，如果想把这种变量就需要把它们变成脚本参数，这就需要将参数传递给另一个脚本，这样实现起来会将脚本调用变得复杂一些，实际上可以在一个脚本中完成截取工作，类似于C/C++中的函数调用，可以在批处理中使用 call 命令搭配标签实现，具体代码如下： 123456789101112131415161718192021222324252627# Albert at home-pc in D:\data\bat [17:37:54]% Get-Content extract1.bat@echo offset OriginStr="C:/Demo/myproject/example.txt"echo %OriginStr%call :extract %OriginStr%goto :eof:extractrem 获取到文件路径echo %~dp1rem 获取到文件盘符echo %~d1rem 获取到文件名称echo %~n1rem 获取到文件后缀echo %~x1# Albert at home-pc in D:\data\bat [17:41:25]% .\extract1.bat"C:/Demo/myproject/example.txt"C:\Demo\myproject\C:example.txt 在这段代码中 :eof 标签是一个默认的标签，表示文件结尾，实际需求中需根据具体要求进行调整。 for语法扩展使用 for 循环是另一种实现方式，因为循环变量也可以支持扩展，可以将需要截取的字符串路径放在循环范围中，然后先循环输出测试下： 1234567891011# Albert at home-pc in D:\data\bat [17:46:29]% Get-Content extract2.bat@echo offset OriginStr="C:/Demo/myproject/example.txt"for %%I in (%OriginStr%) do echo %%I# Albert at home-pc in D:\data\bat [17:46:57]% .\extract2.bat"C:/Demo/myproject/example.txt" 在批处理中的循环变量是 %%I的形式，需要两个 % 才可以，后面的变量名可以换成26个字母中的任意一个，并且字母会区分大小写，然后利用这些循环变量就可以进行扩展，然后完成最开始的需求，实现代码如下： 123456789101112131415161718192021222324# Albert at home-pc in D:\data\bat [17:53:53]% Get-Content extract2.bat@echo offset OriginStr="C:/Demo/myproject/example.txt"for %%I in (%OriginStr%) do echo %%Irem 获取到文件路径for %%I in (%OriginStr%) do echo %%~dpIrem 获取到文件盘符for %%I in (%OriginStr%) do echo %%~dIrem 获取到文件名称for %%I in (%OriginStr%) do echo %%~nIrem 获取到文件后缀for %%I in (%OriginStr%) do echo %%~xI# Albert at home-pc in D:\data\bat [17:54:01]% .\extract2.bat"C:/Demo/myproject/example.txt"C:\Demo\myproject\C:example.txt 这种写法的好处就是无需控制标签跳转流程，通过循环命令 for 就可以获取想要的参数，使用起来会方便很多。 总结 在批处理文件中 %~dp0 表示批处理文件所在的目录，而 %cd% 表示执行命令时所在的目录 在批处理文件中想要截取目录操作可以使用变量扩展来实现，而变量必须是 %i 的形式，其中的 i 是可以是 a~zA~Z0~9 for 表达式中的循环变量在cmd命令行中是 %i 的形式，而在批处理文件中需要协程 %%i 的形式 常用的变量扩展有：获取到文件盘符使用 %~d0，获取到文件名称使用 %~n0，获取到文件后缀使用 %~x0 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 反人性需要不断修炼，逆向思维才能战胜人性的弱点，很多时候事情往往不是你想的那样~ 2021-10-17 18:01:07]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11中的std::atomic保证的原子性是什么]]></title>
    <url>%2Fblog%2F2021%2F10%2F13%2FC-11%E4%B8%AD%E7%9A%84std-atomic%E4%BF%9D%E8%AF%81%E7%9A%84%E5%8E%9F%E5%AD%90%E6%80%A7%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[前言提到atomic这个词，你首先想到的是什么呢？作为一个长时间混迹于编程世界的菜鸟，我首先想到的一个词是“原子性”，接着飞入脑海的是 “ACID” 这个缩写词组，既然提到了 ACID 我们就来简单的复习一下。 ACID 是指事务管理的4个特性，常见于数据库操作管理中，它们分别是：原子性，一致性，隔离性和持久性。 原子性（Atomicity）是指事务是一个不可分割的工作单位，事务中的操作要么都执行，要么都不执行。 一致性（Consistency）是指事务前后数据的完整性必须保持一致，完全符合逻辑原运算。 隔离性（Isolation）是指在多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离，无需感知其他事务的存在。 持久性（Durability）是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对数据造成损坏。 C++中的atomic原子（atom）是在化学反应中不可分割基本微粒，而编程世界中的原子性也是取自这里的不可分割的含义，不可分割与事务管理中的原子性含义一致，指的是一个操作或者一系列操作只能全都执行或者都不执行，不会只执行其中一部分，那么C++11中引入atomic有什么用？不使用atomic能不能保证原子性呢？ 其实C++11中引入atomic主要还是降低了编程的复杂度，如果不使用atomic同样可以使用锁机制来保证原子性，接下来我们来看看为什么需要原子性。 一个简单的自增运算i++ 是个再简单不过的语句了，我们可以使用它来做一个计数器，每次自增加1，假设我们有一个工程项目有两条商品生产的流水线，每个流水线生产出一件商品则需要计数器加1，这时我们用两个线程来模拟两条流水线，每个线程函数来调用自增的计数器，来看看有什么问题？ 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;thread&gt;int i = 0;void func(int n)&#123; for (int k = 0; k &lt; n; k++) i++;&#125;int main(int argc, char* argv[])&#123; int n = argc &gt; 1 ? atoi(argv[1]) : 100; std::thread t1(func, n); std::thread t2(func, n); t1.join(); t2.join(); std::cout &lt;&lt; "i=" &lt;&lt; i &lt;&lt; std::endl;&#125; 测试代码如上所示，执行 g++ -std=c++20 -O0 -pthread main.cpp &amp;&amp; ./a.out 10 命令编译并运行得到结果 i=20，貌似很正常，一共两个线程，每个线程执行10次自增操作，结果就应该是20啊，先别太早下结论，增大自增范围试试。 执行 g++ -std=c++20 -O0 -pthread main.cpp &amp;&amp; ./a.out 100000 得到结果 i=112831，多次执行发现每次运行结果都不太一样，但是数据范围在 100000~200000，这就有些奇怪了，每个线程执行循环执行一条语句，那么程序结果应该等于 2n 才对，为什么结果总是小于 2n 呢，难道有些循环没有执行？ 其实不是这样的，i++从C++语言的层面来看确实是一条语句，但是真正再和机器打交道时一般会解释成类似于下面这样3条汇编指令： 1234// x86 msvc v19.latestmov eax, DWORD PTR _i$[ebp]add eax, 1mov DWORD PTR _i$[ebp], eax 3条指令的含义可以理解为读取、自增，设置共三步，既然不是真正的一条语句，那么在多线的环境下就会生语句的交叉执行，比如第一个线程执行读取变量i的值之后，第二个线程也读取了变量i的值，这样两个线程都进行后续的自增和设置指令后，会发现比预期的值少了一个，这种情况在循环次数较多时尤为明显。 通过加锁把自增变为原子操作既然每个自增操作可能会被分解成3条指令，那么我们可以加锁来将3条指令捆绑，当一个线程执行自增操作时加锁来防止其他进程“捣乱”，具体修改如下，可以在自增操作前直接加锁： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;chrono&gt;#include &lt;mutex&gt;int i = 0;std::mutex mt;void inc()&#123; std::lock_guard&lt;std::mutex&gt; l(mt); i++;&#125;void func(int n)&#123; for (int k = 0; k &lt; n; k++) inc();&#125;int main(int argc, char* argv[])&#123; int n = argc &gt; 1 ? atoi(argv[1]) : 100; std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); std::thread t1(func, n); std::thread t2(func, n); t1.join(); t2.join(); std::cout &lt;&lt; "i=" &lt;&lt; i &lt;&lt; std::endl; std::chrono::duration&lt;double&gt; duration_cost = std::chrono::duration_cast&lt; std::chrono::duration&lt;double&gt; &gt;(std::chrono::steady_clock::now() - start); std::cout &lt;&lt; "total cost " &lt;&lt; duration_cost.count() &lt;&lt; " seconds." &lt;&lt; std::endl; return 0;&#125; 执行 g++ -std=c++20 -O0 -pthread main.cpp &amp;&amp; ./a.out 10000000 命令后运行结果如下： 12i=20000000total cost 2.39123 seconds. 通过加锁，我们已经保证了结果的正确性，但是我们知道加锁的额外消耗还是很大的，有没有其他的方式来实现原子操作呢？ 使用atomic来保证自增的原子性其实在C++11之前可以通过嵌入汇编指令来实现，不过自从C++11引入atomic之后，类似的需求变得简单了许多，可以直接使用autmic这个模板类来实现，代码几乎不需要修改，只需将变量 i 改为 atomic&lt;int&gt; 类型，再把锁去掉就可以了，修改后的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;chrono&gt;#include &lt;mutex&gt;#include &lt;atomic&gt;std::atomic&lt;int&gt; i = 0; // int -&gt; atomic&lt;int&gt;std::mutex mt;void inc()&#123; //std::lock_guard&lt;std::mutex&gt; l(mt); //remove lock i++;&#125;void func(int n)&#123; for (int k = 0; k &lt; n; k++) inc();&#125;int main(int argc, char* argv[])&#123; int n = argc &gt; 1 ? atoi(argv[1]) : 100; std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); std::thread t1(func, n); std::thread t2(func, n); t1.join(); t2.join(); std::cout &lt;&lt; "i=" &lt;&lt; i &lt;&lt; std::endl; std::chrono::duration&lt;double&gt; duration_cost = std::chrono::duration_cast&lt; std::chrono::duration&lt;double&gt; &gt;(std::chrono::steady_clock::now() - start); std::cout &lt;&lt; "total cost " &lt;&lt; duration_cost.count() &lt;&lt; " seconds." &lt;&lt; std::endl; return 0;&#125; 执行 g++ -std=c++20 -O0 -pthread main.cpp &amp;&amp; ./a.out 10000000 命令后运行结果如下： 12i=20000000total cost 1.6554 seconds. 通过对比可以发现，使用 std::atomic 模板类之后，在保证了结果正确的同时，相比于加锁实现原子性速度上有了明显的提升。 总结 ACID 是指事务管理中的原子性，一致性，隔离性和持久性4个特性。 加锁（写锁）的目的通常是将可能同时发生的操作串行化，以此来避免对资源的竞争出现问题 操作的并行加快了任务的处理速度，而“加锁”使部分操作回归到串行，两者相互配合是为了在更短的时间内得到正确的结果 std::atomic 降低了原子性操作编程的难度，同时相比于加锁实现原子性还有了性能的提升 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 时光时光慢些吧，不要再让你变老了，我愿用我一切，换你岁月长留~ 时间对于每个人来说，都是公平的，真的是这样吗？我觉得未必吧！ 2021-10-6 00:32:33–]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>atomic</tag>
        <tag>原子性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dijkstra——通过不断松弛来解决单源最短路径问题的算法]]></title>
    <url>%2Fblog%2F2021%2F09%2F11%2FDijkstra%E2%80%94%E2%80%94%E9%80%9A%E8%BF%87%E4%B8%8D%E6%96%AD%E6%9D%BE%E5%BC%9B%E6%9D%A5%E8%A7%A3%E5%86%B3%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98%E7%9A%84%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言Dijkstra算法又称迪杰斯特拉算法，它采用的是一种贪心的策略，使用广度优先搜索的方式解决带权有向图或者无向图的单源最短路径问题，需要注意的是它不能处理带负边权的情况，核心思想就是“守住现有阵地不断攻占底盘”，这一点可以在后面代码实现中慢慢体会，接下来梳理一下算法思路。 Dijkstra思路Dijkstra 算法的思路是维护一个点集合 S 和一个用来保存起点 m 到各个顶点到各个顶点最短距离的数组 dis，用邻接数组来表示带权图信息。 初始情况时，集合 S 中只包括起点m，通过图信息来初始化 dis 数组，将起点 m 可以直接到达的点设置为边的权值，不能到达的点设置为无穷大，比如点 m 到点 n 的距离是d，则 dis[n] = d。 然后从带权图中选择不在集合S 中的到点 m 距离最近的点，假设为 n，把它加到集合 S 中，然后尝试通过点 n “松弛” 那些不在集合 S 中的点到点 m的距离，更新 dis 数组信息，具体操作就是使用点 n 作为中转，如果距离如果点 m 到任意点 x 通过点 n 中转距离变短了，那么就更新 dis[x] 的值。 之后不断重复上面的“松弛”操作，直到集合 S 中包含了所有得到顶点，至此就通过Dijkstra算法求解出了从点 m 到图中任意点的最短距离。 图解实例 看了上面的关于Dijkstra算法的文字描述可能还是有点蒙，这时候需要画个图来解释一下，对于算法问题，特别是图论方面的算法题，有时候真的是一图胜千言，奈何我真的是不想画图，一方面因为“懒”，另一方面就是图片的搬运比较麻烦，所以对于大部分问题我都是文字描述，但是为了解释这个Dijkstra我还是决定画一画，假如求解从点a 到各个顶点的最短距离，初始图信息如下： 第一步，我们把点 a 添加到集合 S 中变为 S = {a}，然后初始化dis数组为 dis = {0, 1, 12, ∞, ∞, ∞}，加入集合的点用红色表示，操作之后更新如下： 第二步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 b，将点 b 添加到集合 S 中，通过点 b 中转更新 dis 数组，dis[c]变为8，dis[d]变为4，更新后集合为 S = {a, b}， 距离数组为 dis = {0, 1, 8, 4, ∞, ∞}, 图信息如下： 第三步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 d，将点 d 添加到集合 S 中，通过点 d 中转更新 dis 数组，dis[e]变为14，dis[f]变为17，更新后集合为 S = {a, b, d}， 距离数组为 dis = {0, 1, 8, 4, 14, 17}, 图信息如下： 第四步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 c，将点 c 添加到集合 S 中，通过点 c 中转更新 dis 数组，dis[e]变为13，更新后集合为 S = {a, b, d, c}， 距离数组为 dis = {0, 1, 8, 4, 13, 17}, 图信息如下： 第五步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 e，将点 e 添加到集合 S 中，通过点 e 中转更新 dis 数组，通过距离判断发现此次不需要更新dis数组，更新后集合为 S = {a, b, d, c, e}， 距离数组为 dis = {0, 1, 8, 4, 13, 17}, 图信息如下： 第六步，找到距离点 a 最近的且不在 S 中的点，根据 dis 数组计算应该是点 f，将点 f 添加到集合 S 中，至此集合 S 中包含了所有的顶点，Dijkstra算法执行结束，集合信息为 S = {a, b, d, c, e, f}， 距离数组为 dis = {0, 1, 8, 4, 13, 17}, 图信息如下： 代码实现通过上面的图解实例对于Dijkstra的实现应该有了一些思路，那么接下来我们把它转化成代码： 1234567891011121314151617void Dijkstra(vector&lt;vector&lt;int&gt;&gt;&amp; graph)&#123; vector&lt;int&gt; dis = graph[0]; set&lt;int&gt; S; const int n = dis.size(); for (int i = 0, x = 0; i &lt; n; i++, x = 0) &#123; // find minimum weight for (int j = 0; j &lt; n; j++) if (!S.count(j) &amp;&amp; (x == 0 || dis[j] &lt; dis[x])) x = j; S.insert(x); // relax for (int j = 0; j &lt; n; j++) if (!S.count(j) &amp;&amp; dis[x] + graph[x][j] &lt; dis[j]) dis[j] = dis[x] + graph[x][j]; &#125;&#125; 运行上述代码之后我们便得到了节点0到任意点的最短路径长度数组 dis。 从上面的分析我们可以知道从点 a 到点 f 的最短路径长度是 17，那么最短路径怎样求呢？ 其实只要在做松弛操作时记录每个节点是从哪个节点松弛得到的就可以了，比如可以使用一个pre数组来记录这个信息，当计算 dis 结束时通过pre数组反推就可以得到最短路径，简单实现如下： 12345678910111213141516171819202122232425262728293031void Dijkstra(vector&lt;vector&lt;int&gt;&gt;&amp; graph)&#123; vector&lt;int&gt; dis = graph[0]; set&lt;int&gt; S; const int n = dis.size(); vector&lt;int&gt; pre(n, 0); // save previous point index for (int i = 0, x = 0; i &lt; n; i++, x = 0) &#123; // find minimum weight for (int j = 0; j &lt; n; j++) if (!S.count(j) &amp;&amp; (x == 0 || dis[j] &lt; dis[x])) x = j; S.insert(x); // relax for (int j = 0; j &lt; n; j++) if (!S.count(j) &amp;&amp; dis[x] + graph[x][j] &lt; dis[j]) &#123; dis[j] = dis[x] + graph[x][j]; pre[j] = x; &#125; &#125; // output path info vector&lt;int&gt; path&#123;5&#125;; while(path.back() != 0) &#123; path.push_back(pre[path.back()]); &#125; for (auto it = path.rbegin(); it != path.rend(); it++) cout &lt;&lt; *it &lt;&lt; " ";&#125; 总结 Dijkstra算法的时间复杂度为O(N^2)，空间复杂度为 O(N)，如果对时间复杂度有更高要求可以使用堆结构进行优化 Dijkstra是一种求解单源最短路径的算法，在时间复杂度这一项要优于之前所说的 Floyd 算法 Dijkstra不能处理带负边权的情况，不过实际生活中类似于行车路线、管道铺设等问题都不会有负边权，应用还是比较广泛的 该算法仔细分析之后还是比较好理解的，不过还是有一些变型和编程技巧，需要在实际问题中灵活变通 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 历史总是惊人的相似，却不会简单的重复。在柯立芝实行了以放任自流的经济政策之后，紧接着便迎来了1929年的大萧条；而在克林顿到小布什任期内采取的经济自由化的政策，引发了之后2008年的国际金融危机；如今我们抬头看看大洋彼岸那疯狂运转的印钞机，这次的泡泡或许很快就能迎来炸裂的时刻~ 2021-9-12 23:29:46]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Dijkstra</tag>
        <tag>算法</tag>
        <tag>最短路径</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Floyd-Warshall——仅用4行代码就能解决多源最短路径问题的算法]]></title>
    <url>%2Fblog%2F2021%2F09%2F05%2FFloyd-Warshall%E2%80%94%E2%80%94%E4%BB%85%E7%94%A85%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%B0%B1%E8%83%BD%E8%A7%A3%E5%86%B3%E5%A4%9A%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98%E7%9A%84%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言Floyd-Warshall算法简记Floyd算法，又称弗洛伊德算法，是解决任意两点间的最短路径问题的一种常用算法，核心思想就是“不断利用第三者影响原配关系”，这一点在4行核心代码中表现的淋漓尽致，接下来梳理一下算法思路。 Floyd思路从A点走到B点要想路径最短只有两种可能，一种就是直接从A到B，另一种就是通过其他点来中转，Floyd的思路就是先把直接能到达的点固定下来，然后不断的尝试从其他点来中转来降低路程。 Floyd算法实现通常使用一个二维数组来表示任意两点之间的初始距离，每个点到自身的距离为0，若两个点之间没有直接连通，则赋值为 +∞，我们假设这个二维数组是 v，则 v[i][j] 代表了从点 i 到点 j 的初始距离。 假设不允许中转，那么二维数组 v 中的数据就代表了任意两点间的距离。 如果允许中转一次，我们假设只允许从节点1进行中转，那么点 i 到点 j 的最近距离最小为 v[i][j] 或者 v[i][1] + v[1][j]，如果 v[i][1] + v[1][j] 的值更小，我们可以使用它来更新 v[i][j] 的值，这时 v[i][j] 就不仅仅是一个值了，而是隐含着 i-&gt;1-&gt;j 这样一条路径，这个过程实际上翻译成代码就是： 123for(int i = 0; i &lt; n; i++) for(int j = 0; j &lt; n; j++) v[i][j] = min(v[i][j], v[i][1] + v[1][j]); 那么这条路径怎样才能更短呢？ 答案就是引入另一个点，比如我们不仅允许从节点1中转，也允许从节点2中转，从上一步我们知道从从点 i 到点 j 的最短距离是从 i-&gt;1-&gt;j 得到的，实际上经过上面一步，任意两点的距离都是允许从节点1中转条件下的最小值， 那么引入节点2之后就是要看看 v[i][j] 和 v[i][2] + v[2][j] 谁更小一点，然后遍历更新即可，类似的代码可以写成： 123for(int i = 0; i &lt; n; i++) for(int j = 0; j &lt; n; j++) v[i][j] = min(v[i][j], v[i][2] + v[2][j]); 看到套路了没有，就是每个点都作为一个可能中转的点来试一下，整个算法就结束了，好神奇~ 完整4行代码如下： 1234for(int k = 0; k &lt; n; k++) for(int i = 0; i &lt; n; i++) for(int j = 0; j &lt; n; j++) v[i][j] = min(v[i][j], v[i][k] + v[k][j]); 简单粗暴又不失美感！ 示例初始路径及每条边的距离如图： 翻译成二维数组如下： 0 2 7 4 ∞ 0 4 ∞ 9 1 0 ∞ 6 ∞ 15 0 仅通过节点0作为中转，二维数组更新如下： 0 2 7 4 ∞ 0 4 ∞ 9 1 0 13 6 8 13 0 增加节点1作为中转，二维数组更新如下： 0 2 6 4 ∞ 0 4 ∞ 9 1 0 13 6 8 12 0 再增加节点2作为中转，二维数组更新如下： 0 2 6 4 13 0 4 17 9 1 0 13 6 8 12 0 最后增加节点3作为中转，二维数组更新如下： 0 2 6 4 13 0 4 17 9 1 0 13 6 8 12 0 至此我们就求解出了任意两点间的最小距离。 总结 Floyd算法的时间复杂度为O(N^3)，空间复杂度为 O(N^2) Floyd是一种求解多源最短路径的算法，如果是求解单源最短路径这 N^3 的时间复杂度确实有点伤 Floyd可以正确处理有向图或存在负权边的图，但不能处理存在负权回路的图的最短路径问题 4行代码3层循环或许可以助它称为最容易让人理解的最短路径算法 这4行代码只是一个理想化的模型，实际在编码时要注意加法的越界问题，因为两个无穷大相加理论上是无穷大，但在代码里可能就崩溃了 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生到底是追求到达目的地还是准备欣赏沿途的风景，一味地向前奔跑忽略了周围的一切，很多美好的事物就在身边却不自知，我们已经被世俗蒙蔽了双眼，什么时候可以慢下来呢？ 2021-9-7 01:14:05]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>算法</tag>
        <tag>最短路径</tag>
        <tag>Floyd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从一个小题中的应用来体会下std::tie的便利之处]]></title>
    <url>%2Fblog%2F2021%2F08%2F15%2F%E4%BB%8E%E4%B8%80%E4%B8%AA%E5%B0%8F%E9%A2%98%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E6%9D%A5%E4%BD%93%E4%BC%9A%E4%B8%8Bstd-tie%E7%9A%84%E4%BE%BF%E5%88%A9%E4%B9%8B%E5%A4%84%2F</url>
    <content type="text"><![CDATA[前言今天主要学习一下 std::tie 函数的使用方法，之前看到 tie 函数是和 IO 绑定的，最近发现它是和 std::tuple 绑定的，查询资料后发现两个函数虽然名字相同，但是在不同的作用域下，今天学一下和 tuple 有关的这个 tie 函数，不过在学习之前先看一道小题。 解题过程爬楼梯的最少成本这是 LeetCode 上的一道题，题目描述如下： 数组的每个下标作为一个阶梯，第 i 个阶梯对应着一个非负数的体力花费值 cost[i]（下标从 0 开始）。 每当爬上一个阶梯都要花费对应的体力值，一旦支付了相应的体力值，就可以选择向上爬一个阶梯或者爬两个阶梯。 请找出达到楼层顶部的最低花费。在开始时，你可以选择从下标为 0 或 1 的元素作为初始阶梯。 示例 1：123输入：cost = [10, 15, 20]输出：15解释：最低花费是从 cost[1] 开始，然后走两步即可到阶梯顶，一共花费 15 。 示例 2：123输入：cost = [1, 100, 1, 1, 1, 100, 1, 1, 100, 1]输出：6解释：最低花费方式是从 cost[0] 开始，逐个经过那些 1 ，跳过 cost[3] ，一共花费 6 。 提示： 2 &lt;= cost.length &lt;= 1000 0 &lt;= cost[i] &lt;= 999 题目分析这种求解最小花费、最大方案数，最大价值的题目是典型的动态规划题目，这道题也可以使用动态规划的方式来解，既然每次可以选择爬一个或者两个阶梯，那么到达某一个阶梯的花费就等于这个阶梯的花费加上前一个阶梯花费和前两个花费的之间最小值即可，最终的结果取最后一个阶梯和倒数第二个阶梯中的最小值，代码比较简单，实现如下： 12345678910111213class Solution &#123;public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123; int n = cost.size(); vector&lt;int&gt; ans(n); ans[0] = cost[0]; ans[1] = cost[1]; for (int i = 2; i &lt; n; i++) ans[i] = min(ans[i-1], ans[i-2]) + cost[i]; return min(ans[n-1], ans[n-2]); &#125;&#125;; DP优化虽然使用dp数组求解起来很方便，但是从实现上可以看出，每个阶梯的花费只与它前两个阶梯的花费有关，所以使用一个长度为N的数组在空间上有些浪费，其实只要两个变量就可以了，我们用 first 和 second 两个变量分别表示某个阶梯前两个阶梯的花费，可以实现如下代码： 1234567891011121314class Solution &#123;public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123; int n = cost.size(), first = cost[0], second = cost[1]; for (int i = 2; i &lt; n; i++) &#123; int temp = min(first, second) + cost[i]; first = second; second = temp; &#125; return min(first, second); &#125;&#125;; 利用tie进行写法优化使用两个变量优化之后这个算法变成了 O(1) 的空间复杂度，但是在 for 循环中的写法还是有些啰嗦，其实这种写法和交换两个变量值过程非常相似，在GO语言中可以写成 a,b = b,a 来完成交换，但是在C++中这样的写法是错误的，不管是引入第三个变量，还是通过异或解决都需要写三条语句，但是这种情况在遇到 std::tie 函数之后有望得到改变，上面的写法利用 std::tie 可以改写如下： 12345678class Solution &#123;public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123; int n = cost.size(), first = cost[0], second = cost[1]; for (int i = 2; i &lt; n; i++) tie(first, second) = make_tuple(second, min(first, second) + cost[i]); return min(first, second); &#125;&#125;; std::tuple在学习 std::tie 的作用方式之前，先来看一下 std::tuple 是什么。如果你对这个结构有些陌生，可以先想想 std::pair 这个结构。首先 std::tuple 是一个类模板，同时他也是一个固定大小的由各种类型的值组成集合，是 std::pair 的一种泛化实现。 std::pair 中包含两个元素，而 std::tuple 可以同时包含多个元素，它拥有 struct 的表现，但是无需定义实际的 struct，在函数返回多个值时拥有良好的表现。 std::tuple的访问 利用 std::get 函数通过下标访问（C++11） 123auto t = std::make_tuple(110, "excellent", 3.14);std::cout &lt;&lt; "(" &lt;&lt; std::get&lt;0&gt;(t) &lt;&lt; ", " &lt;&lt; std::get&lt;1&gt;(t) &lt;&lt; ", " &lt;&lt; std::get&lt;2&gt;(t) &lt;&lt; ")" &lt;&lt; std::endl; 利用 std::tie 函数进行参数解绑（C++11) 123456auto t = std::make_tuple(110, "excellent", 3.14);int n;std::string s;float d;std::tie(n, s, d) = t;std::cout &lt;&lt; "(" &lt;&lt; n &lt;&lt; ", " &lt;&lt; s &lt;&lt; ", " &lt;&lt; d &lt;&lt; ")" &lt;&lt; std::endl; 利用 std::get 函数通过类型访问（C++14），这种使用方式如果每种类型不唯一会出现编译错误 123auto t = std::make_tuple(110, "excellent", 3.14);std::cout &lt;&lt; "(" &lt;&lt; std::get&lt;int&gt;(t) &lt;&lt; ", " &lt;&lt; std::get&lt;const char*&gt;(t) &lt;&lt; ", " &lt;&lt; std::get&lt;double&gt;(t) &lt;&lt; ")" &lt;&lt; std::endl; 利用结构化绑定的方式来访问（C++17） 123auto t = std::make_tuple(110, "excellent", 3.14);auto [n, s, d] = t;std::cout &lt;&lt; "(" &lt;&lt; n &lt;&lt; ", " &lt;&lt; s &lt;&lt; ", " &lt;&lt; d &lt;&lt; ")" &lt;&lt; std::endl; 以上的几个例子的输出结果都是 (110, excellent, 3.14) std::tie函数中使用std::ignore占位使用 std::tie 函数来获取 std::tuple 参数时，有时不需要所有的参数，这种情况下可以使用 td::ignore 来占位，代替那些不关心的参数，比如 std::set 结构中 insert 函数的返回值。 1234567std::set&lt;int&gt; st;std::pair&lt;std::set&lt;int&gt;::iterator, bool&gt; sp1 = st.insert(4);std::cout &lt;&lt; *sp1.first &lt;&lt; " " &lt;&lt; sp1.second &lt;&lt; std::endl;std::pair&lt;std::set&lt;int&gt;::iterator, bool&gt; sp2 = st.insert(4);std::cout &lt;&lt; *sp2.first &lt;&lt; " " &lt;&lt; sp2.second &lt;&lt; std::endl; 运行结果如下： 124 14 0 如果我们不关心插入的元素是什么，只想知道此次插入操作是否成功，可以利用 std::tie 和 std::ignore 来实现： 12345678std::set&lt;int&gt; st;bool inserted = false;std::tie(std::ignore, inserted) = st.insert(4);std::cout &lt;&lt; inserted &lt;&lt; std::endl;std::tie(std::ignore, inserted) = st.insert(4);std::cout &lt;&lt; inserted &lt;&lt; std::endl; 运行结果如下： 1210 总结 std::tuple 是 std::pair 一种更加通用的实现，std::pair 只能包含两个元素，而 std::tuple 可以包含多个任意类型的元素 tie 本意是系牢、约束、连接、束缚的意思，用在 std::tuple 上却是用来解绑参数的，含义恰好相反了，很有趣是不是 实际上 std::tie 这个函数的作用是把一些左值绑定到 std::tuple 来达到解析参数的目的，函数作用还是 “tie” std::ignore 可以用在 std::tie 函数中作为占位符，用来替代一些不关心的参数 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有些事情反过来想一想，问题可能很快就解决了——记一次拼图游戏中一个对手的高谈阔论 2021-8-15 23:48:37]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>tie</tag>
        <tag>动态规划</tag>
        <tag>DP</tag>
        <tag>tuple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中使用std::sort自定义排序规则时要注意的崩溃问题]]></title>
    <url>%2Fblog%2F2021%2F08%2F07%2FC-%E4%B8%AD%E4%BD%BF%E7%94%A8std-sort%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F%E8%A7%84%E5%88%99%E6%97%B6%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E5%B4%A9%E6%BA%83%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言看到这个标题应该会有很多人一下子就懂了，也会有些人感到迷惑，简简单单排序怎么会奔溃呢？我第一次接触这个问题还是很久以前刚刚参加工作的时候，当时也是写出了导致程序崩溃的代码，通过上网查询解决了问题，至此以后就对这个 sort 函数警惕了一些，一直记得就是在sort的自定义函数中判断条件不要加等号，至于本质的原因一直没有去探究，正好最近又改了一个相关的问题，所以决定从源码和定义的角度来看看为什么会出现这个问题。 sort的使用sort函数真的挺好用，比如像下面这样： 12345678910111213#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;int main()&#123; std::vector&lt;int&gt; values&#123;3, 5, 4, 4, 5, 1&#125;; std::sort(values.begin(), values.end()); for (auto v : values) std::cout &lt;&lt; v &lt;&lt; std::endl; return 0;&#125; 只是 std::sort(values.begin(), values.end()); 这样简简单单的一句就完成了vector数据从小到达的排序，运行结果如下： 12345678albert@home-pc:/data/cpp$ g++ testsort.cpp --std=c++11albert@home-pc:/data/cpp$ ./a.out134455 自定义比较函数上面举的例子是从小到大排序，这是 sort 函数的默认行为，所以不需要额外的参数，如果是想从大到小排序，那么就需要定义一个比较函数了，方法也比较简单，写一个lambda表达式就可以了，比如像下面这样： 1234567891011int main()&#123; std::vector&lt;int&gt; values&#123;3, 5, 4, 4, 5, 1&#125;; std::sort(values.begin(), values.end(), [](int v1, int v2)&#123; return v1 &gt;= v2; &#125;); for (auto v : values) std::cout &lt;&lt; v &lt;&lt; std::endl; return 0;&#125; 按照比较函数定义，我们把数据按照前面大于等于后面的方式排序就完成了从大到小的排序的要求，看看这样写有没有什么问题？如果这里的等号 = 已经引起了你的不适，说明你可能踩过这里的坑，是的，这样写容易造成崩溃，我们来运行一下。 12345678albert@home-pc:/data/cpp$ g++ testsort.cpp --std=c++11albert@home-pc:/data/cpp$ ./a.out554431 咦？怎么没事，我之前用MSVC测试还会崩溃的，难道和编译器有关？ 当我们增大数据量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899std::vector&lt;int&gt; values&#123;3,5,4,4,1,5,4,5,4,5,4,5,4,5,4,5,4,4,5,4,4,4,5,4,5,5,4,5,4,4,5,4,5,4,5,5,5&#125;;// 运行结果如下albert@home-pc:/data/cpp$ g++ testsort.cpp --std=c++11 -galbert@home-pc:/data/cpp$ ./a.out0555555555555555544444444444444444431*** Error in `./a.out': double free or corruption (out): 0x0000000002016c20 ***======= Backtrace: =========/lib/x86_64-linux-gnu/libc.so.6(+0x777f5)[0x7ff5ffef77f5]/lib/x86_64-linux-gnu/libc.so.6(+0x8038a)[0x7ff5fff0038a]/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7ff5fff0458c]./a.out[0x4024e2]./a.out[0x4023ab]./a.out[0x402226]./a.out[0x4020a1]./a.out[0x401edb]./a.out[0x400c67]/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7ff5ffea0840]./a.out[0x400a39]======= Memory map: ========00400000-00403000 r-xp 00000000 00:00 212044 /mnt/d/data/cpp/testsort/a.out00403000-00404000 r-xp 00003000 00:00 212044 /mnt/d/data/cpp/testsort/a.out00603000-00604000 r--p 00003000 00:00 212044 /mnt/d/data/cpp/testsort/a.out00604000-00605000 rw-p 00004000 00:00 212044 /mnt/d/data/cpp/testsort/a.out02005000-02037000 rw-p 00000000 00:00 0 [heap]7ff5f8000000-7ff5f8021000 rw-p 00000000 00:00 07ff5f8021000-7ff5fc000000 ---p 00000000 00:00 07ff5ffb70000-7ff5ffc78000 r-xp 00000000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffc78000-7ff5ffc7a000 ---p 00108000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffc7a000-7ff5ffe77000 ---p 0010a000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffe77000-7ff5ffe78000 r--p 00107000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffe78000-7ff5ffe79000 rw-p 00108000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7ff5ffe80000-7ff600040000 r-xp 00000000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600040000-7ff600049000 ---p 001c0000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600049000-7ff600240000 ---p 001c9000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600240000-7ff600244000 r--p 001c0000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600244000-7ff600246000 rw-p 001c4000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7ff600246000-7ff60024a000 rw-p 00000000 00:00 07ff600250000-7ff600266000 r-xp 00000000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17ff600266000-7ff600465000 ---p 00016000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17ff600465000-7ff600466000 rw-p 00015000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17ff600470000-7ff6005e2000 r-xp 00000000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6005e2000-7ff6005ef000 ---p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6005ef000-7ff6007e2000 ---p 0017f000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6007e2000-7ff6007ec000 r--p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6007ec000-7ff6007ee000 rw-p 0017c000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217ff6007ee000-7ff6007f2000 rw-p 00000000 00:00 07ff600800000-7ff600825000 r-xp 00000000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7ff600825000-7ff600826000 r-xp 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7ff600a25000-7ff600a26000 r--p 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7ff600a26000-7ff600a27000 rw-p 00026000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7ff600a27000-7ff600a28000 rw-p 00000000 00:00 07ff600b70000-7ff600b71000 rw-p 00000000 00:00 07ff600b80000-7ff600b82000 rw-p 00000000 00:00 07ff600b90000-7ff600b91000 rw-p 00000000 00:00 07ff600ba0000-7ff600ba1000 rw-p 00000000 00:00 07ff600bb0000-7ff600bb1000 rw-p 00000000 00:00 07ff600bc0000-7ff600bc1000 rw-p 00000000 00:00 07fffc026e000-7fffc0a6e000 rw-p 00000000 00:00 0 [stack]7fffc10b8000-7fffc10b9000 r-xp 00000000 00:00 0 [vdso]Aborted (core dumped) 这次终于崩溃了，但显示确实内存越界问题，并且排序后第一个元素是0，这不是我们vector中的元素啊，看来肯定是出问题了 反复尝试几次又找到一个测试用例： 1std::vector&lt;int&gt;values&#123;3,5,4,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5,1,4,5&#125;; 运行之后直接得到了 Segmentation fault (core dumped) 错误，没错，这就是我想要的，来从 sort 源码中看看为什么加了 = 就会出现崩溃 sort源码崩溃分析sort 函数的源码还不算太长，我就一点点来看了 1234567891011121314template&lt;typename _RandomAccessIterator, typename _Compare&gt; inline void sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) &#123; // concept requirements __glibcxx_function_requires(_Mutable_RandomAccessIteratorConcept&lt; _RandomAccessIterator&gt;) __glibcxx_function_requires(_BinaryPredicateConcept&lt;_Compare, typename iterator_traits&lt;_RandomAccessIterator&gt;::value_type, typename iterator_traits&lt;_RandomAccessIterator&gt;::value_type&gt;) __glibcxx_requires_valid_range(__first, __last); std::__sort(__first, __last, __gnu_cxx::__ops::__iter_comp_iter(__comp)); 这算是个入口函数，做了一些类型检查，然后就调用了内部的 std::__sort 函数 12345678910111213template&lt;typename _RandomAccessIterator, typename _Compare&gt; inline void __sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) &#123; if (__first != __last) &#123; std::__introsort_loop(__first, __last, std::__lg(__last - __first) * 2, __comp); std::__final_insertion_sort(__first, __last, __comp); &#125; &#125; 当排序范围不为空时，函数会对传入的范围进行排序，为了最大程度的提高效率，结合了快排、堆排和插入排序等多种排序方法，分为 std::__introsort_loop 和 std::__final_insertion_sort 两个阶段。 第一阶段使用“快排+堆排”的方法，当元素个数小于等于 _S_threshold（enum { _S_threshold = 16 }）时，不做处理，交给第二阶段来做，对于元素个数大于_S_threshold的序列，执行快排，当快排的递归深入到一定深度 __depth_limit（通过元素个数计算出来的）时，不再递归深入，对待排序元素执行堆排序，代码如下： 123456789101112131415161718192021/// This is a helper function for the sort routine.template&lt;typename _RandomAccessIterator, typename _Size, typename _Compare&gt; void __introsort_loop(_RandomAccessIterator __first, _RandomAccessIterator __last, _Size __depth_limit, _Compare __comp) &#123; while (__last - __first &gt; int(_S_threshold)) &#123; if (__depth_limit == 0) &#123; std::__partial_sort(__first, __last, __last, __comp); return; &#125; --__depth_limit; _RandomAccessIterator __cut = std::__unguarded_partition_pivot(__first, __last, __comp); std::__introsort_loop(__cut, __last, __depth_limit, __comp); __last = __cut; &#125; &#125; 第二阶段使用“插入排序”，当元素个数小于等于 _S_threshold（enum { _S_threshold = 16 }）时，执行普通的插入排序，当大于 _S_threshold 时，执行两次的“插入”排序操作，首先使用普通的插入排序来排 [first, _S_threshold) 这个范围的元素，然后使用无保护的插入排序，完成 [_S_threshold, last) 这个范围的排序。 1234567891011121314template&lt;typename _RandomAccessIterator, typename _Compare&gt; void __final_insertion_sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) &#123; if (__last - __first &gt; int(_S_threshold)) &#123; std::__insertion_sort(__first, __first + int(_S_threshold), __comp); std::__unguarded_insertion_sort(__first + int(_S_threshold), __last, __comp); &#125; else std::__insertion_sort(__first, __last, __comp); &#125; 其中的普通插入排序没有什么特别的地方，就是遍历前边小于等于_S_threshold个元素进行普通的插入排序，而后面这个无保护的插入排序 std::__unguarded_insertion_sort 往往就是出现问题的地方，代码如下： 123456789101112131415161718192021222324252627template&lt;typename _RandomAccessIterator, typename _Compare&gt; inline void __unguarded_insertion_sort(_RandomAccessIterator __first, _RandomAccessIterator __last, _Compare __comp) &#123; for (_RandomAccessIterator __i = __first; __i != __last; ++__i) std::__unguarded_linear_insert(__i, __gnu_cxx::__ops::__val_comp_iter(__comp)); &#125;template&lt;typename _RandomAccessIterator, typename _Compare&gt; void __unguarded_linear_insert(_RandomAccessIterator __last, _Compare __comp) &#123; typename iterator_traits&lt;_RandomAccessIterator&gt;::value_type __val = _GLIBCXX_MOVE(*__last); _RandomAccessIterator __next = __last; --__next; while (__comp(__val, __next)) &#123; *__last = _GLIBCXX_MOVE(*__next); __last = __next; --__next; &#125; *__last = _GLIBCXX_MOVE(__val); &#125; 这段代码看 __unguarded_insertion_sort 还没有什么问题，但是 __unguarded_linear_insert 中的逻辑就比较迷幻了，只有当 __comp(__val, __next) 的值为false时才会停止。 其中 __comp 就是我们之前自定义的lambda表达式，我们当时写的是 return v1 &gt;= v2;，翻译过来也就是当!(val &gt;= __next) 时，即后一个元素小于前一个元素的时候停止，那么为什么会出问题呢？ 我们知道前_S_threshold个元素我们之前已经按照从大到小排好序了，那么按道理遍历到这个区域就会找到后一个元素小于前一个元素的情况，也就是插入排序遍历到这就会停止，等等！好像有什么不对劲，如果这里的元素都相等就找不到停止的情况了，这就会造成访问的越界，这就是程序崩溃的本质原因了。 那么去掉等号会是个什么情况呢？运行到这里就是要找到满足条件的 !(val &gt; __next)元素时停止，也就是找到后一个元素小于等于前一个元素的时候停止，因为前_S_threshold个元素已经排好序，这个条件是肯定满足的，所以不会出现越界情况，这就是为什么自定义比较函数中，两个元素相等时一定要返回false了。 为什么使用无保护的插入排序既然这里这么容易越界，为什么不判断一下边界条件来防止越界，而是用这种无保护的插入排序呢？ 这里使用无保护的插入排序原因很简单，就是为了提升效率，因为省略掉越界的检查，少了很多次的比较操作，效率肯定有了提升，它的前提是左边必须有已经排好序的元素，所以在函数 __unguarded_insertion_sort 函数之前先调用 __insertion_sort 来完成了[0, _S_threshold) 这个范围的元素排序，便是为了后面这个无保护插入排序的使用。 C++标准要求说到这里sort函数的自定义比较函数还是太容易出错了，有没有什么实现标准呢？其实标准中对这个比较函数的要求写的很详细，具体可以参考 Compare的实现要求。 Compare 是一些标准库函数针对用户提供的函数对象类型所期待的一组要求，其实就是要满足严格若排序关系，翻译成人话就是自定义的比较函数 comp 需要下面三条要求： 对于任意元素a，需满足 comp(a, a) == true 对于任意两个元素a和b，若 comp(a, b)==true 则要满足 comp(b, a)==false 对于任意三个元素a、b和c，若 comp(a, b)==true 且 comp(b, c)==true 则需要满足 comp(a, c)==true 从这条规则也能看出我们之前定义的问题： 123std::sort(values.begin(), values.end(), [](int v1, int v2)&#123; return v1 &gt;= v2;&#125;); 这个自定义的比较函数，当 v1 和 v2 相等时，comp(v1, v2)==true， 但是 comp(v2, v1)的值也是 true，当我们把代码中的等号 = 去掉以后，也就满足了条件2，另外在复杂的比价逻辑中，条件3的传递性问题也是需要注意的问题。 构造一个崩溃的示例理解了前面崩溃的原因，我们就不需要猜了，可以直接构造一个百分之百奔溃的测试用例，因为前16（_S_threshold）个元素会使用正常的插入排序，后面的元素才会使用无保护的插入排序，我们其实构造一个17个相同元素的vector就可以了，下面我们来试一下： 1234567891011int main()&#123; std::vector&lt;int&gt; values&#123;6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6&#125;; std::sort(values.begin(), values.end(), [](int v1, int v2)&#123; return v1 &gt;= v2; &#125;); for (auto v : values) std::cout &lt;&lt; v &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374albert@home-pc:/data/cpp$ g++ testsort.cpp --std=c++11 -galbert@home-pc:/data/cpp$ ./a.out06666666666666666*** Error in `./a.out': double free or corruption (out): 0x0000000001fd9c20 ***======= Backtrace: =========/lib/x86_64-linux-gnu/libc.so.6(+0x777f5)[0x7feaf8ef77f5]/lib/x86_64-linux-gnu/libc.so.6(+0x8038a)[0x7feaf8f0038a]/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7feaf8f0458c]./a.out[0x402446]./a.out[0x40230f]./a.out[0x40218a]./a.out[0x402005]./a.out[0x401e65]./a.out[0x400bf1]/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7feaf8ea0840]./a.out[0x4009e9]======= Memory map: ========00400000-00403000 r-xp 00000000 00:00 211636 /mnt/d/data/cpp/testsort/a.out00403000-00404000 r-xp 00003000 00:00 211636 /mnt/d/data/cpp/testsort/a.out00603000-00604000 r--p 00003000 00:00 211636 /mnt/d/data/cpp/testsort/a.out00604000-00605000 rw-p 00004000 00:00 211636 /mnt/d/data/cpp/testsort/a.out01fc8000-01ffa000 rw-p 00000000 00:00 0 [heap]7feaf4000000-7feaf4021000 rw-p 00000000 00:00 07feaf4021000-7feaf8000000 ---p 00000000 00:00 07feaf8b70000-7feaf8c78000 r-xp 00000000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8c78000-7feaf8c7a000 ---p 00108000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8c7a000-7feaf8e77000 ---p 0010a000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8e77000-7feaf8e78000 r--p 00107000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8e78000-7feaf8e79000 rw-p 00108000 00:00 243923 /lib/x86_64-linux-gnu/libm-2.23.so7feaf8e80000-7feaf9040000 r-xp 00000000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9040000-7feaf9049000 ---p 001c0000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9049000-7feaf9240000 ---p 001c9000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9240000-7feaf9244000 r--p 001c0000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9244000-7feaf9246000 rw-p 001c4000 00:00 243912 /lib/x86_64-linux-gnu/libc-2.23.so7feaf9246000-7feaf924a000 rw-p 00000000 00:00 07feaf9250000-7feaf9266000 r-xp 00000000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17feaf9266000-7feaf9465000 ---p 00016000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17feaf9465000-7feaf9466000 rw-p 00015000 00:00 180347 /lib/x86_64-linux-gnu/libgcc_s.so.17feaf9470000-7feaf95e2000 r-xp 00000000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf95e2000-7feaf95ef000 ---p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf95ef000-7feaf97e2000 ---p 0017f000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf97e2000-7feaf97ec000 r--p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf97ec000-7feaf97ee000 rw-p 0017c000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217feaf97ee000-7feaf97f2000 rw-p 00000000 00:00 07feaf9800000-7feaf9825000 r-xp 00000000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7feaf9825000-7feaf9826000 r-xp 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7feaf9a25000-7feaf9a26000 r--p 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7feaf9a26000-7feaf9a27000 rw-p 00026000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7feaf9a27000-7feaf9a28000 rw-p 00000000 00:00 07feaf9bc0000-7feaf9bc1000 rw-p 00000000 00:00 07feaf9bd0000-7feaf9bd2000 rw-p 00000000 00:00 07feaf9be0000-7feaf9be1000 rw-p 00000000 00:00 07feaf9bf0000-7feaf9bf1000 rw-p 00000000 00:00 07feaf9c00000-7feaf9c01000 rw-p 00000000 00:00 07feaf9c10000-7feaf9c11000 rw-p 00000000 00:00 07ffffb85e000-7ffffc05e000 rw-p 00000000 00:00 0 [stack]7ffffc61d000-7ffffc61e000 r-xp 00000000 00:00 0 [vdso]Aborted (core dumped) 完全符合预期，如果再删除vector中的一个元素就不会崩溃了。 平台差异这篇文章的代码编译和运行都是在Linux下完成的，但是我之前在Windows上测试时，可不需要最少17个元素的前提，这是为什么呢？因为在微软这一套编译环境下，直接检测了Compare中的条件2，并且是以断言的方式给出提示的，所以与Linux上的运行表现还有一些差异。 总结 使用 std::sort 函数自定义比较函数时，需要满足严格弱排序性，若 comp(a, b)==true 则 comp(b, a)==false，那么在比较函数中两个元素相等的情况要返回false 使用 std::sort 函数出现崩溃是往往是不满足严格若排序性，但是在复杂的比较函数中也可能不满足传递性 std::sort 为了把排序效率提高到极致，综合使用了快排、堆排、插入排序等多种排序方法 std::sort 在不同的平台实现不同，当比较函数不满足严格若排序时，gcc环境下至少有17个元素才会崩溃，而 MSVC 则在Debug时没有元素个数限制，会通过断言直接判断这个条件是否满足 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 可是啊 总有那风吹不散的认真 总有大雨也不能抹去的泪痕~ 2021-8-8 23:57:53]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>sort</tag>
        <tag>lambda</tag>
        <tag>crash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用box2dweb做一个下落的小球，宝宝玩的不亦乐乎]]></title>
    <url>%2Fblog%2F2021%2F07%2F21%2F%E4%BD%BF%E7%94%A8box2dweb%E5%81%9A%E4%B8%80%E4%B8%AA%E4%B8%8B%E8%90%BD%E7%9A%84%E5%B0%8F%E7%90%83%EF%BC%8C%E5%AE%9D%E5%AE%9D%E7%8E%A9%E7%9A%84%E4%B8%8D%E4%BA%A6%E4%B9%90%E4%B9%8E%2F</url>
    <content type="text"><![CDATA[前言红球球~ 蓝球球~ 家里的宝宝就喜欢玩球球，特别是蓝色的，而最近正好找到了一个优秀的物理引擎，就拿它来做了一个下落的小球，宝宝玩的不亦乐乎~ 起初我是想模拟一个小球在不同曲线上的运动状态，一直在找一个简易的重力引擎，找了一段时间没有什么收获，也想过自己写个重力系统，可是仔细想想觉得有点太麻烦，后来玩了一些flash游戏，发现这类依靠重力的作为主要玩法的游戏还真不少，肯定有一个好的引擎可以直接拿来用，然后就发现了 Box2D 这个物理引擎，不但带有重力系统，还支持碰撞。 box2dweb其实 cocos2dx 和 unity 中都有使用 Box2D 引擎，只不过自己进行了封装，如果使用这些客户端引擎做出来的东西依赖性比较强，还好顺着 Box2D 这条线发现了一个 box2dweb 引擎，是一个 Box2D 的 js 版本，这就非常方便了，有个浏览器就可以直接运行了，还免去了编译安装的麻烦。 下落的小球开局一张图，后面接着编。。。 把这个下落的小球叫做一个游戏确实有些勉强，但是宝宝就是很喜欢玩啊，游戏本来就是让人开心的，开心就好咯！接下来记录一下这个小球是怎么实现的吧~ 具体实现下载引擎box2dweb 引擎只有一个js文件，可以在github上搜索下载 hecht-software/box2dweb ，不过最近访问缓慢，也可以下载这个我备份的版本 Box2dWeb-2.1.a.3.min.js，下载之后直接引用即可。 引入js引擎新建一个 html 文件命名为 fallball-game.html，编写如下内容，引入 Box2dWeb-2.1.a.3.min.js 文件 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;fall ball game&lt;/title&gt; &lt;script type="text/javascript" src="./Box2dWeb-2.1.a.3.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="./fallball.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;canvas id="mycanvas" width="1600" height="900"&gt;&lt;/canvas&gt;&lt;/body&gt;&lt;/html&gt; 编写小球逻辑在上面的 html 文件中不仅引入了 Box2dWeb-2.1.a.3.min.js 这个物理引擎，还引入了一个 fallball.js 文件，这是一个新建的自定义的js文件，需要我们在这个文件中编写小球的逻辑。 引入常用类型并简写1234567891011121314var b2Vec2 = Box2D.Common.Math.b2Vec2, b2AABB = Box2D.Collision.b2AABB, b2BodyDef = Box2D.Dynamics.b2BodyDef, b2Body = Box2D.Dynamics.b2Body, b2FixtureDef = Box2D.Dynamics.b2FixtureDef, b2Fixture = Box2D.Dynamics.b2Fixture, b2World = Box2D.Dynamics.b2World, b2MassData = Box2D.Collision.Shapes.b2MassData, b2PolygonShape = Box2D.Collision.Shapes.b2PolygonShape, b2CircleShape = Box2D.Collision.Shapes.b2CircleShape, b2DebugDraw = Box2D.Dynamics.b2DebugDraw, b2MouseJointDef = Box2D.Dynamics.Joints.b2MouseJointDef, b2RayCastInput = Box2D.Collision.b2RayCastInput, b2RayCastOutput = Box2D.Collision.b2RayCastOutput 以上这些都是 Box2D 引擎中常用的类，使用简写的变量来引用这些类，这样在后面使用时会方便很多。 创建Box2D世界b2World 是 Box2D 系统模拟物理世界的核心，可以想象成我们生活中的地球，在地球上有各种各样的物理环境，比如从空中自由落体的小球，在水中航行的轮船等等，使用 b2World 就可以创建这样一个世界。 在物理世界中首先需要的是重力，那么先定义一个有方向的力： 1var gravity = new b2Vec2(0, 9.8); 作为一个模拟物理环境的引擎，效率使我们需要考虑的问题，对于静止不动的对象，最好不进行模拟计算来节省CPU运算，这种静止的对象可以让他们在 Box2D 环境中睡觉。12var doSleep = true; 需要的参数都准备好了，这样可以new出一个Box2D世界了。 1var world = new b2World(gravity, doSleep); 创建一个小球 先创建一个物体 12345var bodydef = new b2BodyDef();// 物体类型定义，基本上常用的有两种定义：b2_staticBody 静态物体; b2_dynimacBod动态物体bodydef.type= b2Body.b2_staticBody;// 定义物体位置bodydef.position.Set(0, 0); 再创建一个材质 1234var fixDef = new b2FixtureDef();fixDef.density = 1.0; // desity 密度，如果密度为0或者null，该物体则为一个静止对象fixDef.friction = 0.5; //摩擦力（0~1）fixDef.restitution = 0.2;// 弹性（0~1） 为材质添加一个形状并创建小球 12345678fixDef.shape = new b2CircleShape(0.5);bodyDef.type = b2Body.b2_dynamicBody;bodyDef.position.Set(3, 0);var bomb = world.CreateBody(bodyDef);bomb.userData = "iambomb";bomb.CreateFixture(fixDef) b2PolygonShape为多边形，设置形状大小时对应着SetAsBox(halfWidth, halfHeight)函数，参数半长和半宽，如果自定义多边形可以使用一个SetAsArray(vertexArray,vertexCount)，其中vertexArray为顶点矢量（b2Vec2）数组，vertexCount为顶点数，最多8个 b2CircleSharp为圆形，对应的设置属性为半径，函数为SetRadius(radius) 需要注意的是Box2d中的单位是米，1米是30像素，在绘制材质图片时需注意单位换算 使用 bomb.ApplyForce(force, point); 可以添加一个外力，force是一个b2Vec2的向量代表外力，point一个b2Vec2的向量代表物体的着力点。使用 bomb.SetMassFromShapes(); 可以根据形状计算质量 创建背景刚体物理12345678910111213141516171819202122232425var vertices = [ new b2Vec2(0, 0), new b2Vec2(20, 10), new b2Vec2(0, 10),]fixDef.shape = new b2PolygonShape.AsArray(vertices, 3);//fixDef.shape = new b2PolygonShape();//fixDef.shape.SetAsBox(30, 1);bodyDef.type = b2Body.b2_staticBody;bodyDef.position.Set(0, 0);world.CreateBody(bodyDef).CreateFixture(fixDef);var vertices2 = [ new b2Vec2(19, 10), new b2Vec2(40, 0), new b2Vec2(40, 10),]fixDef.shape = new b2PolygonShape.AsArray(vertices2, 3);bodyDef.type = b2Body.b2_staticBody;bodyDef.position.Set(0, 5);world.CreateBody(bodyDef).CreateFixture(fixDef); 参照小球创建的方式创建两个三角形物体，这里注意一个问题，三角形的坐标需要顺时针给出，否则两个刚体的碰撞会出现问题。 创建调试环境实际上，Box2D只是集成了各种算法，对b2Body对象进行物理模拟计算，并将计算结果存储到这个对象中，但是它并不是DisplayObject的子类，也就意味着我们无法通过addChild()将它添加到舞台上，这时可以借助b2DebugDraw类，绑定一个Sprite对象，Box2D就能帮我们在这个Sprite中，用绘图API绘制出模拟图形，方便我们进行调试。 12345678var debugDraw = new b2DebugDraw();debugDraw.SetSprite(document.getElementById("mycanvas").getContext("2d"));debugDraw.SetDrawScale(30.0);debugDraw.SetFillAlpha(0.5);debugDraw.SetLineThickness(1.0);debugDraw.SetFlags( b2DebugDraw.e_shapeBit | b2DebugDraw.e_jointBit | b2DebugDraw.e_controllerBit | b2DebugDraw.e_pairBit);world.SetDebugDraw(debugDraw); 世界更新12345678910111213141516171819function update () &#123; world.Step(1 / 60, //frame-rate 10, //velocity iterations 10); //position iterations world.DrawDebugData(); for (var b = world.m_bodyList; b != null; b = b.m_next) &#123; if (b.GetUserData()) &#123; context.save(); context.translate(b.GetPosition().x * 30, b.GetPosition().y * 30); context.rotate(b.GetAngle()); context.drawImage(b.GetUserData(), -b.GetUserData().width / 2, -b.GetUserData().height / 2); context.restore(); &#125; &#125; world.ClearForces();&#125;setInterval(update, 1000 / 60); 定义更新函数，注册定时器，用来定时更新。物理引擎的 world.Step 函数是整个 Box2D 引擎的核心, 它驱动了物理世界的运行。而上述代码中 for 循环的部分处理了材质跟随刚体运动的逻辑，可以自定义显示图片，这个已经实验成功，改天可以接着写一写。 总结 Box2D 是一个优秀易用的物理引擎，而 box2dweb 是一个js版本，可以很方便的在html引入使用 box2dweb 不仅能模拟重力环境，还能模拟物体之间的碰撞，可以通过参数调整物体的密度、摩擦力和弹力 物体的形状可以多边形，可以通过给出顶点坐标的形式设置，不过要注意按照顺时针方向给出，否则碰撞失败，具体原因还不太清楚 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 这个世界上，还有很多不被认可的梦想，不被祝福的感情，不被眷顾的孩子，他们不曾犯错，却只能颤颤巍巍，单薄地行走在路上。你我都一样 要承认 我们都很平凡 并且在负重~ 2021-7-25 18:59:01]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>Game</tag>
        <tag>JS</tag>
        <tag>物理引擎</tag>
        <tag>box2dweb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++中的数据类型转换static_cast/dynamic_cast/const_cast/reinterpret_cast]]></title>
    <url>%2Fblog%2F2021%2F07%2F10%2FC-C-%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[前言C/C++属于静态语言，也就是在编译时变量的数据类型即可确定的强类型语言。当不同的数据类型在一起运算或者相互赋值的时候，就需要进行数据类型转换。不同数据类型占用的内存空间不同，而各种数据类型的转换时有规则的，一种通用的规则就是“小转大”自动进行，“大转小”需要强制执行。这里的“大”和“小”指的是数据范围。 为什么会有数据范围大小的区别呢？这就和饭店里的盘子一样，不同的菜肴通常使用不同的盘子，盘子有大有小，如果把小盘子里的菜装到大盘子里没有问题，但是把大盘子里的菜放到小盘子中就会溢出来，假设都使用大盘子就不会产生溢出的问题，但是这样会产生空间的浪费。而C/C++中不同类型的变量占用的内存空间不同与这些盘子非常相似，当范围小的变量赋值给范围大的变量时没有问题，但是反过来也会出现溢出。 数据类型自动转换当不同类型的变量同时运算时就会发生数据类型的自动转换，以常见的 char、short、int、long、float、double 这些类型为例，如果 char 和 int 两个类型的变量相加时，就会把 char 先转换成 int 再进行加法运算，如果是 int 和 double 类型的变量相乘就会把 int 转换成 double 再进行运算。 自动转换的行为如下图所示，方向是从左往右自动进行： 123456graph LR A((char))--&gt;C(unsigned/int) B((short))--&gt;C(unsigned/int) C(unsigned/int)--&gt;D(unsigned/long) D(unsigned/long)--&gt;F[double] E(float)--&gt;F[double] C语言中的强制类型转换前面说了自动转换，从这里开始聊聊强制类型转换，需要强制类型转换往往程序不那么智能了，需要人工进行干预。比如把一个int 类型的变量赋值给 char 类型的变量，或者说把两个 int 相乘时可能会得到一个很大的数，所以需要先把 int 强制转换成 double 计算防止溢出。 强制类型转换的格式为：(new_type_name) expression，其中 new_type_name 为新类型名称，expression为表达式。例如： 123int val = 65535;char ch = (char)val; 或者 12int m = 2147483647, n = 100;double result = (double)m * n; 无论是自动的类型转换还是强制类型转换，都只是为了本次操作或运算而进行的临时转换，转换的结果也会保存到临时的内存空间内，不会改变数据本来的类型或者具体的值。 有些强制类型转换是对原有数据的重新解释，比如： 12345void test(void* p)&#123; char* buffer = (char*)p; // ...&#125; void* 类型的变量p，经过强制类型转换以后变成了char类型的指针，此后就可以把这段内存空间当成字符数组来处理了。 C++中的强制类型转换在C++语言中新增了四个用于强制类型转换的关键字，分别是 static_cast、 dynamic_cast, const_cast、 和 reinterpret_cast，使用语法为 xxxx_cast&lt;new_type_name&gt;(expression)。 相比于C语言中使用小括号()来完成强制类型转换，C++中这几个关键字的引入能更清晰的表明它要完成强制类型转换的意图，容易暴露出隐藏的问题。 其实很长一段时间以来，我对于这四种强转方式区分的不是很清晰，其中 const_cast 的功能还比较容易辨别，但是另外3种经常混作一团，所以才有了这篇总结，而仔细学习后才发现，这4种强转关键字的区别就在他们的名字上，下面逐个来看一下。 static_cast这个关键字的作用主要表现在 static 上，是一种静态的转换，在编译期就能确定的转换，可以完成C语言中的强制类型转换中的大部分工作，但需要注意的是，它不能转换掉表达式的 const、volitale 或者 __unaligned 属性。 它主要有以下几种用法： 用于基本数据类型之间的转换，如把int转换成char，把int转换成double等。 123int val = 110119;char c = static_cast&lt;char&gt;(val);double d = static_cast&lt;double&gt;(val); 将表达式转换成void类型，并将转换后的结果丢弃 12int val = 110119;static_cast&lt;void&gt;(val); 可以用于void* 和其他指针类类型之间的转换，但是不能用于两个无关指针类型的直接转换 1234567// 正常转换int *p = new int;void* p1 = static_cast&lt;void*&gt;(p);char* p2 = static_cast&lt;char*&gt;(p1);// 编译失败 //error: invalid static_cast from type ‘int*’ to type ‘char*’char* p3 = static_cast&lt;char*&gt;(p); 可以用于类继承结构中基类和派生类之间指针或引用的转换，向上转型安全，向下转型由于没有动态类型检查，是不安全的。 123456struct B &#123; &#125;;struct D : B &#123; &#125;;D d;B&amp; rb = d;D&amp; rd = static_cast&lt;D&amp;&gt;(rb); 如果涉及左值到右值、数组到指针或函数到指针的转换，也可以通过static_cast显式执行。 1234template&lt;typename _Tp&gt;inline typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp;move(_Tp&amp;&amp; __t)&#123; return static_cast&lt;typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp;&gt;(__t); &#125; dynamic_cast从名字上看，这个关键字与 static_cast 的静态转换是对立的，这是一个“动态”转换函数，只能对指针和引用的进行转换，并且只用于类继承结构中基类和派生类之间指针或引用的转换，可以进行向上、向下，或者横向的转换。 相比于 static_cast 的编译时转换， dynamic_cast 的转换还会在运行时进行类型检查，转换的条件也比较苛刻，必须有继承关系的类之间才能转换，并且在基类中有虚函数才可以，有一种特殊的情况就是可以把类指针转换成 void* 类型。 关于使用中的常见问题，参考以下几种情况： 普通类型的指针无法转换 12345int val = 100;int *p = &amp;val;// 编译失败 //error: cannot dynamic_cast ‘p’ (of type ‘int*’) to type ‘char*’ (target is not pointer or reference to class)char* pc = dynamic_cast&lt;char*&gt;(p); 继承结构中基类里面没有虚函数无法转换 12345678struct B &#123; &#125;;struct D : B &#123; &#125;;D d;B* pb = &amp;d;// 编译失败 //error: cannot dynamic_cast ‘pb’ (of type ‘struct test1()::B*’) to type ‘struct test1()::D*’ (source type is not polymorphic)D* pd = dynamic_cast&lt;D*&gt;(pb) 指针或引用转换的类型不是正确的类型，如果参数类型是指针会返回目标类型空指针，如果参数类型是引用则会抛出 std::bad_cast 异常。 123456789struct B &#123; virtual void test() &#123;&#125; &#125;;struct D : B &#123; &#125;;B d;B* pb = &amp;d;D* pd = dynamic_cast&lt;D*&gt;(pb);// 编译成功，但是pb指针指向的类型是 B，向下转型失败，输出结果是0，也就是空指针std::cout &lt;&lt; pd &lt;&lt; std::endl; 一个正常转换的例子，包含向上、向下、横向转换 1234567891011121314151617181920struct B &#123; virtual void test() &#123;&#125; &#125;;struct D1 : virtual B &#123; &#125;;struct D2 : virtual B &#123; &#125;;struct MD : D1, D2 &#123; &#125;;D1* pd1 = new MD();std::cout &lt;&lt; pd1 &lt;&lt; std::endl;// 向上转型B* pb = dynamic_cast&lt;B*&gt;(pd1);std::cout &lt;&lt; pb &lt;&lt; std::endl;// 向下转型MD* pmd = dynamic_cast&lt;MD*&gt;(pd1);std::cout &lt;&lt; pmd &lt;&lt; std::endl;// 横向转型D2* pd2 = dynamic_cast&lt;D2*&gt;(pd1);std::cout &lt;&lt; pd2 &lt;&lt; std::endl; 运行结果如下，在横向转换时指针发生了变化，可以看出 dynamic_cast 不是简单的数据强转，还进行了指针的偏移： 123456albert@home-pc:/mnt/d/testconvert$ g++ cppconvert.cppalbert@home-pc:/mnt/d/testconvert$ ./a.out0x15c0c400x15c0c400x15c0c400x15c0c48 const_cast在C/C++中，const限定符通常被用来限定变量，用于表示该变量的值不能被修改，这种限定可以避免程序员犯一些初级错误，但同时也造成了一些不便，比如一些已有函数要求非常量指针，但是掉用这些函数的接口函数中都传递了常量指针，这时候就要对指针类型去常量化。 但需要特别注意的是 const_cast 不能去除变量的常量性，只能用来去除指向常数对象的指针或引用的常量性，且去除常量性的对象必须为指针或引用。 常量指针被转化成非常量指针，并且仍然指向原来的对象，常量引用被转换成非常量引用，并且仍然指向原来的对象；常量对象可能被转换成非常量对象。 尝试去除非指针和引用的类型的常量性会编译失败 1234const int i = 6;// 编译错误 //int j = const_cast&lt;int&gt;(i); 企图用一个指针来修改常量： 1234const int val = 6;//编译错误 //error: invalid conversion from ‘const int*’ to ‘int*’ [-fpermissive]int* cp = &amp;val; 修改一个指针的常量性： 12345678910const int val = 6;std::cout &lt;&lt; "&amp;val=" &lt;&lt; &amp;val &lt;&lt; ", val=" &lt;&lt; val &lt;&lt; std::endl;const int* cp = &amp;val;int *p = const_cast&lt;int*&gt;(cp);*p = 2;std::cout &lt;&lt; "&amp;val=" &lt;&lt; &amp;val &lt;&lt; ", val=" &lt;&lt; val &lt;&lt; std::endl;std::cout &lt;&lt; "p=" &lt;&lt; p &lt;&lt; ", *p=" &lt;&lt; *p &lt;&lt; std::endl; 运行结果如下： 123&amp;val=0x7ffff7446bd4, val=6&amp;val=0x7ffff7446bd4, val=6p=0x7ffff7446bd4, *p=2 运行之后，变量 p 指向了变量val地址，并改变了地址所指向的内存数据，但是打印 val 的值并没有发生变化，这是因为 val 作为常量在编译期使用它的地方就进行了替换，接下来再看另一种情况。 1234567891011int init = 6;const int val = init;std::cout &lt;&lt; "&amp;val=" &lt;&lt; &amp;val &lt;&lt; ", val=" &lt;&lt; val &lt;&lt; std::endl;const int* cp = &amp;val;int *p = const_cast&lt;int*&gt;(cp);*p = 2;std::cout &lt;&lt; "&amp;val=" &lt;&lt; &amp;val &lt;&lt; ", val=" &lt;&lt; val &lt;&lt; std::endl;std::cout &lt;&lt; "p=" &lt;&lt; p &lt;&lt; ", *p=" &lt;&lt; *p &lt;&lt; std::endl; 代码逻辑不变，只在开始的位置使用 init 这个变量来代替 6 这个常数，运行结果如下： 123val=0x7fffe8c71fa0, val=6&amp;val=0x7fffe8c71fa0, val=2p=0x7fffe8c71fa0, *p=2 运行之后 val 本身的变化也应用到了使用它的地方，这里的编译器替换已经不起作用了。 实际上，使用const_cast通常是一种无奈之举，利用const_cast去掉指针或引用的常量性并且去修改原始变量的数值，这是一种非常不好的行为，如果可以的话，尽可能在程序设计阶段就规避这种情况。 reinterpret_cast它被用于不同类型指针或引用之间的转换，或者指针和整数之间的转换，是对比特位的简单拷贝并重新解释，因此在使用过程中需要特别谨慎，比如前面提到的一个例子，static_cast 不能将 int* 直接强转成 char*，使用reinterpret_cast就可以办到。 不同基础类型指针类型之间转换： 1234567int *p = new int;// 编译失败 //error: invalid static_cast from type ‘int*’ to type ‘char*’char* p1 = static_cast&lt;char*&gt;(p);// 编译成功char* p2 = reinterpret_cast&lt;char*&gt;(p1); 基础类型指针与类对象指针之间的转换： 123456789struct B &#123; int val;&#125;;B b&#123;100&#125;;std::cout &lt;&lt; "b.val=" &lt;&lt; b.val &lt;&lt; std::endl;int* p = reinterpret_cast&lt;int*&gt;(&amp;b);std::cout &lt;&lt; "*p=" &lt;&lt; *p &lt;&lt; std::endl; 运行之后可以得到 *p 的值为100，也就是重新解释了变量 b 的地址为整型指针。 将地址值转换成整数 123456789struct B &#123; int val;&#125;;B b&#123;101&#125;;std::cout &lt;&lt; "&amp;b=" &lt;&lt; &amp;b &lt;&lt; std::endl;long addr = reinterpret_cast&lt;long&gt;(&amp;b);std::cout &lt;&lt; "addr=" &lt;&lt; addr &lt;&lt; std::endl; 运行结果如下： 12&amp;b=0x7ffffdc4f270addr=140737450930800 这里的地址 0x7ffffdc4f270 被解释成了整数 140737450930800，因为涉及到字节序，这也是很多文章提到的 reinterpret_cast 不具备一致性的问题，我们需要知道这一个点，只要代码不依赖主机字节序就没有问题。 强转关键字的选择好几个关键字，并且有些功能还是重复的，那么究竟该选哪一个呢？这个真得按照经验来选，我建议使用排除法，按照 const_cast -&gt; dynamic_cast -&gt; reinterpret_cast -&gt; static_cast 的顺序带入选择。 先看是不是要去掉指针或引用的常量属性，如果是只能选择 const_cast 再看转换的是不是继承体系下的多态结构，如果是这种结构下的指针和引用的转换最好使用 dynamic_cast 接着看是不是偏底层的代码，需要将无关类型指针进行转换，或者指针与整数之间进行转换，如果是则选择 reinterpret_cast 前三种情况都不满足，那就只能使用 static_cast 了 总结 C/C++中不同数据类型进行运算或者赋值的时候会发生数据转换，这种转换有些是自动进行的，有些需要进行显示的强制类型转换 在C语言中强制类型转换写成(new_type_name) expression的形式，new_type_name 是要转换的目标类型，expression 是待转换的表达式 在C++中强制类型转换通过更明显的关键字来完成，分别是static_cast、 dynamic_cast, const_cast、 和 reinterpret_cast static_cast 是静态转换，在编译期完成完成转换，与C语言中的强制类型转换重合度最高 dynamic_cast 是动态转换，在运行时转换会进行检查，必须用在有继承关系的多态结构中 const_cast 是常量转换，用于取出指针或引用的常量属性，但是尽量通过设计杜绝它的使用场景 reinterpret_cast 是一种内存数据的重新解释，比较原始，开发者使用它的时候应该明确的知道自己在做什么 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 今夜的雨，好美~ 2021-7-12 00:18:13]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>数据转换</tag>
        <tag>自动</tag>
        <tag>强制类型转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[换个角度来看看C++中的左值、右值、左值引用、右值引用]]></title>
    <url>%2Fblog%2F2021%2F07%2F04%2F%E6%8D%A2%E4%B8%AA%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%9C%8B%E7%9C%8BC-%E4%B8%AD%E7%9A%84%E5%B7%A6%E5%80%BC%E3%80%81%E5%8F%B3%E5%80%BC%E3%80%81%E5%B7%A6%E5%80%BC%E5%BC%95%E7%94%A8%E3%80%81%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言对于左值和右值有一个不太严谨的定义——在赋值表达式 = 左侧是的左值，而在 = 右侧的是右值。通过不断学习和尝试，最近我发现一个新的说法更加贴切，那就是“左值是容器，右值是东西”。对于这个定义我们可以类比一下水杯和水，通过水杯可以操作水杯中的水，操作过程中的中间结果如果想要进一步操作，可以将其放入其他的水杯，如果没有水杯就无法找到曾经操作过的水了，也就无法继续操作了。 123int a = 2;int b = 6;int c = a + b; 在这个例子中，变量 a，b, c 都是水杯，而 2、6、a + b 都是被用来操作的水，只有把这些“水”放到“水杯”中才能被找到，才可以进行下一步操作。 关于左值、右值、左值引用和右值引用的概念可以看看之前的总结： 简单聊聊C/C++中的左值和右值 C++11在左值引用的基础上增加右值引用 虽然温故不一定知新，但绝对可以增强记忆，参照着之前的理解，今天来换一种窥探本质的方式。 汇编代码初探为了熟悉一下汇编代码，我们先写个简单的例子，内容就是上述提到的那一段，新建一个文件 main.cpp，然后编写如下代码： 12345678int main()&#123; int a = 6; int b = 2; int c = a + b; return 0;&#125; 运行 g++ main.cpp --std=c++11 -S -o main.s 编译这段代码，生成汇编文件 main.s，打开文件内容如下： 123456789101112131415161718192021222324252627 .file "main.cpp" .text .globl main .type main, @functionmain:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl $6, -12(%rbp) movl $2, -8(%rbp) movl -12(%rbp), %edx movl -8(%rbp), %eax addl %edx, %eax movl %eax, -4(%rbp) movl $0, %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size main, .-main .ident "GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609" .section .note.GNU-stack,"",@progbits 其中代表定义变量和做加法的语句转换成汇编代码如下： 123456movl $6, -12(%rbp) // 把立即数6放到内存地址为-12(%rbp)的位置，也就是变量a中movl $2, -8(%rbp) // 把立即数2放到内存地址为-8(%rbp)的位置，也就是变量b中movl -12(%rbp), %edx // 把内存地址为-12(%rbp)的位置（变量a）的数据放到寄存器%edx中movl -8(%rbp), %eax // 把内存地址为-8(%rbp)的位置（变量b）的数据放到寄存器%eax中addl %edx, %eax // 把寄存器%edx中的数据加到寄存器%eax中movl %eax, -4(%rbp) // 把寄存器%eax中的计算所得结果数据放到内存地址为-4(%rbp)的位置，也就是变量c中 指针变量首先来看看通过指针来修改变量值的过程，测试代码如下： 123int a = 6;int* p = &amp;a;*p = 2; 转换成汇编代码如下： 12345movl $6, -20(%rbp) // 把立即数6放到内存地址为-20(%rbp)的位置，也就是变量a中leaq -20(%rbp), %rax // 把这个内存地址-20(%rbp)，也就是变量a的地址保存在寄存器%rax中movq %rax, -16(%rbp) // 把寄存器%rax中的保存的变量a的地址，放到内存地址为-16(%rbp)的位置，也就是变量p中movq -16(%rbp), %rax // 把内存地址为-16(%rbp)的位置（变量p）的数据放到寄存器%rax中movl $2, (%rax) // 把立即数2放在寄存器%rax中保存的地址位置中，也就是p所指向的地址，即变量a中 通过汇编代码可以发现，通过指针修改变量的值实际上是在指针变量中保存变量的地址值，修改变量时是通过指针变量直接找到变量所在内存，然后直接修改完成的。 左值引用接着来看下通过引用来修改变量值的过程，测试代码如下： 123int a = 6;int&amp; r = a;r = 2; 转换成汇编代码如下： 12345movl $6, -20(%rbp)leaq -20(%rbp), %raxmovq %rax, -16(%rbp)movq -16(%rbp), %raxmovl $2, (%rax) 看到这里是不是有点意思了，这几行通过引用修改变量值的代码转换成汇编代码以后，居然和之前通过指针修改变量值的汇编代码一模一样。咦？仿佛发现了引用的本质呀！ 常量引用在传统C++中我们知道，引用变量不能引用一个右值，但是常引用可以办到这一点，测试代码如下： 1const int&amp; a = 6; 转换成汇编代码如下： 1234movl $6, %eax //把立即数放到寄存器%eax中movl %eax, -20(%rbp) //把寄存器%eax中的数字6放到内存地址为-20(%rbp)的位置，一个临时变量中leaq -20(%rbp), %rax //把临时变量的内存地址-20(%rbp)放到寄存器%rax中movq %rax, -16(%rbp) //把寄存器%rax中存储的临时变量的内存地址-20(%rbp)放到内存地址为-16(%rbp)的位置 这段代码的翻译结果与前面指针变量的例子很像，首先有一个变量（匿名变量）来存储值，然后是一个新的内存地址来保存之前变量的地址。 右值引用右值引用需要C++11才能使用，与常引用对比的优点就是可以修改右值，实际上我认为还是修改的左值！测试代码如下： 12int&amp;&amp; a = 6;a = 2 转换成汇编代码如下： 123456movl $6, %eax //把立即数放到寄存器%eax中movl %eax, -20(%rbp) //把寄存器%eax中的数字6放到内存地址为-20(%rbp)的位置，一个临时变量中leaq -20(%rbp), %rax //把临时变量的内存地址-20(%rbp)放到寄存器%rax中movq %rax, -16(%rbp) //把寄存器%rax中存储的临时变量的内存地址-20(%rbp)放到内存地址为-16(%rbp)的位置movq -16(%rbp), %rax // 把内存地址为-16(%rbp)的位置（变量p）的数据放到寄存器%rax中movl $2, (%rax) // 把立即数2放在寄存器%rax中保存的地址位置中，也就是p所指向的地址，即变量a中 这段汇编代码与常量引用相比只缺少赋值的部分，与左值引用相比几乎一样，只有在最开始立即数6的处理上有一点点差异，是不是感觉很神奇？ 一点点惊奇对比了前面这些代码的汇编指令后有没有什么想法？什么常量引用，什么右值引用，这些不过都是“愚弄”程序员的把戏，但这些概念的出现并不是为了给程序员们带来麻烦，相反它们的出现使得程序编写更加可控，通过编译器帮助“粗心”的开发者们先暴露了一波问题。 通过汇编代码来看，常量引用其实引用的并非常量，而是引用了一个变量；右值引用引用的也并非右值，同样是一个保存了右值的变量。这年头常量都能变，还有什么不能变的呢？ 来看看下面这段代码，仔细想想常量真的变了吗？运行之后各个变量的值是多少呢？ 123456const int a = 6;int *p = const_cast&lt;int*&gt;(&amp;a);*p = 2;int b = *p;int c = a; 这段代码运行之后的打印结果：a=6, b=2, c=6，变量a作为一个常量没有被改变，貌似常量还是有点用的，哈哈~ 这段代码转换成汇编代码如下： 123456789movl $6, -28(%rbp)leaq -28(%rbp), %raxmovq %rax, -16(%rbp)movq -16(%rbp), %raxmovl $2, (%rax)movq -16(%rbp), %raxmovl (%rax), %eaxmovl %eax, -24(%rbp)movl $6, -20(%rbp) 通过汇编来看你会发现，其实变量a的值已经通过指针 p 修改过了，只不过后面引用a变量的地方，因为它是常量，直接使用立即数6替换了。 改写一下代码，将常量6换成一个变量： 1234567int i = 3;const int a = i;int *p = const_cast&lt;int*&gt;(&amp;a);*p = 2;int b = *p;int c = a; 转换成汇编代码为： 123456789101112movl $3, -28(%rbp)movl -28(%rbp), %eaxmovl %eax, -32(%rbp)leaq -32(%rbp), %raxmovq %rax, -16(%rbp)movq -16(%rbp), %raxmovl $2, (%rax)movq -16(%rbp), %raxmovl (%rax), %eaxmovl %eax, -24(%rbp)movl -32(%rbp), %eaxmovl %eax, -20(%rbp) 这段代码运行的结果为：i=3, a=2, b=2, c=2，看来常量也禁不住我们这么折腾啊 所以从这一点可以看出C++代码中无常量，只要是定义出的变量都可以修改，而常量只是给编译器优化提供一份指导，比如可以把一些字面量在编译期间替换，但是运行时的常量还是能改的。 总结 左值和右值更像是容器与数据的关系，不过C++11提出的将亡值的概念又模糊这两者的界限，将亡值可以看成是即将失去容器的数据 在Ubuntu16.04、GCC5.4.0的环境下，通过左值引用和指针修改一个变量值生成的汇编代码完全一致 C++11中右值引用与常量引用生成的汇编代码一致，与左值引用生成的代码只在初始化时有一点差异 常量并非不可修改，它只是一种“君子协定”，你要知道什么情况下可以改，什么情况下绝对不可以改 const_cast 目的并不是让你去修改一个本身被定义为const的值，这样修改后果是可能是无法预期的，它存在的目的是调整一些指针、引用的权限，比如在函数传递参数的时候 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 身上若无千斤担，谁拿生命赌明天~世间唯一不变的就是变化 2021-7-5 00:36:29]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>左值</tag>
        <tag>右值</tag>
        <tag>右值引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11中std::move和std::forward到底干了啥]]></title>
    <url>%2Fblog%2F2021%2F07%2F04%2FC-11%E4%B8%ADstd-move%E5%92%8Cstd-forward%E5%88%B0%E5%BA%95%E5%B9%B2%E4%BA%86%E5%95%A5%2F</url>
    <content type="text"><![CDATA[前言C++11中的右值引用的出现，在特定情况下减少了对象的拷贝，提升了C++程序的效率，伴随而来的 std::move 和 std::forward 也大量出现在程序代码中，但是这两个函数究竟干了啥呢？其实他们的本质都是转换函数，也就是完成左值和右值之间的转换，需要注意的是左值可以转换成右值，但是右值无法转换成左值。 关于左值、右值、左值引用和右值引用的概念可以看看之前的总结： 简单聊聊C/C++中的左值和右值 C++11在左值引用的基础上增加右值引用 虽然温故不一定知新，但绝对可以增强记忆，本章的内容说起来很绕，我也是边学边总结，有不对的地方还请大佬们指出来。 左值引用和右值引用了解过基础的引用知识之后我们都知道左值引用的形式为 T&amp; t，一般会像成下面这样使用： 123456789101112class A&#123;private: int n;&#125;;void test(A&amp; obj) &#123; //...&#125;A obj;test(obj); 而右值引用是在左值引用的基础上多加一个&amp;，形式变为 T&amp;&amp; t，使用方式如下： 1234567891011class A&#123;private: int n;&#125;;void test(A&amp;&amp; obj) &#123; //...&#125;test(A()); 这种通过 &amp; 的个数区分左值引用和右值引用的方法，在大多数的普通函数中没有问题，但是放到模板参数或者 auto 关键字之后的位置就不太灵了，因为这些地方会推导实际的类型，正是有了参数推导，才使得模板中出现了“万能引用”的说法，也就是下面这样： 12345678910111213141516#include &lt;iostream&gt;using namespace std;template&lt;typename T&gt;void func(T&amp;&amp; val)&#123; cout &lt;&lt; val &lt;&lt; endl;&#125;int main()&#123; int year = 2020; func(year); func(2020); return 0;&#125; 函数 func 即能接受变量 year 这样的左值作为参数，也能接受 2020 这样的常数作为右值，简直太完美。那么这里是怎样推导的呢？这就要请出一个引用的“折叠”规则了，描述如下： A&amp; &amp; 折叠成 A&amp; A&amp; &amp;&amp; 折叠成 A&amp; A&amp;&amp; &amp; 折叠成 A&amp; A&amp;&amp; &amp;&amp; 折叠成 A&amp;&amp; 根据这个规则，func 函数在接受 year 作为参数时应该是一个左值引用，那么模板参数 T 会被推到为 A&amp; 与后面的 &amp;&amp; 折叠为 A&amp;，接受 year 没问题。而这个函数在接受 2020 作为参数时应该是一个右值引用，那么模板参数 T 会被推导成 A，与后面的 &amp;&amp; 形成 A&amp;&amp;，可以接受右值，知道了这些基础知识我们接着往后看。 std::move这个函数听起来好像是一个小人移动时调用的函数，但它却是一个把左值转化成右值的转化函数，我们看一下 std::move 函数的实现： 123456789/** * @brief Convert a value to an rvalue. * @param __t A thing of arbitrary type. * @return The parameter cast to an rvalue-reference to allow moving it.*/template&lt;typename _Tp&gt; constexpr typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; move(_Tp&amp;&amp; __t) noexcept &#123; return static_cast&lt;typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp;&gt;(__t); &#125; 这是一个模板函数，一共才4行，好像最麻烦的就是这个 std::remove_reference&lt;_Tp&gt;::type&amp;&amp; 了，先来看看它是什么，其实它的作用就是，移除类型的引用，返回原始类型。 std::remove_reference它的可能实现如下： 1234567891011121314template &lt;typename T&gt;struct remove_reference &#123; using type = T;&#125;;template &lt;typename T&gt; // 模板特化struct remove_reference&lt;T&amp;&gt; &#123; using type = T;&#125;;template &lt;typename T&gt; // 模板特化struct remove_reference&lt;T&amp;&amp;&gt; &#123; using type = T;&#125;; 它的作用可以参考 cppreference.com - remove_reference，示例如下： 12345678910111213141516171819#include &lt;iostream&gt; // std::cout#include &lt;type_traits&gt; // std::is_sametemplate&lt;class T1, class T2&gt;void print_is_same() &#123; std::cout &lt;&lt; std::is_same&lt;T1, T2&gt;() &lt;&lt; '\n';&#125;int main() &#123; std::cout &lt;&lt; std::boolalpha; print_is_same&lt;int, int&gt;(); print_is_same&lt;int, int &amp;&gt;(); print_is_same&lt;int, int &amp;&amp;&gt;(); print_is_same&lt;int, std::remove_reference&lt;int&gt;::type&gt;(); print_is_same&lt;int, std::remove_reference&lt;int &amp;&gt;::type&gt;(); print_is_same&lt;int, std::remove_reference&lt;int &amp;&amp;&gt;::type&gt;();&#125; 运行结果 123456truefalsefalsetruetruetrue 从这个例子可以清晰的看出 std::remove_reference 就是返回去掉引用的原始类型。 static_cast明白了上面 std::remove_reference 的作用，整个 std::move 函数就剩下一个 static_cast 函数了，其实到这里也就清晰了，std::move 函数的作用就先通过 std::remove_reference 函数得到传入参数的原始类型 X，然后再把参数强转成 X&amp;&amp; 返回即可，参数的 _Tp 的推导参考引用折叠规则。 std::move 到底干了啥通过前面的一通分析我们发现，std::move 的内部只做了一个强制类型转换，除此之外啥也没干，其实就是对传入的参数重新解释了一下，并没有实质性的动作。 那么为什么要使用 std::move 这个名字呢？这个名字更多的是起到提醒的作用，告诉使用者这里可能进行了到右值的转化，相关的对象后续可能发生移动，“被掏空”了，如果你继续使用这个对象，行为是未定义的，后果自负。 std::forwardstd::forward 被称为完美转发，听起来和 “万能引用”一样厉害，使用的头文件为 &lt;utility&gt;，在 /usr/include/c++/5/bits/move.h 文件中的定义如下： 12345678910111213141516171819202122232425/** * @brief Forward an lvalue. * @return The parameter cast to the specified type. * * This function is used to implement "perfect forwarding". */template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp; __t) noexcept &#123; return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125;/** * @brief Forward an rvalue. * @return The parameter cast to the specified type. * * This function is used to implement "perfect forwarding". */template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept &#123; static_assert(!std::is_lvalue_reference&lt;_Tp&gt;::value, "template argument" " substituting _Tp is an lvalue reference type"); return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125; std::forward 用于函数模板中完成参数转发任务，我们必须在相应实参为左值，该形参成为左值引用时把它转发成左值，在相应实参为右值，该形参成为右值引用时把它转发成右值。 有了前面的铺垫我们直接来分析代码吧，第一个版本接受参数苏为左值引用的情况，因为 std::remove_reference&lt;_Tp&gt;::type 是 _Tp 的原始类型，所以 t 就是左值引用类型，调用这个函数时，_Tp 为 X&amp; 类型，经过引用这的 _Tp&amp;&amp; =&gt; X&amp; &amp;&amp; =&gt; X&amp;，所以返回值也是左值引用。 同理，第二个版本接受右值引用参数，返回值也是一个右值引用。 从目前的情况来看，std::forward 好像什么也没做，只是将参数强转以后返回，如果不使用这个函数会有什么问题呢？ 必要性为什么要使用 std::forward 我们可以通过一个例子来看一下： 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;utility&gt;void Print(int&amp; val) &#123; std::cout &lt;&lt; "lvalue refrence: val=" &lt;&lt; val &lt;&lt; std::endl;&#125;void Print(int&amp;&amp; val) &#123; std::cout &lt;&lt; "rvalue refrence: val=" &lt;&lt; val &lt;&lt; std::endl;&#125;template&lt;typename T&gt;void TPrint(T &amp;&amp;t) &#123; return Print(t);&#125;int main() &#123; int date = 1021; TPrint(date); TPrint(501); return 0;&#125; 看到这个例子可以先思考一下，运行结果会是什么呢？可能和你想的有点不一样哦，看看下面的答案： 12lvalue refrence: val=1021lvalue refrence: val=501 有点出乎意料啊，为什么 Print(int&amp;&amp; val) 这个函数没有被调用呢？原因在于“右值引用是一个左值”，很懵对不对，接着往下看： 1234int i = 101;int&amp; li = i;int&amp;&amp; ri = 120; 这段代码中哪些是左值，哪些是右值呢？可以肯定的是 i、li 是左值， 101、120 是右值，而ri也是左值，因为它也一个可以取地址并长期有效的变量啊，只不过这个左值引用了一个右值而已。 接着回到刚才的例子，TPrint(501); 调用模板函数时，T被推导为 int，所以模板被实例化为： 123void TPrint(int&amp;&amp; t) &#123; return Print(t);&#125; 运行到这里，t 实际上是一个左值，所以调用了 void Print(int&amp; val) 这个函数，那么怎样才能调用 void Print(int&amp;&amp; val) 这个版本呢？是时候请出 std::forward 函数了，将模板函数进行如下修改： 1234template&lt;typename T&gt;void TPrint(T&amp;&amp; t) &#123; return Print(std::forward&lt;T&gt;(t));&#125; 修改之后再来分析一下，TPrint(501); 调用模板函数时，T被推导为 int，所以模板被实例化为： 123void TPrint(int&amp;&amp; t) &#123; return Print(std::forward&lt;int&gt;(t));&#125; 这里会调用 std::forward 的这个版本： 12345678template&lt;typename _Tp&gt; constexpr _Tp&amp;&amp; forward(typename std::remove_reference&lt;_Tp&gt;::type&amp;&amp; __t) noexcept &#123; static_assert(!std::is_lvalue_reference&lt;_Tp&gt;::value, "template argument" " substituting _Tp is an lvalue reference type"); return static_cast&lt;_Tp&amp;&amp;&gt;(__t); &#125; 函数的返回类型为 int&amp;&amp;，然后就调用了 void Print(int&amp;&amp; val) 这个版本的打印函数。 疑惑可能有人会说，这不对啊，使用 std::forward 修改之前函数参数就是 int&amp;&amp; 类型，修改之后得到的返回值还是 int&amp;&amp; 类型，这有什么区别吗？ 这里的区别就在于，使用 std::forward 之前的 int&amp;&amp; 是有名字的变量 t，它是一个左值，而使用 std::forward 之后的 int&amp;&amp; 是有个匿名变量，它是一个右值，真正的差距就在这里。 std::forward 到底干了啥它和 std::move 一样，std::forward 也是做了一个强制类型转换，当形参成为左值引用时把它转换成左值引用返回，当形参成为右值引用时把它转换成右值引用返回。 总结 std::move 并没有实际的“移动”操作，只是在内部进行了强制类型转换，返回一个相关类型的右值引用 std::move 的名字主要标识它后续可能会被其他人“掏空”，调用它之后如果继续使用，行为未定义，后果自负 std::forward 的本质也是进行强制类型转换，形参为左值时返回左值引用，形参为右值时返回右值引用 从定义入手可以理解很多花里胡哨的东西，透过现象看其本质。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 日拱一卒无有尽，功不唐捐终入海~ 我们追求的样子：十分沉静，九分气质，八分资产，七分现实，三分颜值，二分糊涂，一份自知之明。 2021-7-18 21:23:01]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>move</tag>
        <tag>forward</tag>
        <tag>完美转发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手把手搭建一个redis集群]]></title>
    <url>%2Fblog%2F2021%2F06%2F26%2F%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AAredis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[前言所谓“难者不会，会者不难”，这不只是一句简简单单的俗语，而是历经人类进化史而得到的历史经验，只有亲身体会过才会越发了解这句话的意义。当你苦思冥想几天几夜难以解决的问题，一句过来人的点拨便可瞬间化解，这是人生路上的需要经历的，愿每个人在关键时刻都能得到善良人的点拨。 人生有四个境界：①不知道自己不知道；②知道自己不知道；③不知道自己知道，④知道自己知道。不同的境界对应着不同的人生，仔细想想这四种境界并没有哪一种是绝对的好与坏，具体怎么选择取决于你想要什么样的人生，想想坐井观天的那只青蛙，属于“不知道自己不知道”的典型，如果它每天逍遥快活，完全没必要非得跳出井来卷入井外的纷争。 长久以来自给自足的小农经济，如果没有战乱，没有暴政，没有入侵，耕田织布的生活不也挺好，但是人性是贪婪的，欲望促使人们侵略、扩张，使得一些人进入了“知道自己不知道”的境界，于是加速了人与人之间的交流。进入现代社会以后，信息爆炸，越来越多的人“觉醒”了，但只是“觉醒”而不付出努力就会使自己陷入焦虑，其实焦虑不可怕，重要的是你要采取什么措施来缓解焦虑。 题外话扯多了，回归正题，写这篇总结的主要还是一个学习记录，也希望这样的记录可以解决一些人的困惑，这些东西并不复杂，也许只是我这个初学者才会关心的问题，但是现在有一种现象，就是大佬们不愿写基础的东西，而小白们想成为大佬从基础学习，却发现路上的坑让他们头破血流。一句学神们口中的“不难证明”，可能你几个月都证明不出来，这个问题是双方面的，一方面小白认为这东西很难，不知道怎么入手，另一方面大神感觉这东西这么简单还需要学？ 屠龙勇士中成恶龙，大神们经过时间的洗礼，很多人已经忘记了自己曾经的头破血流，忘记了自己怎么从小白一步步走来，所以遇到困惑的人不要先入为主，不要认为这个东西太简单了怎么可能有人不会？耐心聆听给出建议，或许可以瞬间打通他的任督二脉。 好了，好了，又跑题了，拉回来，我们从头来搭建一个redis集群，不过搭建之前我们先来看看“安装”这个词。 安装的含义安装软件、安装程序、安装APP，自从电子设备飞入寻常百姓家，我们每天都在安装、安装、安装。但是这个词并不是电子设备出现后才诞生的。这个词自古就有，安装——本意是：把机件、器材等固定在一定的位置。那么软件、APP的安装就是把它们放在一定的位置。 软件是什么？至今我都记得初中的一道计算机的题目，我是04年在学校机房第一次接触到真的计算机，当时有一道题是问：Windows是系统软件，还是应用软件？当时这种题目真的是死记硬背啊，脑子里完全没有概念，每周只有一节课，还不一定每个人都有机器，那时的软件对我来说就是一个迷。 现在接触计算机也这么多年了，对软件也有了一些认识。软件可以说是用来向发出计算机发出任务的东西，当我们对一个人发号任务指令时，能通过声音传递给人，然后接到指令的人可以完成相应的任务，当我们给计算机发送指令时，可以通过键盘、鼠标等把指令输进去，也可以运行软件来完成某些计算机任务，其实软件就是一些提前编好的任务指令的集合。 那么安装软件就是把软件放到对应的地方，为什么要安装？为什么要放到指定的地方？随便放有什么问题？其实还是为了方便。就像生活中我们把床安装在卧室，把马桶安装在厕所一样，大家都是这样，已经成了习惯。如果你把床安装在厕所，并不是不能用来睡觉，只是其他人在来到厕所睡觉这件事上有些不太习惯。 因为软件并不只是一个东西，他还有数据啊、配置啊、服务啊等等，所以这些东西都要放在相应的位置，并且做一个登记，就像你把床放到厕所一样，这种不一样的习惯更要登记下来，否则对于一个普通人来说，肯定会去卧室找床，而不是厕所。 windows中软件的傻瓜式安装降低了软件的安装难度，它有一个注册表，里面记录了很多奇奇怪怪的东西，在windows上安装软件也就是把软件的各部分发到对应的位置，然后通过注册表这个总管登记入册，但是有些软件不太友好，安装的时候一顿乱放，卸载的时候不自己清理干净，导致出现很多问题，比如当年的SQLServer和3DMax，当时为了再次安装不得不重做了好多次系统。 在linux中安装软件就没有注册表这个东西，只要把软件各部分按照类型分别放好就行，比如可执行程序放/usr/bin/，库文件放/usr/lib，配置文件放 /etc/等等，卸载就是把这些新加的东西删除就可以了，相比windows的注册表要简单一些。 好了，接下来我们就来安装配置一下redis-cluster，看了一下，本地机器真的很干净，什么都没有，现在从安装redis开始吧。 安装redis 系统是ubuntu16.04，偷个懒，直接通过命令 sudo apt-get install redis-server 安装吧，当然你也可以通过源码编译安装，这都是可以的，输出信息如下： 12345678910111213141516171819202122232425262728293031323334353637383940albert@home-pc:/usr/src$$ sudo apt-get install redis-serverReading package lists... DoneBuilding dependency treeReading state information... DoneThe following additional packages will be installed: libjemalloc1 redis-toolsSuggested packages: ruby-redisThe following NEW packages will be installed: libjemalloc1 redis-server redis-tools0 upgraded, 3 newly installed, 0 to remove and 125 not upgraded.Need to get 519 kB of archives.After this operation, 1,507 kB of additional disk space will be used.Do you want to continue? [Y/n] YGet:1 http://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial/universe amd64 libjemalloc1 amd64 3.6.0-9ubuntu1 [78.9 kB]Get:2 http://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial-updates/universe amd64 redis-tools amd64 2:3.0.6-1ubuntu0.4 [95.5 kB]Get:3 http://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial-updates/universe amd64 redis-server amd64 2:3.0.6-1ubuntu0.4 [344 kB]Fetched 519 kB in 13s (38.0 kB/s)Selecting previously unselected package libjemalloc1.(Reading database ... 33895 files and directories currently installed.)Preparing to unpack .../libjemalloc1_3.6.0-9ubuntu1_amd64.deb ...Unpacking libjemalloc1 (3.6.0-9ubuntu1) ...Selecting previously unselected package redis-tools.Preparing to unpack .../redis-tools_2%3a3.0.6-1ubuntu0.4_amd64.deb ...Unpacking redis-tools (2:3.0.6-1ubuntu0.4) ...Selecting previously unselected package redis-server.Preparing to unpack .../redis-server_2%3a3.0.6-1ubuntu0.4_amd64.deb ...Unpacking redis-server (2:3.0.6-1ubuntu0.4) ...Processing triggers for libc-bin (2.23-0ubuntu11) ...Processing triggers for man-db (2.7.5-1) ...Processing triggers for ureadahead (0.100.0-19.1) ...Processing triggers for systemd (229-4ubuntu21.27) ...Setting up libjemalloc1 (3.6.0-9ubuntu1) ...Setting up redis-tools (2:3.0.6-1ubuntu0.4) ...Setting up redis-server (2:3.0.6-1ubuntu0.4) ...invoke-rc.d: could not determine current runlevelProcessing triggers for libc-bin (2.23-0ubuntu11) ...Processing triggers for ureadahead (0.100.0-19.1) ...Processing triggers for systemd (229-4ubuntu21.27) ...albert@home-pc:/usr/src$ 安装完查找一下命令安装的位置，redis服务器和客户端应该都在 /usr/bin/ 1234albert@home-pc:/usr/src$ which redis-server/usr/bin/redis-serveralbert@home-pc:/usr/src$ which redis-cli/usr/bin/redis-cli 修改配置文件，设置密码，通过 sudo vim /etc/redis/redis.conf 命令打开文件搜索 requirepass 找到下面这一行，把前面的#去掉，requirepass 后面的就是密码，可以自己修改。 1234567# Warning: since Redis is pretty fast an outside user can try up to# 150k passwords per second against a good box. This means that you should# use a very strong password otherwise it will be very easy to break.## requirepass foobared #把这一行前面的&apos;#&apos;去掉就行，密码我就不改了# Command renaming. 启动redis，直接运行 sudo redis-server /etc/redis/redis.conf，redis服务器就起来了 123albert@home-pc:/mnt/c/Users/Albert$ sudo redis-server /etc/redis/redis.confalbert@home-pc:/mnt/c/Users/Albert$ ps -ef | grep redisroot 663 1 0 13:52 ? 00:00:00 redis-server 127.0.0.1:6379 启动客户端使用redis，连接默认的6379端口，密码foobared 12345678910albert@home-pc:/mnt/c/Users/Albert$ redis-cli -a foobared -p 6379127.0.0.1:6379&gt; set name redis-testOK127.0.0.1:6379&gt; get name"redis-test"127.0.0.1:6379&gt; set age 18OK127.0.0.1:6379&gt; get age"18"127.0.0.1:6379&gt; 至此redis就可以使用了，测试了set和get两个命令如上 redis编译安装忽然发现Ubuntu16.04自带的 redis3.0.6 版本太低了，无法自动配置集群，所以决定编译安装，如果自动安装完redis版本在5以上，那就不需要编译安装了。 新建/usr/local/redis/目录并进入，然后下载redis源码redis-6.2.4.tar.gz 12345678910111213141516albert@home-pc:/usr/local$ mkdir redisalbert@home-pc:/usr/local$ sudo mkdir redisalbert@home-pc:/usr/local$ cd redis/albert@home-pc:/usr/local/redis$ sudo wget https://download.redis.io/releases/redis-6.2.4.tar.gz--2021-06-26 14:10:42-- https://download.redis.io/releases/redis-6.2.4.tar.gzResolving download.redis.io (download.redis.io)... 45.60.125.1Connecting to download.redis.io (download.redis.io)|45.60.125.1|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 2457940 (2.3M) [application/octet-stream]Saving to: ‘redis-6.2.4.tar.gz’redis-6.2.4.tar.gz 100%[========================================&gt;] 2.34M 45.4KB/s in 54s2021-06-26 14:11:37 (44.3 KB/s) - ‘redis-6.2.4.tar.gz’ saved [2457940/2457940]albert@home-pc:/usr/local/redis$ 运行 sudo tar -zxvf redis-6.2.4.tar.gz 命令解压文件 运行 cd redis-6.2.4 进入 redis-6.2.4 目录 直接运行 sudo make &amp;&amp; make install 即可，如果遇到了 ‘/bin/sh: 1: pkg-config: not found’ 错误，先通过 sudo apt-get install pkg-config 安装 pkg-config，再运行 make 即可。 这次 redis-server 和 redis-cli 都被安装到了 /usr/local/bin/ 目录，版本是 6.2.4，接下来我们就用这个版本来搭建redis集群 redis集群为什么要用redis集群？很直接的原因就是一个redis不够用了，需要使用好几个来组合分摊数据量和压力，那么搭建redis集群其实就是启动多个redis程序实例，让他们配合工作就好了，他们之间的配合是redis本身实现的，我们只需要配置启动多个实例就行了，因为redis集群最少需要三主三从，所以我们启动6个实例，： 新建 /usr/local/redis-cluster 目录并进入 12albert@home-pc:/usr/local$ sudo mkdir redis-clusteralbert@home-pc:/usr/local$ cd redis-cluster/ 为了彼此的配置、数据、日志不相互影响，6个实例需要分别配置，我们建立如下的目录结构，按照端口号配置6个配置文件 12345678910111213141516.├── conf│ ├── redis-6301.conf│ ├── redis-6302.conf│ ├── redis-6303.conf│ ├── redis-6304.conf│ ├── redis-6305.conf│ └── redis-6306.conf├── data│ ├── redis-6301│ ├── redis-6302│ ├── redis-6303│ ├── redis-6304│ ├── redis-6305│ └── redis-6306└── log 修改redis配置文件，需要修改的配置文件中的内容如下，首先把下面几项前面的#去掉，然后按照端口号命名相互影响的目录和文件名，以第一个实例端口6301为例： 12345678910pidfile /var/run/redis/redis-server.pidport 6379logfile /var/log/redis/redis-server.logdir /var/ilib/redisdbfilename dump.rdb#requirepass foobared#cluster-enabled yes#cluster-config-file nodes-6379.conf#cluster-node-timeout 15000 文件 redis-6301.conf 修改后的文件内容如下，接下来5个配置文件分别按照6302, 6303, 6304, 6305, 6306来配置： 12345678910pidfile /var/run/redis/redis-6301-server.pidport 6301logfile /usr/local/redis-cluster/log/redis-6301-server.logdir /usr/local/redis-cluster/data/redis-6301dbfilename dump-6301.rdbrequirepass foobaredcluster-enabled yescluster-config-file nodes-6301.confcluster-node-timeout 15000 进入 /usr/local/redis-cluster 目录并启动实例： 1234567891011121314albert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6301.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6302.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6303.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6304.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6305.confalbert@home-pc:/usr/local/redis-cluster$ sudo /usr/local/bin/redis-server conf/redis-6306.confalbert@home-pc:/usr/local/redis-cluster$ ps -ef | grep redisroot 663 1 0 13:52 ? 00:00:00 redis-server 127.0.0.1:6379root 5928 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6301 [cluster]root 5935 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6302 [cluster]root 5942 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6303 [cluster]root 5949 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6304 [cluster]root 5956 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6305 [cluster]root 5963 1 0 14:57 ? 00:00:00 /usr/local/bin/redis-server 127.0.0.1:6306 [cluster] 此时查看目录已经生成了默认的节点文件： 12345678910111213141516171819202122232425262728293031albert@home-pc:/usr/local/redis-cluster$ tree.├── conf│ ├── redis-6301.conf│ ├── redis-6302.conf│ ├── redis-6303.conf│ ├── redis-6304.conf│ ├── redis-6305.conf│ └── redis-6306.conf├── data│ ├── redis-6301│ │ └── nodes-6301.conf│ ├── redis-6302│ │ └── nodes-6302.conf│ ├── redis-6303│ │ └── nodes-6303.conf│ ├── redis-6304│ │ └── nodes-6304.conf│ ├── redis-6305│ │ └── nodes-6305.conf│ └── redis-6306│ └── nodes-6306.conf└── log ├── redis-6301-server.log ├── redis-6302-server.log ├── redis-6303-server.log ├── redis-6304-server.log ├── redis-6305-server.log └── redis-6306-server.log9 directories, 18 files 虽然实例都启动起来了，但是现在每个实例是单独的，需要把它们连接到一起，运行如下命令 redis-cli -a foobared --cluster create 127.0.0.1:6301 127.0.0.1:6302 127.0.0.1:6303 127.0.0.1:6304 127.0.0.1:6305 127.0.0.1:6306 --cluster-replicas 1 即可，命令中的 --cluster-replicas 1 表示为每个主节点都提供一个从节点，中间还需要输入 yes，这样运行完就形成了一个三主三从的redis集群： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152albert@home-pc:/usr/local/redis-cluster$ redis-cli -a foobared --cluster create 127.0.0.1:6301 127.0.0.1:6302 127.0.0.1:6303 127.0.0.1:6304 127.0.0.1:6305 127.0.0.1:6306 --cluster-replicas 1Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe.&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 127.0.0.1:6305 to 127.0.0.1:6301Adding replica 127.0.0.1:6306 to 127.0.0.1:6302Adding replica 127.0.0.1:6304 to 127.0.0.1:6303&gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity[WARNING] Some slaves are in the same host as their masterM: 5d60bdad15a6f8ceec188a1081e9381f181a5c5e 127.0.0.1:6301 slots:[0-5460] (5461 slots) masterM: 8c814d4a32763d47723398fcf8f596d7b6340afc 127.0.0.1:6302 slots:[5461-10922] (5462 slots) masterM: 7bc53b512772c3a1df3217facca283ff9564d32d 127.0.0.1:6303 slots:[10923-16383] (5461 slots) masterS: b39bdb5b6e720e9fbedd43e58b57661910dcc3d7 127.0.0.1:6304 replicates 5d60bdad15a6f8ceec188a1081e9381f181a5c5eS: 5fef2bedd430bf86cdff63cb2f852aeb21e1b18f 127.0.0.1:6305 replicates 8c814d4a32763d47723398fcf8f596d7b6340afcS: 0123b160087743a5296807145b426d9b9cefcf21 127.0.0.1:6306 replicates 7bc53b512772c3a1df3217facca283ff9564d32dCan I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join.&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6301)M: 5d60bdad15a6f8ceec188a1081e9381f181a5c5e 127.0.0.1:6301 slots:[0-5460] (5461 slots) master 1 additional replica(s)S: b39bdb5b6e720e9fbedd43e58b57661910dcc3d7 127.0.0.1:6304 slots: (0 slots) slave replicates 5d60bdad15a6f8ceec188a1081e9381f181a5c5eM: 7bc53b512772c3a1df3217facca283ff9564d32d 127.0.0.1:6303 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: 5fef2bedd430bf86cdff63cb2f852aeb21e1b18f 127.0.0.1:6305 slots: (0 slots) slave replicates 8c814d4a32763d47723398fcf8f596d7b6340afcM: 8c814d4a32763d47723398fcf8f596d7b6340afc 127.0.0.1:6302 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: 0123b160087743a5296807145b426d9b9cefcf21 127.0.0.1:6306 slots: (0 slots) slave replicates 7bc53b512772c3a1df3217facca283ff9564d32d[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. redis集群的使用当使用redis-cli连接集群的时候，直接连3个主节点的任意一个就可以，把cluster当成一个整理来看待，测试如下： 1234567albert@home-pc:/usr/local/redis-cluster$ redis-cli -a foobared -p 6301Warning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.127.0.0.1:6301&gt; get name(error) MOVED 5798 127.0.0.1:6302127.0.0.1:6301&gt; set name cluster-test(error) MOVED 5798 127.0.0.1:6302127.0.0.1:6301&gt; 我们发现测试Redis命令的时候报错了，显示 (error) MOVED 5798 127.0.0.1:6302，这说明 name 这个键不在端口为 6301 这个实例上，而在端口为 6302 这个实例上，使用 cluster nodes 命令可以查询集群的节点信息 1234567127.0.0.1:6301&gt; cluster nodesb39bdb5b6e720e9fbedd43e58b57661910dcc3d7 127.0.0.1:6304@16304 slave 5d60bdad15a6f8ceec188a1081e9381f181a5c5e 0 1624711999000 1 connected7bc53b512772c3a1df3217facca283ff9564d32d 127.0.0.1:6303@16303 master - 0 1624712000000 3 connected 10923-163835fef2bedd430bf86cdff63cb2f852aeb21e1b18f 127.0.0.1:6305@16305 slave 8c814d4a32763d47723398fcf8f596d7b6340afc 0 1624711998971 2 connected5d60bdad15a6f8ceec188a1081e9381f181a5c5e 127.0.0.1:6301@16301 myself,master - 0 1624712000000 1 connected 0-54608c814d4a32763d47723398fcf8f596d7b6340afc 127.0.0.1:6302@16302 master - 0 1624711999978 2 connected 5461-109220123b160087743a5296807145b426d9b9cefcf21 127.0.0.1:6306@16306 slave 7bc53b512772c3a1df3217facca283ff9564d32d 0 1624712000991 3 connected 这种情况下可以手动连接端口为6302的节点，也可以在连接 6301 时添加一个 -c 参数，这样在当前实例找不到指定的键时会自动切换，下面测试一下： 12345678albert@home-pc:/usr/local/redis-cluster$ redis-cli -a foobared -p 6301 -cWarning: Using a password with &apos;-a&apos; or &apos;-u&apos; option on the command line interface may not be safe.127.0.0.1:6301&gt; set name cluster666-&gt; Redirected to slot [5798] located at 127.0.0.1:6302OK127.0.0.1:6302&gt; get name&quot;cluster666&quot;127.0.0.1:6302&gt; 自动跳转到了端口为6302的实例上，这波操作挺666 C++工程连接使用redis-clusterredis-cli 可以在集群的各个节点之间自动跳转，那是人家的本事，如果是你想自己连接集群，首先看看你使用的是那个库了。 如果使用的语言是C/C++，并且用的是最常见的hiredis，那么很抱歉它本身不提供集群的便利访问，像 MGET，MSET 这种操作多个键的命令得自己花点力气了，不过如果你考虑换一个库那么这些问题就解决了。 hiredis-vip 是第一个选择，这个名字听起来好像充了前才能用，实际上是“唯品会 一家专门做特卖的网站”，它先搞出来开源的，可以把集群看成一个整体，连接任一节点就可以操作，但是这个库有个问题，不支持认证，所以后来人在这个基础上进行了二次开发。hiredis-vip源码 hiredis-cluster 就是这个二次创作的产物，现在算是官方推荐产品了，如果你想方便的操作集群，不妨下载这个库hiredis-cluster源码试试，另外redis-plus-plus也属于官方推荐产品，有时间可以了解一下。 hiredis-cluster今天拿 hiredis-cluster 开刀，看看它怎么用，改天再研究那个 redis-plus-plus。 首先创建一个目录 /usr/local/redis-cluster-cli，进入目录下载源码： 123456789101112131415161718albert@home-pc:/usr/local/redis-cluster$ sudo mkdir -p /usr/local/redis-cluster-clialbert@home-pc:/usr/local/redis-cluster$ cd /usr/local/redis-cluster-clialbert@home-pc:/usr/local/redis-cluster-cli$ sudo wget https://github.com/Nordix/hiredis-cluster/archive/refs/tags/0.6.0.tar.gz--2021-06-26 22:01:15-- https://github.com/Nordix/hiredis-cluster/archive/refs/tags/0.6.0.tar.gzResolving github.com (github.com)... 140.82.114.3Connecting to github.com (github.com)|140.82.114.3|:443... connected.HTTP request sent, awaiting response... 302 FoundLocation: https://codeload.github.com/Nordix/hiredis-cluster/tar.gz/refs/tags/0.6.0 [following]--2021-06-26 22:01:16-- https://codeload.github.com/Nordix/hiredis-cluster/tar.gz/refs/tags/0.6.0Resolving codeload.github.com (codeload.github.com)... 13.229.189.0Connecting to codeload.github.com (codeload.github.com)|13.229.189.0|:443... connected.HTTP request sent, awaiting response... 200 OKLength: unspecified [application/x-gzip]Saving to: ‘0.6.0.tar.gz’0.6.0.tar.gz [ &lt;=&gt; ] 69.58K 19.4KB/s in 3.6s2021-06-26 22:01:21 (19.4 KB/s) - ‘0.6.0.tar.gz’ saved [71249] 解压源码，进入目录hiredis-cluster-0.6.0 1234567891011albert@home-pc:/usr/local/redis-cluster-cli$ sudo tar -zxvf 0.6.0.tar.gzhiredis-cluster-0.6.0/hiredis-cluster-0.6.0/.clang-format......hiredis-cluster-0.6.0/tests/test_utils.hhiredis-cluster-0.6.0/win32.halbert@home-pc:/usr/local/redis-cluster-cli$ ls0.6.0.tar.gz hiredis-cluster-0.6.0albert@home-pc:/usr/local/redis-cluster-cli$ cd hiredis-cluster-0.6.0/albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0$ 按照README文档编译库文件 1234567albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0$ sudo mkdir build &amp;&amp; cd buildalbert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0/build$ sudo cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo -DDISABLE_TESTS=ON ..CMake Error at CMakeLists.txt:1 (cmake_minimum_required): CMake 3.11 or higher is required. You are running version 3.5.1-- Configuring incomplete, errors occurred! oh no！要求cmake版本最低3.11，而我的ubuntu16.04自带的cmake是3.5.1，好吧我先去升个级 123456$ wget http://www.cmake.org/files/v3.11/cmake-3.11.3.tar.gz$ tar xf cmake-3.11.3.tar.gz$ cd cmake-3.11.3$ ./configure$ make$ sudo make install 继续编译 hiredis-cluster 库，先运行cmake生成Makefile文件，然后运行 make 命令开始编译： 1234567891011121314151617181920212223242526272829303132333435363738394041albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0/build$ sudo cmake -DCMAKE_BUILD_TYPE=RelWithDebInfo -DDISABLE_TESTS=ON ..-- The C compiler identification is GNU 5.4.0-- The CXX compiler identification is GNU 5.4.0-- Check for working C compiler: /usr/bin/cc-- Check for working C compiler: /usr/bin/cc -- works-- Detecting C compiler ABI info-- Detecting C compiler ABI info - done-- Detecting C compile features-- Detecting C compile features - done-- Check for working CXX compiler: /usr/bin/c++-- Check for working CXX compiler: /usr/bin/c++ -- works-- Detecting CXX compiler ABI info-- Detecting CXX compiler ABI info - done-- Detecting CXX compile features-- Detecting CXX compile features - doneDetected version: 0.5.0Downloading dependency 'hiredis'.....albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0/build$ sudo makeScanning dependencies of target hiredis_cluster[ 5%] Building C object CMakeFiles/hiredis_cluster.dir/adlist.c.o[ 11%] Building C object CMakeFiles/hiredis_cluster.dir/command.c.o[ 17%] Building C object CMakeFiles/hiredis_cluster.dir/crc16.c.o[ 23%] Building C object CMakeFiles/hiredis_cluster.dir/dict.c.o[ 29%] Building C object CMakeFiles/hiredis_cluster.dir/hiarray.c.o[ 35%] Building C object CMakeFiles/hiredis_cluster.dir/hircluster.c.o[ 41%] Building C object CMakeFiles/hiredis_cluster.dir/hiutil.c.o[ 47%] Linking C shared library libhiredis_cluster.so[ 47%] Built target hiredis_clusterScanning dependencies of target hiredis[ 52%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/alloc.c.o[ 58%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/async.c.o[ 64%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/dict.c.o[ 70%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/hiredis.c.o[ 76%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/net.c.o[ 82%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/read.c.o[ 88%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/sds.c.o[ 94%] Building C object _deps/hiredis-build/CMakeFiles/hiredis.dir/sockcompat.c.o[100%] Linking C shared library libhiredis.so[100%] Built target hiredis 进入 /usr/local/redis-cluster-cli/hiredis-cluster-0.6.0 目录，新建 cluster-cli.cpp 文件，编写测试代码如下： 123456789101112131415161718192021222324252627282930#include "hircluster.h"#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main(int argc, char **argv) &#123; struct timeval timeout = &#123;1, 500000&#125;; // 1.5s redisClusterContext *cc = redisClusterContextInit(); redisClusterSetOptionAddNodes(cc, "127.0.0.1:6301"); redisClusterSetOptionConnectTimeout(cc, timeout); redisClusterSetOptionPassword(cc, "foobared"); redisClusterSetOptionRouteUseSlots(cc); redisClusterConnect2(cc); if (cc &amp;&amp; cc-&gt;err) &#123; printf("Error: %s\n", cc-&gt;errstr); exit(-1); &#125; redisReply *reply = (redisReply *)redisClusterCommand(cc, "SET %s %s", "name", "cluster999"); printf("SET: %s\n", reply-&gt;str); freeReplyObject(reply); redisReply *reply2 = (redisReply *)redisClusterCommand(cc, "GET %s", "name"); printf("GET: %s\n", reply2-&gt;str); freeReplyObject(reply2); redisClusterFree(cc); return 0;&#125; 运行如下命令，编译代码，然后运行，结果和预期一致，虽然连接的是端口为6301的实例，也成功取到了端口为6302实例上的数据： 1234albert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0$ sudo gcc cluster-cli.cpp -I ./build/_deps/ -L ./build/ -Wl,-rpath=./build -lhiredis_cluster -L ./build/_deps/hiredis-build/ -Wl,-rpath=./build/_deps/hiredis-build/ -lhiredis -o cluster-clialbert@home-pc:/usr/local/redis-cluster-cli/hiredis-cluster-0.6.0$ ./cluster-cliSET: OKGET: cluster999 总结 安装软件其实就是把软件的各部分数据放到它该放的地方而已，卸载软件就是把这些数据删掉了 redis也是一款软件，与其他软件并无不同，想要使用多个同时使用，启动多个实例就好了，需要注意配置别冲突 单独运行的redis实例之间不会有沟通，想要这些实例联合起来工作，需要把它们组成一个集群 hiredis-cluster 和 cluster-plus-plus 是使用的C/C++连接redis集的官方推荐库 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 一念动山海，一念山海平~ 2021-6-27 00:35:13]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>集群</tag>
        <tag>实践</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cpplint中filter参数的每个可选项的含义]]></title>
    <url>%2Fblog%2F2021%2F06%2F17%2Fcpplint%E4%B8%ADfilter%E5%8F%82%E6%95%B0%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%8F%AF%E9%80%89%E9%A1%B9%E7%9A%84%E5%90%AB%E4%B9%89%2F</url>
    <content type="text"><![CDATA[前言cpplint 是一款优秀的代码格式检查工具，有了它可以统一整个团队的代码风格，完整的工具就是一个Python脚本，如果安装了Python环境，直接使用 pip install cpplint 命令就可以安装了，非常的方便。 具体的使用方法可以通过 cpplint --help 查询，语法如下： 123456789101112131415Syntax: cpplint.py [--verbose=#] [--output=emacs|eclipse|vs7|junit|sed|gsed] [--filter=-x,+y,...] [--counting=total|toplevel|detailed] [--root=subdir] [--repository=path] [--linelength=digits] [--headers=x,y,...] [--recursive] [--exclude=path] [--extensions=hpp,cpp,...] [--includeorder=default|standardcfirst] [--quiet] [--version] &lt;file&gt; [file] ... Style checker for C/C++ source files. This is a fork of the Google style checker with minor extensions. 其中有一句 [--filter=-x,+y,...] 就是本文总结的重点。 filter是什么这个filter究竟是什么呢？我将它强行解释成代码的“过滤器”，cpplint 是一款检查C++源代码风格的工具，遵循的是Google的编码风格，但是这些规则并不是对于所有人都合适，我们应该有目的进行选择，这个filter参数就是用来屏蔽或者启用一些规则的，我们还是从帮助文档里来看，其中有一段 1234567891011121314filter=-x,+y,... Specify a comma-separated list of category-filters to apply: only error messages whose category names pass the filters will be printed. (Category names are printed with the message and look like "[whitespace/indent]".) Filters are evaluated left to right. "-FOO" means "do not print categories that start with FOO". "+FOO" means "do print categories that start with FOO". Examples: --filter=-whitespace,+whitespace/braces --filter=-whitespace,-runtime/printf,+runtime/printf_format --filter=-,+build/include_what_you_use To see a list of all the categories used in cpplint, pass no arg: --filter= 这一段说明了filter参数的用法，就是以+或者 - 开头接着写规则名，就表示启用或者屏蔽这些规则，使用 --filter= 参数会列举出所有规则，我们来看一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970C:\Users\Albert&gt;cpplint --filter= build/class build/c++11 build/c++14 build/c++tr1 build/deprecated build/endif_comment build/explicit_make_pair build/forward_decl build/header_guard build/include build/include_subdir build/include_alpha build/include_order build/include_what_you_use build/namespaces_headers build/namespaces_literals build/namespaces build/printf_format build/storage_class legal/copyright readability/alt_tokens readability/braces readability/casting readability/check readability/constructors readability/fn_size readability/inheritance readability/multiline_comment readability/multiline_string readability/namespace readability/nolint readability/nul readability/strings readability/todo readability/utf8 runtime/arrays runtime/casting runtime/explicit runtime/int runtime/init runtime/invalid_increment runtime/member_string_references runtime/memset runtime/indentation_namespace runtime/operator runtime/printf runtime/printf_format runtime/references runtime/string runtime/threadsafe_fn runtime/vlog whitespace/blank_line whitespace/braces whitespace/comma whitespace/comments whitespace/empty_conditional_body whitespace/empty_if_body whitespace/empty_loop_body whitespace/end_of_line whitespace/ending_newline whitespace/forcolon whitespace/indent whitespace/line_length whitespace/newline whitespace/operators whitespace/parens whitespace/semicolon whitespace/tab whitespace/todo 这些选项还挺多的，一共有69项，但是现在有一个问题，我就一直没找到这些选项都代表什么含义，有些从名字可以推断出来，比如 whitespace/line_length 应该是指每行的长度限制，但是 whitespace/comma 单单从名字你知道他们是什么意思吗？所以我想简单总结一下。 一个小实验都说cpplint非常好用，那么接下来我们看看这个工具要怎么用，先新建一个文件teststyle，在里面随便写一些C++代码，如下： 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;map&gt;using namespace std;class Style&#123; public: void test() &#123; cout &lt;&lt; "This is style class" &lt;&lt; endl; &#125; void showName(string&amp; extraMsg) &#123; cout &lt;&lt; extraMsg &lt;&lt; className &lt;&lt; endl; &#125;public: string className;&#125;;int main()&#123; Style s; string msg_fjakdjfkadjfkadjffjadfkasdjffajsdfkadvljakdjfakdfjkadfjkasdjfkasdfj="class_name:"; s.showName( msg_fjakdjfkadjfkadjffjadfkasdjffajsdfkadvljakdjfakdfjkadfjkasdjfkasdfj ); if(s.className == "") &#123; cout &lt;&lt; "class name for s is empty." &lt;&lt; endl; &#125; return 0;&#125; 这段临时“发挥”的代码可以正常编译运行，然后用cpplint工具检测一下： 1234567891011121314151617181920&gt; cpplint .\teststyle.cpp.\teststyle.cpp:0: No copyright message found. You should have a line: &quot;Copyright [year] &lt;Copyright Owner&gt;&quot; [legal/copyright] [5].\teststyle.cpp:3: Do not use namespace using-directives. Use using-declarations instead. [build/namespaces] [5].\teststyle.cpp:6: &#123; should almost always be at the end of the previous line [whitespace/braces] [4].\teststyle.cpp:7: public: should be indented +1 space inside class Style [whitespace/indent] [3].\teststyle.cpp:8: Tab found; better to use spaces [whitespace/tab] [1].\teststyle.cpp:12: Is this a non-const reference? If so, make const or use a pointer: string&amp; extraMsg [runtime/references] [2].\teststyle.cpp:13: &#123; should almost always be at the end of the previous line [whitespace/braces] [4].\teststyle.cpp:16: public: should be indented +1 space inside class Style [whitespace/indent] [3].\teststyle.cpp:21: &#123; should almost always be at the end of the previous line [whitespace/braces] [4].\teststyle.cpp:23: Lines should be &lt;= 80 characters long [whitespace/line_length] [2].\teststyle.cpp:23: Missing spaces around = [whitespace/operators] [4].\teststyle.cpp:24: Lines should be &lt;= 80 characters long [whitespace/line_length] [2].\teststyle.cpp:24: Extra space after ( in function call [whitespace/parens] [4].\teststyle.cpp:24: Extra space before ) [whitespace/parens] [2].\teststyle.cpp:26: Missing space before ( in if( [whitespace/parens] [5].\teststyle.cpp:27: &#123; should almost always be at the end of the previous line [whitespace/braces] [4].\teststyle.cpp:32: Could not find a newline character at the end of the file. [whitespace/ending_newline] [5]Done processing .\teststyle.cppTotal errors found: 17 这么一小段代码居然报出了17个错误，厉不厉害？刺不刺激？下面来逐个解释一下： .\teststyle.cpp:0: No copyright message found. You should have a line: “Copyright [year] “ [legal/copyright] [5] [legal/copyright] 表示文件中应该有形如 Copyright [year] &lt;Copyright Owner&gt; 版权信息 \teststyle.cpp:3: Do not use namespace using-directives. Use using-declarations instead. [build/namespaces] [5] [legal/copyright] 表示第3行 using namespace std; 应该使用 using-declarations 而不要使用 using-directives，这个规则可以简单的理解为使用命名空间，每次只引用其中的成员，而不要把整个命名空间都引入。 .\teststyle.cpp:6: { should almost always be at the end of the previous line [whitespace/braces] [4] [whitespace/braces] 表示第6行的大括号应该放在上一行末尾 .\teststyle.cpp:7: public: should be indented +1 space inside class Style [whitespace/indent] [3] [whitespace/indent] 表示第7行 public: 应该在行首只保留一个空格 .\teststyle.cpp:8: Tab found; better to use spaces [whitespace/tab] [1] [whitespace/tab] 表示代码中第8行出现了Tab字符，应该使用空格代替 .\teststyle.cpp:12: Is this a non-const reference? If so, make const or use a pointer: string&amp; extraMsg [runtime/references] [2] [runtime/references] 表示代码第12行建议使用常引用 .\teststyle.cpp:23: Lines should be &lt;= 80 characters long [whitespace/line_length] [2] [whitespace/line_length] 表示代码第23行长度超过了80个字符 .\teststyle.cpp:23: Missing spaces around = [whitespace/operators] [4] [whitespace/operators] 表示代码第23行在赋值符号 = 前后应该有一个空格 .\teststyle.cpp:24: Extra space after ( in function call [whitespace/parens] [4] [whitespace/parens] 表示代码第24行在小括号后面出现了多余的空格 .\teststyle.cpp:26: Missing space before ( in if( [whitespace/parens] [5] [whitespace/parens] 表示代码第26行if后面缺少空格 .\teststyle.cpp:32: Could not find a newline character at the end of the file. [whitespace/ending_newline] [5] [whitespace/ending_newline] 表示32行，文件末尾应该是一个空行 按照上面cpplint提示修改代码如下： 12345678910111213141516171819202122232425262728293031// Copyright [2021] &lt;Copyright albert&gt;#include &lt;iostream&gt;#include &lt;map&gt;using std::cout;using std::endl;using std::string;class Style &#123; public: void test() &#123; cout &lt;&lt; "This is style class" &lt;&lt; endl; &#125; void showName(const string&amp; extraMsg) &#123; cout &lt;&lt; extraMsg &lt;&lt; className &lt;&lt; endl; &#125; public: string className;&#125;;int main() &#123; Style s; string msg = "class_name:"; s.showName(msg); if (s.className == "") &#123; cout &lt;&lt; "class name for s is empty." &lt;&lt; endl; &#125; return 0;&#125; 自己指定筛选规则有些人按照上面默认的规则修改代码之后感觉清爽了不少，而有些人却更加郁闷了，因为这些规则是google内部自己根据需要制定的，并不能满足所有人的需求，所以自己需要有目的的做出选择，比如我就决定项目中不写版权信息，那么再使用cpplint 时可以把检测版权信息的规则过滤掉：cpplint --filter=&quot;-legal/copyright&quot; .\teststyle.cpp。 对照表格总体来说规则还是很多的，想要在一段代码中展示出所有的情况不太容易，所以整理了下面的表格，对一些规则做了解释，因为有些情况我也没有遇到，所以先空着，后面再慢慢补充，这也是做这篇总结的目的，当有一种规则需求时先来查一下，越来越完整。 filter 解释 build/class build/c++11 build/c++14 build/c++tr1 build/deprecated build/endif_comment build/explicit_make_pair build/forward_decl build/header_guard ①头文件需要添加只被包含一次的宏，#ifndef、#define build/include build/include_subdir build/include_alpha build/include_order build/include_what_you_use build/namespaces_headers build/namespaces_literals build/namespaces ①不要引入整个命名空间，仅引入需要使用的成员 build/printf_format build/storage_class legal/copyright ①文件中缺少版权信息 readability/alt_tokens readability/braces ①如果if一个分支包含大括号，那么其他分支也应该包括大括号 readability/casting readability/check readability/constructors readability/fn_size readability/inheritance readability/multiline_comment readability/multiline_string readability/namespace readability/nolint readability/nul readability/strings readability/todo ①TODO注释中应包括用户名 readability/utf8 ①文件应该使用utf8编码 runtime/arrays runtime/casting runtime/explicit runtime/int runtime/init runtime/invalid_increment runtime/member_string_references runtime/memset runtime/indentation_namespace runtime/operator runtime/printf ①使用sprintf替换strcpy、strcat runtime/printf_format runtime/references ①确认是否要使用常引用 runtime/string runtime/threadsafe_fn runtime/vlog whitespace/blank_line whitespace/braces ①左大括号应该放在上一行末尾 whitespace/comma ①逗号后面应该有空格 whitespace/comments ①//后应该紧跟着一个空格 whitespace/empty_conditional_body whitespace/empty_if_body whitespace/empty_loop_body whitespace/end_of_line whitespace/ending_newline ①文件末尾需要空行 whitespace/forcolon whitespace/indent ①public、protected、private前需要1个空格 whitespace/line_length ①代码行长度有限制 whitespace/newline whitespace/operators ①操作符前后需要有空格 whitespace/parens ①if、while、for、switch后的小括号前需要有空格。②小括号中的首个参数前和最后参数尾不应有空格 whitespace/semicolon ①分号后缺少空格，比如{ return 1;} whitespace/tab ①使用空格代替tab whitespace/todo ①TODO注释前空格太多。②TODO注释中用户名后应该有一个空格 总结 cpplint 是一个检查c++代码风格的小工具 cpplint.py 其实是一个Python脚本文件，使用前可以先安装Python环境 使用 cpplint 时默认遵循的是Google的代码风格 为了让代码检测符合自己的习惯，需要使用--filter=参数选项，有多种规则可以选择或者忽略 --filter=中的规则是一个大类，比如 whitespace/parens 既检查小括号前缺少空格的情况，也会检查小括号中多空格的情况 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 生活中会有一些感悟的瞬间，娃娃哭闹时大人们总是按照自己的经验来出处理，碰上倔脾气小孩往往毫无作用。其实孩子是最单纯的，想要什么不想要什么都摆在脸上，愿望一旦被满足立马就不哭了，而大人才世界是难处理的，长大的人类善于隐藏和伪装，想要的不一定说出来，说出来不一定是想要的，所以很多人才会羡慕小孩子的天真和无邪~ 努力吧！哪管什么真理无穷，进一步有进一步的欢喜 2021-6-20 18:46:45]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>Python</tag>
        <tag>cpplint</tag>
        <tag>filter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++对我来说简直就是星辰大海，为了避免翻船，我选择从小河沟出发]]></title>
    <url>%2Fblog%2F2021%2F05%2F24%2FC-%E5%AF%B9%E6%88%91%E6%9D%A5%E8%AF%B4%E7%AE%80%E7%9B%B4%E5%B0%B1%E6%98%AF%E6%98%9F%E8%BE%B0%E5%A4%A7%E6%B5%B7%2F</url>
    <content type="text"><![CDATA[你学的越多，不懂的东西反而越多~ 前言以前觉得 C++ 并没有什么复杂的，不就是 C 语言加上类定义、模板、容器、算法函数这些就可以了吗，只要我不用，它就难不倒我，用到了查查文档也就搞定了，真的是年少轻狂啊。 随着学习的深入渐渐发现，即使抛开那些算法函数、那些冗长的模板，单单是 C++ 核心的概念和类型就够喝上好几壶的，随便罗列几个，像 std::furnitrue、std::memory_order、std::packaged_task 等等这些，之前都没听说过，特别是C++20的协程，到现在还是一头雾水。 C++ 缺少了 C 语言的纯粹，总是喜欢在编译时加点料，但是这个协程加的料超多，一时间还有点接受不了。 不过第一次听说协程这个词是在 Lua 中，全称被叫做协同程序，记得没错是在 《Lua程序设计》这本书中看到的，里面专门有一章是讲coroutine的，并且在 Lua 中定义和使用协程很方便，所以决定先复习一下 Lua 中的协程，然后对比着 C++的协程来进行拓展学习。 进程 vs 线程 vs 协程这三者常常被拿来比较，而引入多进程、多线程、多协程有一个简单而纯粹的目的，那就是榨干CPU，不过这三者侧重还有所不同。 进程是资源分配最小单位，每个进程都有独立的地址空间，来维护代码段、堆栈段和数据段，但是创建和切换进程的开销较大，可以在多台物理机和多核CPU上提高效率，依靠管道（pipe）、命名管道（named pipe/FIFO）、信号量（semophore）、消息队列（message queue）、信号（sinal）、共享内存（shared memory）、套接字（socket）、全双工管道等途径来进行通信。 线程是任务调度和执行的最小单位，没有独立的地址空间，但有独立的运行栈和程序计数器（PC），创建和切换线程的开销相比进程来说要小得多，线程之间通信更加方便，除了可以使用进程间通信的方式，还可以简单地通过共享全局变量，静态变量等进行通信，但是需要锁机制、信号量机制、信号机制来控制线程间互斥。 协程这个概念就比较迷了，其实它不像多进程和多线程那样可以在多核机器上提供并行的能力，而是侧重于相互协作共同完成某个任务，同一个线程中可以启动多个协程，但这些协程同一时刻只能有一个在运行。 协程其实可以看成是一个可以被随时停止和唤醒的函数，使用协程是为了在用户层面来控制调用逻辑，对比于多线程程序的线程调度完全看操作系统的心情的处境，多协程的程序就比较自主了，可以由开发者来控制函数执行顺序。 还有一个特性很重要，就是使用协程可以实现用“同步”的方式来写“异步”的代码，这一点不理解没关系，以后会慢慢明白的。说到这，不得不说一下关于同步和异步、阻塞和非阻塞这几个概念，它们常常被大家混在一起来说，实际上只是从不同维度来描述了一件事情，下面简单叙述下。 同步 vs 异步同步和异步指的是消息通信的机制，或者说得到结果的方式。 同步：调用函数后就能返回想要的结果，有点像去食堂买饭，自己去食堂付完钱（调用），饭（结果）就可以被拿回来了，这就是同步调用的方式，与返回结果的时间长短无关，得到结果之后直接执行后面的逻辑（吃饭）就可以了，所以同步的逻辑是最好写的。 异步：调用函数后并不能直接得到想要的结果，需要通过回调或者其他消息来通知，这就有点像定外卖了，打开APP选好饭菜输入地址（注册回调），开始付钱（调用），此时并不能直接得到饭（结果），而是一段时间之后，有外卖小哥将饭（结果）给你送来，这时才能执行后面的逻辑（吃饭）。 总结来说，需要自己取结果的就是同步，依靠别人送结果的就是异步。 阻塞 vs 非阻塞阻塞和非阻塞指的是程序在等待调用结果时的状态，强调在获得结果之前的表现。 阻塞：调用函数后由于不满足某种条件（比如读socket但是没有数据）被挂起，当条件满足（socket来数据了）时被唤醒，并将结果返回。 非阻塞：调用函数后如果不满足指定条件（比如读socket但是没有数据）不挂起，而是返回一个表示没有取到结果的值，你可以按照某种间隔再次调用函数，直到取到结果为止，当然你也可以调用一次就结束了。 总结来说，不满足条件时调用方被挂起就是阻塞调用，否则就是非阻塞调用。 协程学习C++的协程是暂时学不明白了，为了不翻车，我还是从熟悉的 Lua 入手，来举例说明什么是协程？有什么用？为什么这样用？弄明白以后再慢慢用 C++ 来实现相同的目的，毕竟 C++ 这一块需要实现的内容也有点多。 消费者-生产者提到 Lua 的协程就会想到 “消费者-生产者”的例子，网上关于这个的实现有特别多的版本，整体上来说大同小异，基本上都是 《C++程序设计》这本书中的内容，但是这一部分我看了很多遍，感觉这个例子并不太好。 123456789101112131415161718192021222324252627282930313233343536373839404142function receive(prod) -- 激活协同程序 local status,value = coroutine.resume(prod) return valueendfunction send(x) -- 挂起协同程序 coroutine.yield(x)endfunction producer() -- 生产者 return coroutine.create( -- 创建协同程序 function() while true do local x = io.read() -- 产生新值 send(x) end end )endfunction filter(prod) -- 过滤器 return coroutine.create( -- 创建协同程序 function() for line = 1, math.huge do local x = receive(prod) -- 激活协同程序来获取新值 x = string.format("%5d %s",line , x ) -- 过滤规则 send(x) -- 挂起激活程序 end end )endfunction consumer(prod) while true do local x = receive(prod) -- 获取新值 io.write(x, "\n") -- 消费新值 endendp= producer() -- 初始化生产者f = filter(p) -- 初始化过滤器consumer(f) -- 初始化消费者并启动程序 这就是一个消费者驱动的模型，首先由启动消费者，然后调用生产者来生产资源，接着消费者消耗掉新的资源，再控制生产者生产新的资源，以此方式循环往复，其实就是下面代码的复杂化： 12345678function consumer_producer() while true do local x = io.read() -- 产生新值 io.write(x, "\n") -- 消费新值 endendconsumer_producer() -- 启动生产者消费者 这个例子以我现在的菜鸟水平来看没啥用，但是有一点比较好，就是展示了可以用协程来控制程序执行顺序的强大功能，只是这个消费者和生产者强耦合的设计实在是看不明白。 自己想个例子既然他们的例子我都不喜欢，那我就自己想一个，叮铃铃！下面我收到了一个新的需求： 计算1+2+3+4+5+6+7+8+9+10的和，然后等待5秒钟后，将结果显示在控制台上。 乍一听，这个需求太简单了吧，没有一点难度，其实不然，其中蕴含着大量玄机，简直就是一个万能句式： 做一件事情A，然后等待某件事发生，再做一件事情B（可能与A相关） 仔细想想，这样的“句式”在开发中，生活中是不是经常出现？ 下载电影，下载完成后，播放电影 开始加载场景，加载完成后，隐藏加载进度条 发送一个请求，收到回复时，将回复结果显示出来 … 看了吧，现实中有很多这类需求，我们接下来尝试着实现一下 常规写法123456789101112131415161718192021222324252627-- lua 没有 sleep 函数，使用while循环模拟function sleep(n) local t = os.clock() while os.clock() - t &lt;= n do endendfunction task_method_1() print(string.format("program start at %s", os.date("%H:%M:%S"))) -- 求和 local sum = 0; for i=1,10 do sum = sum + i; end -- 等待 sleep(5); -- 展示 print(string.format("program end at %s and sum = %d", os.date("%H:%M:%S"), sum))endfunction main1() task_method_1()endmain1() 代码很简单，为了看起来更连贯这里就不分段展示了，首先模拟一个 sleep 函数，然后实现 task_method_1 函数来完成原始需求——求和、等待、展示，最后通过主函数来调用就可以了。 运行结果如下: program start at 01:30:27program end at 01:30:32 and sum = 55 进阶写法看了上面的代码有没有发现什么问题？这是一种同步的实现方式，整个程序在中间等待的5秒钟什么都不能做，必须等倒计时结束才能做后面的事情，这要是购物APP点了5秒没反应就直接X掉了，这可是赤果果的金钱损失啊，绝不能让这种事情发生。 怎么办呢？我确实需要5秒钟的处理时间，但是又不能让用户卡在那，我可以显示一个进度条，进度一直再变化，用户就不会以为程序卡死了，如果进度走的比较慢，他可能以为手机老旧该换了，没准还促进了手机的销量呢！ 顺着这个思路写出了下面这种实现，这是一种异步的实现方式，通过回调函数来通知最终要显示的结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445function task_method_2() print(string.format("program start at %s", os.date("%H:%M:%S"))) -- 求和 local sum = 0; for i=1,10 do sum = sum + i; end -- 注册回调函数，进行等待 add_callback(5, call_back_print, sum)endfunction call_back_print(data) --展示结果 print(string.format("program end at %s and sum = %d", os.date("%H:%M:%S"), data))endfunction add_callback(inteval, func, data) interval_time = inteval call_back = func msg_data = dataendfunction main2() local t0 = os.clock(); local t = t0; task_method_2() while true do local now = os.clock() if now - t &gt;= 1 then print(string.format("program run %f seconds", now - t0)) t = now; if interval_time and call_back and now - t0 &gt;= interval_time then call_back(msg_data) break; end end endendmain2() 在函数 task_method_2 中计算完求和的结果，并没有等待，而是通过 add_callback 函数注册了等待时间、回调函数、以及回调展示的结果，然后直接返回了调用方，调用主函数 main2 中计算这时间差并展示进度，等倒计时一结束就执行回调函数，进而展示出结果。 运行结果如下，通过打印信息展示处理进度条: program start at 01:44:56program run 1.000000 secondsprogram run 2.001000 secondsprogram run 3.001000 secondsprogram run 4.001000 secondsprogram run 5.001000 secondsprogram end at 01:45:01 and sum = 55 协程写法卡顿的问题解决了，但是添加了一大堆额外的注册和回调函数，有些麻烦啊，怎么把它们去掉呢？ 终于等到协程出场了，同步调用很卡、异步回调很烦，那么协程可以实现用“同步”的方式来写“异步”的代码，既不卡也不烦，下面来看一下实现。 123456789101112131415161718192021222324252627282930313233343536373839function task_method_3() print(string.format("program start at %s", os.date("%H:%M:%S"))) -- 求和 local sum = 0; for i=1,10 do sum = sum + i; end -- 等待 coroutine.yield(5); -- 展示 print(string.format("program end at %s and sum = %d", os.date("%H:%M:%S"), sum))endfunction main3() local t0 = os.clock(); local t = t0; local co = coroutine.create(task_method_3) local status, interval = coroutine.resume(co) while true do local now = os.clock() if now - t &gt;= 1 then print(string.format("program run %f seconds", now - t0)) t = now; if now - t0 &gt;= interval then coroutine.resume(co) break; end end endendmain3() 对比 task_method_3 和 task_method_1 函数，只是将 sleep 函数换成了 coroutine.yield(5)，整个需求函数很紧凑。 程序运行逻辑是这样的，先将 task_method_3 函数包装成协程 co，然后启动 co 执行求和逻辑，执行到 coroutine.yield(5); 这句，协程被暂停并将5返回，主函数 main3 中收到返回值5后开始计时并展示进度值，直到5秒等待期结束再次唤醒协程 co，coroutine.yield(5); 后面的代码继续执行，完成最后的展示需求。 运行结果如下： program start at 01:50:59program run 1.000000 secondsprogram run 2.000000 secondsprogram run 3.000000 secondsprogram run 4.000000 secondsprogram run 5.000000 secondsprogram end at 01:51:04 and sum = 55 总结 多进程/多线程的引入并不是总能降低任务消耗的时间，还要考虑到进程/线程切换的消耗问题，参考Redis实现 多协程的引入本质上是为了更好的控制程序运行的逻辑，虽然它往往也能带来效率上的提升 coroutine.yield 是协程的中核心函数，主动让出CPU，如果协程不自己挂起，外部无法干预 知识的迁移是一项重要的技能，下一步要用C++协程来实现这个需求啦，边学边写喽 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 拨开那一片云，是你未曾实现的梦想，岁月流转，梦想在变，有些事不得不放弃坚守（固执），珍惜眼前的一切，迎接明天的朝阳~ 2021-5-28 00:27:18]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>recursion</tag>
        <tag>C++20</tag>
        <tag>coruntine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用AddressSanitizer搭配addr2line查找C/C++内存泄漏问题]]></title>
    <url>%2Fblog%2F2021%2F05%2F15%2F%E4%BD%BF%E7%94%A8AddressSanitizer%E6%90%AD%E9%85%8Daddr2line%E6%9F%A5%E6%89%BEC-C-%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言指针是C/C++程序中的利器，同时也引入了风险，现代C++中增加了智能指针来降低使用“裸”指针带来的风险，但是智能指针不是一颗银弹，它不能解决所有的指针问题，内存泄漏在C/C++程序开发中依旧是值得注意的，学会合理、合适的方法来查找内存泄漏问题也是一项有用的技能。 通常内存泄漏问题会在开发到一定程度时集中检查，一些检测方法长时间不去使用难免会忘记，所以本文记录一种自己常用的检测方法，方便日后查阅。 AddressSanitizerAddressSanitizer 是什么东西呢？从名字上直接翻译叫“地址消毒剂”，其实就是用来检查地址问题的。 它是一款地址问题检测工具，简称 ASAN，开源项目主地址为 google/sanitizers，是众多检测工具AddressSanitizer, MemorySanitizer, ThreadSanitizer, LeakSanitizer 中的一款，功能非常强大，可以检测出栈上缓冲区溢出、堆上缓冲区溢出、引用已释放内存、内存泄漏等多种地址问题。 今天想记录的是使用 AddressSanitizer 检测内存泄漏的步骤，其实检测内存泄漏的功能目前已经被基本独立成了 LeakSanitizer，不过仍可以通过在 AddressSanitizer 工具中通过参数来开启和关闭使用。 检测步骤其实使用 ASAN 检测内存泄漏还是比较简单的，g++4.8 以上的版本自带了 ASAN 工具，只要编译时指定好参数，编译完成后正常启动运行程序就可以了，只不过有些情况下只从检测报告中无法准确定位问题，需要借助一些工具进一步缩小检测范围。 泄漏发生在可执行程序本身这种情况检测起来比较容易，编写如下测试代码： 12345678910111213141516//test.cpp#include &lt;iostream&gt;void func()&#123; int* p = new int(); // 内存泄漏的位置 p = nullptr;&#125;int main()&#123; func(); std::cout &lt;&lt; "test leak" &lt;&lt; std::endl; return 0;&#125; 使用g++进行编译，编译时添加参数 -fsanitize=leak 就可以了，启动后可以清晰的展示出内存泄漏的位置 test.cpp:5，也就是 test.cpp 文件的第5行。 123456789101112131415albert@home-pc:/mnt/d/data/cpp/testleak$ g++ test.cpp -g -o test --std=c++11 -fsanitize=leakalbert@home-pc:/mnt/d/data/cpp/testleak$ ./testtest leak===================================================================344==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4 byte(s) in 1 object(s) allocated from: #0 0x7fc7d796d815 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd815) #1 0x400967 in func() /mnt/d/data/cpp/testleak/test.cpp:5 #2 0x400985 in main /mnt/d/data/cpp/testleak/test.cpp:11 #3 0x7fc7d722083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 4 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 这里有个小意外，将 int* p = new int(); 这句代码改成 int* p = new int[10]; 可以检测出内存泄漏如下： 123456789101112131415albert@home-pc:/mnt/d/data/cpp/testleak$ g++ test.cpp -fsanitize=leak -g -o test --std=c++11albert@home-pc:/mnt/d/data/cpp/testleak$ ./testtest leak===================================================================416==ERROR: LeakSanitizer: detected memory leaksDirect leak of 400 byte(s) in 1 object(s) allocated from: #0 0x7fdc0c16d975 in operator new[](unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd975) #1 0x400967 in func() /mnt/d/data/cpp/testleak/test.cpp:5 #2 0x40097f in main /mnt/d/data/cpp/testleak/test.cpp:11 #3 0x7fdc0ba2083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 400 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 但是将 int* p = new int(); 这句代码改成 int* p = new int[1024]; 就无法检测是内存泄漏了，只能修改编译选项为 -fsanitize=address 才能检测出泄漏，目前还不知道真正的原因是什么。 123456789101112131415albert@home-pc:/mnt/d/data/cpp/testleak$ g++ test.cpp -fsanitize=address -g -o test --std=c++11albert@home-pc:/mnt/d/data/cpp/testleak$ ./testtest leak===================================================================432==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4096 byte(s) in 1 object(s) allocated from: #0 0x7f1d42b296b2 in operator new[](unsigned long) (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x996b2) #1 0x400b27 in func() /mnt/d/data/cpp/testleak/test.cpp:5 #2 0x400b3f in main /mnt/d/data/cpp/testleak/test.cpp:11 #3 0x7f1d4235083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: AddressSanitizer: 4096 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 泄漏发生在编译所需动态库中如果内存泄漏发生在编译时使用的动态库中，那么这和上一种情况基本一致，可以直接编译后运行就能发现，测试代码如下 123// myadd.hint add(int a, int b); 123456789// myadd.cpp#include "myadd.h"int add(int a, int b)&#123; int* p = new int(); // 内存泄漏的位置 return a + b;&#125; 12345678910// test.cpp#include "myadd.h"#include &lt;iostream&gt;int main()&#123; std::cout &lt;&lt; "519 + 1 = " &lt;&lt; add(519, 1) &lt;&lt; std::endl; return 0;&#125; 添加编译选项 -fsanitize=leak 编译后运行，也可以直接显示出内存泄漏的位置，内存泄漏在 libmyadd.so 动态库中的 add 函数中。 12345678910111213141516albert@home-pc:/mnt/d/data/cpp/testleak$ g++ -shared -fPIC -o libmyadd.so myadd.cppalbert@home-pc:/mnt/d/data/cpp/testleak$ g++ test3.cpp -L. -lmyadd -o test -Wl,-rpath=. -fsanitize=leakalbert@home-pc:/mnt/d/data/cpp/testleak$ ./test519 + 1 = 520===================================================================493==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4 byte(s) in 1 object(s) allocated from: #0 0x7ff1aff6d815 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd815) #1 0x7ff1afd506b7 in add(int, int) (libmyadd.so+0x6b7) #2 0x4009cd in main (/mnt/d/data/cpp/testleak/test+0x4009cd) #3 0x7ff1af61083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 4 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 泄漏发生在自定义加载的动态库中这种情况要想精确定位问题就麻烦一些了，下面是用来测试的代码 123// myadd.hextern "C" int add(int a, int b); 123456789// myadd.cpp#include "myadd.h"extern "C" int add(int a, int b)&#123; int* p = new int(); // 内存泄漏的位置 return a + b;&#125; 12345678910111213141516171819// test.cpp#include "myadd.h"#include &lt;dlfcn.h&gt;#include &lt;iostream&gt;typedef int (*FUNC)(int a,int b);int main() &#123; void* handle = dlopen("./libmyadd.so", RTLD_LAZY); FUNC myadd = (FUNC)dlsym(handle,"add"); int nVal = 0; std::cin &gt;&gt; nVal; std::cout &lt;&lt; "519 + 1 = " &lt;&lt; myadd(519, 1) &lt;&lt; ", input:" &lt;&lt; nVal &lt;&lt; std::endl; dlclose(handle); return 0;&#125; 添加编译选项 -fsanitize=leak 编译后运行，输入数字618，程序运行结束，显示内存泄漏出现在 0x7fc88f0f06b7 (&lt;unknown module&gt;) 1234567891011121314151617albert@home-pc:/mnt/d/data/cpp/testleak$ g++ -shared -fPIC -o libmyadd.so myadd.cpp -galbert@home-pc:/mnt/d/data/cpp/testleak$ g++ test.cpp -ldl -o test -Wl,-rpath=. -g -fsanitize=leakalbert@home-pc:/mnt/d/data/cpp/testleak$ ./test618519 + 1 = 520, input:618===================================================================817==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4 byte(s) in 1 object(s) allocated from: #0 0x7fc89076d815 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd815) #1 0x7fc88f0f06b7 (&lt;unknown module&gt;) #2 0x400bc2 in main (/mnt/d/data/cpp/testleak/test+0x400bc2) #3 0x7fc88fe1083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 4 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ unknown module当使用 dlopen 的方式加载的动态库时，产生的内存泄漏常显示为 (&lt;unknown module&gt;)，那是因为内存检测工具在程序退出时分析泄漏情况，而这时自定义加载的动态库往往已经手动调用 dlclose 关闭了，这时就会显示成 0x7fc88f0f06b7 (&lt;unknown module&gt;) 的显示。 maps针对于出现 (&lt;unknown module&gt;) 的这种情况，可以通过查询 /proc/pid/maps 来辅助查询，maps 文件显示进程映射后的内存区域和访问权限，是程序正在运行时的信息，数据格式如下： 12345678910111213141516171819207f8c7adb6000-7f8c7adba000 rw-p 00000000 00:00 07f8c7adc0000-7f8c7af32000 r-xp 00000000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7af32000-7f8c7af3f000 ---p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7af3f000-7f8c7b132000 ---p 0017f000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7b132000-7f8c7b13c000 r--p 00172000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7b13c000-7f8c7b13e000 rw-p 0017c000 00:00 189413 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.217f8c7b13e000-7f8c7b142000 rw-p 00000000 00:00 07f8c7b150000-7f8c7b153000 r-xp 00000000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b153000-7f8c7b154000 ---p 00003000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b154000-7f8c7b352000 ---p 00004000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b352000-7f8c7b353000 r--p 00002000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b353000-7f8c7b354000 rw-p 00003000 00:00 243909 /lib/x86_64-linux-gnu/libdl-2.23.so7f8c7b360000-7f8c7b39f000 r-xp 00000000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b39f000-7f8c7b3a2000 ---p 0003f000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b3a2000-7f8c7b59e000 ---p 00042000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b59e000-7f8c7b5a0000 r--p 0003e000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b5a0000-7f8c7b5a1000 rw-p 00040000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.07f8c7b5a1000-7f8c7c1f4000 rw-p 00000000 00:00 07f8c7c200000-7f8c7c225000 r-xp 00000000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so7f8c7c225000-7f8c7c226000 r-xp 00025000 00:00 243945 /lib/x86_64-linux-gnu/ld-2.23.so 第一列：7f8c7b360000-7f8c7b39f000，表示本段内存映射的虚拟地址空间范围。 第二列：r-xp，表示此段虚拟地址空间的属性。r表示可读，w表示可写，x表示可执行，p和s共用一个字段，互斥关系，p表示私有段，s表示共享段，-表示没有权限。 第三列：00000000，表示映射偏移。对有名映射，表示此段虚拟内存起始地址在文件中以页为单位的偏移。对匿名映射，它等于0或者vm_start/PAGE_SIZE。 第四列：00:00，表示映射文件所属设备号。对有名映射来说，是映射的文件所在设备的设备号，对匿名映射来说，因为没有文件在磁盘上，所以没有设备号，始终为00:00。 第五列：247365，表示映射文件所属节点号。对有名映射来说，是映射的文件的节点号。对匿名映射来说，因为没有文件在磁盘上，所以没有节点号，始终为0。第六列：/usr/lib/x86_64-linux-gnu/liblsan.so.0.0.0，表示映射文件名或堆、栈。对匿名映射来说，是此段虚拟内存在进程中的角色。[stack]表示在进程中作为栈使用，[heap]表示堆。对有名来说，是映射的文件名。其余情况则无显示。 7f8c7b360000-7f8c7b39f000 r-xp 00000000 00:00 247365 /usr/lib/x86_64-linux-gnu/liblsan.so.0.0.0 这一行就展示了 liblsan.so 这个动态库映射的内存中位置和权限情况，liblsan.so 也就是 ASAN 工具用来检测内存泄漏的工具所依赖的动态库。 具体操作 启动一个终端，然后运行 test 程序，因为程序中要求从控制台读取一个变量，所以运行后程序会一直停留在控制台等待输入 1albert@home-pc:/mnt/d/data/cpp/testleak$ ./test 重新打开一个终端，查询 test 程序的进程id，然后拷贝对应的 maps 文件 1234albert@home-pc:/mnt/d/data/cpp/testleak$ ps -ef | grep testalbert 986 889 0 21:42 pts/0 00:00:00 ./testalbert 988 953 0 21:42 pts/1 00:00:00 grep --color=auto testalbert@home-pc:/mnt/d/data/cpp/testleak$ cp /proc/986/maps testmaps 在第一个终端中输入数字，程序运行结束，显示出内存泄漏信息 123456789101112131415161718albert@home-pc:/mnt/d/data/cpp/testleak$ ./test# 以下为新的信息，输入了515515519 + 1 = 520, input:515===================================================================986==ERROR: LeakSanitizer: detected memory leaksDirect leak of 4 byte(s) in 1 object(s) allocated from: #0 0x7f8c7b36d815 in operator new(unsigned long) (/usr/lib/x86_64-linux-gnu/liblsan.so.0+0xd815) #1 0x7f8c79cf06b7 (&lt;unknown module&gt;) #2 0x400bc2 in main (/mnt/d/data/cpp/testleak/test+0x400bc2) #3 0x7f8c7aa1083f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2083f)SUMMARY: LeakSanitizer: 4 byte(s) leaked in 1 allocation(s).albert@home-pc:/mnt/d/data/cpp/testleak$ 从检测报告中看到 0x7f8c79cf06b7 (&lt;unknown module&gt;)，在备份的 testmaps 文件中查找范围，发现处于下面一段之中 1234567640000000000-640000003000 rw-p 00000000 00:00 07f8c79cf0000-7f8c79cf1000 r-xp 00000000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79cf1000-7f8c79cf2000 ---p 00001000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79cf2000-7f8c79ef0000 ---p 00002000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79ef0000-7f8c79ef1000 r--p 00000000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79ef1000-7f8c79ef2000 rw-p 00001000 00:00 282 /mnt/d/data/cpp/testleak/libmyadd.so7f8c79f00000-7f8c7a000000 rw-p 00000000 00:00 0 至此发现问题出现在 libmyadd.so 这个动态库中，再用 0x7f8c79cf06b7 减去动态链接库基地址 7f8c79cf0000，得到偏移量为 0x6b7，此时使用 addr2line 工具进行转化。 123albert@home-pc:/mnt/d/data/cpp/testleak$ addr2line -C -f -e /mnt/d/data/cpp/testleak/libmyadd.so 0x6b7add/mnt/d/data/cpp/testleak/myadd.cpp:4 至此就找到了内存泄漏的确切位置，在/mnt/d/data/cpp/testleak/myadd.cpp文件第4行的 add 函数之中。 总结 在 C++11 之后尽可能使用智能指针来管理在堆上申请的内存，shared_ptr、weak_ptr、unique_ptr 能帮我们减少许多麻烦 想要检测程序内存用用问题， AddressSanitizer 是一个不错的选择，其中有关内存泄漏的检测已经被整合到 LeakSanitizer 工具中 当程序中的内存泄漏发生在 dlopen 加载的动态库中时，常常出现 (&lt;unknown module&gt;) 的情况，这时需要借助 proc/pid/maps 文件和 addr2line 工具来完成精确定位。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 祝融落地。一百年了，还没有什么事情是做不到的，我们需要的是时间，我等着看你们在真正的力量面前瑟瑟发抖~ 2021-5-16 00:08:24]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>AddressSanitizer</tag>
        <tag>addr2line</tag>
        <tag>内存泄漏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux文件权限简单备忘知识点]]></title>
    <url>%2Fblog%2F2021%2F04%2F30%2Flinux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E7%AE%80%E5%8D%95%E5%A4%87%E5%BF%98%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[前言已经拖更好几天了，有些坚持的事情一旦中断便很难再捡起来了，所以还是尽可能的坚持下去，今天就简单记录一下文件权限相关的操作。 这里先扔出一张图，这么懒的我居然画了一张图！不过前几天感冒状态不太好，这张图画了好几晚上才完成，就凑活看吧。 基本上看注释就能明白各列的含义，关于“文件硬链接数”这一列还有点疑问，在CentOS上测试，确实如注释所言，普通文件显示的硬链接数，目录文件显示的是第一级子目录的个数，但是在WSL上却不符合这个说法，暂时还没有找到原因，接下来还是列举下文件权限相关的操作吧。 基础概念在linux系统中的每个用户必须属于一个组，不能独立于组外。而每个文件的权限区分所有者、所在组、其它人，就像图片中展示的那样 所有者：指创建文件的人，拥有第一组权限 所在组：指的是和所有者在同组的人，拥有第二组权限 其他人：指排除掉前两类的其他人，拥有第三组权限 每组权限都分为读、写、执行三种具体权限，对应数字分别是4、2、1，字母表示为r、w、x，没有权限可以用-表示，也就是0，因为有这些表示方法，修改文件的命令形式也有很多种。 一个文件的访问权限是可以进行修改的，用户可以使用 chmod 命令来重新设定不同的访问权限，可以使用 chown 命令来更改某个文件或目录的所有者，也可以利用 chgrp 命令来更改某个文件或目录的用户组。 chmod 命令上面说到文件的权限分为所有者、同组人、其他人三组，每组权限又有r、w、x三种权限，分别用数字4、2、1表示，举几个对照的例子如下 123rwx r-x r–x 755rw- r–- –-- 640rw- rw- r–- 664 因为表示的多样性，该命令有通常有字母表达式和数字表达式两种用法，格式如下: 1chmod ［who］ ［+ | – | =］ ［mode］ filename 命令中各选项的含义为： 命令中的 who 可是下述字母中的任一个或者它们的组合： u 表示所有者user g 表示同组用户group o 表示其他人other a 表示所有用户all，是系统默认值 权限改变的符号可以是： + 添加某个权限 – 取消某个权限 = 赋予给定权限并取消其他所有权限（如果有的话） 命令中的 mode 所表示的权限可用下述字母的任意组合： r 可读权限 w 可写权限 x 可执行权限 X 只有目标文件对某些用户是可执行的或该目标文件是目录时才追加 x 属性 u 与文件属主拥有一样的权限 g 与和文件属主同组的用户拥有一样的权限 o 与其他用户拥有一样的权限 常见写法如下： 123$ chmod a+x happy.txt$ chmod ug+w file.xml$ chmod 755 a.out chgrp 命令该命令能改变文件或目录所属的组，语法如下： 1chgrp ［option］ group filename option参数可选： -f,–quiet,–silent: 不显示错误信息 -R,–recursive: 递归处理，将指定目录下的所有文件及子目录一并处理 -v,–verbose: 显示指令执行过程 命令中group可以是用户组ID，也可以是/etc/group文件中用户组的组名。文件名是以空格分开的要改变属组的文件列表，支持通配符。如果用户不是该文件的属主或超级用户，则不能改变该文件的组的。 1$chgrp -R mysql /opt/local/my.ini chown 命令该命令可以更改某个文件或目录的属主，也就是所有者。语法如下： 1chown ［option］ groupname|username filename chown可以将指定文件的拥有者改为指定的用户或组。用户可以是用户名或用户ID。组可以是组名或组ID。与 chgrp命令一样，参数文件也是以空格分开的要改变权限的文件列表，支持通配符。 option参数可选： -f: 若该文件拥有者无法被更改也不要显示错误讯息 -h: 只对于连结(link)进行变更，而非该 link 真正指向的文件 -v: 显示拥有者变更的详细资料 -R: 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更) 修改文件所有者的例子如下： 1$ chown redis /ect/redis/redis.config 总结 ls -l 命令执行后的第5列，表示文件的硬连接数 chmod a+x happy.txt 可以修改文件的权限信息 chgrp a+x fruit.txt 可以修改文件的所有组信息 chown redis /ect/redis/redis.config 可以修改文件的所有者信息 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 鲁迅先生说“我向来是不惮以最坏的恶意，来推测中国人的，然而我还不料，也不信竟会下劣凶残到这地步。”其实黑暗中往往也透露着光明~ 2021-5-1 01:26:36]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>chmod</tag>
        <tag>文件</tag>
        <tag>权限</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[protobuf中SerializeToString和SerializePartialToString的区别]]></title>
    <url>%2Fblog%2F2021%2F04%2F18%2Fprotobuf%E4%B8%ADSerializeToString%E5%92%8CSerializePartialToString%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言protobuf是Google提出的序列化方案，此方案独立于语言和平台，目前提供了如c++、go、python等多种语言的实现，使用比较广泛，具有性能开销小，压缩率高等优点，是值得学习的优秀开源库。 protobuf有 v2 和 v3 两个主要的并且差异很大的版本，有一些关于protobuf的文章中并没有说明版本，有些描述的内容给人造成了疑惑，所以在使用protobuf前要明确自己使用的版本，查找对应的特性。 proto2这个版本在编写 .proto 文件时的字段有三种限定符，分别是required、optional 和 repeated。 required：必须设置该字段，如果是在debug模式下编译 libprotobuf，则序列化一个未初始化（未对required字段赋值）的 message 将导致断言失败。在release模式的构建中，将跳过检查并始终写入消息，但解析未初始化的消息将返回false表示失败。 optional：可以设置也可以不设置该字段。如果未设置可选字段值，则使用默认值，也可以用[default = value]进行设置。 repeated：该字段可以重复任意次数（包括零次），可以将 repeated 字段视为动态大小的数组。 message定义定义一个简单的 message 结构如下： 123456message Person &#123; required string name = 1; optional string email = 2; optional int age = 3 [default = 18]; repeated bytes phones = 4;&#125; 观察 message 定义可以看到每个字段后面都有 = 1、= 2 的标记，这些被称为 Tags，在 protobuf 中同一个 message 中的每个字段都需要有独一无二的tag，tag 为 1-15 的是单字节编码，16-2047 使用2字节编码，所以1-15应该给频繁使用的字段。 关于tag的取值，还有一种范围是[1,536870911]的说法，同时 19000 到 19999 之间的数字也不能使用，因为它们是 protobuf 的实现中保留的，也就是 FieldDescriptor::kFirstReservedNumber 到 FieldDescriptor::kLastReservedNumber 指定的范围，如果使用其中的数字，导出 .proto 文件时会报错，此处存疑，需要验证一下。 message扩展在使用的了 protobuf 的项目发布以后，绝对会遇到扩展原有 message 结构的需求，这一点不可避免，除非发布后的项目不再升级维护了，要想扩展就需要兼容之前的代码逻辑，这里有一些必须遵守的规则，否则就达不到兼容的目的。 不能更改任何现有字段的 tag 不能添加或删除任何 required 字段 可以删除 optional 或 repeated 的字段 可以添加新的 optional 或 repeated 字段，但必须使用新的tag，曾经使用过又删除的 tag 也不能再使用了 注意事项proto2 中对 required 的使用永远都应该非常小心。如果想在某个时刻停止写入或发送 required 字段，直接将字段更改为可选字段将会有问题。一些工程师得出的经验是，使用 required 弊大于利，他们更喜欢只使用 optional 和 repeated。 proto3proto3 比 proto2 支持更多语言但更简洁，去掉了一些复杂的语法和特性。 在第一行非空白非注释行，必须写：syntax = &quot;proto3&quot;; 直接从语法层面上移除了 required 规则，取消了 required 限定词 增加了对 Go、Ruby、JavaNano 等语言的支持 移除了 default 选项，字段的默认值只能根据字段类型由系统决定 序列化将 message 结构对象序列化的函数有很多，即使是序列化成字符串也有多个函数可以使用，比如 SerializeToString、SerializePartialToString、SerializeAsString、SerializePartialAsString 等等。 SerializeToString和SerializeAsString区别这两个还是很好区分的，从源码角度一眼就能够分辨出来： 1234567891011121314std::string MessageLite::SerializeAsString() const &#123; // If the compiler implements the (Named) Return Value Optimization, // the local variable 'output' will not actually reside on the stack // of this function, but will be overlaid with the object that the // caller supplied for the return value to be constructed in. std::string output; if (!AppendToString(&amp;output)) output.clear(); return output;&#125;bool MessageLite::SerializeToString(std::string* output) const &#123; output-&gt;clear(); return AppendToString(output);&#125; 从源代码可以很容易看出，两者仅仅是参数和返回值的类型不同，其内部调用的函数都是一样的，SerializePartialToString 和 SerializePartialAsString 两个函数也是这种区别，可以根据外部逻辑所需来调用合适的函数。 12345678910bool MessageLite::SerializePartialToString(std::string* output) const &#123; output-&gt;clear(); return AppendPartialToString(output);&#125;std::string MessageLite::SerializePartialAsString() const &#123; std::string output; if (!AppendPartialToString(&amp;output)) output.clear(); return output;&#125; SerializeToString和SerializePartialToString区别这两个函数的区别在于内部调用的函数不同，一个调用 AppendToString，另一个调用 AppendPartialToString，两个被调用函数的源代码如下： 12345678910111213bool MessageLite::AppendToString(std::string* output) const &#123; GOOGLE_DCHECK(IsInitialized()) &lt;&lt; InitializationErrorMessage("serialize", *this); return AppendPartialToString(output);&#125;bool MessageLite::AppendPartialToString(std::string* output) const &#123; size_t old_size = output-&gt;size(); size_t byte_size = ByteSizeLong(); if (byte_size &gt; INT_MAX) &#123; GOOGLE_LOG(ERROR) &lt;&lt; GetTypeName() &lt;&lt; " exceeded maximum protobuf size of 2GB: " &lt;&lt; byte_size; return false;&#125; 原来 AppendToString 函数调用了 AppendPartialToString, 只是在调用之前先执行了一句 GOOGLE_DCHECK(IsInitialized()) &lt;&lt; InitializationErrorMessage(&quot;serialize&quot;, *this); 这句话什么意思呢？ 其实就是一个调试状态下的检查，类似于 assert 这个断言函数吧，检查的内容是判断这个 message 是否初始化，之前提到 required 修饰的字段必须要设置一个值，否者就是未初始化的状态，那么现在两个函数的区别就知道了，带有 “Partial” 函数其实是忽略 required 字段检查的，另外还有没有别的不同需要再进一步研究下源码了。 总结 protobuf有 v2 和 v3 两个主要的并且差异较大的版本，使用前请注意版本号 proto3 直接从语法层面上移除了 required 规则，移除了 default 选项，字段的默认值只能根据字段类型由系统决定 SerializeToString和SerializeAsString区别在于参数和返回值的不同，内部调用的函数是相同的 SerializeToString和SerializePartialToString区别在于SerializePartialToString会忽略 required 字段必须赋值的要求 在应用过程中尽可能重用 message 结构，这样protobuf内部实现中内存的重用 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 没有什么捷径，继续努力就好了短期内不会看到什么成果，甚至说一辈子都可能看不到，但只有努力了才有可能看得到 2021-4-18 21:31:34]]></content>
      <categories>
        <category>protobuf</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>protobuf</tag>
        <tag>序列化</tag>
        <tag>required</tag>
        <tag>message</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中一些可以在偷懒时直接使用的函数]]></title>
    <url>%2Fblog%2F2021%2F04%2F11%2FC-%E4%B8%AD%E4%B8%80%E4%BA%9B%E5%8F%AF%E4%BB%A5%E5%9C%A8%E5%81%B7%E6%87%92%E6%97%B6%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言在解决一些算法题时，会遇到一些“嵌套”问题，也就是一个题目中包含多个小的算法知识点，比如计算一个整数的二进制表示中1的个数，或者计算两个数的最大公约数，如果这些小问题本身就是题目，那么就只能“手撕”了。 但是如果这些内容只是解决题目中的一小部分，我们其实是可以偷个懒的，有很多函数已经被纳入函数库，可以直接拿过来使用，接下来我们可以简单看几个。 求解最大公约数自定义实现求最大公约数的一种常用方法叫做辗转相除法，又名欧几里德算法(Euclidean algorithm)，算法本身并不复杂，可以写成如下逻辑实现： 12345678int my_gcd(int x, int y) &#123; while (y != 0) &#123; int z = x % y; x = y; y = z; &#125; return x;&#125; 或者简单写成递归的实现： 123int my_gcd(int x, int y) &#123; return y ? my_gcd(y, x%y) : x;&#125; 因为计算机处理加减法的性能要远高于计算乘除法，所以辗转相除法有很多变形实现，比如辗转相减、用移位运算代替除法计算等。 库函数其实在C++17中，最大公约数计算已经被加到了函数库中，头文件为 &lt;numeric&gt;，直接调用 std::gcd() 就可以了，本身是一个模板函数，定义如下： 12template&lt; class M, class N&gt;constexpr std::common_type_t&lt;M, N&gt; gcd(M m, N n); 计算一个整数的二进制表示中有多少个1自定义实现这也是一道经典的算法题了，常见的实现如下： 12345678int count1(int n) &#123; int cnt = 0; while (n &gt; 0) &#123; cnt++; n &amp;= (n-1); &#125; return cnt;&#125; 这种实现方法不能说最优解法，但是也算的上是一个优秀的实现思路了。 内建函数关于二进制的形式的各种操作，GCC提供了一系列的builtin函数，可以实现一些简单快捷的功能来方便程序编写，并且可用来优化编译结果。 __builtin_popcount12// 返回n的二进制表示形式中1的个数int __builtin_popcount(unsigned int n) __builtin_ffs12// 返回n的二进制表示形式中最后一位1的是从后向前第几位int __builtin_ffs(unsigned int n) __builtin_clz12// 返回n的二进制表示形式中前导0的个数int __builtin_clz(unsigned int n) __builtin_ctz12// 返回n的二进制表示形式中结尾0个个数int __builtin_ctz(unsigned int n) __builtin_parity12// 返回n的奇偶校验位，即n的二进制表示形式中的1的个数模2的结果int __builtin_parity(unsigned int n) 上述列举的这些函数参数都是 unsigned int 类型，如果参数为 usigned long 或者 usigned long long，只需要在函数名后面加上 l 或 ll 就可以了，比如 __builtin_popcountl。 遗憾的是，这些builtin函数一般没有可移植性，使用时要注意。 库函数但值得庆幸的是，这些优秀的函数在C++20中得以转正，成为了C++的标准函数，比如 std::popcount，定义在头文件 &lt;bit&gt; 中，函数定义如下： 12template&lt;class T&gt;constexpr int popcount(T x) noexcept; 更快速的源码计算一个整数的二进制表示中包含1的个数，除了前面提到的 n &amp;= (n-1) 外，还有下面这种变形的二分法实现： 123456789unsigned popcount (unsigned int u)&#123; u = (u &amp; 0x55555555) + ((u &gt;&gt; 1) &amp; 0x55555555); u = (u &amp; 0x33333333) + ((u &gt;&gt; 2) &amp; 0x33333333); u = (u &amp; 0x0F0F0F0F) + ((u &gt;&gt; 4) &amp; 0x0F0F0F0F); u = (u &amp; 0x00FF00FF) + ((u &gt;&gt; 8) &amp; 0x00FF00FF); u = (u &amp; 0x0000FFFF) + ((u &gt;&gt; 16) &amp; 0x0000FFFF); return u;&#125; 采用这种二分法的实现，基本上可以媲美单字节打表的速度了，上述二分法是利用变量u来分组统计1的个数，两两合并到一起进而得到最后结果的。 总结 计算两个数的最大公约数可以在C++17环境下使用 std::gcd() 函数 计算一个整数二进制表示中1的个数可以在C++20环境下使用 std::popcount() 函数 __builtin 开头的函数是GCC提供的方便程序编写的函数，并且可用来优化编译结，但是使用时要注意不可移植性 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 在繁华中自律在落魄中自愈谋生的路上不抛弃良知谋爱的路上不抛弃尊严 2021-4-11 21:27:25]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>语法糖</tag>
        <tag>函数</tag>
        <tag>函数库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写给自己的KMP——C++版本]]></title>
    <url>%2Fblog%2F2021%2F04%2F04%2F%E5%86%99%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84KMP%E2%80%94%E2%80%94C-%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[一、前言又翻到了这个算法，一个常用的子串（子数组）匹配算法，看一遍学一遍，学一遍忘一遍，反反复复，不过每次回忆起来所用的时间越来少了，其本质上就是在暴力搜索的基础上加上 next 数组加速匹配，算法的关键在于 next 数组的理解和求解方法。 不想画图，缺少图解的算法很难给初学者讲清楚，所以本文也仅仅是个人的笔记而已，用于记录算法中关键点、帮助回忆或者理解其中的一些关键因素，如果想从头学习 KMP，还是去搜索其他资料吧，相关的内容有很多，有些文章写的很详细的。 今天的示例代码用C++来写，上一版的自己写的KMP我查了一下是C语言版本的，初看起来已经有点费劲了，随着时间的推移，我决定根据理解再写一次，写完才发现，和之前的风格判若两人。 二、暴力搜索在原字符串中搜索模式串，最容易想到的就是暴力搜索，匹配则向后移动，不匹配则原串回溯，模式串归0，代码很容易实现，列举如下： 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;int violence_find(string s, string p) &#123; int m = s.size(), n = p.size(), i = 0, j = 0; while (i &lt; m &amp;&amp; j &lt; n) &#123; if (s[i] == p[j]) &#123; // match character ++i; ++j; &#125; else &#123; // mismatch i -= j - 1; j = 0; &#125; &#125; return j == n ? i - j : -1;&#125;int main() &#123; string s("abdfdjfdkekfdaa5gsdsf"); string p("fdkekfd"); cout &lt;&lt; violence_find(s, p) &lt;&lt; endl; return 0;&#125; 典型的 O(MN) 解法，这种解法慢就慢在原字符串的回溯上，也就是语句 i -= j - 1; 的效果，当出现失配时，原字符串之前的匹配几乎“白费”，每次最多移动一个字符，而 KMP 算法决定利用之前的“努力成果”。 三、KMP算法在 KMP 算法中先利用模式串构建一个 next 数组，当出现失配情况时根据模式串前缀和后缀情况，最大程序利用已经匹配的部分来达到加速查找的目的，只需要求一个 next 数组，其他部分和暴力匹配的代码很像： 1234567891011121314151617181920int kmp_tmp(string s, string p) &#123; int m = s.size(), n = p.size(), i = 0, j = 0; vector&lt;int&gt; next = std::move(gen_next(p)); while (i &lt; m &amp;&amp; j &lt; n) &#123; if (s[i] == p[j]) &#123; ++i; ++j; &#125; else &#123; // mismatch j = next[j]; if (j == -1) &#123; ++i; j = 0; &#125; &#125; &#125; return j == n ? i - j : -1;&#125; 和暴力搜索的代码对照下，只有 else 中语句块不太一样，这个 i 只前进不后退了，其实这个里的 j == -1 语句可以合并到判定相等的 if 语句块中，完成 KMP 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;using namespace std;vector&lt;int&gt; gen_next(string p) &#123; int n = p.size(), i = 0, j = -1; vector&lt;int&gt; next(n, -1); while (i &lt; n - 1) &#123; if (j == -1 || p[i] == p[j]) &#123; ++i; ++j; next[i] = j; &#125; else j = next[j]; // mismatch, move j &#125; return next;&#125;int kmp(string s, string p) &#123; int m = s.size(), n = p.size(), i = 0, j = 0; vector&lt;int&gt; next = std::move(gen_next(p)); while (i &lt; m &amp;&amp; j &lt; n) &#123; if (j == -1 || s[i] == p[j]) &#123; ++i; ++j; &#125; else j = next[j]; // mismatch, move j &#125; return j == n ? i - j : -1;&#125;int main() &#123; string s("abdfdjfdkekfdaa5gsdsf"); string p("fdkekfd"); cout &lt;&lt; kmp(s, p) &lt;&lt; endl; return 0;&#125; 四、关键点记录 next[i] 中记录的实际上是 p[0,i-1] 这个字符串中所有前缀和所有后缀交集中最长字符串的长度，比如&#39;fdkekfd&#39; 这个字符串所有前缀和所有后缀交集中最长字符串是 &#39;fd&#39;，其长度是2。 字符串的前缀和后缀不包括字符串本身。 next[0] 初始化成-1仅仅是一个编程技巧，你可以初始化成任意值，只要你分辨出是失配的情况即可，这里初始成 -1 正好可以和 s[i] == p[j] 这种情况合并，所以初始化成 -1 会常用一点。 在 KMP 算法中原串索引 i 比较傲娇，它只前进不会回溯，这也是 KMP 速度快的一个主要原因。 当出现失配时，模式子串的前缀和后缀有重合，可以直接移动模式串的前缀到刚刚匹配的后缀部分，但要记住如果没有重合的前缀和后缀，失配时移动模式串的速度会更快，这里容易弄反。 五、总结 KMP 算法的关键是求解 next 数组，是一个被称为部分匹配表(Partial Match Table)的数组 KMP 算法相比暴力匹配时间复杂度提升到了O(N+M)，但是并不是最优秀的字符串匹配算法 想要更快或者选择更合适的算法可以了解下从模式串的尾部开始匹配的 BM算法，以及从模式串的头部开始匹配的 Sunday算法 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 想要看到更高的风景，除了让自己跳的更高以外，还可以选一个更高的平台站上去。找到一个2米高的平台并努力爬上去，远比你原地起跳2米要容易的多~ 2021-4-5 00:22:35]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>find</tag>
        <tag>算法</tag>
        <tag>KMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中常见的字符判断与处理方法]]></title>
    <url>%2Fblog%2F2021%2F03%2F28%2FC-%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E5%AD%97%E7%AC%A6%E5%88%A4%E6%96%AD%E4%B8%8E%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言字符串处理是编程世界中一项基础技能，特别是对于C/C++的程序员们，远没有那么多华丽的工具可以使用，大多数时候都需要一个个字符来判断和处理，甚至对于C语言来说都没有字符串类型，字符数组是其常见的等价结构，所以稳扎稳打的基本功尤其重要。 对于C++而言，确实有string这个字符串类型，在使用的时候有一些技巧和函数可以使用，比C语言要方便许多了，只是有些时候我们并不知道可以这样用，有时一些很朴素的写法会让程序更加简洁，而一些技巧的表达当明白之后也会感叹自己曾经的无知。 ASCII 码作为字符编码的基础，ASCII码是需要先弄明白的，即使不能把所有的ASCII码对应的字符都记住，也要把常见的字母、数字、特殊字符记住，这样在处理字符问题时可以得心应手，常见的ASCII码对照表如下： 其中需要注意的知识点： 前32个为非打印控制字符，后面的字符为打印字符 数字字符 &#39;0&#39;-&#39;9&#39; 对应的ASCII码范围是48-57 大写字母 &#39;A&#39;-&#39;Z&#39; 对应的ASCII码范围是65-90 小写字母 &#39;a&#39;-&#39;z&#39; 对应的ASCII码范围是97-122 NULL 对应ASCII码0，回车的ASCII码是13，换行的ASCII码是10 仔细观察这个ASCII表你会发现很多“秘密”，比如 windows 中的文件放到 linux 上打开时常常显示许多的 ^M，其实这就是\r 的表现，因为在 windows 上用 \r\n 表示换行，而 linux 上使用 \n 换行，那么多余的 \r 在 linux 上就会显示成 ^M。 再比如小写字母 a 和大写字母 A 中间差了32，为什么不是26呢？为什么要在中间插入几个别的字符，搞成连续的不好吗？之前没想过这个问题，但是前两天看了一个高手的代码后，我大概明白了，这个32的差距应该是一种“炫技”的表现，它可以使得许多代码逻辑变得简单。 判断字符范围的函数C 语言C语言中判断的字符范围的函数都在头文件 &lt;ctype.h&gt; 中，常见的有下面这些 int isalnum(int c)：检查所传的字符是否是字母和数字 int isalpha(int c)：检查所传的字符是否是字母 int isdigit(int c)：检查所传的字符是否是十进制数字 int islower(int c)：检查所传的字符是否是小写字母 int isupper(int c)：检查所传的字符是否是大写字母 int ispunct(int c)：检查所传的字符是否是标点符号字符 C++C++ 中其实大部分还是引用C语言里的这些函数，但是头文件的名字为 &lt;cctype&gt;，在C++11中加了一个 int isblank(int c) 函数。 字符判断技巧判断两个字符互为大小写看到这个问题第一直觉是什么？很简单的问题有木有？因为知道一个字母的大小写差了32，所以会写出下面的代码： 1234int isOk(char x, char y) &#123; return x - y == 32 || y - x == 32;&#125; 不过我前两天看到一段代码，它是这样写的： 1(x ^ 32) == y; 看到这里你还以为 A 和 a 之间差32而不是26感到迷惑吗？简单的字符编排透露着巨大的智慧。 哨兵的使用比如取出一个字符串 string s 中所有的数字，问题很简单，但是结尾字符的处理往往体现了编程的功底，加上一个哨兵字符可以使得编程逻辑简单许多，无须再对结尾字符特殊判断。 1234567891011void find(string s) &#123; s = s + 'A'; string ans; for (auto c : s) &#123; if (isdigit(c)) ans += c; else &#123; cout &lt;&lt; ans &lt;&lt; endl; ans = ""; &#125; &#125;&#125; 总结 &#39;0&#39; 的ASCII码是48，&#39;A&#39; 的ASCII码是65，&#39;a&#39; 的ASCII码是97 isdigit 可以判断字符是否是数字，isalpha 可以判断字符是否为字母 一个字母的大小写对应的ASCII码正好差32，判断互为大小写时可以使用异或符号 (x ^ 32) == y 字符串结尾加哨兵字符可以使得处理逻辑更加简单统一，这种编程技巧在其他结构中也常常出现 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 今天看到一个一直作为榜样的知识输出者宣布财富自由，满心羡慕，是真的羡慕！关键人家比我年轻，比我工作时间还短，已经依靠短短4、5年的努力达到了自由状态，不过了解他的经历会发现他确实付出了很多，而我们大多数作为普通人太安于现状了，有时候选择比努力重要，如果选择对了又付出了加倍的努力，那…… 2021-3-28 23:27:32]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>string</tag>
        <tag>char</tag>
        <tag>判断</tag>
        <tag>查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中反向遍历map时怎样删除元素]]></title>
    <url>%2Fblog%2F2021%2F03%2F21%2FC-%E4%B8%AD%E5%8F%8D%E5%90%91%E9%81%8D%E5%8E%86map%E6%97%B6%E6%80%8E%E6%A0%B7%E5%88%A0%E9%99%A4%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[前言今天在解决一个问题 《5710. 积压订单中的订单总数》 时用到了map的反向遍历，看到问题时首先想到了优先队列，但是需要维护一个大根堆和一个小根堆，感觉操作起来比较麻烦，突发奇想使用map就能够解决。map本身就是有序的，正向遍历可以得到从小到大的序列，而反向遍历就可以得到从大到小的序列，这个思路本身没有错，但是解题时卡在了反向遍历时如何删除元素的知识点上，特此记录一下。 map的正向遍历map的正向遍历是一个基础知识点了，先简单复习一下，不管是用 for 还是 while，只要控制迭代器持续前进就可以了。 12345678910111213map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (map&lt;int, string&gt;::iterator it = mp.begin(); it != mp.end(); ++it) &#123; cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;&#125;/* 输出内容1 A2 E3 I4 O6 U*/ 引入 auto 关键字以后，定义表示式的时候会更加方便一点 12345map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it = mp.begin(); it != mp.end(); ++it) &#123; cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;&#125; 引入冒号以后表达式更加简短，要注意的是这里的 it 已经不是指针了，而是 value_type 类型，所以需要是用 . 来访问 12345map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it : mp) &#123; cout &lt;&lt; it.first &lt;&lt; " " &lt;&lt; it.second &lt;&lt; endl;&#125; 引入了结构化绑定声明之后，遍历方式还可以写成下面这样 12345map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto&amp; [a, b] : mp) &#123; cout &lt;&lt; a &lt;&lt; " " &lt;&lt; b &lt;&lt; endl;&#125; map 遍历时删除元素map 遍历时删除需要注意迭代器失效问题，常用的有下面两种写法 1it = mp.erase(it); 或者1mp.erase(it++); 遍历删除时的例子： 1234567891011121314151617map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it = mp.begin(); it != mp.end();) &#123; if (it-&gt;second == "I") mp.erase(it++); else it++;&#125;for (auto it : mp) cout &lt;&lt; it.first &lt;&lt; " " &lt;&lt; it.second &lt;&lt; endl;/* 输出内容1 A2 E4 O6 U*/ map 的反向遍历map 反向遍历时可以使用 reverse_iterator 迭代器，配合 rbegin() 和 rend() 方法就可以完成反向遍历 123456789101112map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it = mp.rbegin(); it != mp.rend(); it++) &#123; cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;&#125;/* 输出内容6 U4 O3 I2 E1 A*/ map 反向遍历时删除元素一开始也是用 erase 函数来删除元素，但是会报下面的编译错误 123error: no matching function for call to ‘std::map&lt;int, std::__cxx11::basic_string&lt;char&gt; &gt;::erase( std::reverse_iterator&lt;std::_Rb_tree_iterator&lt;std::pair&lt;const int, std::__cxx11::basic_string&lt;char&gt; &gt; &gt; &gt;)’ mp.erase(it++); 查询文档发现，erase 函数重载只有下面几种实现: 12345678void erase( iterator pos ); (until C++11)iterator erase( const_iterator pos ); (since C++11)iterator erase( iterator pos ); (since C++17)void erase( iterator first, iterator last ); (until C++11)iterator erase( const_iterator first, const_iterator last ); (since C++11)size_type erase( const key_type&amp; key ); 参数是迭代器的函数并不支持 reverse_iterator，需要将 reverse_iterator 转化成 iterator 才可以，这时就需要用到 base 函数，对 reverse_iterator 类型的迭代器使用 base 函数得到的是上一个元素“原始指针”，这一点比较有意思，具体的解释可以参考 《std::reverse_iterator::base》，这种操作决定了我们遍历删除的写法，应该是先自增再调用 base 函数，代码如下； 12345678910111213141516map&lt;int, string&gt; mp&#123;&#123;1, "A"&#125;, &#123;2, "E"&#125;, &#123;3, "I"&#125;, &#123;4, "O"&#125;, &#123;6, "U"&#125;&#125;;for (auto it = mp.rbegin(); it != mp.rend();) &#123; if (it-&gt;second == "I") mp.erase((++it).base()); else it++;&#125;for (auto it = mp.rbegin(); it != mp.rend(); it++) cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;/* 输出内容6 U4 O2 E1 A*/ 总结 map 默认会按照 key 排序，是一个常用的有序容器 配合使用 rbegin() 和 rend() 函数可以完成 map 的反向遍历 对 reverse_iterator 类型迭代器使用 base() 函数，可以转化成 iterator 相关类型，然后进行删除操作 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 搬起砖，我抱不了你，放下砖 … 我尽力！ 2021-3-21 19:44:27]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>iterator</tag>
        <tag>reverse_iterator</tag>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决git命令会将结果输出到单独窗口必须按q才能退出的问题]]></title>
    <url>%2Fblog%2F2021%2F03%2F14%2F%E8%A7%A3%E5%86%B3git%E5%91%BD%E4%BB%A4%E4%BC%9A%E5%B0%86%E7%BB%93%E6%9E%9C%E8%BE%93%E5%87%BA%E5%88%B0%E5%8D%95%E7%8B%AC%E7%AA%97%E5%8F%A3%E5%BF%85%E9%A1%BB%E6%8C%89q%E6%89%8D%E8%83%BD%E9%80%80%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言平时会在 windows+linux 两种环境下进行开发，版本控制软件用 git 比较多，但是一直有个小问题，在 windows 下使用 git Bash 比较顺手，但是在 linux 使用 git 部分命令的时候，常常会打开一个新的窗口，按q键才能退出，比如 git branch、git log、git show 等等。 如果是使用 git log 查询日志后想进行合并或者回退等操作时，因为日志信息已经退出没有显示在当前窗口，这种情况会比较麻烦，有时候还需要额外再查询一次，还有就是 git branch 命显示内容常常较少，单独打开一个窗口也没有必要，所以想单独设置这种情况，后来查询资料发现，这与 git 的 pager 设置有关，特此记录一下。 什么是 pagerpager 其实就是分页器，也就是对一大段内容进行分页显示的工具，git 在一些版本中默认使用的是 less 工具，不同的版本默认设置会有差异，这也就是造成我在 windows 下没有自动分页，而在 linux 下会打开新窗口进行分页的原因。 git 的分页器可以通过 core.pager 来进行设置，他会被 git 命令行解释，影响分页器的变量有多个，他们起作用的顺序依次是 $GIT_PAGER 环境变量，core.pager git配置，$PAGER 环境变量，如果这些都没有设置，默认会选择编译时的选项（通常为less），具体细节可以参考官方文档 git core.pager。 设置 core.pager了解了上面的原理，我们就知道只要单独修改 git 配置就可以了，默认的分页器是 less，我们只要设置了 core.pager 就可以影响结果，所以在 git Bash 中执行下面的语句即可。 1git config --global core.pager '' 其实就是将分页器清空就行了，这样再执行 git branch 的时候就不会出现分页的情况了。 更精细的设置设置 core.pager 这项配置后对 less 分页器进行了全局屏蔽，虽然 git branch 这种显示内容少的命令比较方便了，但是执行 git show 的时候不分页反而会显得混乱，有没有单独设置每个 git 命令的配置呢？ 答案当然是肯定的，比如上面提到的这种情况，我们只想屏蔽 git branch 命令的分页，而想保留git show 和 git log 的分页显示，就可以单独执行下面的命令。 1git config --global pager.branch false 这样就可以达到只屏蔽 git branch 命令的分页结果了。 less and more临时插播个知识点，less 和 more 都是 linux 的文本显示工具，那么它们谁更厉害一点呢？从名字上来看应该是 more 更厉害，但实际上是 less 更厉害，less 在 more 的基础上加上了后退功能（据说最初版本more不能后退，现在的常见版本已经支持后退功能了），支持上下键翻页，并且速度更快一点，所以在 linux 的世界一直流传着 “less is more” 这句话。另外 more 退出后会在 shell 上留下刚显示的内容，而 less 不会。 总结 使用 git config --global core.pager &#39;&#39; 命令可以屏蔽 git 默认的分页器 less 使用 git config --global pager.branch false 命令可以只关闭 git branch 命令的分页显示 less 命令比 more 命令更加强大，支持上下键翻页，退出后不会在 shell 显示刚才的内容 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 学而不思则罔，思而不学则殆。动而不思则徒，思而不动则颓。 2021-3-14 17:14:55]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>branch</tag>
        <tag>log</tag>
        <tag>config</tag>
        <tag>pager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中int、str、bytes相互转化，还有2进制、16进制表示，你想要的都在这里了]]></title>
    <url>%2Fblog%2F2021%2F03%2F05%2FPython%E4%B8%ADint%E3%80%81str%E3%80%81byte%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96%EF%BC%8C%E8%BF%98%E6%9C%892%E8%BF%9B%E5%88%B6%E3%80%8116%E8%BF%9B%E5%88%B6%E8%A1%A8%E7%A4%BA%EF%BC%8C%E4%BD%A0%E6%83%B3%E8%A6%81%E7%9A%84%E9%83%BD%E5%9C%A8%E8%BF%99%E9%87%8C%E4%BA%86%2F</url>
    <content type="text"><![CDATA[前言数据类型转换是个很基础的操作，很多语言中都要做这些转换，例如前一段时间刚刚总结了《C/C++中string和int相互转换的常用方法》，python 自从分离出 python3 版本之后，str 和 bytes 两个类型弄蒙了一大票人，在这两种类型的转换上我可是花了不少时间，记住一点，别随随便便使用 str() 函数，很多数据使用 str() 变成字符串之后再想恢复可就难了。 本文所有示例均在 Python 3.7.5 上测试，Python2 已经被我抛弃了，我来试着把常见的转换都放到一起，把踩过的坑拿来开心一下，如果有些常用的类型转换这里没有的话，也欢迎小伙伴们提出来，我将持续补充，好了，可以开始了。 数据类型转化数字中除了整数，还有浮点数、复数等，但是 int 是最常见的类型，所有转换中的数字只涉及 int 数字类型。 int -&gt; str使用 str() 函数12345num = 10val = str(10)print(type(val), val)#&lt;class 'str'&gt; 10 使用 format() 函数12345num = 10val = '&#123;0&#125;'.format(num)print(type(val), val)#&lt;class 'str'&gt; 10 使用 hex() 转换成16进制形式12345num = 10val = hex(num)print(type(val), val)#&lt;class 'str'&gt; 0xa 使用 bin() 转换成2进制形式12345num = 10val = bin(num).replace('0b','')print(type(val), val)#&lt;class 'str'&gt; 1010 str -&gt; int这个转换比较专一，只使用 int() 函数就可以了，这个函数实际上有两个参数，第二个参数表示进制，默认是10进制，你可以改成2进制或者16进制，甚至是3进制、5进制等等 使用 int() 进行各进制数字转换1234567891011121314151617181920val = int('10')print(type(val), val)val = int('0xa', 16)print(type(val), val)val = int('a', 16)print(type(val), val)val = int('0b1010', 2)print(type(val), val)val = int('1010', 2)print(type(val), val)val = int('101', 3)print(type(val), val)val = int('60', 5)print(type(val), val)# 结果均为 &lt;class 'int'&gt; 10 使用 int() 函数的时候要主要注意一点，如果提供的字符串不能转换成指定进制的数字，那么会报异常，就像下面这样，所以在使用这个函数的时候最好放到 try 语句中。 123456789val = int('128', 2)'''Traceback (most recent call last): File "D:\python\convert\convert.py", line 41, in &lt;module&gt; val = int('128', 2)ValueError: invalid literal for int() with base 2: '128'[Finished in 0.1s with exit code 1]''' 什么是bytes在列举 bytes 相关的转化前，我们来先认识一下这个类型，在 Python3 中 int、str、bytes 类型的变量实际上都是一个 “类” 的对象，而 bytes 相比 str 而言更接近底层数据，也更接近存储的形式，它其实是一个字节的数组，类似于 C 语言中的 char []，每个字节是一个范围在 0-255 的数字。 bytes 其实就是这样一连串的数字，计算机世界所有的信息都可以用这样一串数字表示，一幅画，一首歌，一部电影等等，如果对编码感兴趣可以看看这篇《简单聊聊01世界中编码和解码这对磨人的小妖儿》，现在清楚bytes是什么了，我们可以看看和它相关的转化了。 int -&gt; bytes使用 to_bytes() 转换成定长bytes12345num = 4665val = num.to_bytes(length=4, byteorder='little', signed=False)print(type(val), val)#&lt;class 'bytes'&gt; b'9\x12\x00\x00' 这段代码就是把数字 4665 转化成定长的4个字节，字节序为小端，我们来简单看一下是怎么转换的： 上面我们提到 bytes 类型一串 0-255 范围的数字，4665 肯定超出了这个范围，可以先转化成256进制，就变成了 ，也就是 4665 = 18 * 256 + 57，我们发现两个字节就能存储这个数字，一个18，一个57，要想组成4个字节的数组需要补充两个空位，也就是补充两个0，这时就涉及到一个排列顺序，是 [0,0,18,57] 还是 [57, 18, 0, 0] 呢，这就是函数参数中的字节序 byteorder，little 表示小端，big 表示大端，这里选择的小端 [57, 18, 0, 0] 的排列。 看到这里可能会迷糊，好像和结果不一样啊，其实这只是一个表示问题，57 的 ASCII 码对应这个字符 ‘9’，18 表示成16进制就是 ‘0x12’，这里写成 b’9\x12\x00\x00’ 只是便于识别而已，实际上内存存储的就是 [57, 18, 0, 0] 这一串数字对应的二进制编码 ‘00111001 00010010 00000000 00000000’。 使用 bytes() 函数把int数组转成bytes参考上面的生成的数组，可以通过数组生成相同的结果 12345num_array = [57, 18, 0, 0]val = bytes(num_array)print(type(val), val)#&lt;class 'bytes'&gt; b'9\x12\x00\x00' 使用 struct.pack() 函数把数字转化成bytes12345num = 4665val = struct.pack("&lt;I", num)print(type(val), val)#&lt;class 'bytes'&gt; b'9\x12\x00\x00' 这里的 &quot;&lt;I&quot; 表示将一个整数转化成小端字节序的4字节数组，其他的类型还有： 参数 含义 &gt; 大端序 &lt; 小端序 B uint8类型 b int8类型 H uint16类型 h int16类型 I uint32类型 i int32类型 L uint64类型 l int64类型 s ascii码，s前带数字表示个数 bytes -&gt; int明白了上面的转化过程，从 bytes 转化到 int 只需要反着来就行了 使用 from_bytes() 把 bytes 转化成int12345bys = b'9\x12\x00\x00'val = int.from_bytes(bys, byteorder='little', signed=False)print(type(val), val)#&lt;class 'int'&gt; 4665 使用 struct.unpack() 把 bytes 转化成int12345bys = b'9\x12\x00\x00'val = struct.unpack("&lt;I", bys)print(type(val), val)#&lt;class 'tuple'&gt; (4665,) str 和 bytes前面的这些转化还算清晰，到了字符串str 和字节串 bytes，就开始进入了混沌的状态，这里会出现各种编码，各种乱码，各种报错，牢记一点 str 到 bytes 是编码过程，需要使用 encode() 函数， bytes 到 str 是解码过程，需要使用 decode() 函数，请勿使用 str 函数，否则后果自负。 使用 encode() 函数完成 str -&gt; bytes12345s = '大漠孤烟直qaq'val = s.encode('utf-8')print(type(val), val)# &lt;class 'bytes'&gt; b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq' 使用 decode() 函数完成 bytes -&gt; str12345bys = b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq'val = bys.decode('utf-8')print(type(val), val)# &lt;class 'str'&gt; 大漠孤烟直qaq 假如使用了 str() 函数从上面来看字符串和字节串的转化蛮简单的，甚至比整数的转化都要简单，但是你如果把一个 bytes 变量用 str() 转化成字符串，你就得手动来处理了，这个函数写过n次了，暂时还没找到好的处理办法。 123456789101112131415161718192021222324bys = b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq's = str(bys)print(type(s), s)#&lt;class 'str'&gt; b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq'def str2bytes(str_content): result_list = []; pos = 0 str_content = str_content.replace("\\n", "\n").replace("\\t", "\t").replace("\\r", "\r") content_len = len(str_content) while pos &lt; content_len: if str_content[pos] == '\\' and pos + 3 &lt; content_len and str_content[pos + 1] == 'x': sub_str = str_content[pos + 2: pos + 4] result_list.append(int(sub_str, 16)) pos = pos + 4 else: result_list.append(ord(str_content[pos])) pos = pos + 1 return bytes(result_list)val = str2bytes(s[2:-1])print(type(val), val)# &lt;class 'bytes'&gt; b'\xe5\xa4\xa7\xe6\xbc\xa0\xe5\xad\xa4\xe7\x83\x9f\xe7\x9b\xb4qaq' 什么时候会遇到这种情况，就是有些数据是以 bytes 的形式给的，但是经过中间人复制转发变成了字节流形式的字符串，格式还不统一，有些已经翻译成了字符，有些还保留了0x或者\x形式，这时就要手工处理了。 转化表格上面的转化方式和解释穿插在一起有些乱，这里总结一个表格，便于今后拿来就用 源类型 目标类型 方式 结果 int str str(10)、&#39;{0}&#39;.format(10) 10 =&gt; &#39;10&#39; int str（16进制） hex(10) 10 =&gt; &#39;0xa&#39; int str（2进制） bin(10).replace(&#39;0b&#39;,&#39;&#39;) 10 =&gt; &#39;1010&#39; str int int(&#39;10&#39;) &#39;10&#39; =&gt; 10 str（16进制） int int(&#39;0xa&#39;, 16) &#39;0xa&#39; =&gt; 10 str（2进制） int int(&#39;1010&#39;, 2) &#39;1010&#39; =&gt; 10 int bytes num.to_bytes(length=4, byteorder=&#39;little&#39;, signed=False) 4665 =&gt; b&#39;9\x12\x00\x00&#39; int bytes struct.pack(&quot;&lt;I&quot;, 4665) 4665 =&gt; b&#39;9\x12\x00\x00&#39; bytes int int.from_bytes(b&#39;9\x12\x00\x00&#39;, byteorder=&#39;little&#39;, signed=False) b&#39;9\x12\x00\x00&#39; =&gt; 4665 bytes int struct.unpack(&quot;&lt;I&quot;, b&#39;9\x12\x00\x00&#39;) b&#39;9\x12\x00\x00&#39; =&gt; 4665 int[] bytes bytes([57, 18, 0, 0]) [57, 18, 0, 0] =&gt; b&#39;9\x12\x00\x00&#39; bytes int[] [x for x in b&#39;9\x12\x00\x00&#39;] b&#39;9\x12\x00\x00&#39; =&gt; [57, 18, 0, 0] str bytes &#39;美好&#39;.encode(&#39;utf-8&#39;) &#39;美好&#39; =&gt; b&#39;\xe7\xbe\x8e\xe5\xa5\xbd&#39; str bytes bytes(&#39;美好&#39;, &#39;utf-8&#39;) &#39;美好&#39; =&gt; b&#39;\xe7\xbe\x8e\xe5\xa5\xbd&#39; bytes str b&#39;\xe7\xbe\x8e\xe5\xa5\xbd&#39;.decode(&#39;utf-8&#39;) b&#39;\xe7\xbe\x8e\xe5\xa5\xbd&#39; =&gt; &#39;美好&#39; bytes bytes（无\x） binascii.b2a_hex(b&#39;\xe7\xbe\x8eqaq&#39;) b&#39;\xe7\xbe\x8eqaq&#39; =&gt; b&#39;e7be8e716171&#39; bytes bytes（有\x） binascii.a2b_hex(b&#39;e7be8e716171&#39;) b&#39;e7be8e716171&#39; =&gt; b&#39;\xe7\xbe\x8eqaq&#39; bytes str（hex） b&#39;\xe7\xbe\x8eqaq&#39;.hex() b&#39;\xe7\xbe\x8eqaq&#39; =&gt; &#39;e7be8e716171&#39; str（hex） bytes bytes.fromhex(&#39;e7be8e716171&#39;) &#39;e7be8e716171&#39; =&gt; b&#39;\xe7\xbe\x8eqaq&#39; 总结 Python3 对字符串和二进制数据流做了明确的区分，不会以任意隐式的方式混用 str 和 bytes bytes 类型是一种比特流，它的存在形式是 01010001110 的形式，需要解码成字符才容易被人理解 struct 模块中的 pack() 和 unpack() 可以实现任意类型和 bytes 之间的转换 binascii.b2a_hex 和 binascii.a2b_hex 可以实现16进制 bytes 的不同形式转换，不过转换前后长度发生了变化 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 初识不知曲中意，再闻已是曲中人 2021-3-6 20:36:10]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>int</tag>
        <tag>hex</tag>
        <tag>Python</tag>
        <tag>str</tag>
        <tag>bytes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下的mount命令到底有什么玄机]]></title>
    <url>%2Fblog%2F2021%2F02%2F22%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84mount%E5%91%BD%E4%BB%A4%E5%88%B0%E5%BA%95%E6%9C%89%E4%BB%80%E4%B9%88%E7%8E%84%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[前言linux 环境下的 mount 命令可以挂载一个文件系统，这个命令目前是我所知命令中最陌生的一个，至今没有使用过，真的一次都没有挂载过，平时用的最多是的 Windwos 系统，完全不需要挂载，安装新硬盘自动就会出现盘符，稍微复杂点就是通过磁盘管理器来分区，或者使用魔术分区助手搞一点复杂的分区或格式化操作。 而 linux 系统下，无论是申请的开发机，还是购买的服务器，硬盘都是挂载好的，至今没有遇到需要手动挂载的情况，所以对这个命令并不是很熟悉，今天决定抽时间学一下，总结一下设备相关的知识，特别是磁盘相关的操作。 文件类型“Everything is a file”（一切皆文件）是 linux 中的特点，所以在学习磁盘、设备之前先来看看文件类型，在 linux 并不通过后缀名来判断文件类型的，可以利用 ll （或 ls -l）命令可以列举指定目录下的内容，通过每一行的首个字母就可以判断出文件的大类型： -rw-r-r-：-开头的都普通文件 brw-r-r-：b开头的是块设备文件 crw-r-r-：c开头的是字符设备文件 drw-r-r-：d开头的是目录文件 lrw-r-r-：l开头的是软链接文件，也叫符号链接 prw-r-r-：p开头的是管道文件 srw-r-r-：s开头的是socket文件 我们可以查一下CentOS系统设备目录 dev，这里的文件类型非常丰富，由于内容非常多，这里只展示一部分内容： 123456789101112131415161718192021222324252627[root@VM-0-3-centos ~]# ll /devtotal 0crw------- 1 root root 10, 235 Feb 9 11:13 autofsdrwxr-xr-x 2 root root 120 Feb 9 11:13 blockdrwxr-xr-x 2 root root 60 Feb 9 11:13 bsgcrw------- 1 root root 10, 234 Feb 9 11:13 btrfs-controldrwxr-xr-x 3 root root 60 Feb 9 11:13 buslrwxrwxrwx 1 root root 3 Feb 9 11:13 cdrom -&gt; sr0drwxr-xr-x 2 root root 2560 Feb 9 11:13 charcrw------- 1 root root 5, 1 Feb 9 11:13 consolelrwxrwxrwx 1 root root 11 Feb 9 11:13 core -&gt; /proc/kcoredrwxr-xr-x 3 root root 60 Feb 9 11:13 cpucrw------- 1 root root 10, 61 Feb 9 11:13 cpu_dma_latencycrw------- 1 root root 10, 62 Feb 9 11:13 crashdrwxr-xr-x 6 root root 120 Feb 9 11:13 diskdrwxr-xr-x 2 root root 60 Feb 9 11:13 dricrw-rw---- 1 root video 29, 0 Feb 9 11:13 fb0lrwxrwxrwx 1 root root 13 Feb 9 11:13 fd -&gt; /proc/self/fdcrw-rw-rw- 1 root root 1, 7 Feb 9 11:13 fullcrw-rw-rw- 1 root root 10, 229 Feb 9 11:13 fusecrw------- 1 root root 10, 228 Feb 9 11:13 hpetdrwxr-xr-x 2 root root 0 Feb 9 11:13 hugepagescrw------- 1 root root 10, 183 Feb 9 11:13 hwrnglrwxrwxrwx 1 root root 25 Feb 9 11:13 initctl -&gt; /run/systemd/initctl/fifodrwxr-xr-x 3 root root 240 Feb 9 11:13 inputcrw-r--r-- 1 root root 1, 11 Feb 9 11:13 kmsgsrw-rw-rw- 1 root root 0 Feb 9 11:13 log 其实还有一个更精准的，专门用来查询文件类型的命令，那就是 file 命令，下面可以测试一下，执行后会输出关于文件类型的描述： 123456789[root@VM-0-3-centos ~]# lltotal 12-rwxr-xr-x 1 root root 67 Feb 9 13:44 connecttendis.shdrwxr-xr-x 2 root root 4096 Feb 9 13:44 tarlistdrwxr-xr-x 4 root root 4096 Feb 9 13:40 tendis[root@VM-0-3-centos ~]# file connecttendis.shconnecttendis.sh: Bourne-Again shell script, ASCII text executable[root@VM-0-3-centos ~]# file tendis/tendis/: directory 描述展示说明 connecttendis.sh 是一个 shell 脚本，而 tendis/ 是一个目录。 文件系统说完文件类型还得说说文件系统，什么是文件系统，其实从名字就可以看出来，文件系统就是管文件的呗，不同的文件系统所支持的最大容量、对单个文件的大小限制、存取性能、是否可压缩、是否提供校验等都是不太一样的，当我们做系统或者烧制U盘、格式化硬盘时常常提到文件系统这个词。 在 Windows 中常用的文件系统有 FAT16、FAT32、NTFS、exFAT 等，而 Linux 中常用的文件系统有 ext2、ext3、ext4、tmpfs、proc 等，这些文件系统无需全部记住，在 Linux 上可以通过查看 /proc/filesystems 文件得知当前系统都支持哪些文件系统。 123456789101112131415161718192021222324252627[root@VM-0-3-centos ~]# cat /proc/filesystemsnodev sysfsnodev rootfsnodev ramfsnodev bdevnodev procnodev cgroupnodev cpusetnodev tmpfsnodev devtmpfsnodev debugfsnodev securityfsnodev sockfsnodev daxnodev bpfnodev pipefsnodev configfsnodev devptsnodev hugetlbfsnodev autofsnodev pstorenodev mqueue ext3 ext2 ext4 iso9660nodev binfmt_misc 观察上述结果，第一列为空的文件系统需要挂载到块设备上才能访问其中的内容，之后才可以正常使用。 挂载硬盘Windows 中的目录是分盘符的，每个分区可以指定一个盘符，每个盘符就是这个分区的根，可以通过各个盘符依次向下访问其中的内容。在 Linux 中是没有盘符概念的，只有一个根目录 /，只存在一棵完整的目录树，硬盘设备需要挂载到这棵目录树上才能被正常使用。 我们知道 /dev 目录下存放着几乎所有的设备文件，从中就可以找到硬盘设备： 123456[root@VM-0-3-centos ~]# ll /dev/ | grep diskdrwxr-xr-x 6 root root 120 Feb 9 11:13 diskbrw-rw---- 1 root disk 7, 0 Feb 9 11:13 loop0crw-rw---- 1 root disk 10, 237 Feb 9 11:13 loop-controlbrw-rw---- 1 root disk 253, 0 Feb 9 11:13 vdabrw-rw---- 1 root disk 253, 1 Feb 9 11:13 vda1 从命令结果可以看出，这台计算机只有一块普通硬盘 vda，硬盘只有一个分区 vda1，这是目前一种通用的命名方式，hd 是指IDE接口的硬盘，sd 是指SATA接口的硬盘、vd 指 virtio 磁盘，现在的内核一般都会把硬盘，移动硬盘，U盘等识别为sdX的形式，第一块硬盘使用 a 作为后缀，例如 sda，后面的硬盘依次命名为 sdb、sdc 等，硬盘的第一个分区后缀为1，后面分区号依次递增。 查硬盘分区硬盘作为一种设备可以在 /dev 目录下查询，但是可以通过 fdisk 做更细致的查询 1234567891011[root@VM-0-3-centos dev]# fdisk -lDisk /dev/vda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000d64b4 Device Boot Start End Blocks Id System/dev/vda1 * 2048 104857566 52427759+ 83 Linux 通过 fdisk -l 命令查询出的结果与 /dev 目录下查到的信息一致 查看当前挂载的文件系统查看当前挂载的所有文件系统只需要一个 mount 命令就够了，也可以加 -t tmpfs 参数查看指定类型 123456789101112131415161718[root@VM-0-3-centos ~]# mountsysfs on /sys type sysfs (rw,relatime)proc on /proc type proc (rw,relatime)devtmpfs on /dev type devtmpfs (rw,nosuid,size=930496k,nr_inodes=232624,mode=755)securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)devpts on /dev/pts type devpts (rw,relatime,gid=5,mode=620,ptmxmode=000)tmpfs on /run type tmpfs (rw,nosuid,nodev,mode=755)tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)...cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)configfs on /sys/kernel/config type configfs (rw,relatime)/dev/vda1 on / type ext4 (rw,noatime,data=ordered)debugfs on /sys/kernel/debug type debugfs (rw,relatime)hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime)mqueue on /dev/mqueue type mqueue (rw,relatime)tmpfs on /run/user/0 type tmpfs (rw,nosuid,nodev,relatime,size=188204k,mode=700)binfmt_misc on /proc/sys/fs/binfmt_misc type binfmt_misc (rw,relatime) 输出每一行格式一致，均包含6列内容，通过空格来分割，具体形式为： &lt;块设备或者其他文件系统&gt; on &lt;挂载目录&gt; type &lt;文件系统类型&gt; &lt;(与文件系统内容相关的选项)&gt; 以 ‘/dev/vda1 on / type ext4 (rw,noatime,data=ordered)’ 这一行为例，就表示 /dev/vda 这个硬盘的第一个分区挂载到了一个 类型为 ext4 的文件系统上，挂载点是根目录 /，括号中的 rw 表示可读写，data=ordered 表示有序，relatime 是一种更新文件访问时间属性的一种方式，此外还有 noatime 和 lazytime 类型，这一块内容也比较多，此处就不展开了，有兴趣可以转到 《文件系统中 atime,lazytime,relatime 详聊》 这篇文章中了解一下。 格式化磁盘分区使用 mkfs 可以格式化一个磁盘分区，格式化的同时可以修改文件系统的类型 1[root@VM-0-3-centos ~]# mkfs -t ext4 /dev/sda2 挂载新的磁盘分区使用 mount 命令可以将新的磁盘分区挂载到目录树上 普通挂载12[root@VM-0-3-centos ~]# mkdir -p /mnt/data[root@VM-0-3-centos ~]# mount /dev/sda2 /mnt/data 以只读方式挂载1[root@VM-0-3-centos ~]# mount -o ro /dev/sda2 /mnt/data 只读挂载改为读写模式1[root@VM-0-3-centos ~]# mount /mnt/data -o rw,remount 挂载光驱挂载光驱的方式与挂载硬盘是一样的，只不过光驱是单独的设备，对应着不同的文件 1[root@VM-0-3-centos ~]# mount /dev/cdrom /media/cdrom 挂载 windows 共享文件挂载 windows 共享文件时需要用到 windows 计算机的IP、用户名和密码 12[root@VM-0-3-centos ~]# mkdir -p /mnt/share[root@VM-0-3-centos ~]# mount -t cifs -o username=admin,password=94741 //10.2.49.172/share /mnt/share 挂载 ios 文件直接挂载 iso 文件就不需要光驱了，使用起来更加方便，先模拟创建一个 iso 文件 12345678910111213141516[root@VM-0-3-centos ~]# genisoimage -o test.iso tendis/I: -input-charset not specified, using utf-8 (detected in locale settings)Using TENDI000.;1 for tendis/bin/tendisplus (tendisplus_static) 2.77% done, estimate finish Sat Feb 27 23:03:08 2021 5.53% done, estimate finish Sat Feb 27 23:03:08 2021 8.30% done, estimate finish Sat Feb 27 23:03:08 2021... 49.78% done, estimate finish Sat Feb 27 23:03:10 2021... 99.55% done, estimate finish Sat Feb 27 23:03:11 2021Total translation table size: 0Total rockridge attributes bytes: 0Total directory bytes: 6144Path table size(bytes): 50Max brk space used 1a000180825 extents written (353 MB) 把 iso 直接挂载到目录就可以了，之后就可以通过 /mnt/iso 目录访问 test.iso 文件中的内容了 123[root@VM-0-3-centos ~]# mkdir -p /mnt/iso[root@VM-0-3-centos ~]# mount test.iso /mnt/isomount: /dev/loop0 is write-protected, mounting read-only 计算机设备中的概念计算机科学中的概念有很多，常常记不清或者记混它们的内容，比如扇区、块、簇、页等，每次看完过后就忘记了，在此简单总结一下。 扇区硬盘的读写以扇区为基本单位，属于物理层面的概念，操作系统是不直接与扇区交互的，可以通过 fdisk -l 查看扇区大小 1234567891011[root@VM-0-3-centos dev]# fdisk -lDisk /dev/vda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000d64b4 Device Boot Start End Blocks Id System/dev/vda1 * 2048 104857566 52427759+ 83 Linux 其中 Sector size，就表示扇区大小，从结果来看还分逻辑扇区和物理扇区，不过本例中为均为 512 bytes。 块、簇块和簇其实是一种东西，通常 linux 系统叫做块，而 windows 系统叫做簇，它是文件系统读写数据的最小单位，每个磁盘块可以包括相邻的 2、4、8、16、32 或 64 个扇区，是操作系统所使用的逻辑概念，可以通过命令 stat /boot 来查看 123456789[root@VM-0-3-centos mnt]# stat /boot File: ‘/boot’ Size: 4096 Blocks: 8 IO Block: 4096 directoryDevice: fd01h/64769d Inode: 18 Links: 5Access: (0555/dr-xr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2018-08-08 11:20:35.109000000 +0800Modify: 2021-02-09 11:14:21.799238058 +0800Change: 2021-02-09 11:14:21.799238058 +0800 Birth: - 结果中 IO Block 就是磁盘块大小 4096 Bytes，也就是 4K，这样将相邻扇区合并为块来存取数据是为了更高效地、更好地管理磁盘空间。操作系统规定一个磁盘块中只能放置一个文件，这就会造成空间的浪费，那就意味着大多数情况会出现文件所占用的磁盘空间大于文件大小的情况。 页这是一个内存相关的概念，是内存的最小存储单位。它大小通常为磁盘块大小的 $2^n$ 倍，可以通过命令 getconf PAGE_SIZE 来获取页的大小 12[root@VM-0-3-centos mnt]# getconf PAGE_SIZE4096 总结 mount 是 linux 系统中非常重要的一个命令，但是我感觉用的比较少 系统新挂载的硬盘关机后会被卸载，可以将挂载信息添加到 /etc/fstab 文件实现开机后自动挂载 可以把多个设备挂载到同一个目录，默认后面挂载的内容会隐藏前面挂载的内容，卸载后面的挂载，前面挂载的内容还会出现 可以把同一个设备挂载到不同的目录，并在挂载时可以指定不同的权限，这样在不同的目录中看到的是同样的内容，但是权限不同 扇区是硬盘的读写的基本单位，属于物理概念 块/簇是操作系统读写数据的基本单位，属于逻辑概念 页是内存的最小存储单位，通常为磁盘块大小的 $2^n$ 倍 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 万里山河都踏过 天下又入谁手 分分合合 不过几十载春秋~ “你在教我做事啊” 2021-2-27 23:41:34]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mount</tag>
        <tag>unmount</tag>
        <tag>挂载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go环境配置时遇到的GOPATH路径以及包管理问题]]></title>
    <url>%2Fblog%2F2021%2F02%2F19%2FGo%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84GOPATH%E8%B7%AF%E5%BE%84%E4%BB%A5%E5%8F%8A%E5%8C%85%E7%AE%A1%E7%90%86%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言过了个年回到工作岗位，发现之前好好的 Go 环境无法进行调试了，于是又重新配置了一次，弄好之后的新参数与原来有一些不一样的地方，甚至还有一些矛盾的地方，真搞不清楚原来的配置参数怎么能成功调试的，也是奇了怪了。配置过程中还遇到了一些问题，特别记录一下，防止今后遇到类似问题还要苦苦寻找。 Go 的环境变量说起环境变量，写 Java 的时候倒是配置了不少，一般都需要配置 JAVA_HOME 和 CLASSPATH，后来 C/C++ 写的比较多，完全不需要这个东西，直接使用 include 把绝对路径或者相对路径引用进来就可以，或者使用VS的项目属性界面配置包含路径，也可以将包含路径写在 CMakeLists.txt 中。但是为了能找到和使用各种工具和软件，它们所在的路径一般会加到环境变量Path中。 Go 中也有两个环境变量非常重要，它们分别是 GOROOT 和 GOPATH，其中 GOROOT 比较好理解，就是 Go 软件安装的目录，可以类比一下 JAVA_HOME/bin，而 GOPATH 就是一个神奇的存在，在go1.12版本之前，Go 语言编写的项目代码和下载的包都必须在 GOPATH 目录下，想像一下，在一个 GOPATH 目录下无数个项目go文件，那感觉真是酸爽。 GOPATH 的设计GOPATH设计的出发点是好的，将代码包统一存储到一个目录下，直接引用包名就可以了，可是这样设计也缺少了自由，下载的第三方包和自己的项目文件混在一起虽然可以方便查看代码，但是结构看起来确实很乱。 什么？你说 GOPATH 可以指定多个目录，确实很多资料说 GOPATH 支持多个目录，下载的包会默认放在 GOPATH 指定的第一个目录下，需要注意的一个点是在windows下指定多个目录需要用分号分隔，而Linux下指定多个目录需要用冒号分隔，并且指定的目录需要是用绝对路径，如果指定的目录中包含相对路径，会报一个 go: GOPATH entry is relative; must be absolute path: &quot;... 错误，记得 GOPATH 变量末尾不要加 : 或者 ;。 春节前我就是配置的多个目录，本来调试用的好好的，结果过完年现在不让用了，一调试就会报错 unexpected directory layout:，具体的报错内容结构如下： 1234567unexpected directory layout: import path: _/go/src/firstgo root: /go/src dir: /go/src/firstgo expand root: /go expand dir: /go/src/firstgo separator: / 后来参考了下面两篇文章，把 GOPATH 改成单一目录就好了。 cmd/go: unexpected directory layout while building project go get 报错 unexpected directory layout 也就是在linux下的 ~/.profile 文件中把 export GOPATH=/home/albert/go:/home/albert/WorkSpace/go 改成 export GOPATH=/home/albert/go 就可以了。 GOPATH 的发展早期版本的 GOPATH 设计所有包下载到指定的目录，并且没有版本号，如果多个项目引用的同一个包的不同版本那就歇菜了，所以说这时的 Go 管理仅仅处于能用的状态，也就相当于一个下载器，达不到软件包管理器的及格水平，后来出现了一些例如 dep、Godep 的包管理工具，均属于官方推荐的第三方管理工具，都非 Go 语言自带。 Go 的包管理工具并不像 Python 的 pip，或者 JS 的 npm 那样统一，本质上还是设计不同导致的，Go 想做的包管理是一种分布式的，没有Python 或者 JS 那种中心仓库，这样又带来了一个弊端，如果包的提供者频繁提交新版本怎么办，所以在包管理的工作中，版本号是必须要存在的。 关于之前GO项目为什么非要放在 GOPATH 下，以及 GO的包管理发展历程可以参考下面文章： GO问答之为什么项目要在 GOPATH/src 目录下 Go 包管理的前世今生 直到 go mod 出现以后，在Go 中引入第三方模块算是方便了不少，参考《拜拜了，GOPATH君！新版本Golang的包管理入门教程》 go mod使用 go mod 怎么导入本地其它项目的包？ 谈谈go.sum Go 的常用命令Go 作为一种语言，同时也代表了一系列工具和生态环境，它的命令有不少，下面列举一些常见的： go env: 打印go的环境信息 go fmt: 运行gofmt对go代码进行格式化 go build: 编译包和依赖 go run: 编译并运行go程序 go version: 显示go程序的版本 go help: 打印命令的帮助信息 go get: 下载并安装包和依赖（-v 显示操作流程的日志及信息；-u 下载丢失的包，但不会更新已经存在的包） 其中 go help 不仅仅打印了这些命令的基本信息，还可以打印出一些概念的帮助信息， 例如 go help gopath，内容比较多，下面只列举一小部分: 1234567891011121314151617181920...Here's an example directory layout: GOPATH=/home/user/go /home/user/go/ src/ foo/ bar/ (go code in package bar) x.go quux/ (go code in package main) y.go bin/ quux (installed command) pkg/ linux_amd64/ foo/ bar.a (installed package object)...... 运行 go env 展示一下当前使用的环境，方便以后做个对照： 123456789101112131415161718192021222324252627282930313233343536$ go envGO111MODULE=""GOARCH="amd64"GOBIN=""GOCACHE="/home/albert/.cache/go-build"GOENV="/home/albert/.config/go/env"GOEXE=""GOFLAGS=""GOHOSTARCH="amd64"GOHOSTOS="linux"GOINSECURE=""GOMODCACHE="/home/albert/WorkSpace/go1/pkg/mod"GONOPROXY=""GONOSUMDB=""GOOS="linux"GOPATH="/home/albert/WorkSpace/go1"GOPRIVATE=""GOPROXY="https://proxy.golang.org,direct"GOROOT="/usr/local/go"GOSUMDB="sum.golang.org"GOTMPDIR=""GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"GCCGO="gccgo"AR="ar"CC="gcc"CXX="g++"CGO_ENABLED="1"GOMOD=""CGO_CFLAGS="-g -O2"CGO_CPPFLAGS=""CGO_CXXFLAGS="-g -O2"CGO_FFLAGS="-g -O2"CGO_LDFLAGS="-g -O2"PKG_CONFIG="pkg-config"GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build602227063=/tmp/go-build -gno-record-gcc-switches" VSCode Remote 时修改的环境变量不生效这个问题也是在这次配置 Go 调试环境时遇到的，我是在 ~/.profile 文件中修改的 GOPATH的内容，但是在VSCode中调试时就是报错，其表现就跟修改的变量未生效一致，后来查资料发现，原来出现这种情况和配置文件的加载顺序有关，具体参考下列文章： VSCode Remote环境变量加载——续 remote-ssh: .profile not sourced for bash shells, only .bashrc? #83 What is the difference between interactive shells, login shells, non-login shell and their use cases? 其实在linux中的shell有 interactive shell、 non-interactive shell、 login shell、 non-login shell 的区分，每种情况下调用的初始化脚本不同，涉及到 /etc/profile、~/.bash_profile ~/.bash_login / ~/.profile等等，而在VSCode远程连接Linux时还会继承之前的环境，多次尝试之后还是不起效果，此时不得不高呼“重启大法好”，重启能解决80%的问题，剩下的20%只能靠重做系统来解决了。 在我这修改脚本内容不生效，脚本间调用还搞出了死循环的问题，不知道是不是因为我使用 zsh 这个 shell作为默认环境出的问题，但重启大法依然奏效，电脑重启后环境变量成功修改了。 C++ 的包管理器捣鼓这么久 Go，突然想到一个问题，C++ 有没有包管理器呢？答案是有的，并且有很多，但是都存在着这样或那样的问题，可以重点看一下 conan，具体使用可以搜索官网，或者看看下面这些总结。 从零开始的C++包管理器CONAN上手指南 conan使用(一)–安装和应用 conan 是一款使用 python 开发的包管理工具，所以需要依赖 Python 环境，CentOS 平台上安装 Python 环境可以参考 Centos安装python3.6和pip步骤记录，虽然CentOS即将被放弃，但是目前在各种云服务器上依旧是主流系统，安装 Python 环境时需要注意一步步跟着做，最好不要投机取巧，我就是因为少安装了一个依赖，导致我使用 pip install conan命令安装 conan 是报错 ModuleNotFoundError: No module named &#39;_ctypes&#39;，此时可以安装依赖，重新编译安装 ·conan· 就可以了，也就是运行 yum install libffi-devel -y，重新 make clean &amp;&amp; make &amp;&amp; make install 就可以了。 总结 Go 的项目在 1.12 版本之后不必放到 GOPATH 路径中了，灵活度大大提高 C++ 也是有包管理器的，其中 conan 排名比较靠前，它是由 Python 语言编写的 go get 是安装依赖包常常要用到的 Go 命令， go help 会提供 Go 相关的许多知识 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有时正确的选择比刻苦努力更加重要，用战术上的勤奋来掩盖战略上的懒惰，其结果只是感动了自己，而不会带我们达到目标。将者，智信仁勇严也~ 2021-2-21 01:19:01]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>GOPATH</tag>
        <tag>mod</tag>
        <tag>包管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP/IP协议簇中的子网掩码有什么作用]]></title>
    <url>%2Fblog%2F2021%2F02%2F14%2FTCP-IP%E5%8D%8F%E8%AE%AE%E7%B0%87%E4%B8%AD%E7%9A%84%E5%AD%90%E7%BD%91%E6%8E%A9%E7%A0%81%E6%9C%89%E4%BB%80%E4%B9%88%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言前几天在配置数据库主从结构时查询了一些IP配置，后来编写远程执行脚本时又配置了一些IP数据，在linux上使用 ifconfig 命令查询IP配置时发现子网掩码不是常用的3个255，而是 255.255.254.0，只是简单记得这样配置扩大了子网的范围，但是具体子网掩码怎么使用还是记不清楚了，所以查询了这些还给老师的知识，在此总结一下。 IP地址分类关于IPv4的地址分类之前有专门的总结，可以参考这一篇《IP地址常见分类：A类、B类、C类、D类、E类》，其中常用到的也就是A类、B类、C类这三种，默认的子网掩码分别为 255.0.0.0、255.255.0.0、255.255.255.0，通过IP地址和子网掩码进行与运算可以得出IP地址所对应的网络地址。 子网掩码子网掩码通过它的名字来判断肯定和子网有关系，掩码又有遮遮掩掩之意，合在一起其实就是通过其作用来进行的命名，也就是对IP地址进行遮掩，然后得到子网地址的作用。 掩码的作用刚刚说通过子网掩码和IP地址可以获得网络地址，那么没有子网掩码可不可以呢？实际上如果进行某些约定就可以不使用子网掩码来获得网络地址，比如IPv4是通过4个字节来表示一个IP地址的，分为网络号和主机号两部分，A类IP地址可以使用第一字节表示网络号，B类地址使用前两个字节表示网络号，C类地址使用前3个字节表示网络号，其实这也是默认的表示方法。 加入子网掩码只是为了更灵活的配置子网，如果都按照默认的“规矩”进行划分就太死板了，难以适应复杂的需求环境，比如一个A类地址 10.0.48.36 默认的子网 10.0.0.0，可以容纳16777214台主机，这样如果一个子网的所需IP数量远远小于这个数字就会产生巨大的浪费。 再比如一个C类地址 192.168.0.1 的默认子网是 192.168.0.0，可以容纳254台主机，如果一个公司或者组织有500人，那么一个C类的子网就无法满足要求，需要分配B类地址才能解决问题，所以才有了子网掩码的出现，可以使子网的划分更加合理。 掩码表示掩码的表示比较简单，它由四个字节构成，表示成二进制形式时为前1后0，可以像IP地址一样的格式来书写，也可以写出掩码中有多少个1，举个例子，下面表示一个IP和子网： 192.168.1.1 和 255.255.254.0 也可以将掩码中1的个数写在IP地址后面表示成： 192.168.1.1/23 子网拆分增加子网掩码中1的个数可以将子网进行拆分，比如地址 192.168.1.1 和 192.168.1.130 在子网掩码是 255.255.255.0时都属于 192.168.1.0 这个子网，但是将子网掩码换成 255.255.255.128， 那么这两个IP地址就分别属于 192.168.1.0 和 192.168.1.128 这两个子网。 子网合并减少子网掩码中1的个数可以将子网进行合并，比如地址 192.168.1.1 和 192.168.0.1 在子网掩码是 255.255.255.0时分别属于 192.168.1.0 和 192.168.0.0 这两个子网，但是将子网掩码换成 255.255.254.0， 那么这两个IP地址就都属于 192.168.0.0 这个子网了。 网络包的传播网络包在子网内是以广播的形式传播，靠的是MAC地址，通过ARP协议可以获得，但是子网间的传播依赖IP地址，需通过路由或者三层交换机才能实现。 总结 通过IP地址和子网掩码进行按位与运算可以计算出IP地址所在的网络地址 通过子网掩码可以更合理的划分子网，对默认子网进行拆分与合并，一定程度上减少浪费和拥堵 子网内通信需要MAC地址，广播的形式来传递消息，这就是为什么网卡设置为混合模式可以截获到发给他人的消息 子网间通信需要借助IP地址才能实现，不过MAC地址也是需要的，通常被设置为网管的MAC地址 平时使用的 ping 命令，其实是 ICMP 协议的一部分，它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 勤劳的人总能挤出时间来坚持自己的习惯，懒惰的人总能找到借口来逃避应该做的事情，认定的目标就要大胆的开始，无论什么想法只要迈出了第一步就不算晚~ 2021-2-14 23:57:52]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>ping</tag>
        <tag>network</tag>
        <tag>ip</tag>
        <tag>子网掩码</tag>
        <tag>MAC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++中string和int相互转换的常用方法]]></title>
    <url>%2Fblog%2F2021%2F02%2F08%2FC-C-%E4%B8%ADstring%E5%92%8Cint%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言字符串操作是各种算法题中的常客，很多数据常常以字符串形式给出，其中有的需要自己转化成整数，而一些整型数据有时转换成字符串处理起来更加方便，比如判断一个整数是否是回文数，所以字符串和整数的转换是一些问题处理的基础步骤，C++ 在处理这类问题时并不像 Python 那样方便，但是也有许多方法能够实现，为了今后查找方便，整理如下。 int 转 string通过 std::to_string() 函数转换12345678#include &lt;iostream&gt;int main()&#123; int num = 123; std::cout &lt;&lt; std::to_string(num); return 0;&#125; 这种方式在 C++11 中才能使用，编译时记得加上 --std=c++11 的选项 通过 ostringstream 转换1234567891011#include &lt;iostream&gt;#include &lt;sstream&gt;int main()&#123; int num = 123; std::ostringstream ss; ss &lt;&lt; num; std::cout &lt;&lt; ss.str(); return 0;&#125; 这是一种通过字符流的方式将整数转换成字符串，这种方式在C++11之前也可以使用 通过 sprintf 转换1234567891011#include &lt;stdio.h&gt;int main()&#123; int num = 123; char buffer[256]; sprintf(buffer, "%d", num); printf("%s", buffer); return 0;&#125; 这是一种C语言中的转换方式，sprintf 也可以换成更安全的 snprintf 函数 string 转 int通过 istringstream 转换1234567891011121314#include &lt;iostream&gt;#include &lt;sstream&gt;int main()&#123; std::string str = "668"; int num = 0; std::istringstream ss(str); ss &gt;&gt; num; std::cout &lt;&lt; num; return 0;&#125; 使用 istringstream 可以从字符流中读取整数，与 ostringstream 是一种相反的操作 使用 sscanf 来转化123456789101112#include &lt;iostream&gt;#include &lt;stdio.h&gt;int main()&#123; std::string str = "668"; int num = 0; sscanf(str.c_str(), "%d", &amp;num); std::cout &lt;&lt; num; return 0;&#125; 注意 sscanf 函数的第一个参数类型是 const char *，string类型的参数需要转换一下 使用 atoi 转换123456789#include &lt;iostream&gt;#include &lt;stdlib.h&gt;int main()&#123; std::string str = "668"; std::cout &lt;&lt; atoi(str.c_str()); return 0;&#125; atoi 函数的头文件是 stdlib.h，同样是一个C语言中的函数 总结 itoa 不是c语言标准函数，在跨平台的整数转字符串的代码中不要使用这个函数 atoi 是一个标准函数，需要将它和 itoa 区别开来，这一点很容易记混的 如果是在C++环境中进行转换，推荐使用 stringstream 字符流的形式和 to_string 函数 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 这个世界真的很有趣，只要活着，就会有无限可能~ 2021-2-9 00:44:34]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>string</tag>
        <tag>int</tag>
        <tag>相互转化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中一些方便的算法函数和吃不够的语法糖]]></title>
    <url>%2Fblog%2F2021%2F01%2F30%2FC-%E4%B8%AD%E4%B8%80%E4%BA%9B%E6%96%B9%E4%BE%BF%E7%9A%84%E7%AE%97%E6%B3%95%E5%87%BD%E6%95%B0%E5%92%8C%E5%90%83%E4%B8%8D%E5%A4%9F%E7%9A%84%E8%AF%AD%E6%B3%95%E7%B3%96%2F</url>
    <content type="text"><![CDATA[前言C++由于其复杂性，学习成本很高。但是作为接近底层的语言，学会之后能做的事情相当多，C++给了开发者极大的自由，只要符合规范就可以尽情的折腾，不过对于日常使用来说确实不太“方便”，特别是相比于Python、JS这类脚本语言，处理一些小问题时前奏太长，很多常用操作都需要自己造轮子，这一点在刷题时感觉很明显，C++一碰到字符串分析就“头大”，Python用一行搞定，用C++则需要N行。 变化其实很多人对C++的认识还停留在 C++98 或者 C++03 版本，然而从 C++11 版本开始C++就发生了翻天覆地的变化，近期在使用的过程中发现C++也有很多方便的函数，越高的版本越方便，语法糖也越多，今天先总结几个，后续再补充吧。 优秀函数和语法糖使用find系列实现trim函数trim函数在很多语言中都是内置函数，可以去除收尾两端的空格，在C++中是没有trim函数的，需要自己实现一下，需要用到的工具函数有下面两个： find_first_not_of：在字符串s中找到第一个不等于指定字符序列ACDE..的位置 find_last_not_of：在字符串s中找到最后一个不等于指定字符序列ACDE..的位置 find_first_not_of(&quot;hello world&quot;, &quot;he&quot;) 指的就是找到第一个不等于 h 且不等于 e 字母的字符位置，要想去除字符串首尾空格就需要找到第一个不等于空格的位置，和最后一个不等于空格的位置，保留这两个位置中的部分即可，实现如下： 123456std::string&amp; trim(std::string &amp;s) &#123; if (s.empty()) return s; s.erase(0, s.find_first_not_of(" ")); return s.erase(s.find_last_not_of(" ") + 1);&#125; 测试代码如下： 12345678910111213141516171819#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;std::string&amp; trim(std::string &amp;s) &#123; if (s.empty()) return s; s.erase(0, s.find_first_not_of(" ")); return s.erase(s.find_last_not_of(" ") + 1);&#125;int main() &#123; string s(" Hello world "); cout &lt;&lt; "before trim ==&gt;" &lt;&lt; s &lt;&lt; "&lt;&lt;==" &lt;&lt; endl; cout &lt;&lt; "after trim ==&gt;" &lt;&lt; trim(s) &lt;&lt; "&lt;&lt;==" &lt;&lt; endl; return 0;&#125; 测试结果如下： 1234albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ g++ stringtrim.cpp -o stringtrimalbert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ ./stringtrimbefore trim ==&gt; Hello world &lt;&lt;==after trim ==&gt;Hello world&lt;&lt;== 使用regex实现split函数split 也是一个常用但C++不提供的函数，在C语言和早期的C++中一般通过 strtok 函数来实现，但是从 C++11 开始可以通过 regex 来实现，可以看下面这个例子： 12345678910111213141516#include &lt;regex&gt;#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; string s("c++11 test split"); regex reg(" "); // "\\s+" for blank vector&lt;string&gt; v(sregex_token_iterator(s.begin(), s.end(), reg, -1), sregex_token_iterator()); for (auto str : v) cout &lt;&lt; "==&gt;" &lt;&lt; str &lt;&lt; "&lt;&lt;==" &lt;&lt; endl; return 0;&#125; 测试结构如下： 12345albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ g++ stringsplit.cpp -o stringsplit --std=c++17albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ ./stringsplit==&gt;c++11&lt;&lt;====&gt;test&lt;&lt;====&gt;split&lt;&lt;== 使用auto遍历map结构很早以前C++中遍历map、set等复杂结构的时候需要写很长的代码来定义迭代器，自从出现了auto之后这种遍历简单了许多，最近发现针对map的遍历还有更简单的方法，使用方法如下： 123456789101112131415#include &lt;map&gt;#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;int main() &#123; map&lt;string, int&gt; m&#123;&#123;"tom", 20&#125;, &#123;"albert", 18&#125;, &#123;"bella", 19&#125;, &#123;"bily", 30&#125;&#125;; for (auto&amp; [name, age] : m) &#123; if (age &gt; 18) cout &lt;&lt; name &lt;&lt; endl; &#125; return 0;&#125; 测试结果如下： 12345albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ g++ autonew.cpp -o autonew --std=c++17albert@home-pc:/mnt/d/data/cpp/cplusplusadvance$ ./stringsplitbellabilytom 总结 find_first_not_of、find_last_not_of、find_first_of 这系列函数功能虽简单，但是使用它们可以简化代码逻辑 正则表达式是处理字符串查找的强有力的工具，合理的使用正则表达式可以达到事半功倍的效果，C++中请使用 std::regex C++17中引入了结构化绑定声明，可以使用auto来声明多个变量，所有变量都必须用中括号括起来 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 做自己认为对的事情，剩下的交给时间~ 2021-1-31 23:04:17]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>auto</tag>
        <tag>algorithm</tag>
        <tag>语法糖</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启用make多任务参数让构建过程加速完成]]></title>
    <url>%2Fblog%2F2021%2F01%2F23%2F%E5%90%AF%E7%94%A8make%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%8F%82%E6%95%B0%E8%AE%A9%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B%E5%8A%A0%E9%80%9F%E5%AE%8C%E6%88%90%2F</url>
    <content type="text"><![CDATA[前言是不是有时候觉得使用make命令编译时太慢了，特别是紧急修改BUG的时候，恨不得钻进电脑里帮着编译器一起干活，其实make命令是可以加速的，使用 -j 选项即可指定make启动的任务数，它是 --jobs[=jobs] 的缩写形式，比如 make -j4 就表示同时启动4个任务并行构建，虽然达不到4倍的关系，但是要比原来快多了。 寻找寻找这样一个参数的原因还是觉得有时编译太慢了，特别是修改一个公共的头文件时，几乎要从头编译到尾，启用 -j 参数的过程异常顺利，没有任何报错，迅速的就构建完成，这让我想起了之前一个项目中遇到过的问题。 同样是编译过程比较慢，但是老大哥告诉我启用多线程编译会报错，作为新手小白的我就默默记住了，也没有过多的探究，毕竟编译的次数不是那么多，偶尔长时间编译一次也没有什么关系，但是现在突然想知道当时为什么使用多线程编译会报错呢？ 可能的原因当知识渐渐丰富以后，面对这样的问题还是有些头绪的，启用 make -jn时被称为多任务并行构建，也有的文章会写多线程编译或者多进程编译，从表现来看至少是多进程的，因为在任务列表可以看到不同的进程id，不过这里的名字不是重点，重点是有多个任务在同时干活。 当启用多任务构建时原来的串行构建逻辑就变成了并行，那么此时构建失败多是由依赖关系指定不正确导致，这种依赖关系通常有两种： B模块编译需要用到A模块的函数 B模块构建过程中需要的临时数据由A模块构建时产生，两种共用临时数据，但是有序 总结来说就是逻辑上需要A构建完了，才能开始构建B，如果此时先构建B任务就会导致出错，这就能解释为什么使用 make 可以成功，但是使用 make -j4 就构建失败，也能解释为什么失败之后，多次执行这个命令可能还会成功，因为多次执行以后可能会把A模块先构建完，这样后面再构建B就不会出错了。 加速从上面的分析可以得知，模块间的依赖关系决定了多任务构建时应有的顺序，那么是不是所有的构建任务都可以通过 -j 来加速呢？答案当然是否定的，如果要构建项目的所有模块的依赖关系完全是线性的，那么就没有办法并行完成，比如下面这样的： 123456graph TB A--&gt;B; B--&gt;C; C--&gt;D; D--&gt;E; E--&gt;F; 但是整个任务如果可以进行拆分，整个依赖图中出现分叉，那么就可以通过这种方式来加速，比如像这样的依赖关系： 123456graph TB A--&gt;B; A--&gt;C; B--&gt;D; B--&gt;E; C--&gt;F; 上面所表示的关系中，虽然 B 和 C 都需要依赖A完成，但是当 A 完成后，B 和 C 的构建就可以并行开始，这样就可以达到加速构建的目的。 视网膜效应之前也没有注意到 make 命令这个 -j 的选项，自从在项目中使用了一次，我发现在很多项目说明中都看到了这个参数，比如安装 global 的时候，编译 tendis 的时候等等，之前也有这样的情况，就是当你刚接触一个事物，或者进入一个新领域的时候，发现其实周围很多人都在讨论这些事物，自己以前都没注意到，上网查了一下，原来这叫做视网膜效应。 视网膜效应这个学术名词用白话文来讲就是“心眼”，每一个人的眼睛，都是跟着心走！简单地说，这种效应的意思就是一个人的身心状况会影响他的视线，当他自己拥有一件东西或一项特征时，他就会比平常人更会注意到别人是否跟他一样具备这种特征，即越关注什么就越出现什么。 视网膜效应是一种狭隘视野与思维的反映，它会导致看问题不全面，甚至会出现牛走羊肠道、鼠钻牛角尖的极端现象。其实每个人的特质中，都有很多优点和缺点。当一个人只知道自己的缺点是什么，而不知发掘优点时，视网膜效应就会促使这个人发现他身边也有许多人拥有类似的缺点，进而使他的人际关系无法改善，生活也不会快乐。 随便聊聊make从刚接触这个命令的时候就认为它是编译的意思，其实这是一种先入为主的思想，因为之前在linux安装软件时常需要下面这三步： 123./configure./make./make install 从源码安装软件就需要进行编译，所以一直认为这三步是配置、编译、安装的意思，其实 make 本身并不会编译，它只是编译命令的搬运工。 它的真实含义应该是构建，这个构建可不一定是编译，可以是任何逻辑化的事物，只不过常常用 make 来完成编译任务，所以把它和编译绑定到了一起，构建时需要图纸的，这个图纸就是 Makefile 文件，只要我们画好了Makefile图纸，那么 make 命令就可以根据它来完成任务。 所以当你运行 make 命令时，仿佛在说: “Here’s your drawing, go go go！”，又仿佛在说：“图纸搁这儿呢，可劲儿造吧”，以这个观点来看 make 的一系列命令就有意思了： make：图纸搁这儿呢，可劲儿造吧 make clean：把你弄得这堆破烂儿，拾掇拾掇 make install：把你鼓捣出的那玩意，搬到旮旯去 make dist：赶紧把那玩意打包拉走 cmake既然 make 是编译命令的搬运工，那么 cmake 又是什么意思呢？大胆猜测他就是 config make 的意思，它的作用是生成 Makefile 文件，换句话来说就是给 make 造图纸的。 总结 使用 make -j4 命令可以开启4个任务并行构建，大大加快构建速度 make 本身并不能进行编译，它只是各种编译命令的搬运工，需要Makefile图纸才能进行工作 cmake 的作用是生成 make 所需的图纸，有了它可以更快更方便的生成一些规范的Makefile文件 视网膜效应指的是越关注什么就越出现什么的效应，是一种狭隘视野与思维的反映，会导致看问题不全面 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 未经他人苦，莫劝他人善，世上总有一些你无法理解，但却真实存在的生活~ 2021-1-23 17:24:15]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>linux</tag>
        <tag>编译</tag>
        <tag>make</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次解决Intel 9462无线网卡的笔记本安装Ubuntu16.04后无法连接WIFI问题的艰难历程]]></title>
    <url>%2Fblog%2F2021%2F01%2F16%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E8%A7%A3%E5%86%B3Intel-9462%E6%97%A0%E7%BA%BF%E7%BD%91%E5%8D%A1%E7%9A%84%E7%AC%94%E8%AE%B0%E6%9C%AC%E5%AE%89%E8%A3%85Ubuntu16-04%E5%90%8E%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5WIFI%E9%97%AE%E9%A2%98%E7%9A%84%E8%89%B0%E9%9A%BE%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言一台新申请的ThinkPad-X390笔记本需要安装Ubuntu系统，为了统一工作开发环境，选择了大家一直都在使用Ubuntu16.04版本，虽然这个系统版本旧一点，不过同一个版本出现开发环境问题也好排查，但没想到的是这里埋下了一个坑，导致成功安装系统后却不能上网，足足折腾了一整天才搞定，记录一下，没事回来看看还能乐呵乐呵。 安装环境 ThinkPad-X390 笔记本 Intel® Wireless-AC 9462 无线网卡 Ubuntu 16.04.7 系统 解决方案根本原因：Ubuntu 16.04内核版本太低，无法自动识别网卡，需要手动升级内核，安装无线网卡驱动。如果你遇到的情况也是新版本无线网卡，安装老版本系统时无法连接WIFI，请参考这个无私博主的解决方法，有这一篇足够了 联想 Yoga C740：：关于Ubuntu16.04下无法识别Intel WIFI6 AX201无线网卡的解决方案，https://blog.csdn.net/dieju8330/article/details/101422743 安装过程下面开始碎碎念内容，虽然现在看来只要是上面那一个链接就可以解决问题，但是这却是浏览了几百个网页后，不断尝试才证明它是有效的，所以我也简单记录一下这个过程，以下的内容可能对你解决问题可能没什么帮助了，不过你要是喜欢听故事倒是可以继续瞧瞧。 初始环境一台预装了Win10系统的笔记本，连接WIFI可以正常上网，一开始是打算装双系统的，但是台式机是Win10，笔记本再保留一个Win10有些浪费空间，准备只安装一个Ubuntu算了。 又要装Ubuntu了，我在CSDN的第一篇文章的就是有关Ubuntu的，《ubuntu 12.04系统黑屏，登录界面黑屏，命令行界面可用》，当时靠着几个命令把问题解决了，这次解决问题同样需要几个命令，但是这个过程就复杂多了。 安装Ubuntu16.04.2本地找到了之前在虚拟机中安装使用过的 ubuntu-16.04.2-desktop-amd64.iso 镜像，使用UltraISO刻录U盘发现一直占用，遂改用rufus来刻录，发现这个软件操作也很方便，顺利完成启动盘制作，安装过程也比较顺利，网上很多大佬的教程都可以拿来参考，比如下面这几篇： 《Win10 + Ubuntu 16.04双系统完美安装教程【详细】》 《Windows10安装ubuntu16.04双系统教程》 《卸载win10装Ubuntu笔记》 注意，前两篇是装双系统的，和我们的只安装一个Ubuntu16.04的目标不太一致，但是绝大部分步骤都是一致的，网络上的知识就是这样，要想完全一样太难了，需要学会变通。 比较懒我就不截图了，去大佬文章里看吧，说一说其中的几个关键点： 安装Ubuntu需要关闭BOIS中的Secure Boot 选项 注意一下安装双系统和单个系统时进行分区操作的差异，选对主分区和逻辑分区 烧制U盘和安装系统时涉及到分区表GPT和MBR的选择，烧制U盘与硬件分区模式一致就好，在windows通过命令或者“管理”都很容易查到 启动方式有Legacy和UEFI区别，记住UEFI是新的，能用它就用它，不行就换成Legacy，大不了重新装一次，当然你根据硬件情况能准确判断最好 安装过程中有一步是勾选“为图形或无线硬件…安装第三方软件”，看到很多教程都没勾选，我也没选（伏笔1），感觉我已经入坑了 非常自信的干掉了Win10系统（伏笔2），干干净净的只安装了Ubuntu，安装很快完成，顺利进入系统，期间跳出来几行错误，我都没看清就闪过去了，应该是不重要，进去后准备部署开发环境，发现没有WIFI列表，这就奇怪了，网卡肯定没问题，之前Win10是可以上网的，肯定是设置的不对，上网查查为什么吧。 硬件禁止问题这是我看到的第一个解决方案，需要改 /etc/Network/NetworkManage.conf 文件中的 manage=false 改成 manage=true, 参考下面这帖子 解决Ubuntu下的WiFi列表不显示问题（硬件禁止问题） 现在回过头来想想我根本就不是这个问题，很多特点都对不上，完全是病急乱投医的处理方式，使用 rfkill list 命令根本找不到我的网卡，这一点也是后来才发现的 激活无线网卡很多文章开局就是下面这两个命令 12sudo apt-get updatesudo apt-get install --reinstall bcmwl-kernel-source 要知道使用 sudo apt-get update 是需要联网的，现在我连不上无线，不存在有线，所以上不去网，然后翻了N个页面之后找到了大神离线安装的方法，其实这个安装文件在镜像里就有，所以打开刚刚烧制的U盘就行了，但是直接安装会有依赖问题，解决这个问题就是一个个在镜像中找到，然后安装上。 《Ubuntu离线安装网卡驱动》 《【菜鸟向】Ubuntu无法连接wifi问题的解决》 一步步操作之后完全没有作用，现在来看如果能起作用还怪了，这是bcm博通网卡的处理方法，而我的网卡是Intel的，又是做了无用功 更新软件列表附加驱动这种解决方案网上流传的最多，其实就是依靠Ubuntu的软件仓库来解决 Ubuntu16.04无法连接Wifi解决方案 （绝对有效） ubuntu16.04中不能连接无线网络 解决这个问题的关键就是先联网，仿佛陷入了死循环，我现在就是要解决无线网卡连不上网的问题，你却让我先联网更新，其实对于可以连接有线网络的电脑来说没什么问题，但是我这里连不上有线网啊！ 然后我就学到了一个逆天的操作，原来USB可以供网，可以通过USB线让笔记本使用手机的4G网络，瞬间打开了一扇大门，说干就干，先更新一下 Ubuntu 的软件源，可以参考下面这个文章，其实有很多源的，比如常用的清华源、阿里源等等，但是要注意版本，网上流传着一些错误版本的地址，需要自己甄别一下。 Ubuntu16.04更换国内源 这个软件源其实类似于应用商店的地址，换了软件源就相当于换了一个应用商店，还完源更新之后就可以从软件仓库中有哪些软件，下次再安装软件时就可以找到了，在这列举一个常用的Ubuntu16.04的软件源，替换 /etc/apt/source.list 文件内容就好了。 123456789101112131415deb http://mirrors.aliyun.com/ubuntu/ xenial maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial maindeb http://mirrors.aliyun.com/ubuntu/ xenial-updates maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates maindeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security maindeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe 准备就绪，USB连接手机，然后共享网络，执行 sudo apt-get update 命令，照着教程操作一通，发现并没有在附加驱动选项卡中发现可用的驱动，失败了，同时耗费了1G流量导致手机网络报警。 还是回过头来看看，很多人说这种方式解决了问题，但我没有成功，看看成功的那些截图还是博通的网卡，而我这个Intel的网卡有点难搞啊 更新内核1从现在开始渐渐步入正确轨道了，因为我发现的网上各种设置网卡参数的命令在我这没有任何反应，比如执行 rfkill list 结果显示根本没有网卡，只有两个蓝牙，执行 lshw -c network 之后发现 *-network UNCLAIMED 内容，开始搜索相关内容，然后发现了下面的帖子： Network unclaimed on Ubuntu 提问者根据回答人提供的步骤一步步解决了自己的问题，而我在安装这个内核版本时提示已经最新无法继续安装，并且在回答中提到安装 16.04.4 版本的镜像不需要做任何设置就可以使用无线网了，想到自己安装的是 16.04.2，就怀疑是这里的问题，准备安装高版本镜像 安装Ubuntu16.04.7由于Ubuntu官网下载比较慢，所以找了国内的镜像网站下载了 16.04.7版本，刻录U盘启动盘，准备安装 中科大ubuntu镜像站 几个ubuntu16.04镜像下载地址 这次安装的熟练了一些，完成后进入系统并未有什么改变，依旧连不上网，尝试更新软件试试，于是再一次连接手机USB网络，执行下面命令 12sudo apt-get updatesudo apt-get upgrade 又是几百兆流量进去了，依然没有效果 查找网卡版本至此虽然已经连不上网，但是基本确定了问题的原因，不是网卡设置的问题，而是驱动版本不对，系统无法识别网卡型号，没有办法正常工作。 可是怎么才能知道网卡的型号呢？如果正常驱动的无线网卡可以使用 lspci | grep Net 这个命令，但是我这里不行，只显示Intel设备，无法显示设备类型和具体型号，倘若我的win10还在，直接在设备管理器中查就可以了，但是他已经被我自信的格式化了，此处捶胸顿足。 那怎么办呢？问问旁边的小伙伴？关键电脑型号不一样啊！撬开电脑后盖？估计无线网卡在哪我都找不到。对了，去联想官网看看配置，应该能找到的吧！ 说干就干，打开联想官网居然找不到这个具体的型号，只能找到 ThinkPad-X390 系列，而我这个子系列是20SD，官网上有20SC、20SX、20Q0等等，就是没有20SD，没办法了，看看这个相似的这几个无线网卡都是什么型号吧，发现大多数都是 Intel® 9560AC，部分提到了 Intel® Wi-Fi 6 AX200，这两个网卡差的可是有点多，需求的Ubuntu内核版本差了一个大版本，他们的对应关系可以参考Intel提供的这个文档 Linux* Support for Intel® Wireless Adapters 联想的官网只提供Windows上的驱动下载，Linux版本只能自己去Intel上找了，现在需要确定网卡版本来下载相应的驱动，但是目前查不到这个网卡的具体型号，还能怎么办呢？去找联想客服吧！ 在联想的官网上找到了联想的售前，说明问题后给出的答案是大概率是 Intel® 9560AC 型号，然后转接技术处理，等待N久之后查了SN码说应该是 Intel® 9560AC，但是我认为应该是网卡型号应该是 Intel® Wi-Fi 6 AX200，如果是 Intel® 9560AC 型号的网卡，需要 Ubuntu的内核是 4.14+，而我安装的内核版本是 4.15.7，系统没理由不认识这个网卡。 想想还有什么办法查网卡型号呢？既然windows上可以看，那我省点事安装个WinPE进系统看一下。下载常用的微PE工具箱，烧制U盘后进了系统，点开设备管理器，一大串设备都带着问号，原来在WinPE里也没有这些设备的驱动，我又跑去联想官网下载了Windows上的网卡驱动，放到WinPE上安装，结果因缺少DLL而失败，这条路也行不通了。 只能重新做个win10的系统看看了，再次对删除win10前没看网卡型号而捶胸顿足，下载了5个多G的镜像文件，刻录Win10启动盘，然后安装发现引导不进去，也是醉了，应该是文件系统格式不正确，导致系统无法引导，算了，死马当活马医吧，我直接把无线网卡当成最高的版本，按照Intel® Wi-Fi 6 AX200进行处理，把内核升级了应该就可以了 更新内核2WinPE方式失败后决定升级内核，于是按照文章开头提供的那篇解决方案来处理，但是内核文件安装失败了，无法升级，内心崩溃…… Ubuntu应用—安装 Intel Wireless-AC 9462 无线网卡驱动（无法连接wifi，完美解决） 这一篇也是类似的，同样无法成功更新。 安装Ubuntu20.04.1既然升级内核失败了，我干脆安装个自带高内核版本的系统算了，下载了 Ubuntu20.04.1 的镜像 清华大学开源软件镜像站 经过漫长的等待下载完成，刻录Ubuntu启动盘，安装设置一气呵成，搞定了，无线网的小雨伞出现了，简直泪流满面… 然后通过 lspci | grep Net 来查看网卡型号，结果出现了 Intel Corporation Wireless-AC 9462，看到这里我都惊呆了，原来这个网卡比 Intel® 9560AC 版本还要低，联想客服的回答是错误的，Intel官方驱动的对应关系也不准确，真是没什么可信的了。 安装Ubuntu18.04.5既然无线网卡都可以识别了，为什么还要折腾呢？因为Ubuntu20.04.1这个版本跟周围小伙伴使用的系统版本差的有点多，并且之前出现过项目编译问题，所以换个低一点的版本试试。 下载镜像，刻录U盘，安装系统轻车熟路，真是越来越顺利了，安装完成后WIFI正常，果然是内核版本的事情，高版本的系统直接就可以使用，根本不需要任何设置。 再次安装Ubuntu16.04.7因为不服输，我又折腾回来了，既然是内核版本的问题，我就再试试在这个低版本系统上升级内核，这次安装时勾选了“为图形或无线硬件…安装第三方软件”，因为我突然意识到其他教程不让勾选都是台式机，而我是笔记本，我的个乖乖啊，感觉被坑了。 这次一步步操作，安装完系统就不再做任何设置，直接升级内核，访问内核软件网站，下载下面4个包： linux-headers-5.2.16-050216_5.2.16-050216.201909190832_all.deb linux-headers-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.deb linux-image-unsigned-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.deb linux-modules-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.deb 依次按照下面的命令安装，这次居然成功了 1234sudo sudo dpkg -i linux-headers-5.2.16-050216_5.2.16-050216.201909190832_all.debsudo sudo dpkg -i linux-headers-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.debsudo sudo dpkg -i linux-modules-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.debsudo sudo dpkg -i linux-image-unsigned-5.2.16-050216-generic_5.2.16-050216.201909190832_amd64.deb 然后从网站下载linux固件版本 linux-firmware_1.190_all.deb 使用如下命令安装成功 1sudo sudo dpkg -i linux-firmware_1.190_all.deb reboot重启系统，WIFI的小雨伞终于出现了，功夫不负有心人，彻底搞定了，至于WIFI可用却连不上公司的无线网，那就是另一个悲伤的故事了，和技术无关，就不在此展开了。 踩过的其他坑其实整个过程远不止上面提到的这些，安装了10几次系统，至少翻了几百个页面，有一些处理方式尝试过但没有什么效果，也列举在这里，留个纪念 Ubuntu16.04无附加驱动无法连接WiFi总结 Ubuntu-server 10.04.4查看无线网卡型号 Ubuntu安装后未发现wifi适配器解决 ubuntu16.04+intel无线网卡无法连接WiFi解决方法，下载驱动，升级内核（亲测有效） ubuntu18.04 无法连接有线 总结 现在安装Ubuntu特别溜，刻录U盘启动盘+安装完系统也就10分钟 手机通过USB连接笔记本可以直接给笔记本提供网络，这波操作有点优秀 认识了一堆之前没用过的命令，特别是 lspci 显示设备信息和 iwconfig 管理无线网络 出现问题时还是要找到根本原因，不能病急乱投医，一些博通网卡的处理方式用到Intel上显然不合适 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当个人实力撑不起自己的野心时，需埋下头来刻苦修炼，当目前的能力无法满足自己的欲望时，需抑制一些不切实际的想法，但愿多年之后回想这些隐忍的瞬间能内心充满感恩而不是悔恨~ 2021-1-16 22:48:02]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ubuntu</tag>
        <tag>系统</tag>
        <tag>WIFI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搬迁声明]]></title>
    <url>%2Fblog%2F2021%2F01%2F12%2F%E6%90%AC%E8%BF%81%E5%A3%B0%E6%98%8E%2F</url>
    <content type="text"><![CDATA[我的博客即将同步至 OSCHINA 社区，这是我的 OSCHINA ID：osc_57262839，邀请大家一同入驻：https://www.oschina.net/sharing-plan/apply]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>工作</tag>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在比较Linux和Windows命令差异时意外发现了Windows Terminal]]></title>
    <url>%2Fblog%2F2021%2F01%2F05%2F%E5%9C%A8%E6%AF%94%E8%BE%83Linux%E5%92%8CWindows%E5%91%BD%E4%BB%A4%E5%B7%AE%E5%BC%82%E6%97%B6%E6%84%8F%E5%A4%96%E5%8F%91%E7%8E%B0%E4%BA%86Windows-Terminal%2F</url>
    <content type="text"><![CDATA[前言目前工作时的开发环境是 Windows + Linux，常常需要切换测试环境，比如写一个脚本既要在 Windows 使用，同时也要在 Linux 下运行，命令的使用是 Linux 的强项，它自带了很多方便的命令程序，比如过滤字符串的 grep，统计文件行数的 wc，查看目录文件的 ls 等等，如果要在 Windows 中运行相同逻辑的脚本，那我们需要先找到这些命令在 Windows 上的等价写法。 常用命令下面列举几个常见的命令，功能上基本等价，如果有更好的写法也欢迎小伙伴们留言补充呀，有些命令在命令框里执行感觉没有作用，但是放到脚本中就很方便了，比如下面第一个： 显示当前目录 linux 中的 pwd 12albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ pwd/mnt/d/app/ScreenToGif/Logs windows 中 cd 12D:\app\ScreenToGif\Logs&gt;cdD:\app\ScreenToGif\Logs 这个显示当前目录的命令在这来看确实没什么用，因为在命令提示框里已经显示的当前所在的目录，但是用到脚本中时就可以发挥作用了，windows 中的 cd 命令不仅可以切换目录，当命令后面不接任何参数时就可以显示当前所在的目录位置。 显示目录下内容 linux 中的 ls 12albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ ls20_04_06 11_09_57_942.txt 20_04_06 11_09_58_049.txt 20_04_06.txt windows 中 dir 12345678910111213D:\app\ScreenToGif\Logs&gt;dir 驱动器 D 中的卷没有标签。 卷的序列号是 0E68-747E D:\app\ScreenToGif\Logs 的目录2020/04/06 23:09 &lt;DIR&gt; .2020/04/06 23:09 &lt;DIR&gt; ..2020/04/06 23:09 507 20_04_06 11_09_57_942.txt2020/04/06 23:09 518 20_04_06 11_09_58_049.txt2020/04/06 23:10 2,754 20_04_06.txt 3 个文件 3,779 字节 2 个目录 56,929,013,760 可用字节 这两个命令都可以显示当前目录下的内容，但是windows下的 dir 更详细一些，当然，linux 下的 ls 命令也可以附加一些参数来达到显示详细信息的目的。 过滤文本 linux 中的 grep 123albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ ls -l | grep 09_-rwxrwxrwx 1 albert albert 507 Apr 6 2020 20_04_06 11_09_57_942.txt-rwxrwxrwx 1 albert albert 518 Apr 6 2020 20_04_06 11_09_58_049.txt windows 中 findstr 123D:\app\ScreenToGif\Logs&gt;dir | findstr 09_2020/04/06 23:09 507 20_04_06 11_09_57_942.txt2020/04/06 23:09 518 20_04_06 11_09_58_049.txt 这两个命令基本上是一样的，都是按行过滤文本，当不加参数时效果基本一致 查看文件内容 linux 中的 cat 12345678910111213141516171819albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ cat "20_04_06 11_09_57_942.txt"► Title - Automatic feedback▬ Message - Value cannot be null.Parameter name: path1○ Type - System.ArgumentNullException♦ [Version] Date/Hour - [2.14.1] 04/06/2020 23:09:57▲ Source - mscorlib▼ TargetSite - System.String Combine(System.String, System.String, System.String)♠ StackTrace - at System.IO.Path.Combine(String path1, String path2, String path3) at ScreenToGif.Model.ApplicationViewModel.SendFeedback()---------------------------------- windows 中 type 12345678910111213141516171819D:\app\ScreenToGif\Logs&gt;type "20_04_06 11_09_57_942.txt"► Title - Automatic feedback▬ Message - Value cannot be null.Parameter name: path1○ Type - System.ArgumentNullException♦ [Version] Date/Hour - [2.14.1] 04/06/2020 23:09:57▲ Source - mscorlib▼ TargetSite - System.String Combine(System.String, System.String, System.String)♠ StackTrace - at System.IO.Path.Combine(String path1, String path2, String path3) at ScreenToGif.Model.ApplicationViewModel.SendFeedback()---------------------------------- 统计文件行数 linux 中的 wc -l 12albert@home-pc:/mnt/d/app/ScreenToGif/Logs$ cat "20_04_06 11_09_57_942.txt" | wc -l19 windows 中 find /c /v &quot;&quot; 12D:\app\ScreenToGif\Logs&gt;type "20_04_06 11_09_57_942.txt" | find /c /v ""19 wc 这个命令没什么好说的，专门为统计字符数、字节数、行数而生的，而 windows 下的 find 命令通过 /c 显示匹配函数，/v &quot;&quot; 查找非空行这样的参数组合也能达到统计行数的目的。 计算文件摘要 linux 中的 [md5sum|sha1sum|sha256] 文件名 1234567albert@home-pc:/mnt/d/data/cpp$ md5sum simple.cppa00eba0276e396de58fabc92b325672a simple.cppalbert@home-pc:/mnt/d/data/cpp$ sha1sum simple.cpp7acf1e59ca2608b7591ec526d48ce041cddf49d2 simple.cppalbert@home-pc:/mnt/d/data/cpp$ sha256sum simple.cppf07bcd585fa7e49897676105797dd984c12f63411f0ba8db62a57f6ef03bbaec simple.cppalbert@home-pc:/mnt/d/data/cpp$ windows 中 certutil -hashfile 文件名 [MD5|SHA1|SHA256] 1234567891011121314d:\data\cpp&gt;certutil -hashfile simple.cpp MD5MD5 的 simple.cpp 哈希:a00eba0276e396de58fabc92b325672aCertUtil: -hashfile 命令成功完成。d:\data\cpp&gt;certutil -hashfile simple.cpp SHA1SHA1 的 simple.cpp 哈希:7acf1e59ca2608b7591ec526d48ce041cddf49d2CertUtil: -hashfile 命令成功完成。d:\data\cpp&gt;certutil -hashfile simple.cpp SHA256SHA256 的 simple.cpp 哈希:f07bcd585fa7e49897676105797dd984c12f63411f0ba8db62a57f6ef03bbaecCertUtil: -hashfile 命令成功完成。 在 linux 上不同的摘要算法对应着不同的程序，但是在 windows 上是同一个程序有着不同的参数。 其他命令 功能 linux命令 windows命令 拷贝 cp copy/xcopy 移动 mv move/rename 删除 rm del/rd 创建目录 mkdir md/mkdir 先整理这么多，后续用到新的等价命令再补充吧！ 半路杀出个PowerShell一开始接触 PowerShell 把它认为是 cmd 的升级版，感觉就是原来的 cmd 太寒酸了，然后又搞出个 PowerShell 稍微好看了一点，功能又增加了一些而已，但是随着使用次数的增多，我发现之前的理解不太对，PowerShell 和 cmd 它俩完全没关系啊，唯一的相同点就是都被叫做命令行而已。 PowerShell 通常被描述成是面向对象的脚本语言，并且是在 .Net公共语言运行时(CLR-Common Language Runtime)和.Net Framework的基础上构建的，可以接受和返回.Net对象，引入了cmdlet的概念，是cmd的超集。 看到一些说法提到“cmd命令都可以在Powershell中执行”，但事实并非如此，比如我最近发现在cmd运行的 type a.txt | find /c /v &quot;&quot; 统计行数的命令在 PowerShell 中就不管用了，所以我认为把 PowerShell 作为 cmd 的升级不太准确。 PowerShell完全是一个新发明的东西，它里面有一些原来cmd中常用的命令，比如 cd、dir，同时它还兼容了一些 Linux 上的常用命令，比如 ls、man 等，关于这些可以简单看几个例子。 显示目录下内容123456789101112131415161718192021222324albert@home-pc D:\app\ScreenToGif\Logs&gt; ls 目录: D:\app\ScreenToGif\LogsMode LastWriteTime Length Name---- ------------- ------ -----a---- 2020/4/6 23:09 507 20_04_06 11_09_57_942.txt-a---- 2020/4/6 23:09 518 20_04_06 11_09_58_049.txt-a---- 2020/4/6 23:10 2754 20_04_06.txtalbert@home-pc D:\app\ScreenToGif\Logs&gt; dir 目录: D:\app\ScreenToGif\LogsMode LastWriteTime Length Name---- ------------- ------ -----a---- 2020/4/6 23:09 507 20_04_06 11_09_57_942.txt-a---- 2020/4/6 23:09 518 20_04_06 11_09_58_049.txt-a---- 2020/4/6 23:10 2754 20_04_06.txt 我们可以看到这两个命令在 PowerShell 中执行之后结构完全一样，然后我们使用 man 命令来查看一下两个命令的帮助文档 查看帮助文档123456789101112131415161718192021222324252627282930313233343536373839404142434445464748albert@home-pc D:\app\ScreenToGif\Logs&gt; man dir名称 Get-ChildItem语法 Get-ChildItem [[-Path] &lt;string[]&gt;] [[-Filter] &lt;string&gt;] [&lt;CommonParameters&gt;] Get-ChildItem [[-Filter] &lt;string&gt;] [&lt;CommonParameters&gt;]别名 gci ls dir备注 Get-Help 在此计算机上找不到该 cmdlet 的帮助文件。它仅显示部分帮助。 -- 若要下载并安装包含此 cmdlet 的模块的帮助文件，请使用 Update-Help。 -- 若要联机查看此 cmdlet 的帮助主题，请键入: "Get-Help Get-ChildItem -Online" 或 转到 https://go.microsoft.com/fwlink/?LinkID=113308。albert@home-pc D:\app\ScreenToGif\Logs&gt; man ls名称 Get-ChildItem语法 Get-ChildItem [[-Path] &lt;string[]&gt;] [[-Filter] &lt;string&gt;] [&lt;CommonParameters&gt;] Get-ChildItem [[-Filter] &lt;string&gt;] [&lt;CommonParameters&gt;]别名 gci ls dir备注 Get-Help 在此计算机上找不到该 cmdlet 的帮助文件。它仅显示部分帮助。 -- 若要下载并安装包含此 cmdlet 的模块的帮助文件，请使用 Update-Help。 -- 若要联机查看此 cmdlet 的帮助主题，请键入: "Get-Help Get-ChildItem -Online" 或 转到 https://go.microsoft.com/fwlink/?LinkID=113308。 结果还是完全一样，原来在PowerShell中，ls 和 dir 都是命令 Get-ChildItem 的别名，这时我才发现，原来PowerShell自己的命令其实都是这种动宾短语的形式，原来 cmd 中的命令和一些扩展的 Linux 命令大多是以别名的形式存在的。 所以从这里来看，PowerShell是一款新产品，并不是cmd的简单升级，只是这个新的产物将原来大部分的cmd命令以别名的方式进行了兼容处理而已 统一命令行的渴望上面提到了 cmd、PowerShell、linux Shell 等等，每天使用这些工具时都要开很多窗口，比较麻烦，有没什么途径把它们统一一下呢？这里的统一只是想使用的更方便而已，语法上暂时没办法统一了，偶然间发现了 Windows Terminal 这个工具，一下子打开了新的世界。 Windows TerminalWindows Terminal 是一个全新的、功能强大的命令行终端工具。包含多 Tab 支持、富文本、多语言支持、可配置、主题和样式等诸多特性，并且属于微软的亲儿子，支持的力度你懂得。 安装直接在 Windows 上打开应用商店安装即可，这时我第二次打开应用商店了，上一次是安装 WSL 的时候。 官方提供的截图也很炫酷： 我自己也尝试着美化了一下，效果还不错 命令行窗口的统一确实不想开很多个命令窗口，比如执行windows脚本命令的cmd、PowerShell，版本管理工具git bash，windows子系统WSL命令行，远程服务器命令行工具XShell等，要是把它们都放在一起就好了，使用Windows Terminal可以轻松实现这个愿望，其中cmd、PowerShell、WSL命令行都是Windows Terminal自带的，git bash 要想显示在 Windows Terminal 中需要手动配置下，运行效果如下： 配置和美化方法比较简单，网上搜索会有很多教程可以学习 遇到的问题目前使用 Windows Terminal 替代各种命令行工具只遇到了一个问题，就是它不知道 Zmodem 协议，所以无法使用 sz、rz，对于我这种需要经常上传脚本文件到服务器的人来说不太方便。 好消息是在2019年的时候已经有人在 Windows Terminal 的 github 项目中提了issue，据说会考虑这个功能，但是还得评估下，但是目前已经1年多过去了还没有反馈，焦急等待中，有了它就可以不使用XShell了。 总结 cmd 和 PowerShell 是两个不相同的命令行程序，有些cmd命令无法直接在PowerShell中运行 cmd 能做的 PowerShell 几乎都能做，但是不能直接拿过来就运行，有时需要简单的改写才可以 PowerShell 的命令采用动宾的格式，并且使用别名的形式兼容 cmd 命令和一些 Linux 命令 Windows Terminal 目前来看很优秀，可以将cmd、PowerShell、WSL Shell等运行在同一个窗口中 由于 Windows Terminal 不支持 Zmodem 协议，所以暂时还不能使用 sz、rz 命令来传送文件 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 不过是大梦一场空，不过是孤影照惊鸿，不过是白驹之过一场梦，梦里有一些相逢……歌词写的真好！ 2021-1-10 00:33:11]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>windows</tag>
        <tag>ternimal</tag>
        <tag>powershell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020年终总结！新的起航，新的征程]]></title>
    <url>%2Fblog%2F2020%2F12%2F27%2F2020%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%EF%BC%81%E6%96%B0%E7%9A%84%E8%B5%B7%E8%88%AA%EF%BC%8C%E6%96%B0%E7%9A%84%E5%BE%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[奇变偶不变，符号看象限，真正的知识就是这么朴实无华~ 前言从2019年开始意识到年终总结的意义，通过反思自己的得失进而确定今后的方向，既然去年开了头，今年也不能中断，依旧写写每天的流水账，细数最近一年发生的故事。 今年最直观的、印象最深的感受就是时间不够用，一首《时间都去哪了》旋律时常出现在脑海中，对比之前长久不变的工作环境，今年确实紧张了许多，习惯了原来的安逸，对各种可能发生的问题轻车熟路的就能解决掉，但来到新环境，一切变得不是那么轻松了。 回顾2020毕竟到了年终时刻，之前的flag可不能忘，依旧是从工作、学习、生活三个方面来回忆一下去年flag的完成情况。 工作 2020 flag：新的一年不能再碌碌无为，真的需要去闯一闯了 完成度：95% 年初如愿的换了一份新工作，之前的公司呆的时间太长了，工作内容单一，熟悉了安逸的环境，虽然每天也在学习，但是进步缓慢，在2019年末开始寻找新的发展环境，在2020初就成功找到了新的工作机会，提前完成flag内容。 与老东家分手过程可以说不太愉快，给不了梦想，撑不起现实，一味的拖着可不是解决问题的办法，不过事情已经过去，不想再提，毕竟也为之奋斗过，近2000个日日夜夜，梦开始的地方，也是认清现实的地方。 离开原公司来到新公司算是无缝衔接吧，中间在家只待了3天，如果能早点意识到离职流程这么狗血，我真应该在家躺一个月，不过新公司的入职流程还真是暖心，提前先网上入职，帮我把社保问题解决了，感谢流程中出现的每一个人。 新的工作内容刚开始肯定要吃力一些，首先是熟悉开发环境，大体和原来相同，不过复杂的申请制度较原来规范了许多，同时也麻烦了许多，版本控制是一个重要的改变，之前主要使用SVN，如今换成了 Git + SVN，以前经常使用的 git 三部曲——add、commit、push 已经不够用了，随着不断的使用学习，渐渐熟悉了 checkout、merge、rebase、cherry-pick reset 等等子命令，还是那句话多用才能学会，如果只停留在纸面上，只是眼睛学会了，闭上眼睛就忘记了。 选择这份新的工作也是想拓宽自己的知识面，之前一直在做分区分服的游戏，想学习一下分布式游戏的开发和一些需要需要注意的问题，所以说这份工作是用来补充我这方面知识短板的，从学习了一年的结果来看，确实达到了这个目的，虽然现在的规模还没有那么大，有些逻辑来不太完善，但是这已经给我做出了示范，在今后的一段时间内，将沿着这条路不断的前行，去探索自己未知的领域。 新的工作认识新的伙伴，大家相处起来还是很友好的，一年来的工作算是中规中矩，不过年底这俩月有点滑铁卢的味道，接连在同一个位置摔倒几次，还是老大给背了锅，内心愧疚不已，无以为报，愿以更饱满的热情投入工作中，实践当初并肩战斗的诺言，得一伙伴不易，愿共同进步。 今年的工作强度明显高于以往，倒不是工作时间长了，而是在新环境中，长时间聚精会神的盯在一件事上本来就很累，头发掉的明显比之前多了，眼睛的视力也下降的厉害，这两点要注意一下了，可以调整一下工作的节奏，注意劳逸结合，我可不想“聪明绝顶”。 学习 2020 flag：看两本有关分布式知识的图书，多看一些开源项目的代码 完成度：85% 有关分布式架构的书今年只看了一本，另外基本都是和育儿知识相关，他们分别是： 从零开始学架构 ——照着做，你也能称为架构师 你就是孩子最好的玩具 正面管教 小狗钱钱2 今年的1本技术书籍对比去年的7本少了很多，一方面换了新工作后需要花很多时间来熟悉新的业务逻辑，用来读书的时间被侵占了一部分，另一方面宝宝一天天长大，需要花些时间去陪伴她，虽然技术书读的少了，可是故事书我可没少读，一年读了好几十本： 这些故事书中描述的故事小时候没觉得有什么不妥，不过以现在成年人的身份来看，有些故事太离奇了，比如大灰狼和七只小羊的故事，山羊妈妈居然剪开狼的肚皮救出了被吃掉的六只小山羊，然后在狼的肚子里装满石头缝上了，整个过程大灰狼都没有醒，我感觉山羊妈妈可能是个麻醉科的护士。 虽然故事内容离奇，但是并不妨碍小娃娃听到津津有味，小孩子的世界还真是单纯，有故事听就老老实实的等着，而我化身为一个播报员，一遍一遍的重复着书中的故事。 博客总结今年也没有丢，一共写了45篇，比去年还要多几篇，总体来看类别很多，总结的内容并不难懂，写到博客中主要是为了方便日后的查找，这一年的总结绝大部分都是晚上和周末花时间写的，只有尝试过才明白，想写好一篇总结需要花费很多很多时间，今年的成长可以对比两年数据来看一下。 C++11的使用在这一年里变得更加熟练，之前的项目中无法使用C++11的特性，所以很多知识都是自己额外花时间来测试学习，来到新项目可以参考已有的代码，进一步巩固C++11的使用，知识还是越用掌握的越好。 语言方面还有Python今年用的比较多，得益于工作中的多次锻炼，一些常用的函数，类型可以很熟练的写出来了，虽然没有用来写过什么大的软件，但是也在不断尝试着使代码更规范，比如使用class、装饰器、继承、记录运行日志等等。 Go语言今年算是简单入门吧，之前只能算是听说过，今年快速的浏览了一遍Go语言的语法，编写了一些测试程序，但是对于Go语言的条条框框还是不太习惯，需要慢慢适应下。 刷题今年一直在坚持着，之前使用的国际账号一时找不到了，今年新注册了国内版LeetCode账号，本着刷简单冲中等的态度，一直在默默的洗刷刷，不过今年参加了几次竞赛，一直是两题选手，只有一次题比较简单全答出来了，当时还是很开心的，继续加油吧。 阅读开源项目源码方面，今年也有了很大的进步，在新项目中接触了好几个之前没有使用过的开源库，虽然没有完全整明白，但最起码开阔了眼界，比如 easyloging++、nolhmann json库等等，另外今年还意外获得了 Github 的 Arctic Code Vault Contributor 勋章，看来我写的BUG要被雪藏千年了，哈哈。 生活 2020 flag：尽最大可能陪陪家人、投资达到2019的水平 完成度：55% 多陪陪家人这一项今年应该算完成了，因为疫情今年也没有出去疯，就是出门买买菜，其他的闲暇时间都和家人在一起，天气好的时候去附近的公园逛逛，对比2019年，每天晚上回家的时间晚了一点，但是多了每周多了一整天可以和家人一起度过。 现在每天还是很充实的，早上起来妈妈已经做好早饭，赶紧洗漱和家人一起吃个早饭，然后和宝宝告别去上班。晚上回来大多数情况宝宝已经睡了，不过有时候也能挺到我回家，和我玩一会再睡。周末陪宝宝出去玩，透透新鲜空气，等宝宝睡觉的时候对近期所学的知识做一个总结，基本上周末的时间陪娃、总结55开吧。 这个flag完成度不高主要是投资理财这块今天基本上是停滞了，未达到2019年水平，每天除了工作就是学习，账户情况没怎么看，整个一个过山车行情，所以基本上处于不赔不赚的情况。 节前的最后一天是大涨行情，算是一个好的结尾，全年放养的状态收益率3个多点，勉强跑赢余额宝的收益，估计跑不过今年的通货膨胀了！ 目前的生活状况就是每天都相似，但确实很满足，早上起床一家人吃早饭，和宝宝告别后送媳妇上班，然后自己骑个自行车来公司上班，努力完成一天的工作再骑个小车回家，如果宝宝没睡还可以陪她玩一会儿，等宝宝睡着开始一天的总结，温暖而又充实。 2020年初养了一年的栀子花开花了，正好在我过年回老家的时候开的花，之前我好好浇水通风的时候它却连个花骨朵都没有，好像再告诉我只要我不管它就能好好开花，可是在10月份的时候枯死了。还有一盆文竹在夏天涨势良好，修剪了几次，但没有熬过寒冷的冬天，在11月份左右干枯了。 在上两盆花相继离开之后，我赶紧又补充了新的生机，一盆栀子花和一盆茉莉花，目前长势良好，茉莉花已经开花了，希望它俩在新的一年里花香不断。 展望2021工作 脚踏实地做好本职工作 额外挤出时间去尝试技术提升（优化、解决痛点） 在熟悉业务的同时更多参与设计的工作，拓宽自己的认知范围 学习 博客总结继续，基本保持在1周一篇，可以适当偷懒，一年懒10次可以产出40篇 开源代码还是要继续学习，libevent需要详细看一下，今年的出镜率太高 读2本技术类书籍，可以是开阔眼界的，也可以是现有技能提升的 读2本经济学、金融理财相关的书籍 生活 陪娃娃，陪家人，工作内容适应后可以多拿出一点时间和家人在一起（需要比2020多一些） 投资理财还是要多花一点时间研究下，目标7%（靠工资是不可能财富自由的，必须开源才行） 注重身体的保养，身体是本钱，可不能把身体搞垮了，愿丢掉体检时的小毛病~ 总结 2020年在工作上是一个新的开始，同时也面临着新的挑战 2020年的flag完成度大概70%，大部分愿望已经实现，未实现部分还需努力 2021年已经悄然开始，新的flag已经在路上，为了新的目标加油努力吧 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人生旅途中运气与实力都很重要，但是强大的实力可以帮助你提升运气，减少不确定性（记一次事故后提心吊胆的等待），比如买一注彩票中一千万很难，但是如果你的实力可以强大到买下大部分甚至是所有的组合，那么要中一千万只需要等到开奖就可以了~ 凡是过去，皆为序幕 2020-12-27 00:32:07]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[system_clock::now()和time()时间函数混用带来的踩坑经历]]></title>
    <url>%2Fblog%2F2020%2F12%2F13%2F%E6%97%B6system-clock-now-%E5%92%8Ctime-%E6%97%B6%E9%97%B4%E5%87%BD%E6%95%B0%E6%B7%B7%E7%94%A8%E5%B8%A6%E6%9D%A5%E7%9A%84%E8%B8%A9%E5%9D%91%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[前言时间是一个可怕的东西，听说能用来杀猪。在编程世界中，时间也控制着一个维度，常常伴随着程序运行而流逝，有时也会影响着程序的运行的逻辑，所以在程序中处理时间时还是要仔细一些，最近连续踩坑，总结一下给自己提个醒，有些逻辑还是需要抱着怀疑的态度去看待。 时间函数混用我们在写一个小程序时基本不会去混用时间函数，比如只用 time(NULL) 去控制时间，或者只使用 chrono::system_clock::now() 来记录时间消耗，关于 chrono 的用法，之前简单总结过，可传送至 C++11中的时间库std::chrono。 但是当程序变得复杂起来，这个时间函数混用的高压线还是有可能触碰到的，当程序逻辑对时间要求越发精确时，混用所带来的后果将越发严重。在此记录一个结果：连续调用 time(NULL) 和 chrono::system_clock::now() 两个函数得到的时间戳可能是不同的。 可能你会说，函数是先后调用的，肯定是不同的，后面的函数调用时的时间戳要比前面的大，但事实却是两个函数所取得的时间戳大小不确定，可能是第一个函数的时间戳比较大，也可能是第二个时间戳更大一些。 测试的例子下面展示一段代码，先后调用两个时间函数，打印所获得的时间戳，可以看看有什么特点： 12345678910111213141516171819#include &lt;stdint.h&gt;#include &lt;sys/time.h&gt;#include &lt;time.h&gt;#include &lt;iostream&gt;#include &lt;chrono&gt;using namespace std;int main()&#123; int64_t t1, t2; while (true) &#123; t1 = chrono::duration_cast&lt;chrono::milliseconds&gt;(chrono::system_clock::now().time_since_epoch()).count(); t2 = time(0); if (t1/1000 != t2) cout &lt;&lt; t1 &lt;&lt; " " &lt;&lt; t2 &lt;&lt; endl; &#125; return 0;&#125; 编译运行结果如下： 1234567891011albert@home-pc:testtime$ g++ testtime.cpp -std=c++11albert@home-pc:testtime$ ./a.out1607779917993 16077799181607779957999 16077799581607780080001 16077800791607780103001 16077801021607780150001 16077801491607780202001 16077802011607780327999 16077803281607780440001 1607780439... 运行之后很快就出现了一些不一致，对比可以发现，两个时间戳一个是毫秒，一个是秒，同时把单位转化成秒来比较时，两者大小不定，从仅有的这几行结果来看，最大的误差是7毫秒。 再加一个时间函数除了上面提到的两个函数，还有一个 gettimeofday() 函数也是在获取时间时常常使用的，把它也放到测试函数中对比一下： 1234567891011121314151617181920212223#include &lt;stdint.h&gt;#include &lt;sys/time.h&gt;#include &lt;time.h&gt;#include &lt;iostream&gt;#include &lt;chrono&gt;using namespace std;int main()&#123; int64_t t1, t2, t = 0; struct timeval tv; while (true) &#123; t1 = chrono::duration_cast&lt;chrono::milliseconds&gt;(chrono::system_clock::now().time_since_epoch()).count(); t2 = time(0); gettimeofday(&amp;tv, NULL); if (t1/1000 != t2 || t2 != tv.tv_sec) if (t != t1) cout &lt;&lt; t1 &lt;&lt; " " &lt;&lt; t2 &lt;&lt; " " &lt;&lt; tv.tv_sec &lt;&lt; "," &lt;&lt; tv.tv_usec &lt;&lt; endl; t = t1; &#125; return 0;&#125; 运行后查看结果： 1234567891011121314151617181920albert@DESKTOP-6746UC3:/mnt/d/data/cpp/testtime$ g++ testtime.cpp --std=c++11albert@DESKTOP-6746UC3:/mnt/d/data/cpp/testtime$ ./a.out1607876993000 1607876992 1607876993,21607876994000 1607876993 1607876994,31607876995000 1607876994 1607876995,31607876996000 1607876995 1607876996,21607876997000 1607876996 1607876997,11607876998000 1607876997 1607876998,21607876999000 1607876998 1607876999,21607877000000 1607876999 1607877000,31607877001000 1607877000 1607877001,11607877002000 1607877001 1607877002,31607877003000 1607877002 1607877003,21607877004000 1607877003 1607877004,21607877005000 1607877004 1607877005,11607877006000 1607877005 1607877006,31607877007000 1607877006 1607877007,21607877008000 1607877007 1607877008,111607877009000 1607877008 1607877009,3... 真是各不相同，这要是在发射火箭时混用两个时间函数，那估计探月卫星就凉凉了…… 总结 常用来获取时间戳的函数有 time()、chrono::system_clock::now() 和 gettimeofday() 时间函数不要混用，否则会给精密计算带来巨大的麻烦，造成计算结果的不可控 测试发现 chrono::system_clock::now() 和 gettimeofday() 时间非常接近，有微秒级别的误差，但也不建议混用 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有的人走了只留下一撮灰烬，有的人离开却千古留名，但在时间长河中就是那么一瞬，意义何在，有差吗？ 2020-12-14 00:12:01]]></content>
      <tags>
        <tag>C++</tag>
        <tag>时间</tag>
        <tag>time</tag>
        <tag>system_clock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++中有符号数隐式类型转换成无符号数需注意的问题]]></title>
    <url>%2Fblog%2F2020%2F12%2F07%2FC-C-%E4%B8%AD%E6%9C%89%E7%AC%A6%E5%8F%B7%E6%95%B0%E9%9A%90%E5%BC%8F%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E6%88%90%E6%97%A0%E7%AC%A6%E5%8F%B7%E6%95%B0%E9%9C%80%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言隐式类型转换转换是一个挺基础的概念，即使对于初学者来说都不会陌生，一般情况下是指数据类型的转换是由编译系统自动进行的，不需要人工干预的类型转换方式。与之相对的是强制类型转换，在进行转换时必须使用强制类型转换运算符进行转换，这种也被称为显式转换。 举例隐式转换12short sn = 999;int n = sn; 显示转换12float f = 110.741f;int n = (int)f; 这两种转换方式平时经常用到，不管是函数传参时进行转换，还是数学计算时进行强转，一直也没有发现有什么问题，直到昨天遇到了一个有符号数隐式转换成无符号数时，才发现这里也是一个知识盲点，当时脑瓜儿嗡嗡的，怎么连隐式类型转换也这么陌生了呢？ 其实隐式类型转换一般发生在小类型转换成大类型时，有个常用的关系链 char -&gt; short -&gt; int -&gt; long -&gt; float -&gt; double，当关系链条中出现无符号数字时，情况有些难以理解了（实际上是有符号数字的锅）。 问题看一下这几行代码，如果你能准确说出程序的输出值，那么你已经掌握了这个知识点，后面的内容可以不用看了： 1234567891011#include &lt;iostream&gt;using namespace std;int main()&#123; char c = 128; unsigned int n = c; cout &lt;&lt; n &lt;&lt; endl; return 0;&#125; 这段代码的输出值是 4294967168，发生了啥？也就是说老板给你发工资时，本来想发128块，但是发工资的函数参数是 unsigned int 类型的，结果就给你发了 4294967168，一下就实现了40多个小目标。 查找原因针对上面的代码我们改写一下，把变量 c 换成无符号类型： 1234567891011#include &lt;iostream&gt;using namespace std;int main()&#123; unsigned char c = 128; unsigned int n = c; cout &lt;&lt; n &lt;&lt; endl; return 0;&#125; 这次的输出值变成了 128, 符合我们的预期，回过头来再看看刚才出错的代码，区别就是变量c是否有符号，结果差了好几十亿。 这里导致结果差异的原因实际上是符号位引起的，如果是无符号数字，从小类型到大类型隐式类型转换的结果数字都不会变，但是如果是有符号的数字，在转换成大类型数字的时候就要考虑符号位了，就以第一段代码为例来解释这个现象。 char c = 128; 这一句实际上已经超出了变量 c 的范围，因为变量c是有符号数字，所以它的范围是-128~127，这里赋值成128，实际在内存中的bit排列是 10000000，而有符号数的第一位bit表示正负号，这里是1表示这是一个负数，计算机存储负数是以补码的形式存储的，那么把这个数据按位取反再加1，得到 1000000 还是原来的数字，好神奇哦！ 不过这里就可以计算出 c 实际上代表-128，那么它在隐式类型转换成更大的有符号数字时，需要保证值不变，一个int的-128怎么表示呢？根据补码的定义应该是11111111 11111111 11111111 10000000，这个数字再转换成 unsigned int 就是前面提到的 4294967168 啦。 总结 有符号数字在转换成范围更大的无符号数字时需要注意转换所得数值是否正确，失之毫厘差之千里。 总结一个规律，有符号的整形数字在进行隐式类型转换时实际上是在数字的二进制表示前面补充符号位。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 一个人不能做完所有的事情，但是所有人都可以做一些事情，怕什么真理无穷，进一寸有进有一寸的欢喜~ 2020-12-8 00:04:05]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>符号</tag>
        <tag>隐式类型</tag>
        <tag>转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单聊聊01世界中编码和解码这对磨人的小妖儿]]></title>
    <url>%2Fblog%2F2020%2F11%2F28%2F%E7%AE%80%E5%8D%95%E8%81%8A%E8%81%8A01%E4%B8%96%E7%95%8C%E4%B8%AD%E7%BC%96%E7%A0%81%E5%92%8C%E8%A7%A3%E7%A0%81%E8%BF%99%E5%AF%B9%E7%A3%A8%E4%BA%BA%E7%9A%84%E5%B0%8F%E5%A6%96%E5%84%BF%2F</url>
    <content type="text"><![CDATA[前言在程序员生活的01世界中有两大Boss级难题，分别是缓存失效和命名问题，对比这两大难题来说，编码和解码只能算是小妖儿了，只不过这两个小妖儿出镜率很高，有时确实很磨人的，得多花些时间捋顺一下。 编码问题不仅仅出现在计算机中，广义的说，编码问题涉及到人类社会的方方面面，比如古人规定指定长度是一寸，然后规定十寸为一尺，其实就是当时人们对长度的一种编码，但是由于每个地方的编码不统一，导致人们在交流的时候出现了很多问题，直到秦始皇统一了文字、度量衡，相当于统一了描述当时社会的编码，使得知识和文明得以快速传播。 计算机中的编码今天想说的编码和解码特指计算中使用的编码和解码，通俗点说：编码是给计算机看的，解码是为了让人能看懂的。可能大家对这句话还不太理解，不过没关系，这个说法本身不太严谨，也可以举出一些反例，但是大部分情况下确实是这样的。 为什么要编码，想想谍战片里近代社会中的发电报过程，滴答、滴滴答、滴滴滴答答就这个样子，怎么来表达“敌人发动进攻了”，这时候就用到了编码，提前约定好“滴答”代表“敌”，“滴滴答”代表“人”，这样在收到“滴答、滴滴答”你就知道了“敌人”这个信息，那个密码本记录的内容和规则其实就是对所有电传信息的一种编码。 计算机中的编码也是一样的，从我们开始接触到计算机的时候就听说过计算机只认识0和1，虽然现代计算机技术发展迅速，但是计算机只认识0和1这一点一直未变，所以你想让他看懂你的信息，保存你的数据，就要把这些信息和数据编码成0和1，计算机才能进行处理和存储。 所以计算机中为什么要对数据进行编码，这里可以给一个狭义的理解：计算机编码是为了让数据便于传输、存储和处理。 那有为什么要进行解码呢？其实就是为了人能看懂，给你一串二进制 01010111100011111111...，相信你即使有最强大脑也不能迅速把所有数据解开，这可能是一篇优美的散文、一幅美丽的图画，或者是一部励志的电影，这一切都需要解码后才能知道。 本来想画一幅“编码”和“解码”这两个小妖的画像，但是作为灵魂画手的我还没构思好，此处留空，后面补充。。。 补上了&gt;&gt; 初识编码问题自从接触计算机就开始接触编码问题，比如你抄同学发来的作业文档，打开后却发现是一堆乱码，那时仅仅知道是编码错了，但是不知道怎么解决，或者直接让同学再发一份算了，后来在工作中需要做游戏多语言版本时才真正开始处理编码问题。 解决第一个编码问题大概是14年，当时做上线游戏的多语言版本、配置文件中的中文保存为 ANSI 编码，相同的配置文件放到日韩的系统上居然变成了其他的含义，查询解决方案决定使用 UTF-8 编码来保存配置文件，所以当时利用工具将所有的配置文件转换成了UTF-8编码，也是那个时候第一次接触到了Python，转换之后将其中的中文翻译成日韩的语言，从此知道了 UTF-8 这个编码方式，也清楚了在中日韩、越南、缅甸这个圈做产品，千万要远离 ANSI 编码。 其实 ANSI 并不是某一种特定的字符编码，而是一个编码集合，在不同的系统中，可以表示不同的编码，比如英文系统中的 ASCII编码，简体中文系统中的 GBK编码，韩文系统中 EUC-KR 编码等等 编码变迁小八卦计算机是美国人发明用于科学计算的，所以他们也是第一批考虑编码的，而英文只有26个字母，所以他们发明了ASCII码，只使用了0-127这128个空间就表示了所有可能用的字符，但后来计算机技术飞速发展，已经不仅仅用于科学计算，已经融入到社会的方方面面，并且迅速在全球流行。 随着计算机火遍全球，其它国家发现自己国家经常使用的字符，在 ASCII 码中找不到啊，于是就有人想啊，ASCII 码中的一个字节中不是才用了一半吗，我们使用这个最高位来扩展把，于是很多国家就开始用最高位来扩展这个 ASCII 编码以便能够表示自己国家的一些字符，但是对于我博大精深的中国文化来说，这一个字节远远不够啊，我们的汉字那可就有好几万个，你就给我一个字节，我肯定不干。 既然一个字节搞不定，那我们就用两个字节好了，我们规定一个小于等于127的字符的意义与原来相同，此处为了兼容ASCII码，但两个大于127的字符连在一起时，就表示一个汉字，前一个字节从0xA1用到0xF7，后面一个字节从0xA1到0xFE，我们将常用的6000多汉字在这个范围内定义代码点，并将这种编码方式称为 GB2312。 在 GB2312 这种编码中我们考虑了数学符号、希腊字母、全角标点等等，但是只有简体字没有繁体字啊，这下对面海岸的同胞们不乐意了，自己搞了一套 Big5 编码，用来处理繁体字。 后来随着电脑深入各个领域，常用汉字已经不能满足使用需求了，所以又把 GB2312 编码中没有使用的位置拿出来又进行代码点定义，处理了20000多个汉字，发明了 GBK 编码，但没过多久（2000年）发现还是不够用，又提出了变长的 GB18030 编码，每个字符占用1、2、4个字节。 大统一的Unicode刚刚简单提到了在中日韩这个圈里，每个国家都对 ASCII编码进行了扩充，也就是对 ANSI 编码进行了自己的定义，通常是用两个字节来表示一个文字和符号，这样就出现了一种情况，相同的两个字节在不同的系统上显示了不同的文字，如果每个国家的人只使用自己的语言也是没问题的，但是当中日韩文字混排的时候就出现了问题，这两个字节到底应该转换成中日韩哪个国家的符号呢？ 为了解决这种混乱的局面，大佬们设计了一种名为 Unicode 的字符集，又称万国码或者统一码。Unicode 的诞生是为整合全世界的所有语言文字。理论上任何字符在Unicode中都对应一个值，这个值被称为代码点，通常写成 \uABCD 的格式。 UCS-4 和 UCS-2起初使用两个字节来表示代码点，其取值范围为 \u0000～\uFFFF，这种文字和代码点之间的对应关系被描述为UCS-2，也就是 Universal Character Set Coded in 2 octets 的缩写，最多可以记录65536个字符的代码点。 后来为了能表示更多的文字，人们又提出了UCS-4，即用四个字节表示代码点。它的范围为 \u00000000～\u7FFFFFFF，其中 \u00000000～\u0000FFFF和UCS-2是一样的。 从这里可以看出 UCS-4 与 UCS-2 只是一种扩展的关系，UCS-4 是兼容 UCS-2 的，在 UCS-2 的每个代码点加入两个值为0的字节就变成了 UCS-4。 UCS-2 LE 和 UCS-2 BE这里的 LE 和 BE 指的是计算机中常提到的小端字节序和大端字节序，因为 UCS-4 是 UCS-2 的扩展，所以 UCS-4 也存在大端和小端的问题。 小端字节序，是指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中，而大端字节序，是指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中，这和我们平时的阅读习惯一致。 如果没接触过大端和小端可能会有点懵，举个例子就明白了，C++中一个int类型的数字通常占4个字节，假如一个int类型的变量值是256，那么他再内存中是怎样表示的呢？我们知道计算机中除了1就是0，这在计算机中对应一个bit，而计算机中表示数据的单位是字节，每个字节有8个bit大小，那么int变量值 256 翻译成二进制是 00000000 00000000 00000001 00000000 一共占用4个字节。 对照前面大端和小端的定义，这4个字节在内存中如果从高到低排列，就是小端字节序，如果这4个字节在内存中如果从低到高排列，就是大端字节序。因为UCS-2是两个字节表示一个代码点，所以在表示的时候存在字节排列顺序问题，对于值为 256 的这个代码点，可以是0x0100，也可以是0x0001。 Unicode 和 UCS-2Unicode 是一个字符集，这一点应该很好理解，它表示的是字符和代码点的对应关系，比如简体字“汉”对应的Unicode代码点是 \u6C49，而 UCS-2 究竟是一种字符集还是一种编码方式呢？ 我个人偏向于它是一种编码方式，因为它存在大端、小端这种说法，如果是一种字符集只会考虑对应关系，不会考虑字节序，这只是我个人观点，有些软件上确实是这样标注的，但有些文章也会把UCS-2当成一种字符集，这样也能说的通，不用太纠结这里的区别。 其实 UCS-2 编码对应的字符集是UCS，这些是历史原因导致的，一方面国际标准化组织（ISO）于1984年创建ISO/IEC JTC1/SC2/WG2工作组，试图制定一份通用字符集（Universal Character Set，简称UCS），并最终制定了ISO 10646标准。而另一方面统一码联盟，也很想做这个统一编码的武林盟主，由Xerox、Apple等软件制造商于1988年组成，并且开发了Unicode标准。 然后1991年左右，两个项目的参与者都认识到，世界不需要两个不兼容的字符集。于是，它们开始合并双方的工作成果，并为创立一个单一编码表而协同工作。从Unicode 2.0开始，Unicode采用了与ISO 10646-1相同的字库和字码。ISO也承诺，ISO 10646将不会替超出\u10FFFF的UCS-4编码赋值，以使得两者保持一致。两个项目仍都独立存在，并独立地公布各自的标准。不过由于Unicode这一名字名字起的好，比较好记，因而它使用更为广泛。 从这段历史我们可以看到，虽然 UCS-4 将 UCS-2 从2个字节扩展成了4个字节，但是范围并没到使用到 \u00000000～\uFFFFFFFF，而是将范围集中到 \u000000～\u10FFFF 内，保证了 UCS 和 Unicode 各个字符代码点的统一，也奠定了UTF-8实现标准Unicode时最多需要4个字节的基础。 UTF-8 的诞生按理说 Unicode 已经给世界范围内的所有字符定义了代码点，无论是什么字符，使用4个字节都能表示出来，为什么要搞出一个UTF-8呢？是因为使用者发现，对于ASCII码范围内的字符，本来1个字节就能正确表示，现在居然要4个字节表示，即使使用 UCS-2编码，占用的空间也扩大了1倍，有些太浪费了。 为了解决这种空间浪费问题，就出现了一类变长的通用转换格式，即UTF（Universal Transformation Format），常见的UTF格式有：UTF-7，UTF-7.5，UTF-8，UTF-16 以及 UTF-32。 这类格式中最常见的就是 UTF-8 编码了，UTF-8 是针对于 Unicode 字符集中各个代码点的编码方式，是一种 Unicode 字符的实现方式，采用变长字节来表示Unicode编码，最长使用4个字节来表示标准的Unicode代码点，在有些资料中可能会看到5、6个字节的编码方式，这些都是非标准的Unicode代码点，根据规范，这些字节值将无法出现在合法 UTF-8序列中。 Unicode 和 UTF-8UTF-8在对标准Unicode字符编码时最多使用4个字节，其代码点范围与UTF-8编码后的形式对应如下： Unicode/UCS-4（十六进制） 字节数 UTF-8编码格式（二进制） 000000-00007F 1 0xxxxxxx 000080-0007FF 2 110xxxxx 10xxxxxx 000800-00FFFF 3 1110xxxx 10xxxxxx 10xxxxxx 010000-10FFFF 4 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx UTF-8编码示例只看上面这种对应关系，可能还不太清楚是怎样表示，接下来可以举一个例子试一下，比如一个常用的简体中文字——“好”，查询它的Unicode代码点是 \u597D，对照上面的表格发现在 000800-00FFFF 这个范围，应该采用3个字节的表现形式。 先把这个数值翻译成二进制为 0101100101111101，然后按照3个字节的形式分成3组，0101、100101 和 111101，把这些内容天填充到xxx这样的空位中就得到了“好”这个字的UTF-8编码—— 11100101 10100101 10111101，表示成十六进制就是 0xE5A5BD。 这个过程还是比较简单的，其他编码要转换成UTF-8编码都要经过Unicode这一步中转，先通过转换表查到其他编码对应字符的Unicode编码，然后再转换成UTF-8的表示格式。 优点和缺点根据 UTF-8 的编码规则，任何一个 byte 漏传，多传，传错只影响当前字符，前后字符都不受影响，而 Unicode 如果从一个字的中间截断会导致接下来所有的字符解析都是错的，这使得UTF-8编码的数据在不够可靠的网络传输中是有利的。 兼容ASCII，并且是字节顺序无关的。它的字节顺序在所有系统中都是一样的，因此它实际上并不需要BOM，不过在文件开头常常保存 0xEFBBBF 三个字节来表明文件编码是UTF-8。 缺点是因为UTF-8是一种变长编码，无法从直接从Unicode字符直接判断出UTF-8文本的字节数。除了ASCII字符集内的字符，其他情况实际上都增加了固定的头数据，占用了无效空间。 编码和解码在编程中的应用编码和解码在网站页面和数据库存储时用的非常多，一不小心就搞出一堆乱码，这种编码和解码操作在Python3中很直观，Python2中 string 和 bytes 混合在一起，编码和解码操作不太明显，而在python3中 string 和 bytes 是完全不同的两个类型，string编码成bytes，而bytes解码成string。 相比于python3中的编码、解码对应两个类型，C++中的编码和解码操作的前后都是字符串，这在一定程度上会给人造成误解，接下来我们使用Python3来简单测试一下编码和解码操作。 编码操作编码通常是把人类可以理解的字符转换成计算机可以认识二进制数据，这个过程在python3中对应的是把string转化成bytes，测试如下： 12345word = '好好'print(type(word), word)result = word.encode('utf-8')print(type(result), result) 运行结果如下： 12&lt;class &apos;str&apos;&gt; 好好&lt;class &apos;bytes&apos;&gt; b&apos;\xe5\xa5\xbd\xe5\xa5\xbd&apos; 解码操作解码操作通常是把计算机中存储和传输的数据转换成人类能看懂的字符，这个过程在python3中对应的是把bytes转化成string，测试如下： 12345data = b'\xe5\xa5\xbd\xe5\xa5\xbd'print(type(data), data)result = data.decode('utf-8')print(type(result), result) 运行结果如下： 12&lt;class &apos;bytes&apos;&gt; b&apos;\xe5\xa5\xbd\xe5\xa5\xbd&apos;&lt;class &apos;str&apos;&gt; 好好 乱码产生从上面的两个例子来看编码和解码非常简单，那怎么还能出现乱码呢？计算机说到底还是一种工具，你在把可见字符编码后交给计算机存储和传输时，你要记住这些二进制的编码方式，在你想看这些数据时还要用相反的方式进行解码，否则就会出现乱码，比如下面这种使用 utf-8 编码，却使用 gbk 这种方式来解码，就得不到你想要的数据。 12345678word = '好好'print(type(word), word)result = word.encode('utf-8')print(type(result), result)new_word = result.decode('gbk')print(type(new_word), new_word) 运行结果如下： 123&lt;class &apos;str&apos;&gt; 好好&lt;class &apos;bytes&apos;&gt; b&apos;\xe5\xa5\xbd\xe5\xa5\xbd&apos;&lt;class &apos;str&apos;&gt; 濂藉ソ 虽然结果是可以看得见的字符，但是这不是我们想要的数据，所以 濂藉ソ 对于我们来说也是一种乱码，在处理字符编码时我们必须清楚知道要用什么方式来进行编码和解码，如果编码和解码的方式不一致，那么就会产生乱码现象。 总结 Unicode 是一种字符集，描述了人类范围内用于交流的所有字符的代码点，给与唯一的数字进行对应 Unicode 规定的代码点范围是 \u000000-\u10FFFF，这与 UCS-4 规定的范围达成了统一，共定义了17个Plan UTF-8 是Unicode字符集的一种实现，采用变长的方式，标准规范最多使用4个字节表示一个Unicode字符 编码是为了把人类用来交流的字符转换成二进制数据便于存储和传输 解码是为了把存储在计算机中的二进制数据转换成人们能看得懂的字符 编码和解码不一致时就会造成乱码，比如使用UTF-8编码，使用GBK来解码就会造成乱码现象 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 对未知的事物充满恐惧，过于保守的看待当下的一切，有时候太稳反而会失去很多~ 2020-11-29 19:23:13]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>编码</tag>
        <tag>解码</tag>
        <tag>encode</tag>
        <tag>decode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下使用netstat命令查看网络信息]]></title>
    <url>%2Fblog%2F2020%2F11%2F22%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8netstat%E5%91%BD%E4%BB%A4%E6%9F%A5%E7%9C%8B%E7%BD%91%E7%BB%9C%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言netstat 这个命令一直以为是 net status 的缩写，今天一查发现并没有找到官方的这种说法，然后参考了 man 手册，发现这个词更像是 net statistics 的缩写，命令的作用是显示网络连接、路由表、接口连接、无效连接和多播成员关系的，man 手册中描述这个命令如下： netstat - Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships 这个命令很强大，但是我经常使用的参数很简单，通常用来查询端口占用问题，命令为 netstat -anp | grep xxxPORT，因为在我测试自己项目程序的时候，总有一些进程企图占用我使用的端口，比如那个 被我 kill 了 n 次的 TIM 客户端，使用 netstat 可以方便的找到是哪个进程占用了你的端口。 虽然这个命令经常使用，但是其中的这些参数含义却不是很清楚，所以特地总结一下，综合其他常见的用法，记录下来以备后续查找使用。 参数选项 -a：显示所有连接，包括 LISTEN 状态的连接 -l：仅显示 LISTEN 状态的连接 -t：仅显示tcp相关选项 -u：仅显示udp相关选项 -n：拒绝显示别名，能显示数字的全部转化成数字 -o：显示信息中包括与网络计时器相关的信息 -e：显示扩展信息，例如uid等 -p：显示建立相关链接的程序名 -r：显示路由信息，路由表 -s：按各个协议进行统计 -c：每隔一个固定时间，执行该netstat命令。 无参数执行该命令无参数执行时显示数据会少一些，便于我们看清命令执行的结果，内容如下： 123456789101112131415161718[root@node1 ~]# netstatActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 192.168.0.201:ssh 178.129.126.124.b:19450 ESTABLISHEDtcp 0 52 192.168.0.201:ssh 178.129.126.124.b:17626 ESTABLISHEDtcp 0 0 192.168.0.201:57784 101.200.35.175:https TIME_WAITtcp 0 0 192.168.0.201:42298 100.125.2.72:https ESTABLISHEDActive UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 11550 /run/systemd/shutdowndunix 2 [ ] DGRAM 13355412 /var/run/chrony/chronyd.sockunix 3 [ ] DGRAM 1228 /run/systemd/notifyunix 2 [ ] DGRAM 1230 /run/systemd/cgroups-agentunix 5 [ ] DGRAM 1241 /run/systemd/journal/socketunix 16 [ ] DGRAM 1243 /dev/logunix 3 [ ] STREAM CONNECTED 15663unix 3 [ ] STREAM CONNECTED 15662... 输出结果可以分为 Active Internet connections 和 Active UNIX domain sockets 两个部分： Active Internet connections 指有效的网络连接，默认显示6列内容： Proto：协议名字，包括tcp, udp, udpl, raw等 Recv-Q：表示网络接收队列，表示收到的数据已经在本地接收缓冲，还有多少没有被应用程序取走 Send-Q：表示网络发送队列，表示存在本地缓冲区，但对方没有收到的数据或者没有 ACK 的 Local Address：本地IP地址和端口 Foreign Address：外部IP地址和端口 State：网络连接状态，包括 ESTABLISHED、SYN_SENT、SYN_RECV、FIN_WAIT1、FIN_WAIT2、TIME_WAIT、CLOSE、CLOSE_WAIT、LAST_ACK、LISTEN、CLOSING、UNKNOWN 等状态 Active UNIX domain sockets 是指本地套接口，我们知道 socket 也可用于同一台主机的进程间（IPC）通讯，但是 socket 用于IPC更有效率：不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号和应答等，只是将应用层数据从一个进程拷贝到另一个进程并且它是是全双工的，API接口语义丰富，相比其它进程间通信机制有明显的优越性。 常用命令组合查询端口占用12[root@node1 /]# netstat -anp | grep 8889tcp 0 0 0.0.0.0:8889 0.0.0.0:* LISTEN 27584/tinyproxy 这是我目前最常用的命令，在windows可以改为 netstat -ano | findstr 8889 显示tcp连接123456789101112[root@node1 /]# netstat -atActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:ddi-tcp-2 0.0.0.0:* LISTENtcp 0 0 localhost:smtp 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:ssh 0.0.0.0:* LISTENtcp 0 52 192.168.0.201:ssh 178.129.126.124.b:19450 ESTABLISHEDtcp 0 0 192.168.0.201:ssh 178.129.126.124.b:17626 ESTABLISHEDtcp 0 0 192.168.0.201:42298 100.125.2.72:https ESTABLISHEDtcp6 0 0 [::]:squid [::]:* LISTENtcp6 0 0 localhost:smtp [::]:* LISTENtcp6 0 0 [::]:ssh [::]:* LISTEN 显示处于 LISTEN 状态的端口123456789101112131415161718192021[root@node1 /]# netstat -lActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:ddi-tcp-2 0.0.0.0:* LISTENtcp 0 0 localhost:smtp 0.0.0.0:* LISTENtcp 0 0 0.0.0.0:ssh 0.0.0.0:* LISTENtcp6 0 0 [::]:squid [::]:* LISTENtcp6 0 0 localhost:smtp [::]:* LISTENtcp6 0 0 [::]:ssh [::]:* LISTENudp 0 0 0.0.0.0:bootpc 0.0.0.0:*udp 0 0 0.0.0.0:ntp 0.0.0.0:*udp 0 0 localhost:323 0.0.0.0:*udp 0 0 0.0.0.0:56034 0.0.0.0:*udp6 0 0 [::]:42035 [::]:*udp6 0 0 localhost:323 [::]:*Active UNIX domain sockets (only servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ACC ] STREAM LISTENING 11533 /run/lvm/lvmpolld.socketunix 2 [ ACC ] STREAM LISTENING 6848304 /var/run/rpcbind.sockunix 2 [ ACC ] STREAM LISTENING 11584 /run/lvm/lvmetad.socket... 分类统计每种协议的信息1234567891011121314151617181920212223242526272829303132333435363738394041[root@node1 /]# netstat -sIp: 7902622 total packets received 60675 forwarded 127 with unknown protocol 0 incoming packets discarded 7841813 incoming packets delivered 7270606 requests sent out 8 dropped because of missing routeIcmp: 928210 ICMP messages received 25426 input ICMP message failed. InCsumErrors: 8 ICMP input histogram: destination unreachable: 71154 timeout in transit: 484 echo requests: 856165 echo replies: 337 timestamp request: 54 896502 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 40039 echo request: 244 echo replies: 856165 timestamp replies: 54Tcp: 274517 active connections openings 66347 passive connection openings 187800 failed connection attempts 90950 connection resets received 3 connections established 6359177 segments received 5808198 segments send out 494062 segments retransmited 4 bad segments received. 452720 resets sentUdp: 539313 packets received 14902 packets to unknown port received.... 每秒显示一次信息12345678910[root@node1 /]# netstat -cActive Internet connections (w/o servers)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 52 192.168.0.201:ssh 178.129.126.124.b:19450 ESTABLISHEDtcp 0 0 192.168.0.201:ssh 178.129.126.124.b:17626 ESTABLISHEDtcp 0 0 192.168.0.201:42298 100.125.2.72:https ESTABLISHEDActive UNIX domain sockets (w/o servers)Proto RefCnt Flags Type State I-Node Pathunix 2 [ ] DGRAM 11550 /run/systemd/shutdownd... 显示核心路由信息1234567[root@node1 /]# netstat -rKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Ifacedefault 192.168.0.1 0.0.0.0 UG 0 0 0 eth0169.254.169.254 192.168.0.254 255.255.255.255 UGH 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 显示网络接口列表123456[root@node1 /]# netstat -iKernel Interface tableIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgdocker0 1500 36248 0 0 0 33647 0 0 0 BMUeth0 1500 9119246 0 0 0 8277212 0 0 0 BMRUlo 65536 27700 0 0 0 27700 0 0 0 LRU 总结 netstat -anp | grep 8889 命令可用于查询8889端口被哪个进程占用了，在Windows上翻译为 netstat -ano | findstr 8889 netstat 命令查询出的网络连接信息中，Recv-Q 和 Send-Q 通常应该为0，如果长时间不为0可能是有问题的，需要尽快排查 如果 Recv-Q 数值一直处于不为0的状态，可能是遭受了拒绝服务 DOS 攻击，导致本地消息处理过慢 如果 Send-Q 数值一直处于不为0的状态，可能是有应用向外发送数据包过快，或者是对方接收处理数据包不够快 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 以史为鉴可以知兴替，以铜为鉴可以正衣冠，以人为鉴可以明得失。人的成长需要对比，总有人比你更加优秀~ 2020-11-23 01:17:59]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>sort</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习cmake从成功编译一个小程序开始]]></title>
    <url>%2Fblog%2F2020%2F11%2F14%2F%E5%AD%A6%E4%B9%A0cmake%E4%BB%8E%E6%88%90%E5%8A%9F%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[前言在 Windows上开发我使用最多的IDE还是 Visual Studio，编写、编译一条龙服务，导致了不少编译流程知识的缺失，这种大型的IDE确实好用，诸多配置通过在界面上勾选一下就可以了，但是在编译细节的掌握上还是漏掉了一些知识。 在 linux 开发环境下通常会使用 gcc 或者 g++ 进行编译，可是编译选项有点多，当工程非常大的时候需要写的编译参数太多了，这时可以使用make命令来帮助我们编译 C++ 程序，编译时依赖一些规则，这些规则就写在一个叫 Makefile 的文件中。 后来发现写 Makefile 还是太麻烦了，这个文件也相当大。于是“懒惰”的程序员们又开发出了各种各样的工具用来生成 Makefile 文件，我使用过的目前就只有 automake 和 cmake。 生成Makefile之前使用的生成 Makefile 文件的工具是 automake，被称为是“八股文”一样的操作，每次操作都是固定的几个步骤，比如每次都要运行 autoscan、aclocal、autoconf、automake、./confiugre等命令，需要个人发挥的地方并不多，之前使用的时候也不是完全从0开始一点点写的，往往是写一个项目模板之后，对照着在Makefile.am文件中修改几个参数就好了。 现在新的工作内容中使用 cmake 来生成 Makefile，这个 cmake 之前还确实接触过一些，大概是2012年的时候，那时在编译 OpenCV 库还有增强现实插件的时候用过几次，当时感觉安装起来太麻烦了，对那个红绿蓝的图标记忆犹新，感觉和当时的新闻联播的图标有些亲戚关系。 其实当时根本分不清什么是编译器，什么是 Makefile，对于各种库文件的编译完全是按照文档来操作，现在回过头来看看 cmake 生成 Makefile 还是比较简单的，最起码要比 automake 省了很多步骤，只要编写一个 CMakeLists.txt 文件就好了。 编写CMakeLists.txt生成Makefile为了练习使用编写CMakeLists.txt生成Makefile，进而编译C++项目，我们可以从头来实现一个小例子，目标是编写一个计算加法的静态库和一个计算减法静态库，然后实现一个测试工程来使用这两个函数库，整个工程使用 cmake 来生成 Makefile，然后使用 make 命令完成编译。 实现简单的代码文件加法和减法都是常用的简单计算，用来举例子很容易理解，接下来展示要用到的几个文件内容，每个文件只有几行，只为了说明问题，文件内容如下： 123//myadd.hint add(int a, int b); 1234567//myadd.cpp#include "myadd.h"int add(int a, int b) &#123; return a + b;&#125; 123//mysub.hint sub(int a, int b); 1234567//mysub.cpp#include "mysub.h"int sub(int a, int b) &#123; return a - b;&#125; 123456789101112//test.cpp#include "myadd.h"#include "mysub.h"#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; "happy birthday!" &lt;&lt; std::endl; std::cout &lt;&lt; "519 + 1 = " &lt;&lt; add(519, 1) &lt;&lt; std::endl; std::cout &lt;&lt; "1320 - 6 = " &lt;&lt; sub(1320, 6) &lt;&lt; std::endl; return 0;&#125; 使用常规方法编译首先使用最简单 g++ 命令来编译这个样例程序： 查看目录下文件 12albert@home-pc:testcmake$ lsmyadd.cpp myadd.h mysub.cpp mysub.h test.cpp 将 myadd.h 和 myadd.cpp 编译成静态库 libmyadd.a 12345albert@home-pc:testcmake$ g++ -c myadd.cppalbert@home-pc:testcmake$ ar crv libmyadd.a myadd.oa - myadd.oalbert@home-pc:testcmake$ lslibmyadd.a myadd.cpp myadd.h myadd.o mysub.cpp mysub.h test.cpp 将 mysub.h 和 mysub.cpp 编译成静态库 libmysub.so 1234albert@home-pc:testcmake$ g++ -c mysub.cppalbert@home-pc:testcmake$ g++ -shared -fPIC -o libmysub.so mysub.oalbert@home-pc:testcmake$ lslibmyadd.a libmysub.so myadd.cpp myadd.h myadd.o mysub.cpp mysub.h mysub.o test.cpp 编译链接静态库 libmyadd.a、动态库 libmysub.so 和测试文件生成可执行程序 test 123albert@home-pc:testcmake$ g++ test.cpp libmyadd.a -L. -lmysub -o test -Wl,-rpath=.albert@home-pc:testcmake$ lslibmyadd.a libmysub.so myadd.cpp myadd.h myadd.o mysub.cpp mysub.h mysub.o test test.cpp 运行查看结果，成功计算表达式的值 1234albert@home-pc:testcmake$ ./testhappy birthday!519 + 1 = 5201320 - 6 = 1314 使用cmake方式上面展示了最原始的编译方法，每次都要敲这些命令，接下来编写一个 CMakeLists 文件，使用 cmake 生成Makefile，以后只要运行 make 命令就可以完成编译了。 调整一下目录结构如下： 123456789albert@home-pc:testcmake$ tree.|-- myadd| |-- myadd.cpp| `-- myadd.h|-- mysub| |-- mysub.cpp| `-- mysub.h`-- test.cpp 进入 myadd 目录新建 CMakeLists.txt 编写内容如下： 1234aux_source_directory(. SRC_LIST) #将此目录的源文件集合设置为变量SRC_LISTadd_library(myadd STATIC $&#123;SRC_LIST&#125;) #库的名称，库的类型，静态库的源文件列表set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/lib) #库的输出路径为根目录下的lib文件夹 进入 mysub 目录新建 CMakeLists.txt 编写内容如下： 1234aux_source_directory(. SRC_LIST) #将此目录的源文件集合设置为变量SRC_LISTadd_library(mysub SHARED $&#123;SRC_LIST&#125;) #库的名称，库的类型，动态库的源文件列表set(LIBRARY_OUTPUT_PATH $&#123;PROJECT_SOURCE_DIR&#125;/lib) #库的输出路径为根目录下的lib文件夹 在工程主目录下新建 CMakeLists.txt 编写内容如下： 12345678910111213141516171819202122232425262728293031323334# 指定cmake版本cmake_minimum_required(VERSION 3.5)# 指定项目的名称，一般和项目的文件夹名称对应project(testcmake)# 指定子目录add_subdirectory(myadd)add_subdirectory(mysub)# 添加c++ 11标准支持set(CMAKE_CXX_FLAGS "-std=c++11" )# 特殊宏，之前编译mysqlcppconn8用到过add_definitions(-DGLIBCXX_USE_CXX11_ABI)# 头文件目录include_directories(myadd mysub)# 源文件目录aux_source_directory(. DIR_SRCS)# 设置环境变量，编译用到的源文件全部都要放到这set(TEST_MATH $&#123;DIR_SRCS&#125;)# 库文件目录link_directories(lib)# 添加要编译的可执行文件add_executable($&#123;PROJECT_NAME&#125; $&#123;TEST_MATH&#125;)# 添加可执行文件所需要的库target_link_libraries($&#123;PROJECT_NAME&#125; myadd)target_link_libraries($&#123;PROJECT_NAME&#125; mysub) 新建build目录和lib目录，整个工程目录关系如下： 12345678910111213141516albert@home-pc:testcmake$ tree.|-- CMakeLists.txt|-- build|-- lib|-- myadd| |-- CMakeLists.txt| |-- myadd.cpp| `-- myadd.h|-- mysub| |-- CMakeLists.txt| |-- mysub.cpp| `-- mysub.h`-- test.cpp4 directories, 8 files 进入 build 目录下依次运行 cmake .. 和 make 命令 123456789101112131415161718192021222324252627282930313233343536albert@home-pc:testcmake/build$ cmake ..-- The C compiler identification is GNU 5.4.0-- The CXX compiler identification is GNU 5.4.0-- Check for working C compiler: /usr/bin/cc-- Check for working C compiler: /usr/bin/cc -- works-- Detecting C compiler ABI info-- Detecting C compiler ABI info - done-- Detecting C compile features-- Detecting C compile features - done-- Check for working CXX compiler: /usr/bin/c++-- Check for working CXX compiler: /usr/bin/c++ -- works-- Detecting CXX compiler ABI info-- Detecting CXX compiler ABI info - done-- Detecting CXX compile features-- Detecting CXX compile features - done-- Configuring done-- Generating done-- Build files have been written to: testcmake/buildalbert@home-pc:testcmake/build$ makeScanning dependencies of target mysub[ 16%] Building CXX object mysub/CMakeFiles/mysub.dir/mysub.cpp.o[ 33%] Linking CXX shared library ../../lib/libmysub.so[ 33%] Built target mysubScanning dependencies of target myadd[ 50%] Building CXX object myadd/CMakeFiles/myadd.dir/myadd.cpp.o[ 66%] Linking CXX static library ../../lib/libmyadd.a[ 66%] Built target myaddScanning dependencies of target testcmake[ 83%] Building CXX object CMakeFiles/testcmake.dir/test.cpp.o[100%] Linking CXX executable testcmake[100%] Built target testcmakealbert@home-pc:testcmake/build$ ./testcmakehappy birthday!519 + 1 = 5201320 - 6 = 1314albert@home-pc:testcmake/build$ 至此，使用cmake方式编译工程的例子就写完了。 总结 cmake 和 automake 本身不提供编译功能，只是可以按照编写的 CMakeLists.txt 文件生成 Makefile make 可以根据 Makefile 文件调用 gcc/g++ 命令对源代码进行编译工作 -Wl,-rpath=. 这个选项可以指定可执行文件查找动态库的路径，感觉比 export LD_LIBRARY_PATH 要方便一点 -DGLIBCXX_USE_CXX11_ABI 这个宏可坑了我不少时间，编译使用libmysqlcppconn8的时候，如果不禁用会报编译错误 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有你，真好~ 2020-11-15 23:55:35]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>linux</tag>
        <tag>编译</tag>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下sed命令的基础用法]]></title>
    <url>%2Fblog%2F2020%2F11%2F07%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8Bsed%E5%91%BD%E4%BB%A4%E7%9A%84%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言每次看到sed这个词就想起“种子”，心里明知道是把它和seed弄混了，但是先入为主的思想怕是改不过来了，不过现在还好，虽然把第一印象的意思弄错了，但还是很快能反应过来这是一个很“厉害”的linux命令，也有一些网友评论到，每次看到这个命令就双腿发抖，我虽然没抖，但是谈到这个命令还是有些挠头，心里有些发怵。 一味地逃避困难是不可取的，虽然心里感觉这是个很难的命令，但是今天还是要硬着头皮学一下，边学边记录，易于下次复习，那些打败不了我的困难终将使我更加强大。 sed功能其实sed并不是一个单词，而是 stream editor 的缩写，本意为面向字符流的编辑器，说白了sed就是用来编辑文件的命令，编辑文件是我们每天经常做的工作，但是如果每天的编辑工作都类似，我们就要考虑使用sed工具来提高工作效率了，比如说把今天新增的100个文件的第一行都加上版本信息，虽然手动编辑也能做，但是你想体验一下敲个命令瞬间搞定这件事情的快感吗，我们来学习sed命令吧？ 命令格式1sed [OPTION]... &#123;script-only-if-no-other-script&#125; [input-file] sed 的选项不是太多，最常用的有下面两种形式： sed -e &#39;sed命令脚本&#39; input-file sed -f &#39;sed命令脚本文件&#39; input-file 命令选项 -e ：命令行模式，选项后直接跟sed编辑脚本，在只有一组脚本的情况下可以省略 -f ：脚本文件模式，选项后跟写有sed编辑脚本的文件名，运行后会执行脚本文件内的编辑动作 -i ：直接修改文件内容，如果不加这个选项是不修改源文件的，只将修改后的文件输出 -n ：只打印模式匹配的行，便于查看所作修改 以上列举只是一些常见选项，还有些比如 -l 指定每行长度，-s 指定换行的分隔符等等，用到了再来分析学习。 寻找匹配既然是编辑文件，首先要找到需要编辑的位置，在sed命令中可以使用行号，或者字符查找等方式找到需要修改的位置，然后再执行编辑动作，常见的范围： x：指定的行号，表示第x行 x,y：指定的行号范围，表示第x行到y行 /pattern：查询到包含指定模式的行 x,y!：指定的行号范围，表示不包括第x行到y行 sed操作sed几乎可以实现文件的所有编辑工作，接下来尝试一些常见的用法： 打印内容使用编辑命令 p，可以向匹配行后面插入内容。 打印文件第2行和第3行的内容，命令为sed -n &#39;2,3p&#39; data.txt 123456789albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed -n '2,3p' data.txtabcxyz 追加内容使用编辑命令 a，可以向匹配行后面插入内容。 在第2行后面添加文本newline，命令为sed &#39;2anewline&#39; data.txt 12345678910111213albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '2anewline' data.txt1234abcnewlinexyz1==123 在最后一行后面添加文本endline，命令为sed &#39;$aendline&#39; data.txt 1234567891011121314albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '$aendline' data.txt1234abcxyz1==123endlinealbert@DESKTOP-6746UC3:/mnt/d/data/shell$ 插入内容使用编辑命令 i，可以在匹配的那一行插入内容。 在第1行插入文本firstline，命令为sed &#39;1ifirstline&#39; data.txt 12345678910111213albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '1ifirstline' data.txtfirstline1234abcxyz1==123 在包含文本 “123” 的行插入文本insertline，命令为sed &#39;/123/iinsertline&#39; data.txt 1234567891011121314albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '/123/iinsertline' data.txtinsertline1234abcxyz1==insertline123 更改行内容使用编辑命令 c，可以修改匹配行的内容。 将包含文本 “123” 的行替换为 “456”，命令为sed &#39;/123/c456&#39; data.txt 123456789101112albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '/123/c456' data.txt456abcxyz1==456 将3、4、5行内容更改为newworld，命令为sed &#39;3,5cneworld&#39; data.txt 12345678910albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '3,5cneworld' data.txt1234abcneworld 替换行内容使用编辑命令 s，可以替换匹配行的内容，需要注意和 c 的区别，c 是整行的内容都改变，而 s 是只替换命令中指定的部分。 将文件中的文本 “123” 替换为 “456”，命令为sed &#39;s/123/456/g&#39; data.txt 123456789101112albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed 's/123/456/g' data.txt4564abcxyz1==456 综合运用 删除空行并给所有内容是 “123” 的文本加上小括号 12345678910111213albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed -e 's/123/(&amp;)/g' -e '/^$/d' data.txt(123)4abcxyz1==(123) 删除内容使用编辑命令 c，可以删除匹配行。 删除空行，命令为sed &#39;/^$/d&#39; data.txt 12345678910111213albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '/^$/d' data.txt1234abcxyz1==123 从第一行开始，每两行删除掉一行，命令为sed &#39;1~2d&#39; data.txt 123456789albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '1~2d' data.txtabc1== 删除2行和3行以外的行，命令为sed &#39;2,3!d&#39; data.txt 123456789albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '2,3!d' data.txtabcxyz 删除指定行数范围内的匹配行，命令为sed &#39;1,3{/123/d}&#39; data.txt 1234567891011albert@DESKTOP-6746UC3:/mnt/d/data/shell$ cat data.txt1234abcxyz1==123albert@DESKTOP-6746UC3:/mnt/d/data/shell$ sed '1,3&#123;/123/d&#125;' data.txtabcxyz1==123 总结 sed 是 stream editor 的缩写，表示为面向字符流的编辑器 sed 命令常用的几个选项，-e、-f、-i、-n sed 命令常用的几个编辑动作，也就是选项后的常用命令有 p（打印）、a（追加）、i（插入）、c（改变）、s（替换）、d（删除） ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 不能打败我的困难终将使我更加强大，绊不倒我的石头最后只会被拿来踩踏，拥抱一个个困难，生活本来就是一条打怪升级之路，那有什么一帆风顺~ 2020-11-7 22:47:01]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>文件</tag>
        <tag>sed</tag>
        <tag>编辑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于数据一致性的思考]]></title>
    <url>%2Fblog%2F2020%2F10%2F24%2F%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[前言也不知道是谁这么有才，居然发明了1024这个程序员的节日，其他的节日都是买买买，唯独这个节日促销活动少的可怜，连早上买菜都是占了第二天重阳节的便宜，除了程序员们自嗨，也没人来给码农们庆祝了。 今天也嗨了一把，程序员的节日必须在工位上，飞速的敲着代码才是对1024最大的尊重，在这一天的结尾之际还是聊聊最近开发中的一些问题，其中数据一致性的问题确实需要梳理一下。 什么是一致性关于一致性常常在两个地方听到，一个是数据库，另一个是分布式，两者都叫一致性，但是含义却不同。 数据库一致性数据库中的一致性其实代表不破坏完整性，所有的数据从一个状态转化到另一个状态时不发生逻辑问题，比如说A通过手机银行给B转了100万，这件事情发生后A账户少了100万，B账户多了100万，这样就保证了数据的一致，如果转账结束A账户的钱少了100万，B账户却只多了100块，那完蛋了，A和B肯定一起去找银行打架去了。 分布式一致性很多资料对于分布式一致性理解的都是数据冗余副本，当所有副本的数据一样时，那么此时的状态就是一致的。按照我自己的理解，这里的冗余副本不一定指的是数据形式完全一样，比如玩家在游戏服拥有金币资产200万，然后全服排行榜上的展示面板上显示资产也是200万，可能具体数据的形式不同，但这应该也是一种数据一致性的表现。 两个概念容易混淆，因为经常在分布式的架构下更新数据库，两种一致性也常常在同一个操作中有所体现。其实我也经常混着用，反正知道这个意思就好了，最近遇到的问题也是两个概念的集合，不过还是先来理解一下分布式的一致性吧 分布式一致性分类 强一致性： 要求无论更新操作是在哪一个节点副本上执行，之后获取的数据都是最新的。 弱一致性： 能容忍部分或全部节点都看不到最新数据，数据改变时尽量通知可能多的节点。 最终一致性： 是弱一致性的一种特例表现，需要保证用户最终能够读取到最新的数据。 我们当然希望能实现强一致性，但这样需要付出相当大的代价，往往要通过牺牲可用性才能达到。 一致性的保证如果要想达到强一致性，那么就得保证任何数据在改变之后必须通知所有节点，等待所有节点更新完毕后才能给用户提供服，这就要在开始更新时加一把大锁，先锁住数据，等待所有节点完成更新时释放锁，这样才能提供数据的强一致性保证。 如果节点太多的话，这个锁的机制将会消耗大量的时间来等待，可能导致应用长时间不能提供正常服务，在一些应用上显然是不合适的，所以是否要保证强一致性需要根据具体的业务逻辑来选择。 还有一个经常听到的观点就是在分布式系统中一致性和可用性我们只能选择一个，这一般是从CAP理论中得到的结论，但是这样说是不准确的，关于CAP理论最初版的大意为：“对于一个分布式计算系统，不可能同时满足一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance ）三个设计约束”。 通过CAP理论告诉我们分布式系统只能选择CP或者AP，但其实这里的前提是分布式系统发生了“分区”现象。如果当前系统没有发生分区现象，我们没有必要放弃C或者A，应该C和A都可以保证。 还有一点个人的理解，由于数据传输是需要时间的，那么当一个节点修改了数据同步到另一个节点时不可能瞬间完成，所以数据不一致总是时刻存在，而我们前面提到的数据一致总是指对用户而言的，虽然数据在传输过程中是不一致的，但是我们可以规定在数据完成同步前，用户看到的都是旧数据，这样就对用户而言数据就是一致的。 而数据同步过程中的不一致，如果在不一致期间还发生了中断、崩溃等问题，就必须通过日志来恢复了，个人觉得，总是有那么一种极限情况，连日志都救不了你，毕竟记录日志的也是一种程序，但是这类事情发生的概率也比较小了。 总结 程序世界的一致性常常指数据库中的一致性和分布式中的一致性 CAP理论告诉我们分布式系统在发生了分区现象时，才需要选择CP或者AP，否则应该可以保证CA ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 欲穷千里目，更上一层楼。最近越来越发现古诗的精妙之处了，随着阅历的增加，之前背诵的古诗有些突然就明白了，不知道应该开心还是难过~ 2020-10-26 00:27:02]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>游戏</tag>
        <tag>架构设计</tag>
        <tag>一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下使用sort命令完成常见排序操作]]></title>
    <url>%2Fblog%2F2020%2F10%2F14%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E4%BD%BF%E7%94%A8sort%E5%91%BD%E4%BB%A4%E5%AE%8C%E6%88%90%E5%B8%B8%E8%A7%81%E6%8E%92%E5%BA%8F%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言linux 系统下的命令常常给人一种短小精悍的感觉，使用起来就像一把把锋利的小刀，在自己专注的领域做到极致，今天要聊的就是 linux 环境下的排序命令 sort，处理文本按列排序非常方便，最近使用 sort命令来排序日志查找问题，为了防止一段时间不用又会忘记，所以记录下来便于下次查找。 命令作用sort 命令默认会将待排序内容以空格划分为多个列，然后对内容进行按列排序，命令本身不会修改待排序内容，而是将排序结果重新输出，如果想修改待排序源文件的内容，可以通过重定向命令来实现。 命令格式为： 1sort [选项] 文件名 常见选项sort 作为一个强大的命令，参数选项还挺多的，不过我只列举一些常见的参数，方便日常使用即可。 -k： 指定排序依据的列数，可以分多次指定 -o： 将排序后的结果存入指定文件 -c： 检查指定文件是否已经排好序 -u： 删除所有重复行 -b： 忽略每行或字段前面开始出的空格字符 -f： 排序比较时忽略大小写 -n： 转化为数字，按照数值的大小排序 -r： 反向排序，从大到小 -t： 指定排序时划分列数的分隔字符 数据文件为了展示 sort 命令的作用，专门利用 ls 命令产生了一段数据，并保存在了 data.txt 文件中，之后会利用这个文件来展示 sort 的用法，文件内容展示如下： 123456789101112131415albert@home-pc:~$ cat data.txt-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrcdrwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwx------ 1 albert albert 4096 Jul 16 00:52 .config/-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw------- 1 root root 2257 Jul 16 01:10 .viminfodrwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority 这个文件中的内容在使用 sort 命令排序时默认以空格分割，所以共有9列，在指定列数时从1开始，接下来我们用这些数据来测试一下排序命令的用法。 核心参数对于我来说 sort 命令的核心参数是 -k，其完整的参数列表为 -k START_F[.START_C][OPTIONS][,END_F[.END_C][OPTIONS]]，参数列表很长，但是不要恐惧，逐步分析就可以了。 -k 后面这已打算都是用来指定排序依据的范围的，其中 START_F 和 END_F 表示开始和结束的字段，也就是列数，.START_C 和 .END_C 表示指定字段开始和结束的字符数，OPTIONS 是由一个或多个单个字母排序的选项[bdfgiMhnRrV]，这些选项中常用的已经列举在前面了，写在此处的选项会覆盖全局排序选项。 这样文字叙述有些枯燥，可以看下这个参数 -k 6.2b,6.3b，这个排序选项的含义是把内容按照第6列的第2个字符到第6列的第3个字符排序，查找字符位置的时候要去掉前面的空白。 用法展示看了以上的参数可能还是不太清楚具体怎样用，所以举了下面这些例子，可以方便的处理常用的排序工作。 按照指定列排序这是最普通的排序要求了，也是我用的最多的情况，需要使用-k参数 按照第3列排序 123456789101112131415albert@home-pc:~$ sort -k3,3 data.txtdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrc-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw------- 1 root root 2257 Jul 16 01:10 .viminfo root 已经被排到了所有albert的后面 将排序结果存入指定文件12345678910111213141516albert@home-pc:~$ sort -k3,3 data.txt -o dst.txtalbert@home-pc:~$ cat dst.txtdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrc-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw------- 1 root root 2257 Jul 16 01:10 .viminfo 排序结果已经被存储到了文件 dst.txt 中，其实这个命令还可以改写成 sort data.txt &gt; dst.txt 查看文件是否已经排序好 测试没排好序的文件 12albert@home-pc:~$ sort -k3,3 data.txt -csort: data.txt:2: disorder: -rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history 测试已经排序的文件 12albert@home-pc:~$ sort -k3,3 dst.txt -calbert@home-pc:~$ 对于已经拍好序的文件使用 -c 参数没有任何输出，如果是未排序的文件则会给出提示 去掉排序结果中的重复行123albert@home-pc:~$ sort -k3,3 data.txt -u-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw------- 1 root root 2257 Jul 16 01:10 .viminfo 这里的重复行参考是你指定排序依据的列数，也就是第3列如果重复就会认为是重复行，结果中只能出现一次 按照数值结果进行排序123456789101112131415albert@home-pc:~$ sort -k5n,5 data.txt-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrcdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro* 这里使用了 -k5n,5 作为排序选项，其中的 n 表示以数值方式排序，如果不加 n 的排序结果如下： 123456789101112131415albert@home-pc:~$ sort -k5,5 data.txt-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrcdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out* 数据看起来很乱，其实也是按照第5列排好序的，仔细分析你会发现是把这些数字当成字符串排的序 反向排序123456789101112131415albert@home-pc:~$ sort -k3,3 data.txt -r-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrc-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rw------- 1 albert albert 35 Jul 19 14:14 .lesshstdrwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwx------ 1 albert albert 4096 Jul 16 00:52 .config/ 按照第3列反向排序，root就排到了所有albert的前面 自定义分割字符sort 命令默认是以空格作为列的分割符号的，可以使用 -t 选项自定义分割符，比如我们使用 : 作为分隔符，然后以第二列进行排序 123456789101112131415albert@home-pc:~$ sort -t ":" -k2,2 data.txtdrwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw------- 1 albert albert 35 Jul 19 14:14 .lesshstdrwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrcdrwx------ 1 albert albert 4096 Jul 16 00:52 .config/-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful 结果是以分钟数进行的排序 综合排序学习了上面这么多参数，可以做一个综合的例子，以第6列的月份从小到大排序，以第5列文件大小逆序排列，通过组合上面的参数，可以使用下面的命令： 123456789101112131415albert@home-pc:~$ sort -k6,6 -k5rn,5 data.txt-rwxrwxr-x 1 albert albert 20328 Jul 18 19:49 mainpro*-rwxrwxr-x 1 albert albert 9272 Jul 18 22:27 a.out*drwx------ 1 albert albert 4096 Jul 16 00:52 .config/drwxrwxrwx 1 albert albert 4096 Jul 16 01:10 .cache/drwxrwxrwx 1 albert albert 4096 Jul 19 21:19 .vscode-server/-rw-r--r-- 1 albert albert 3771 Jul 16 00:52 .bashrc-rw------- 1 root root 2257 Jul 16 01:10 .viminfo-rw-r--r-- 1 albert albert 655 Jul 16 00:52 .profile-rw-r--r-- 1 albert albert 220 Jul 16 00:52 .bash_logout-rw-rw-r-- 1 albert albert 195 Jul 18 22:27 mainpro.cpp-rw------- 1 albert albert 35 Jul 19 14:14 .lesshst-rw-r--r-- 1 albert albert 0 Jul 16 00:52 .sudo_as_admin_successful-rw------- 1 albert albert 7064 Oct 17 22:14 .bash_history-rw------- 1 albert albert 61 Sep 20 09:42 .Xauthority 总结 sort 命令中的 -k 选项是最重要的参数，可以指定排序依据的列数 sort 命令中的 -n 选项也是常用的参数，可以进行数值比较 在实际问题中常常需要综合运用这些参数，参考综合例子中的方式逐步确定参数选项就可以了。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 业精于勤，荒于嬉；行成于思，毁于随。没有人能随随便便成功~ 2020-10-18 15:43:51]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>sort</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言在解决实际问题时的优点与不便]]></title>
    <url>%2Fblog%2F2020%2F10%2F14%2FGo%E8%AF%AD%E8%A8%80%E5%9C%A8%E8%A7%A3%E5%86%B3%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%E6%97%B6%E7%9A%84%E4%BC%98%E7%82%B9%E4%B8%8E%E4%B8%8D%E4%BE%BF%2F</url>
    <content type="text"><![CDATA[前言Go语言，全称golang，是Google开发的一种静态强类型、编译型、并发型并具有垃圾回收功能的编程语言。 从2007年末由 Robert Griesemer、Rob Pike、Ken Thompson 主持开发，其中的 Ken Thompson 可是和 Dennis Ritchie 一起发明了C语言的大佬。Go 语言于2009年11月正式宣布成为开放源代码项目， 并在2012年初，发布了Go 1.0稳定版本，此后便开启了稳步发展的道路。 Go 语言作为一个从2007年开始诞生的语言，在庞大的语言家族中算是一个晚辈，和C++、Python这种老牌语言相比查了将近20年，和 C 语言相比资历就更低了，但是这个新晋的语言在 Google 光环的强大加持下也在飞速发展着，由于前辈们在发展的途中趟了很多坑，所以 Go 在发明之初就避免了其他语言的很多不便，可以说是站在巨人的肩膀上发展起来的。 但是即便这样，Go 语言的特点也不能被所有人喜欢，和许多人一样，我在学习这门语言的过程中也发现一些很方便特性和一些不太方便的特点，下面简单说几个点，有不对的地方希望小伙伴能及时指出，防止我在错误的思想上越走越远（怎么有种新闻发言稿的感觉~）。 不便之处这里的不便之处只是我在使用过程中感觉不太方便，可能很多人并没有这个感觉，或许还有很多其他的解决方法和替代方案，烦请小伙伴能指点一下。 三目运算符这个三目运算符是个很常用的逻辑处理部件，也是我在逻辑中经常使用到的，在Python、Lua等语言中也不存在，但是我都找到了简单的替代方式，但是在Go 中不得不写成中规中矩的 if 条件判断，这让很多算法的解题代码看起来并不那么优雅，比如一个简单的约瑟夫环问题： N个人围成一圈，从第一个人从开始报数，报到m的人出圈，剩下的人继续从1开始报数，报到m的人出圈；如此往复，直到所有人出圈，输出最后一个出圈人最初始的编号。 这个问题的解法最简单的是模拟法，使用数组模拟一个环来按照规则运行，最后一个出圈的人的编号就可以输出到结果，还有一种思路就是找规律，可以找到出圈前后的序号对应关系，进而写出一行代码的解决方案。 1234// 索引从0开始，只要对结果加1就好了int joseph_ring(int n, int m) &#123; return n == 1 ? 0 : (joseph_ring(n - 1, m) + m) % n&#125; 但是在没有三目运算符的 Go 中要实现这个算法，就不得不多写几行了，和 C 语言相比就没有那么简洁了。 1234567func joseph_ring(n int, m int) int &#123; if n == 1 &#123; return 0 &#125; return (joseph_ring(n - 1, m) + m) % n&#125; if 单行语句也要加大括号Go 语言本身带有自己的格式化命令，可以保证编写时不同的缩进样式格式化之后得到相同的代码，if 后面的条件语句可以不加小括号，但是后面的语句块必须加大括号，这样的规定对于我经常写的代码有点不太友好，比如下面这些C++代码： 12345678if (a &gt; 0) break;if (b &lt; 0) continue;if (c == 0) return; 有时候为了看起来紧凑，可能会写成这样： 123if (a &gt; 0) break;if (b &lt; 0) continue;if (c == 0) return; 但是放到 Go 语言中，就不得不写成好几行了，并且还要加大括号，看起来代码有些松散。 1234567891011if a &gt; 0 &#123; break;&#125;if b &lt; 0 &#123; continue;&#125;if c == 0 &#123; return;&#125; 优秀特性上面提到了 Go 语言中不方便的地方，现在可以来说说 Go 语言相对于 C、C++ 更优越的特性： 多个变量同时赋值在 C++ 中交换两个变量的通常使用中间变量来完成，比如交换 a、b 两个变量的值： 123456int a = 1;int b = 6;int tmp = a;a = b;b = tmp; 针对于这种整形的变量，一些大牛们发明了特殊的算法来处理，避免使用中间变量： 123456int a = 1,;int b = 6;a = a ^ bb = b ^ aa = a ^ b 但是在 Go语言中这种情况非常好处理，直接从左到右依次赋值就好了 1234var a int = 1var b int = 6b, a = a, b defer 声明defer 可以用于在当前函数返回前执行一些清理代码，而不管此函数如何退出。defer 在函数中可以随时出现，这使得清理代码可以尽可能在需要清理的地方运行，比如我们常常要释放申请的资源，常见的需要释放的资源有文件描述符： 12345file, err := os.Open(fileName)if err != nil &#123; return&#125;defer file.Close() 有了 defer 终于不再在担心，资源没回收的问题，也不用在各个提前返回的条件分支中添加释放资源的重复代码了。 goroutine 并发goroutine 是Go并行设计的核心，说到底其实就是协程，但是它比线程更小并且在Go语言内部帮你实现了这些 goroutine 之间的内存共享。执行 goroutine 只需要极少的栈内存，可同时运行成千上万个并发任务。goroutine 一定程度上比 thread 更易用、更高效、更轻便。 使用起来也非常方便，创建 goroutine 只需在函数调用语句前添加 go 关键字，就可以创建并发执行单元。开发人员无需了解任何执行细节，调度器会自动将其安排到合适的系统线程上执行，这是解放生产力的又一创举，简单示例代码如下： 12345678910111213141516171819202122package mainimport ( "fmt" "time")func new_task() &#123; for i := 0; i &lt; 10; i++ &#123; fmt.Println("this is a newTask") time.Sleep(time.Second) //延时1s &#125;&#125;func main() &#123; for i := 0; i &lt; 3; i++ &#123; go new_task() //新建一个协程， 新建一个任务 &#125; time.Sleep(time.Second * 15) //延时15s fmt.Println("this is a main goroutine")&#125; 总结 Go 语言作为编程语言中的新晋小弟，吸收了前人的经验，现阶段发展迅猛 虽然 Go 出于一些目的规定了语言的标准，但是类似于没有三目运算符这种特点还是有些不方便 Go 这门语言还很年轻，相信随着不断发展它会越来越优秀，但没有任何语言是完美无缺的 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 陪伴是最长情的告白，而守护是最沉默的陪伴。国庆中秋双节合并，放假了，陪家人待在一起真的很开心，什么都不用做，就静静的待在一起很满足，聊聊天、抬抬杠，假期嗖嗖嗖地溜掉了~]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Go</tag>
        <tag>优点</tag>
        <tag>缺点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Spreadsheet Compare工具对比Excel文件差异]]></title>
    <url>%2Fblog%2F2020%2F10%2F04%2F%E4%BD%BF%E7%94%A8Spreadsheet-Compare%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94Excel%E6%96%87%E4%BB%B6%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[前言与 Spreadsheet Compare 这个工具的相遇是通过 TortoiseSVN 来牵线的，在使用 SVN 管理 Excel 表格时发现，TortoiseSVN自带的比较工具不能对比Excel文件的版本差异，这对于通过日志查找问题来说非常不方便，通过搜索发现了 Spreadsheet Compare 这款优秀的工具，特此记录一下，方便日后查找并快速配置。 Spreadsheet CompareSpreadsheet Compare 是 Microsoft Office 自带的一款工具软件（2013版本以后），可以用来比较不同 Excel 表格的差异，既能显示数据的不同，也可以显示出表结构的不同。这是一款带界面的工具软件，布局分为左右两部分，与 Beyond Compare 这个工具界面类似，但是功能更加强大。 Beyond Compare 也可以用来对比表格差异，但是只能比较两个Excel的当前工作表，如果每个 Excel 文件中包含多个工作表时就会对比错误的情况，而 Spreadsheet Compare 在这一点上更加优秀，可以对比多个表格数据。 Spreadsheet Compare 使用起来也非常简单，可以操作的按钮很少，界面简洁， 启动软件后单击左上角的 Compare Files 按钮，选择要对比的文件即可，非常方便，数据差异、结构差异等都会用不同的颜色标记出来，还可以导出对比结果。 命令模式这种模式对于是提供给 TortoiseSVN 使用的前提，因为 TortoiseSVN 无法像人一样一步步操作选择待比较的 Excel 表格，而是需要一个命令脚本，将要比较的参数传给 Spreadsheet Compare 工具进而完成比较工作。 找到工具想要编写命令脚本，首先要找到这个比较的工具，我找到的路径是在 &quot;C:\Program Files (x86)\Microsoft Office\Root\Office16\DCF\SPREADSHEETCOMPARE.EXE&quot;，相信大家的路径都差不多，在 Office 工具目录下应该就能找到了，可以在开始菜单中找到工具，然后通过属性找到可执行程序所在目录，工具的可执行文件名字叫做 SPREADSHEETCOMPARE.EXE。 编写脚本编写脚本之前有一点需要强调一下，SPREADSHEETCOMPARE.EXE 有点奇怪，大多数软件在比较差异的时候会将两个文件作为参数使用，但是 SPREADSHEETCOMPARE.EXE 在比较之前，需要将两个待比较的文件名分成两行写入一个文件，再将这个文件作为参数传给工具使用，比如要比较 ExcelA.xlsx 和 ExcelB.xlsx 两个文件，需要将两个文件写入一个临时文件 ExcelCompare.txt 中: ExcelA.xlsxExcelB.xlsx 然后再把这个文件作为参数传给工具： 1SPREADSHEETCOMPARE.EXE ExcelCompare.txt 脚本内容有了上面的说明，我们就可以写出一个较为通用的版本，比如我的脚本名字是 SC.bat，内容如下： 1234567@echo offchcp 65001set batpath=%~dp0echo %~1&gt; "%batpath%ExcelCompare.txt"echo %~2&gt;&gt; "%batpath%ExcelCompare.txt""C:\\Program Files (x86)\\Microsoft Office\\Root\\Office16\\DCF\\SPREADSHEETCOMPARE.EXE" "%batpath%ExcelCompare.txt" 脚本执行直接在 cmd 命令行中输入以下命令就可以对比 ExcelA.xlsx 和 ExcelB.xlsx 两个文件了： 12D:\data\bat&gt;D:\data\bat&gt;SC.bat ExcelA.xlsx ExcelB.xlsx 供给SVN调用个人比较懒，不喜欢截图，在 TortoiseSVN 工具的设置中找到“差异查看器”选项，选择该选项然后点击界面上的高级设置，点击增加按钮，增加根据扩展名指定差异比较程序，填写 .xlsx 和所需命令 D:\data\bat&gt;SC.bat %base %mine 命令中的 %base 和 %mine 参数是 TortoiseSVN 提供的，代表原始文件和自己修改的文件，这次再通过 SVN 查看表格差异就可以启动 Spreadsheet Compare 程序方便地查看两个表格的差异啦。 总结 Spreadsheet Compare 是一款强大的表格比较工具，在表格比较时比 Beyond Compare 还要优秀 脚本调用 SPREADSHEETCOMPARE.EXE 程序时参数是一个包含了待比较文件名的临时文件，这一点和其他的比较工具有些不同 脚本中我们生成的临时文件无需手动处理，再打开待比较文件后会自动删除 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 急需找到一个看得见摸得着的目标为之努力，不然真的有点止步不前了，至今还未找到可以废寝忘食之事，长此以往终将碌碌无为，继续找寻，此事可以不伟大，但应该有趣~ 2020-10-12 00:16:52]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>SVN</tag>
        <tag>Excel</tag>
        <tag>Spreadsheet-Compare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[东拉西扯01世界的沧海桑田]]></title>
    <url>%2Fblog%2F2020%2F09%2F26%2F%E4%B8%9C%E6%8B%89%E8%A5%BF%E6%89%AF01%E4%B8%96%E7%95%8C%E7%9A%84%E6%B2%A7%E6%B5%B7%E6%A1%91%E7%94%B0%2F</url>
    <content type="text"><![CDATA[前言此篇非技术总结，但却与技术相关，写这篇总结的起因是前两天看了一节关于虚幻四的公开课，这节课也不是讲开发技术，更多的是讲创作艺术，课程开始前看到虚幻编辑器的画面，还是有一种很熟悉的感觉，毕竟使用了3年多的时间，外观几乎没有变化，使用方式依旧是原来的步骤，但随着课程的进行，我发现它变了。 其中有一段内容提到，虚幻四已经不再将自己作为一款游戏开发引擎，而是定位成一个艺术创作和开发的平台，也就是说它不仅仅可以做游戏，同时可以用来出影视剧、国漫、特定素材等等，它已经将自己的势力范围扩张，变得丰富而强大，后来又提到虚幻五带来的种种提升。 什么？虚幻五已经出了，这是我之前不知道的，我知道今年 Redis 出了最新的 6.0 版本，MySQL 一跃发行了 8.0 版本，IOS 也更新到的 14.0.1 版本，似乎各种技术都在飞速的发展着，但是人的精力毕竟有限，很难把它们成长历程尽收眼底，一不留神就发现某种技术已经悄悄从你身边跨了过去。 技术的发展记得我第一篇博客记录的是处理 Ubuntu 黑屏的解决方案，我去翻了翻当时记录的版本是 12.04，时间已经过去了7年，Ubuntu 20.04 已经发行，当年解决黑屏的经验或许已经毫无用处。 10年接触的第一种计算机编程语言是 C 语言，当时开发环境是 Turbe C 2.0，后来使用 VC++6.0，接着就是VS系列，期间用过 Dev-C++、CodeBlocks等编辑器，但是 VS 还是用的最多的，直到目前使用的 VS2017，可是刚刚一个好学的小朋友问我 VS2019 相关的问题，我发现这款用了这么久的工具，之前一直无变化的菜单布局在 VS2019 版本上发生了改变。 之前一直号称单线程内存数据库 Redis 在今年5月份发布的 6.0 版本中，加入了网络多线程，使得整体性能提升近一倍，这被认为是 Redis 最大的一次改版。 MySQL 直接从 5.7 版本跳到了 8.0 版本，因为之前一直是 5.6 、5.7 这样的小版本提升，一跃跳到 8.0 一时让人好奇到底改了什么？其实 6.0 是一个过渡版本，而 7.0 是作为集群的保留版本，所以这次直接到了 8.0。其中一个亮点增加了 MySQL 文档存储，可以存储 json 格式，开始支持向 NoSQL 格式转化。 差点忘了C++，这个庞然大物目前已经从最开始的 C++98，发展到现在的 C++20，我在想它如果真的存活到 2098 年，应该怎么命名它呢？新标准的内容很长，需要慢慢来消化，可以发现一些很好玩的东西，比如三向比较运算符 &lt;=&gt;，也叫飞船运算符，感兴趣的可以去了解下。 经历的和未经历的变化还有很多，想好了再来补充。。。 技术公司的发展当年找工作的听说过的巨头就是 BAT，而今天晚上问一个即将毕业进入工作岗位的同学，哪些是他心目中的大厂，他给出了四个名字，“阿里、腾讯、字节、美团”，很明显百度已经掉队，但是瘦死的骆驼比马大，短时间内百度的技术底蕴不会消失殆尽，这些手握资源和技术的大厂很早就给自己挖好了护城河，一般企业很难追赶的上。 相比早期的 BAT，我感觉后来的字节、美团能够赶上他们实属不易，记得当年开玩笑说 TX 除了发明了一套钻石收费系统，其他的都是抄的，任何公司有了好点子，不是被他合并了就是被他抄走了，所以说能在某个它无法掌控的赛道上超越它也是非常厉害了。 有些东西是其他公司无法做到的，比如疫情期间的健康宝，每天上班、去商场、去公园都要看，都要打开微信和支付宝的APP，这个日活的数据放到其他任意一款软件上都是庞大的数字，可是他们就在这两大巨头这自然的发生着。 总结 技术每时每刻都在发展，有时你发现它陌生了，其实只是你关注的少了 好的赛道大多数已被别人占领，从夹缝中寻找到机会还需好好把握，才能做出一定的成绩 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 年年岁岁花相似，岁岁年年人不同 2020-9-27 00:20:42]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++求解组合数的具体实现]]></title>
    <url>%2Fblog%2F2020%2F09%2F19%2FC-%E6%B1%82%E8%A7%A3%E7%BB%84%E5%90%88%E6%95%B0%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言很少写关于具体算法的总结笔记，因为很难把一个算法从头到尾的叙述清晰并且完整，容易造成误解。这次想总结一下组合数的具体实现，原因是最近总是碰见组合数，所以决定来写写，免得每次从头推导公式耽误时间。排列组合经常会作为一个问题解决方案中一部分，通常是求某个问题有多少个解，达到某种状态有多少种操作方式等等。 问题起因今天下午解一道简单题，难度简直刷新了我的认知，其中需要用到组合数，但这仅仅是解题的一小部分，没办法，从头推导的，简单优化下，写出了如下代码： 1234567int C(int a, int b)&#123; int ans = 1; for (int i = a; i &gt; a - b; i--) ans *= i; for (int i = b; i &gt; 1; i--) ans /= i; return ans;&#125; 因为时间紧迫，范围也比较小，同时可以控制 a 和 b 的大小，所以临时写下的这段代码可以运行，不然这段代码会出现各种错误的。 组合公式既然是想做总结，还是从头来看看组合公式，根据原始公式实现算法，并尝试优化它，当熟悉这个套路之后，就可以直接拿来用了，可以节省不少时间，组合公式的常见表示方式如下： $$C^m_n = \frac{n!}{m!(n-m)!} = C^{n-m}_n,(n \geq m \geq 0)$$ 这个公式写出来清晰多了，n!表示n的阶乘，计算方式为 n*(n-1)*(n-2)*(n-3)*…*3*2*1， 相信很多人都清楚，我们只要把这个数据公式翻译成代码就可以了： 12345678int C2(int n, int m)&#123; int a = 1, b = 1, c = 1; for (int i = n; i &gt;= 1; --i) a *= i; for (int i = m; i &gt;= 1; --i) b *= i; for (int i = n-m; i &gt;= 1; --i) c *= i; return a/(b*c);&#125; 代码比较简单，依次计算公式中三个数的阶乘，然后再做乘除法就可以了，但是你有没有思考过一个问题，int 类型的整数最大能表示的阶乘是多少？是12!，它的值是 479,001,600，它是 int 表示范围内最大的阶乘数，看来这种实现方式局限性很大，如果 n 大于12就没有办法计算了。 公式变形实际上根据阶乘的定义，n! 和 (n-m)! 是可以约分的，将这两个式子约分后，公式可以化简为： $$C^m_n = \frac{n!}{m!(n-m)!} = \frac{n(n-1)(n-2)…(n-m+1))}{m!},(n \geq m \geq 0)$$ 公式写成这样之后可以少计算一个阶乘，并且计算的范围也会缩小，代码实现和一开始展示的代码思想是一样的： 1234567int C3(int n, int m)&#123; int a = 1, b = 1; for (int i = n; i &gt; n - m; --i) a *= i; for (int i = m; i &gt;= 1; i--) b *= i; return a/b;&#125; 这段代码虽然经过了化简，但是当 n 和 m 非常接近的时候，分子还是接近于 n!，所以表示的范围还是比较小。 递推公式直接给出的公式经过化简后还是受制于计算阶乘的范围，得想个办法看看能不能绕过阶乘计算，方法总是有的，并且前辈们已经给我们整理好了，我们总是站在巨人的肩膀上，下面就是递推公式： $$\begin{cases} {C^m_n} = 1,\qquad\qquad\qquad (m=0 或 m=n) \ {C^mn} = {C^m{n-1}} + {C^{m-1}_{n-1}},\qquad(n &gt; m &gt; 0) \end{cases}$$ 递归实现有了上面的分段函数表示，就满足了递归的条件，既有递归调用缩小规模，也有递归出口，这样实现起来很简单，代码如下： 12345int C4(int n, int m)&#123; if (n == m || m == 0) return 1; return C4(n-1, m) + C4(n-1, m-1);&#125; 这两行代码是不是很秀？不过使用递归常常会出现一问题，那就是相同子问题多次计算，导致效率低下，这个计算组合数的方式同样存在重复计算子问题的缺点，我们以调用C4(5, 3)为例，看看下面的调用关系图： 1234567891011121314151617181920graph TB A(5,3)--&gt;B(4,3) B(4,3)--&gt;C(3,3); B(4,3)--&gt;D(3,2); D(3,2)--&gt;E(2,2); D(3,2)--&gt;F(2,1); F(2,1)--&gt;G(1,1); F(2,1)--&gt;H(1,0); A(5,3)--&gt;O(4,2) O(4,2)--&gt;P(3,2) O(4,2)--&gt;Q(3,1) P(3,2)--&gt;R(2,2) P(3,2)--&gt;S(2,1) Q(3,1)--&gt;T(2,1) Q(3,1)--&gt;U(2,0) S(2,1)--&gt;V(1,1) S(2,1)--&gt;W(1,0) T(2,1)--&gt;X(1,1) T(2,1)--&gt;Y(1,0) 从这个图可以清晰看出C4(3, 2) 和 C4(2, 1) 都被计算了多次，当 m 和 n 的数字比较大的时候，会进行更多次的重复计算，严重影响计算的效率，有没有什么办法解决重复计算的问题呢？ 备忘递归解决重复计算的常用方法是利用一个备忘录，将已经计算式子结果存储起来，下次再遇到重复的计算时直接取上次的结果就可以了，我们可以将中间结果简单存储到map中。 假设 n 不超过10000，这比12已经大太多了，我们可以使用 n * 10000 + m 作为map的键，然后将结果存储到map中，每次计算一个式子前先看查询备忘录，看之前有没有计算过，如果计算过直接取结果就可以了，代码简单实现如下： 1234567891011121314int C5(int n, int m, map&lt;int, int&gt;&amp; memo)&#123; if (n == m || m == 0) return 1; auto itora = memo.find((n-1)*10000+m); int a = itora != memo.end() ? itora-&gt;second : C4(n-1, m); if (itora == memo.end()) memo[(n-1)*10000+m] = a; auto itorb = memo.find((n-1)*10000+m-1); int b = itorb != memo.end() ? itorb-&gt;second : C4(n-1, m-1); if (itorb == memo.end()) memo[(n-1)*10000+m-1] = b; return a + b;&#125; 使用 map 作为备忘录可以避免重复计算，这是解决递归效率低下的常用方法，那么有了递推公式不使用递归实现可不可以呢？当然可以了，针对于这个问题，有了递推公式我们还可以使用动态规划（dp）的方式来实现。 动态规划动态规划常常适用于有重叠子问题和最优子结构性质的问题，试图只解决每个子问题一次，具有天然剪枝的功能。基本思想非常简单，若要解一个给定问题，我们需要解其不同子问题，再根据子问题的解以得出原问题的解。 再回顾一下递推公式： $$\begin{cases} {C^m_n} = 1,\qquad\qquad\qquad (m=0 或 m=n) \ {C^mn} = {C^m{n-1}} + {C^{m-1}_{n-1}},\qquad(n &gt; m &gt; 0) \end{cases}$$ 翻译成人话就是，当m等于0或者等于n的时候，组合数结果为1，否则组合数结果等于另外两个组合数的和，我们可以采用正向推导的方式，将 n 和 m 逐步扩大，最终得到我们想要的结果，定义dp表格如下： n\m (0) (1) (2) (3) (4) (5) (0) 1 (1) 1 1 (2) 1 2 1 (3) 1 3 3 1 (4) 1 4 6 4 (5) 1 5 10 ==&gt;10 从表格可以清晰的看出求解 C(5,3) 只需要计算5行3列（从0开始）的数据，其余的值可以不用计算，这样我们就可以对照着表格写代码啦，定义一个dp数组，然后双重for循环就搞定了： 123456789101112int C6(int n, int m)&#123; if (n == m || m == 0) return 1; vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(m+1)); for (int i = 0; i &lt;= n; i++) for (int j = 0; j &lt;= i &amp;&amp; j &lt;= m; j++) if (i == j || j == 0) dp[i][j] = 1; else dp[i][j] = dp[i-1][j] + dp[i-1][j-1]; return dp[n][m];&#125; 至此，我们就采用了非递归的方式求解出了组合数的结果，但是这里的空间有点浪费，每次都要花费O(mn)的空间复杂度，有没有办法降低一点呢？我们可以找找规律进行压缩。 压缩DP观察之前的动态规划实现的代码，我们发现求解第 i行的数据时只与第 i-1 行有关，所以我们可以考虑将二维数据压缩成一维，还是逐行求解，只不过可以用一维数组来记录求解的结果，优化代码如下： 123456789101112int C7(int n, int m)&#123; if (n == m || m == 0) return 1; vector&lt;int&gt; dp(m+1); for (int i = 0; i &lt;= n; i++) for (int j = min(i, m); j &gt;= 0; j--) if (i == j || j == 0) dp[j] = 1; else dp[j] = dp[j] + dp[j-1]; return dp[m];&#125; 这样我们就将空间复杂度降低到了O(m)，需要注意的是在计算dp时，因为采用了压缩结构，为防止前面的修改影响后续结果，所以采用里倒序遍历，这是一个易错的点。 其他优化代码实现到这里，我们的时间复杂度是O(nm)，空间复杂是O(m)，其实还有进一步的优化空间： 减小m： 因为题目是求解C(n, m)，但是我们知道组合公式中，C(n, m) 和 C(n, n-m) 相等，所以当 n-m 小于 m 的时候求解C(n, n-m)可以降低时间复杂度和空间复杂度。 部分剪枝： 观察函数int C7(int n, int m)，实际上当i为n时，j没必要遍历到0，只需要计算j等于m的情况就可以了，可以提前计算出结果。 缩小计算范围： 从上面的剪枝操作得到启示，其实每一行没必要全部计算出来，以 C(5,3) 为例，我们只需要计算出表格中有数字的位置的结果就可以了： n\m (0) (1) (2) (3) (4) (5) (0) 1 (1) 1 1 (2) 1 2 1 (3) 3 3 1 (4) 6 4 (5) ==&gt;10 这样来看每行最多需要计算3个值，那么时间复杂度可以降低到 O(3n)，去掉常数，时间复杂度降为 O(n)。 总结 计算组合数可以采用逆向递归和正向递推两种方式，递归时注意写好递归出口 采用正向递推方法时利用动态规划思想，使用子问题的解拼凑出最终问题的解 计算组合数若使用了计算阶乘应注意范围，避免在计算时产生溢出，int最多能表示 12! 使用动态规划方法时可以逐步优化空间和时间，这其实就是优化算法的过程，也是提升的过程 关于组合数的求解方式，我们可以找到时间复杂度O(n)、空间复杂度O(m)的非递归解法 补充感谢 @小胡同的诗 同学的补充和提醒，让我再次感受到数学力量的深不可测，原来求解组合数还有这样一个递推公式： $$\begin{cases} {C^m_n} = 1,\qquad\qquad\qquad (m=0 或 m=n) \ C_n^m=\frac{n-m+1}{m}C_n^{m-1},\qquad(n &gt; m &gt; 0) \end{cases}$$ 这个公式厉害就厉害在它是一个线性的，不存在分叉的情况，也就是说即使递归也不会出现重复的计算，我们简单实现一下。 反向递归12345int C8(int n, int m)&#123; if (n == m || m == 0) return 1; return C8(n, m-1) * (n-m+1) / m;&#125; 代码非常紧凑，也不存在重复计算的情况，当然我们也可以使用正向计算的方式来实现。 正向递推12345678910int C9(int n, int m)&#123; if (n == m || m == 0) return 1; int ans = 1; m = min(m, n-m); for (int i = 1; i &lt;= m; i++) ans = ans * (n-i+1) / i; return ans;&#125; 这段代码将时间复杂度降到了O(m)，空间复杂度降到了O(1)，不过特定的场景还是要选择特定的实现，虽然C9函数在时间复杂度和空间复杂度上都优于 C5 函数，但是如果一个实际问题中需要用到多个组合数的时候，C5 这种采用缓存的方式可能会是更好的选择。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 想讲故事？没人倾听？那是因为你还未到达一个指定的高度，当你在某个领域站稳了脚跟，做出了成绩，自然有的是时间去讲故事或者“编”故事，到时候随便一句话都会被很多人奉为圭臬，甚至会出现一些鸡汤莫名其妙的从你嘴里“说”出来。在你拥有了讲故事权利的同时，批判的声音也将随之而来~ 2020-9-20 12:32:37]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>combination</tag>
        <tag>dp</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中优先队列priority_queue的基础用法]]></title>
    <url>%2Fblog%2F2020%2F09%2F11%2FC-%E4%B8%AD%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97priority-queue%E7%9A%84%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言学习优先队列之前先看个单词队列 queue， 这个单词的读法很多人都能读对吧，音标是 /kjuː/ ，再看一个双端队列 deque，它的音标是 /dek/，应该有人读错了吧，反正我是没读对，刚开始看见一次错一次，现在还好了，基本能记住怎么读了，可是这些队列怎么用呢？ 队列就不用多说了，一个先进先出的经典数据结构，那么优先队列是个什么鬼，其实它就是在队列的基础上加上优先两个字，想想怎样才能优先呢？没错——排队！只有排好了队伍才会有落后和优先之分，否则一团乱糟糟的，怎么才能分出优先的，所以优先队列一定应用了排序。 可是排序要怎样实现呢？其实排序这个底层逻辑你是不用管的，你只要把想要的数据放到优先队列里，然后取出的必定是当前状态下最优的，当然，究竟什么是最优的条件是需要你来设定的，也就是说我们需要定义排序的规则。 头文件优先队列 priority_queue 是队列 queue 的一个变种，头文件是#include &lt;queue&gt;，使用优先队列必须要包含这个头文件。 结构定义优先队列的结构定义是一个模板类，需要提供三个类型参数： 12345template&lt; class T, class Container = std::vector&lt;T&gt;, class Compare = std::less&lt;typename Container::value_type&gt;&gt; class priority_queue; 从定义可以看出，虽然要结构是三个参数，但是后两个参数带了默认值，所以针对于普通的数据类型，一般情况下指提供第1个参数就可以了，比如 priority_queue&lt;int&gt; 实际上等价于 priority_queue&lt;int, vector&lt;int&gt;, less&lt;int&gt;&gt;。 这三个参数的含义分别为：数据类型，容器类型和比较函数，实际上优先队列就是维护了一个装有 T 类型元素的容器 Container，并在入队和出队时对容器内元素使用 Compare 比较函数进行了排序。 这3个参数还要满足一定的要求，并且在使用过程中有些注意事项： 如果类型 T 和 Container 容器中元素类型不一致，那么行为未定义，所以要避免这种情况。 Container 必须是序列容器，其实C++中序列容器很多的，比如std::array、std::vector、std::deque、std::list等 Container 还必须要支持随机访问，并且有 front()、push_back()、pop_back() 等函数 这样来看只有 std::vector、std::deque 满足容器条件了，而优先队列中使用的默认参数也是 std::vector。 队列排序一直在说优先队列里使用了排序，而常用的容器是 std::verctor，那么究竟用的是什么排序，又是在什么时候进行的排序呢？实际上这里的排序并不是我们通常拿到数据后使用的冒泡排序、快速排序等，优先队列中的排序本质上是堆排序，但是它不是每次都进行完整的堆排序，而是通过 Container 维护了一个堆结构，每次入队和出队时都进行一次堆调整，所花时间为 log(n)，所以用在数据量大的地方，速度比较快。 优先队列使用当我们大概了解了优先队列的原理后，可以通过使用来进一步熟悉这个结构，下面来看几个例子。 实现排序1234567891011121314151617#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;void common_sort()&#123; int source_data[10] = &#123;3, 5, 8, 1, 10, 2, 9, 15, 13, 16&#125;; // 默认大根堆，实现由大到小排序 priority_queue&lt;int&gt; q; for (auto n : source_data) q.push(n); while (!q.empty()) &#123; cout &lt;&lt; q.top() &lt;&lt; endl; q.pop(); &#125;&#125; priority_queue&lt;int&gt; 默认构建的是一个大根堆，所以每次从头取数据得到的是一个从大到小的队列排序 123456789101112albert@home-pc:/mnt/c++/datastruct$ g++ priorityqueue.cpp -o commonsort -std=c++11albert@home-pc:/mnt/c++/datastruct$ ./commonsort16151310985321 如果是完整排序使用优先队列就有些麻烦了，还不如直接调用 std::sort 函数，但是如果只取部分数据的话，优先队列还是非常方便快速的，比如下面这个问题。 取出数组中最大的k个数这是一个经典的算法题，最容易想到的办法就是遍历，先找到最大的，然后排出这个数再找到最大的，这样找k次就好了，所需时间大概表示为 O(kN)。 还有一个方法是排序，使用 std::sort 排序后，然后依次取出前 k 个数就行了，排序使用快速排序的话可以达到所需时间为 O(Nlog(N))，其实这样已经很优秀了，但是还可以通过优先队列来加速，下面来写一下代码： 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;void max_k_num()&#123; int source_data[10] = &#123;3, 5, 8, 1, 10, 2, 9, 15, 13, 16&#125;; int k = 5; // 小根堆 priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q; for (auto n : source_data) &#123; if (q.size() == k) &#123; if (n &gt; q.top()) &#123; q.pop(); q.push(n); &#125; &#125; else q.push(n); &#125; while (!q.empty()) &#123; cout &lt;&lt; q.top() &lt;&lt; endl; q.pop(); &#125;&#125; 这里是定义了一个小根堆，堆顶是最小值，当有新元素大于堆顶元素时，并且队列中元素等于k个，需要移除堆顶元素，然后插入新的元素，这样就能保证优先队列中始终拥有最大的k个数，运行结果如下： 1234567albert@home-pc:/mnt/c++/datastruct$ g++ priorityqueue.cpp -o max_k_num -std=c++11albert@home-pc:/mnt/c++/datastruct$ ./max_k_num910131516 因为这里控制堆的规模最大为k，所以这个算法的执行时间大概是O(Nlog(k))，绝大多数情况是由于快速排序的。 自定义结构使用优先队列时常常要用到自定义结构，这时候就需要自己来写比较函数了，比如输出成绩最好的三个人的信息： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;struct student &#123; string name; int score;&#125;;struct cmp_custom &#123; bool operator()(student&amp; x, student&amp; y) &#123; return x.score &gt; y.score; &#125;&#125;;void max_k_score()&#123; vector&lt;student&gt; stu_list = &#123;&#123;"Andy", 89&#125;, &#123;"Bella", 79&#125;, &#123;"Cary", 92&#125;, &#123;"Dick", 60&#125;, &#123;"Ray", 70&#125;&#125;; int k = 3; // 小根堆 priority_queue&lt;student, vector&lt;student&gt;, cmp_custom&gt; q; for (auto stu : stu_list) &#123; if (q.size() == k) &#123; if (stu.score &gt; q.top().score) &#123; q.pop(); q.push(stu); &#125; &#125; else q.push(stu); &#125; while (!q.empty()) &#123; cout &lt;&lt; q.top().name &lt;&lt; ":" &lt;&lt; q.top().score &lt;&lt; endl; q.pop(); &#125;&#125; 输出结果如下，每个人的名字后面跟着分数，结果是分数最大的3个人的信息： 12345albert@home-pc:/mnt/c++/datastruct$ g++ priorityqueue.cpp -o max_k_score -std=c++11albert@home-pc:/mnt/c++/datastruct$ ./max_k_scoreBella:79Andy:89Cary:92 自定义比较函数的另一种写法看到上个例子中自定义比较函数的写法比较怪，一般我们在排序时定义的比较函数使用lambda表达式就可以，而这里是不能直接这样写的，需要多转化一步，写成下面这种形式： 12auto cmp = [](student&amp; x, student&amp; y) &#123; return x.score &gt; y.score; &#125;;priority_queue&lt;student, vector&lt;student&gt;, decltype(cmp)&gt; q(cmp); 虽然看起来还是有点怪，但总比下面这样要好看的多： 123456struct cmp_custom &#123; bool operator()(student&amp; x, student&amp; y) &#123; return x.score &gt; y.score; &#125;&#125;;priority_queue&lt;student, vector&lt;student&gt;, cmp_custom&gt; q; 常用函数优先队列的常用函数与队列类似，常用的有以下这些，如果想了解详细的用法，请戳在线文档 函数名 含义 top 访问队列的头部元素 empty 判断优先队列内是否有元素 size 返回优先队列内元素个数 push 向优先队列中插入元素 emplace 在优先队列中构造元素 pop 从优先队列头部弹出元素 swap 与其他容器交换元素 总结 优先队列在一些需要部分排序的场景可以加快访问速度，降低时间复杂度 优先队列加速所付出的代价就是构建堆结构所需的内存，时间和空间总是一对矛盾共同体 以自定义结构作为元素的优先队列需要单独编写比较函数，可以使用lambda表达式，并用 decltype(cmp) 推导类型 需要注意的是这里的优先队列定义，第三个参数的需要的是比较函数的参数类型，而不是比较函数，区分与 std::sort 的不同 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 人在比较中奋进，同在比较中消亡，起初面临差距时会奋起直追，但是当努力过后发现距离反而越来越远时，便会麻木懈怠，曾经的努力没有用吗？我觉得不是，努力过不一定会成功，但是努力的过程已经印在了骨子里，这本身就是生活的一部分。你可以选择这条艰苦的路，同样也可以选择跳过，至于跳过时错失了什么，谁又知道呢？毕竟人生无法再来过，重新读档只发生在游戏世界中~ 2020-9-12 17:06:10]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>queue</tag>
        <tag>priority_queue</tag>
        <tag>heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git log根据特定条件查询日志并统计修改的代码行数]]></title>
    <url>%2Fblog%2F2020%2F09%2F05%2Fgit-log%E6%A0%B9%E6%8D%AE%E7%89%B9%E5%AE%9A%E6%9D%A1%E4%BB%B6%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%E5%B9%B6%E7%BB%9F%E8%AE%A1%E4%BF%AE%E6%94%B9%E7%9A%84%E4%BB%A3%E7%A0%81%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言随着年龄的增长和知识的积累，最近常常有种豁然开朗的感觉，或者对一个已经存在的事物突然有了新的认识，比如统计这个词很早就接触了，从没考虑过它是什么意思，而这篇总结的题目中用了统计一词，第一感觉应该是汇总、记录的意思，后来去查了词条定义，也确实就是类似的解释，从没有刻意去学这个词的含义，但是在每天的生活中已经潜移默化地归纳、总结出来了。 想要统计就得有数据源，而 git log 命令恰恰就能提供这个数据源，git log 本身就是一个显示日志的命令，日志记录的是代码库变化的数据，类似于描述代码库变化的 “史书”，想要描述历史就需要大量的数据支撑，想要统计修改的代码行数，只要我们从历史记录中找到需要计算的部分就可以了。 git log在统计之前我们需要先整理数据，杂乱无章的数据不是不能统计，只是计算起来更加的麻烦，所以在统计前需要先将数据规范化，所以我们需要先学习一下 git log 的相关操作。 我们以 redis 开源库为例，切换到 6.0 分支，提交记录定位到 7bf665f125a4771db095c83a7ad6ed46692cd314，以此为数据源，学习一下git log 的常用的查询方法，其实使用不同的条件查询就是整理、归类数据的过程。 git log 的用法多种多样，我们主要关心两个大类，分别是条件筛选和显示格式。 条件筛选git log 条件筛选的选项非常多，使用条件筛选的选项会影响显示的提交记录的范围，查找到想要显示的提交记录。 查询最近几条log使用 -number 参数可以查询最近几条提交提交记录： 1234567891011121314151617181920$ git log -3commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6.commit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branchcommit e15528bf1da1f1232fd08801ad382c915be94662Author: Itamar Haber &lt;itamar@redislabs.com&gt;Date: Thu Jul 16 21:31:36 2020 +0300 Adds SHA256SUM to redis-stable tarball upload (cherry picked from commit 5df0a64d30e7815c0a4a75a80f165fdee0bd1db6) 查询指定作者提交使用 --author 参数可以查询指定作者的提交记录： 12345678910111213Albert@DESKTOP-6746UC3 MINGW64 /d/data/maingit/redis (6.0)$ git log -2 --author='Oran Agra'commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6.commit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branch 查询指定时间段的日志这个可选参数比较多，比如 --since、--until、--before、--after 等等，从意思很容易分辨怎么使用： 查询2020-01-01到2020-04-01的提交记录 123456789101112$ git log -2 --after=2020-01-01 --before=2020-04-01commit 957e917a84ac9979f18145a4d0b53386f5ce4fd9 (tag: 6.0-rc3)Author: antirez &lt;antirez@gmail.com&gt;Date: Tue Mar 31 17:56:04 2020 +0200 Redis 6.0-RC3.commit ef1b1f01a84e969ea368e7fdbaf0d10615743269Author: antirez &lt;antirez@gmail.com&gt;Date: Tue Mar 31 17:41:23 2020 +0200 cast raxSize() to avoid warning with format spec. 恰好逮到了原作者的提交~ 查询1年前的提交记录 1234567891011121314151617181920$ git log -2 --until=1.year.agocommit 86aade9a024c3582665903d0cc0c5692c6677cfdMerge: 89ad0ca56 3bfcae247Author: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Thu Sep 5 13:30:26 2019 +0200 Merge pull request #6364 from oranagra/fix_module_aux_when Fix to module aux data rdb format for backwards compatibility with old check-rdbcommit 3bfcae247a1c51788940bd4d2f32751ead451e42Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Thu Sep 5 14:11:37 2019 +0300 Fix to module aux data rdb format for backwards compatibility with old check-rdb When implementing the code that saves and loads these aux fields we used rdb format that was added for that in redis 5.0, but then we added the 'when' field which meant that the old redis-check-rdb won't be able to skip these. this fix adds an opcode as if that 'when' is part of the module data. 查询包含指定描述内容的提交记录这里用可以使用 --grep 参数，可以过滤出包含指定内容的提交记录，这里指的是在 commit 描述中筛选符合条件的提交，比如查找提交描述中包含 client 的提交记录： 123456789101112131415161718192021222324$ git log -2 --grep='client'commit 0f75036c07db48dfcf605e090216a4447edc38fcAuthor: Wen Hui &lt;wen.hui.ware@gmail.com&gt;Date: Wed Jul 15 05:38:47 2020 -0400 correct error msg for num connections reaching maxclients in cluster mode (#7444) (cherry picked from commit d85af4d6f5fbe9cb9787b81583627cd74b47f838)commit f89f50dbd06247677b8cb3927cbb88c1b5384061Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Tue Jul 14 20:21:59 2020 +0300 diskless master disconnect replicas when rdb child failed (#7518) in case the rdb child failed, crashed or terminated unexpectedly redis would have marked the replica clients with repl_put_online_on_ack and then kill them only after a minute when no ack was received. it would not stream anything to these connections, so the only effect of this bug is a delay of 1 minute in the replicas attempt to re-connect. (cherry picked from commit a176cb56a3c0235adddde33fcbaee2369a5af73e) 查找指定分支的提交记录使用 git log 默认查找的是当前分支的提交记录，如果想查询其他分支的记录直接在命令后面加上分支名字就行，比如查询 arm 分支上的提交记录： 123456789101112131415161718192021222324$ git log -2 armcommit 7329cc39818a05c168e7d1e791afb03c089f1933 (origin/arm, arm)Author: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Sun Feb 19 15:07:08 2017 +0000 ARM: Avoid fast path for BITOP. GCC will produce certain unaligned multi load-store instructions that will be trapped by the Linux kernel since ARM v6 cannot handle them with unaligned addresses. Better to use the slower but safer implementation instead of generating the exception which should be anyway very slow.commit 4e9cf4cc7ed4b732fc4bb592f19ceb41d132954eAuthor: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Sun Feb 19 15:02:37 2017 +0000 ARM: Use libc malloc by default. I'm not sure how much test Jemalloc gets on ARM, moreover compiling Redis with Jemalloc support in not very powerful devices, like most ARMs people will build Redis on, is extremely slow. It is possible to enable Jemalloc build anyway if needed by using "make MALLOC=jemalloc". 其实在 git 体系中，分支名、commit、标签等拥有几乎相同的含义，所以在很多场景下可以扩展互换，比如 git log 后面加上分支名就可以查询指定分支的提交记录，如果加上 commit 就会查询这个 commit 之前的提交记录，如果加上标签，就可以查询这个标签之前的提交记录，比如我们加一个 commit 试试： 123456789101112131415161718192021222324$ git log -2 7329cc39818a05c168e7d1e791afb03c089f1933commit 7329cc39818a05c168e7d1e791afb03c089f1933 (origin/arm, arm)Author: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Sun Feb 19 15:07:08 2017 +0000 ARM: Avoid fast path for BITOP. GCC will produce certain unaligned multi load-store instructions that will be trapped by the Linux kernel since ARM v6 cannot handle them with unaligned addresses. Better to use the slower but safer implementation instead of generating the exception which should be anyway very slow.commit 4e9cf4cc7ed4b732fc4bb592f19ceb41d132954eAuthor: Salvatore Sanfilippo &lt;antirez@gmail.com&gt;Date: Sun Feb 19 15:02:37 2017 +0000 ARM: Use libc malloc by default. I'm not sure how much test Jemalloc gets on ARM, moreover compiling Redis with Jemalloc support in not very powerful devices, like most ARMs people will build Redis on, is extremely slow. It is possible to enable Jemalloc build anyway if needed by using "make MALLOC=jemalloc". 因为 commit id 就是之前的 arm 分支最新的记录，所以这个命令等价于 git log -2 arm 查询指定 commit 之间的提交记录如果想查询两个 commit 之前的提交记录，可以将两个 commit id 依次放在命令后面并用 .. 连接就可以了，格式为 git log commit1..commit2，需要注意的是这样查询出来的提交记录列表中不包含 commit1，其实列举出的就是 commit1 之后又做了哪些修改提交。 123456789101112$ git log e15528bf1da1f1232fd08801ad382c915be94662..7bf665f125a4771db095c83a7ad6ed46692cd314commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6.commit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branch 这个特性有一个应用就是在 merge 分支之前可以查询究竟会 merge 哪些记录，常见的用法比如 git log feature..dev 就是列举出 feature 分支合并到 dev 分支将要合并的提交记录有哪些。 123456$ git log 6.0..unstablecommit 324e22accf457edc996971bc97f5474349cd7c4c (unstable)Author: antirez &lt;antirez@gmail.com&gt;Date: Fri Dec 20 12:29:02 2019 +0100 Fix ip and missing mode in RM_GetClusterNodeInfo(). 查询指定文件的提交记录查询指定文件的提交记录一般直接在 git log 命令后面跟上文件名就可以，但是为了避免和分支名产生分歧，所以通常在文件名前面加上 -- 用来区分，-- 这个标识符就是用来防止混淆的，放在 -- 前面的是分支名，放在后面的是文件名，相同的作用不仅仅在 git log 命令中，在其他命令比如 git checkout 中也有相同的用法。 12345678910111213141516171819$ git log -2 -- redis.confcommit 7a536c2912be1fd9f62b26b7022a00644c88ef8bAuthor: Yossi Gottlieb &lt;yossigo@users.noreply.github.com&gt;Date: Fri Jul 10 11:33:47 2020 +0300 TLS: Session caching configuration support. (#7420) * TLS: Session caching configuration support. * TLS: Remove redundant config initialization. (cherry picked from commit 3e6f2b1a45176ac3d81b95cb6025f30d7aaa1393)commit 8312aa27d47c0befcf69eb74d0a5dc19745ffd32Author: antirez &lt;antirez@gmail.com&gt;Date: Mon Jun 22 11:21:21 2020 +0200 Clarify maxclients and cluster in conf. Remove myself too. (cherry picked from commit 59fd178014c7cca1b0c668b30ab0d991dd3030f3) 显示格式git log 除了可以筛选提交记录，还可以控制显示格式，普通不加参数，会显示作者、邮件、提交描述信息、日期等信息。 123456$ git log -1commit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6. 通过添加参数可以控制和改变显示格式，下面来看几条常见的 显示单行信息git log 默认会显示多行信息，使用 --oneline 后每条提交记录只显示一行信息，可以在一屏幕中查看到更多的信息 1234567891011$ git log -10 --oneline7bf665f12 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0) Redis 6.0.6.a5696bdf4 Run daily CI on PRs to release a branche15528bf1 Adds SHA256SUM to redis-stable tarball uploade28aa99af Support passing stack allocated module strings to moduleCreateArgvFromUserFormat (#7528)305143004 Send null for invalidate on flush (#7469)29b20fd52 Notify systemd on sentinel startup (#7168)5b3668121 Add registers dump support for Apple silicon (#7453)0f75036c0 correct error msg for num connections reaching maxclients in cluster mode (#7444)b1a01fda9 Fix command help for unexpected options (#7476)83f55f61a Refactor RM_KeyType() by using macro. (#7486) 显示每条记录中文件修改的具体行数和行体统计使用 --stat 参数就可以显示每条记录的中修改文件的具体行数和行数统计 1234567891011121314151617181920$ git log -2 --statcommit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6. 00-RELEASENOTES | 245 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ src/help.h | 4 +- src/version.h | 2 +- 3 files changed, 248 insertions(+), 3 deletions(-)commit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branch .github/workflows/daily.yml | 6 ++++-- 1 file changed, 4 insertions(+), 2 deletions(-) 显示每条提交记录中文件的增加行数和删除行数使用 --numstat 参数会把 --stat 参数中合并显示的修改行数拆分成增加行数和删除行数 123456789101112131415161718$ git log -2 --numstatcommit 7bf665f125a4771db095c83a7ad6ed46692cd314 (HEAD -&gt; 6.0, tag: 6.0.6, origin/6.0)Author: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 14:00:20 2020 +0300 Redis 6.0.6.245 0 00-RELEASENOTES2 2 src/help.h1 1 src/version.hcommit a5696bdf4f2687ab45f633ccb7cdc4ee9c2f957dAuthor: Oran Agra &lt;oran@redislabs.com&gt;Date: Sun Jul 19 15:33:21 2020 +0300 Run daily CI on PRs to release a branch4 2 .github/workflows/daily.yml 依次罗列各提交记录中每个文件中增加的行数和删除的行数要想达到这个目的需要用到 --prety=tformat: --numstat 参数，这样的显示格式便于统计 12345$ git log -2 --pretty=tformat: --numstat245 0 00-RELEASENOTES2 2 src/help.h1 1 src/version.h4 2 .github/workflows/daily.yml 统计修改的代码行数有了前面的铺垫，想要统一修改的行数就容易了，只要配合 awk 工具就可以完成统计了 12$ $ git log -2 --pretty=tformat: --numstat | awk '&#123;adds += $1; subs += $2; diffs += $1 - $2&#125; END &#123;printf "added lines: %s removed lines: %s, diff lines: %s\n", adds, subs, diffs&#125;'added lines: 252 removed lines: 5, diff lines: 247 还可以统计两个分支相差的代码行数 12$ git log 6.0..unstable --pretty=tformat: --numstat | awk '&#123;adds += $1; subs += $2; diffs += $1 - $2&#125; END &#123;printf "added lines: %s removed lines: %s, diff lines: %s\n", adds, subs, diffs&#125;'added lines: 5 removed lines: 2, diff lines: 3 到这里可以发现前面的知识都可以用上，前面筛选的参数变了，得到的结果就变了，我们可以根据需求来调整不同的参数 总结 git log 就是一部代码库记录的“史书”，对于曾经所做的修改可以做到有史可查 git log 的选项参数可以分为筛选参数和格式参数，筛选参数可以选择记录范围，格式参数可以控制显示样式 统计就是按照一定规律来将数据进行汇总，在进行汇总前需要将数据进行整理，这样汇总的工作才会更加顺利 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 慌慌张张，匆匆忙忙，原来生活就是这样~ 2020-9-7 00:05:18]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>awk</tag>
        <tag>log</tag>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中时间戳、时间字符串、时间结构对象之间的相互转化]]></title>
    <url>%2Fblog%2F2020%2F08%2F27%2FPython%E4%B8%AD%E6%97%B6%E9%97%B4%E6%88%B3%E3%80%81%E6%97%B6%E9%97%B4%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E6%97%B6%E9%97%B4%E7%BB%93%E6%9E%84%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BA%92%E8%BD%AC%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言使用 Python 写程序的时候常常要查看中间结果，需要把一些内容记录到日志中，为了不让新产生的日志覆盖之前的日志文件，所以通常使用时间字符串来作为文件名，类似于 2020827_233842.log 这种格式，就是表示程序在 2020-8-27 23:38:42 启动时产生的日志文件。 日志文件名需要一个字符串，但是这个时间字符串不能直接得到，需要将时间戳经过转化才可以，每次用到都要查一次有些浪费时间，总结到一起方便自己今后快速查找。 通过学习总结发现，操作时间和日期常用的模块有 time 和 datetime 这两个，并且 time 模块与 C 语言中的时间处理函数颇为相似，下面来一起看一下吧。 时间的表示形式显示生活中的时间表示形式多种多样，比如15分钟可以说成是1刻钟，半夜12点可以叫做子时，在程序中也有几种常用的表示形式，比如 python 中的时间戳、时间结构对象和时间字符串，分别对应 C 语言中的time_t、struct tm 和 char[]，处理函数的名字也很相近，自己可以扩展学习下，本文只列举 Python 的用法了 时间戳在Python中被实现成一个浮点数，表示从1970年1月1日00:00:00到当前时间所经历的秒数，因为是浮点数所以可以表示不足1秒的时间，而在有些语言中，比如C 语言中使用整数来表示这个值，在 python 中使用 time.time() 函数来获取时间戳： 12345678import timeval = time.time()print(val, type(val))'''输出结果1598769108.8337526 &lt;class 'float'&gt;''' 时间结构对象在 python 中使用 time.struct_time 这个类用来表示时间结构，其实是一个九元组，可以参考C语言中的 struct tm结构，表现形式相同，在 python 中这个九元组中元素依次表达的含义是：4位数年份、1-12月、1-31日、0-23小时、0-59分钟，0-59秒，0-6一周第几日，1-366一年第几日，{-1, 0, 1}夏令时标志。 通过代码我们可以尝试构造如下，不够时间使用时通常是通过函数转化，很少直接构造 time.struct_time 对象。 123456789import timeval = time.struct_time([2020, 8, 30, 14, 45, 30, 6, 243, 0])print(val, type(val))'''输出结果time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=14, tm_min=45, tm_sec=30, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;''' 时间字符串时间字符串是本质上是一种普通的字符串，因为用来表示时间所以感觉上有些不同，实际使用过程时会通过转化函数来生成时间字符串，然后就可以当场普通字符串来使用了，比如记录日志时间，作为文件名等都是常见用法。 12345678import timeval = '2019-08-30 15:04:00'print(val, type(val))'''输出结果2019-08-30 15:04:00 &lt;class 'str'&gt;''' 举例说明仅仅认识了这三种类型还是不够的，还要学习经常使用的转化函数才可以，上面提到的这三种类型一般不会从时间戳到字符串或者从字符戳到时间戳，都是通过时间结构对象来转化的，所以常见的转化是时间戳和时间结构对象的转化、时间结构对象和时间字符串的转化，需要用到的函数展示如下图： 示例代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import time# 生成时间戳t = time.time()print(t, type(t))'''1598775821.840567 &lt;class 'float'&gt;'''# 生成时间结构对象(本地时间)l = time.localtime()print(l, type(l))'''time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=16, tm_min=23, tm_sec=41, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;'''# 时间戳 -&gt; 时间结构对象(本地时间)l = time.localtime(t)print(l, type(l))'''time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=16, tm_min=23, tm_sec=41, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;'''# 生成时间结构对象(格林威治时间)g = time.gmtime()print(g, type(g))'''time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=8, tm_min=23, tm_sec=41, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;'''# 时间戳 -&gt; 时间结构对象(格林威治时间)g = time.gmtime(t)print(g, type(g))'''time.struct_time(tm_year=2020, tm_mon=8, tm_mday=30, tm_hour=8, tm_min=23, tm_sec=41, tm_wday=6, tm_yday=243, tm_isdst=0) &lt;class 'time.struct_time'&gt;'''# 生成时间字符串s = time.strftime("%Y-%m-%d %X")print(s, type(s))'''2020-08-30 16:23:41 &lt;class 'str'&gt;'''# 时间结构对象 -&gt; 时间字符串s = time.strftime("%Y-%m-%d %X",time.localtime())print(s, type(s))'''2020-08-30 16:23:41 &lt;class 'str'&gt;''''================================================================='# 定义时间字符串s = '2022-02-18 09:30:00'# 时间字符串 -&gt; 时间结构对象l = time.strptime(s, '%Y-%m-%d %X')print(l, type(l))'''time.struct_time(tm_year=2022, tm_mon=2, tm_mday=18, tm_hour=9, tm_min=30, tm_sec=0, tm_wday=4, tm_yday=49, tm_isdst=-1) &lt;class 'time.struct_time'&gt;'''# 时间结构对象 -&gt; 时间戳t = time.mktime(l)print(t, type(t))'''1645147800.0 &lt;class 'float'&gt;''''================================================================='# 生成固定格式(%a %b %d %H:%M:%S %Y)时间字符串s = time.asctime(time.localtime())print(s, type(s))'''Sun Aug 30 16:23:41 2020 &lt;class 'str'&gt;'''s = time.ctime(time.time())print(s, type(s))'''Sun Aug 30 16:23:41 2020 &lt;class 'str'&gt;''' 格式化符号将时间转化成字符串表示形式的时候，需要使用格式化符号，为了查找方便整理如下： 格式 含义 %a 本地（locale）简化星期名称 %A 本地完整星期名称 %b 本地简化月份名称 %B 本地完整月份名称 %c 本地相应的日期和时间表示 %d 一个月中的第几天（01 - 31） %H 一天中的第几个小时（24小时制，00 - 23） %I 第几个小时（12小时制，01 - 12） %j 一年中的第几天（001 - 366） %m 月份（01 - 12） %M 分钟数（00 - 59） %p 本地am或者pm的相应符 %S 秒（00 - 59） %U 一年中的星期数。（00 - 53星期天是一个星期的开始。） %w 一个星期中的第几天（0 - 6，0是星期天） %W 和%U基本相同，不同的是%W以星期一为一个星期的开始。 %x 本地相应日期 %X 本地相应时间 %y 去掉世纪的年份（00 - 99） %Y 完整的年份 %Z 时区的名字（如果不存在为空字符） 总结 时间戳和时间字符串的转化，通常要经过时间结构对象作为中间结果。 时间戳也可以通过 time.ctime() 函数直接转化为时间字符串，但格式固定。 常用来表示文件名的时间字符串写法：time.strftime(&quot;%Y%m%d_%H%M%S&quot;, time.localtime()) ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有时选择的意义确实高于努力的结果，认清这一点，学会适当的放下，会让焦躁的生活更美好一点，毕竟全部都坚持真的太累了，有时收益真的不高~ 2020-8-30 21:50:19]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>time</tag>
        <tag>datetime</tag>
        <tag>转化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB调试指北-启动调试或者附加到进程]]></title>
    <url>%2Fblog%2F2020%2F08%2F17%2FGDB%E8%B0%83%E8%AF%95%E6%8C%87%E5%8C%97-%E5%90%AF%E5%8A%A8%E8%B0%83%E8%AF%95%E6%88%96%E8%80%85%E9%99%84%E5%8A%A0%E5%88%B0%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言要想使用 gdb 调试程序，必须让 gdb 程序和被调试程序建立联系，这种联系可以通过程序的可执行文件、core文件或者正在运行的进程来建立，具体调试的时候使用的选项不同，涉及到参数的传递，选项的顺序，多进程启动前的设置等等，接下来可以看一些常见用法。 测试样例首先来写一段简单的但是会自动崩溃的代码，主要是为了展示core文件的调试方法，通过调试崩溃产生的core文件是一种很直接的查找问题的方法，可以帮助我们快速定位到问题的栈帧，进而找到具体的逻辑代码。 代码内容新建文件 examplepro.cpp，编写代码内容如下： 123456789101112131415161718#include &lt;iostream&gt;using namespace std;int main(int argc, char* argv[])&#123; if (argc &gt; 1) cout &lt;&lt; "argv[1] = " &lt;&lt; argv[1] &lt;&lt; endl; int a = 3, b = 4; int c = a + b; cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl; int *p = NULL; *p = c; return 0;&#125; 代码编译1g++ examplepro.cpp -o examplepro -g 运行程序123albert@home-pc:~/WorkSpace/cpp$ ./exampleproc = 7Segmentation fault (core dumped) 我们发现程序在运行之后发生了段错误，这是一种比较常见的BUG，通常由访问无效内存导致，查看程序目录下内容，多了一个叫 core 的文件。 12albert@home-pc:~/WorkSpace/cpp$ lscore examplepro examplepro.cpp 通过这一步你可能看不到这个 core 文件，需要检查两点，第一是编译的时候需要加 -g 选项，第二是使用 ulimit -c unlimited 命令设置core文件占用空间的最小限制，默认大小为0，也就是不产生 core 文件，需要改为 unlimited 才可以，如果你确定产生的 core 文件不会太大，也可以设置一个具体的数值。 使用gdb调试有了上面的程序我们就可以进行调试了，因为已经产生了 core 文件，所以先来调试一下 core 文件，看下程序崩溃的原因。 使用gdb调试core文件启动程序的语法如下，gdb 命令之后跟程序名，然后后面跟着 core 文件的名字： 1gdb examplepro core 具体调试的时候需要换成自己的崩溃的程序名，而core文件大多数是 core.进程id 的形式。 调试过程12345678910111213141516171819202122albert@home-pc:~/WorkSpace/cpp$ gdb examplepro coreGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word"...Reading symbols from examplepro...done.[New LWP 19786]Core was generated by `./examplepro'.Program terminated with signal SIGSEGV, Segmentation fault.#0 0x0000000000400932 in main (argc=1, argv=0x7ffd23cc3a18) at examplepro.cpp:1515 *p = c;(gdb) 从调试信息来看一下就定位到了问题，在代码的第15行发生了段错误，也就是我们刚刚给野指针赋值的代码。 使用gdb直接启动程序这种情况就是调试运行，相当于在 gdb 的监控下启动程序，一旦发生错误，gdb 会给出响应的提示，启动方式很简单，gdb 命令之后直接跟着程序名字就可以了。 1gdb examplepro 调试过程123456789101112131415161718192021222324albert@home-pc:~/WorkSpace/cpp$ gdb exampleproGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word"...Reading symbols from examplepro...done.(gdb) runStarting program: /home/albert/WorkSpace/cpp/exampleproc = 7Program received signal SIGSEGV, Segmentation fault.0x0000000000400932 in main (argc=1, argv=0x7fffffffdd18) at examplepro.cpp:1515 *p = c;(gdb) 这种情况下，启动之后需要输入 run 命令才可以运行程序，这时发现程序又崩溃了。 如果被调试的程序有参数的话，需要将启动的命令进行修改，写成 gdb --args examplepro testparam1，加上 --args 选项，然后将参数罗列在后面就好了，因为看这些声明很麻烦，我们利用之前学过的 -q 选项来屏蔽启动说明，测试如下： 1234567891011albert@home-pc:~/WorkSpace/cpp$ gdb -q --args examplepro NBReading symbols from examplepro...done.(gdb) runStarting program: /home/albert/WorkSpace/cpp/examplepro NBargv[1] = NBc = 7Program received signal SIGSEGV, Segmentation fault.0x0000000000400932 in main (argc=2, argv=0x7fffffffdd08) at examplepro.cpp:1515 *p = c;(gdb) 还有一种写法就是启动 gdb 之后再传参数，具体操作方法如下： 1234567891011albert@home-pc:~/WorkSpace/cpp$ gdb -q exampleproReading symbols from examplepro...done.(gdb) run NBStarting program: /home/albert/WorkSpace/cpp/examplepro NBargv[1] = NBc = 7Program received signal SIGSEGV, Segmentation fault.0x0000000000400932 in main (argc=2, argv=0x7fffffffdd08) at examplepro.cpp:1515 *p = c;(gdb) 这种情况是先启动 gdb，然后在执行 run 命令的时候传递参数。 使用gdb调试正在运行的文件这时需要获得被套是程序的进程id，可以使用 ps、top 或者 pidof 命令来获取进程id，然后通过 attch 的方式附加到进程。 比如查到需要调试的 examplepro 程序进程号是 3598，那么可以直接启动 gdb 附加到这个进程： 1gdb examplepro 3598 也可以先启动 gdb，然后使用 attach 命令附加到进程： 123albert@home-pc:~/WorkSpace/cpp$ gdb -q exampleproReading symbols from examplepro...done.(gdb) attach 3598 如果此时提示进程拒绝被附加通常是权限问题，可以使用所属账号调试，或者可以尝试 sudo 命令。 语法对比常见的调试方式就文中提到的这几种，特整理成表格方便对比和查找： 语法 解释 gdb examlepro 直接 gdb 调试启动 gdb examlepro core.3598 调试崩溃的 core 文件 gdb examlepro 3598gdb -p 3598 附加到正在运行的程序进程上 gdb attach 3598 先启动gdb，后附加到程序上 总结 gdb 不但可以调试 core 文件，还可以调试正在运行的程序，这对于难重现的 bug 来说非常有帮助 在调试正在运行的程序时可以使用 pidof 命令来直接获取被调试程序的进程号 gdb 调试附加的进程的时候要注意权限问题，如果不成功可以尝试 sudo 命令 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 兜兜转转又换了一个住所，匆匆忙忙如蝼蚁般迁徙，路程短了，可选的路却少了。回头看看，一个窝、一段事、一群人而已~ 2020-8-25 00:24:01]]></content>
      <categories>
        <category>GDB</category>
      </categories>
      <tags>
        <tag>GDB</tag>
        <tag>manual</tag>
        <tag>调试</tag>
        <tag>启动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis源码-BFS方式浏览main函数]]></title>
    <url>%2Fblog%2F2020%2F08%2F05%2FRedis%E6%BA%90%E7%A0%81-BFS%E6%96%B9%E5%BC%8F%E6%B5%8F%E8%A7%88main%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言欠下的技术债慢慢还，继续为去年吹过的牛而努力。去年年末的时候意识到自己掌握的知识还不够深入，决定开始看一些开源项目的源码，因为当时 Redis 的兴起，所以瞄准了准备从它下手，之后确实看了一部分内容，比如跳表、网络事件库等等，后来过年就鸽了。今年开始一直熟悉新的业务，比较懒没跟进，最近间歇性踌躇满志又发作了，准备抽时间再捋顺一遍，老规矩，还是从 main() 函数下手。 对于 C/C++ 程序一定是从 main() 函数开头的，这是我们切入的一个点，至于怎么找到 main 函数，每个人有不同的方法，最暴力的方法当然就是全文搜索了，不过较为成熟的项目一般搜索出来都不止一个 main 函数，因为整个项目完整构建下来不止一个程序。 像 redis 这个项目最起码有服务器和客户端两个程序，源码中至少包含了两个 main 函数，再加上一些测试程序，main 函数在源码中会有很多。再比如 Lua 的源代码中包含和解释器和编译器，如果直接搜索至少会找到两个 main 函数。 redis 服务器程序的 main 函数在文件 src/server.c 中，之前好像是在 redis.c 文件中后来改名了，这都不重要，反正你需要从搜索出来的 main 函数中找到一个开始的地方，这个花不了多少时间。 看代码的方式标题中提到了 BFS 方式看代码，而 BFS 指的是广度优先搜索，与之相对应的是 DFS 深度优先搜索，对于不含异步调用的单线程程序来说，执行代码是以深度优先搜索的方式，遇到一个函数就调用进去，在函数中又遇到另一个函数再调用进去，当函数执行完成返回到上一层。 为什么选择 BFS 方式看代码呢？因为这样可以在短时间内更全面的了解代码结构，我们先看第一层，当第一层浏览完成之后再进入到第二层，比如我们先看 main 函数，即使 main 函数调用了很多不认识的函数也不要去管，从名字大概判断一些作用就可以了，不用纠结具体的实现内容，当 main 函数全部看完了再进入到第二层去了解它调用的那些函数。 总之使用 BFS 方式看代码就要有一种“不懂装懂”的态度，不然容易陷入细节，无法整体把握。 Redis 服务器的 main 函数redis 服务器的 main 函数代码量不是很大，总共 200 行左右，我选择了 6.0.6 这个版本 7bf665f125a4771db095c83a7ad6ed46692cd314，因为只是学习源码，没有特殊情况就不更新版本了，保证环境的统一，我先把代码贴一份在这，后面再来慢慢看。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214int main(int argc, char **argv) &#123; struct timeval tv; int j;#ifdef REDIS_TEST if (argc == 3 &amp;&amp; !strcasecmp(argv[1], "test")) &#123; if (!strcasecmp(argv[2], "ziplist")) &#123; return ziplistTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "quicklist")) &#123; quicklistTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "intset")) &#123; return intsetTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "zipmap")) &#123; return zipmapTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "sha1test")) &#123; return sha1Test(argc, argv); &#125; else if (!strcasecmp(argv[2], "util")) &#123; return utilTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "endianconv")) &#123; return endianconvTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "crc64")) &#123; return crc64Test(argc, argv); &#125; else if (!strcasecmp(argv[2], "zmalloc")) &#123; return zmalloc_test(argc, argv); &#125; return -1; /* test not found */ &#125;#endif /* We need to initialize our libraries, and the server configuration. */#ifdef INIT_SETPROCTITLE_REPLACEMENT spt_init(argc, argv);#endif setlocale(LC_COLLATE,""); tzset(); /* Populates 'timezone' global. */ zmalloc_set_oom_handler(redisOutOfMemoryHandler); srand(time(NULL)^getpid()); gettimeofday(&amp;tv,NULL); crc64_init(); uint8_t hashseed[16]; getRandomBytes(hashseed,sizeof(hashseed)); dictSetHashFunctionSeed(hashseed); server.sentinel_mode = checkForSentinelMode(argc,argv); initServerConfig(); ACLInit(); /* The ACL subsystem must be initialized ASAP because the basic networking code and client creation depends on it. */ moduleInitModulesSystem(); tlsInit(); /* Store the executable path and arguments in a safe place in order * to be able to restart the server later. */ server.executable = getAbsolutePath(argv[0]); server.exec_argv = zmalloc(sizeof(char*)*(argc+1)); server.exec_argv[argc] = NULL; for (j = 0; j &lt; argc; j++) server.exec_argv[j] = zstrdup(argv[j]); /* We need to init sentinel right now as parsing the configuration file * in sentinel mode will have the effect of populating the sentinel * data structures with master nodes to monitor. */ if (server.sentinel_mode) &#123; initSentinelConfig(); initSentinel(); &#125; /* Check if we need to start in redis-check-rdb/aof mode. We just execute * the program main. However the program is part of the Redis executable * so that we can easily execute an RDB check on loading errors. */ if (strstr(argv[0],"redis-check-rdb") != NULL) redis_check_rdb_main(argc,argv,NULL); else if (strstr(argv[0],"redis-check-aof") != NULL) redis_check_aof_main(argc,argv); if (argc &gt;= 2) &#123; j = 1; /* First option to parse in argv[] */ sds options = sdsempty(); char *configfile = NULL; /* Handle special options --help and --version */ if (strcmp(argv[1], "-v") == 0 || strcmp(argv[1], "--version") == 0) version(); if (strcmp(argv[1], "--help") == 0 || strcmp(argv[1], "-h") == 0) usage(); if (strcmp(argv[1], "--test-memory") == 0) &#123; if (argc == 3) &#123; memtest(atoi(argv[2]),50); exit(0); &#125; else &#123; fprintf(stderr,"Please specify the amount of memory to test in megabytes.\n"); fprintf(stderr,"Example: ./redis-server --test-memory 4096\n\n"); exit(1); &#125; &#125; /* First argument is the config file name? */ if (argv[j][0] != '-' || argv[j][1] != '-') &#123; configfile = argv[j]; server.configfile = getAbsolutePath(configfile); /* Replace the config file in server.exec_argv with * its absolute path. */ zfree(server.exec_argv[j]); server.exec_argv[j] = zstrdup(server.configfile); j++; &#125; /* All the other options are parsed and conceptually appended to the * configuration file. For instance --port 6380 will generate the * string "port 6380\n" to be parsed after the actual file name * is parsed, if any. */ while(j != argc) &#123; if (argv[j][0] == '-' &amp;&amp; argv[j][1] == '-') &#123; /* Option name */ if (!strcmp(argv[j], "--check-rdb")) &#123; /* Argument has no options, need to skip for parsing. */ j++; continue; &#125; if (sdslen(options)) options = sdscat(options,"\n"); options = sdscat(options,argv[j]+2); options = sdscat(options," "); &#125; else &#123; /* Option argument */ options = sdscatrepr(options,argv[j],strlen(argv[j])); options = sdscat(options," "); &#125; j++; &#125; if (server.sentinel_mode &amp;&amp; configfile &amp;&amp; *configfile == '-') &#123; serverLog(LL_WARNING, "Sentinel config from STDIN not allowed."); serverLog(LL_WARNING, "Sentinel needs config file on disk to save state. Exiting..."); exit(1); &#125; resetServerSaveParams(); loadServerConfig(configfile,options); sdsfree(options); &#125; serverLog(LL_WARNING, "oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo"); serverLog(LL_WARNING, "Redis version=%s, bits=%d, commit=%s, modified=%d, pid=%d, just started", REDIS_VERSION, (sizeof(long) == 8) ? 64 : 32, redisGitSHA1(), strtol(redisGitDirty(),NULL,10) &gt; 0, (int)getpid()); if (argc == 1) &#123; serverLog(LL_WARNING, "Warning: no config file specified, using the default config. In order to specify a config file use %s /path/to/%s.conf", argv[0], server.sentinel_mode ? "sentinel" : "redis"); &#125; else &#123; serverLog(LL_WARNING, "Configuration loaded"); &#125; server.supervised = redisIsSupervised(server.supervised_mode); int background = server.daemonize &amp;&amp; !server.supervised; if (background) daemonize(); initServer(); if (background || server.pidfile) createPidFile(); redisSetProcTitle(argv[0]); redisAsciiArt(); checkTcpBacklogSettings(); if (!server.sentinel_mode) &#123; /* Things not needed when running in Sentinel mode. */ serverLog(LL_WARNING,"Server initialized"); #ifdef __linux__ linuxMemoryWarnings(); #endif moduleLoadFromQueue(); ACLLoadUsersAtStartup(); InitServerLast(); loadDataFromDisk(); if (server.cluster_enabled) &#123; if (verifyClusterConfigWithData() == C_ERR) &#123; serverLog(LL_WARNING, "You can't have keys in a DB different than DB 0 when in " "Cluster mode. Exiting."); exit(1); &#125; &#125; if (server.ipfd_count &gt; 0 || server.tlsfd_count &gt; 0) serverLog(LL_NOTICE,"Ready to accept connections"); if (server.sofd &gt; 0) serverLog(LL_NOTICE,"The server is now ready to accept connections at %s", server.unixsocket); if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123; if (!server.masterhost) &#123; redisCommunicateSystemd("STATUS=Ready to accept connections\n"); redisCommunicateSystemd("READY=1\n"); &#125; else &#123; redisCommunicateSystemd("STATUS=Waiting for MASTER &lt;-&gt; REPLICA sync\n"); &#125; &#125; &#125; else &#123; InitServerLast(); sentinelIsRunning(); if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123; redisCommunicateSystemd("STATUS=Ready to accept connections\n"); redisCommunicateSystemd("READY=1\n"); &#125; &#125; /* Warning the user about suspicious maxmemory setting. */ if (server.maxmemory &gt; 0 &amp;&amp; server.maxmemory &lt; 1024*1024) &#123; serverLog(LL_WARNING,"WARNING: You specified a maxmemory value that is less than 1MB (current value is %llu bytes). Are you sure this is what you really want?", server.maxmemory); &#125; redisSetCpuAffinity(server.server_cpulist); aeMain(server.el); aeDeleteEventLoop(server.el); return 0;&#125; main 函数分段解释函数名及参数12345678int main(int argc, char **argv) &#123; struct timeval tv; int j; //... //... return 0&#125; 这就是一个标准的 main 函数，参数 argc 和 argv 对于一个命令行程序来说可以是重头戏，肯定会拿来做重度解析的，函数开头还定义了 tv 和 j 两个变量，不知道干嘛的，接着往下看吧。 启动测试程序12345678910111213141516171819202122232425#ifdef REDIS_TEST if (argc == 3 &amp;&amp; !strcasecmp(argv[1], "test")) &#123; if (!strcasecmp(argv[2], "ziplist")) &#123; return ziplistTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "quicklist")) &#123; quicklistTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "intset")) &#123; return intsetTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "zipmap")) &#123; return zipmapTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "sha1test")) &#123; return sha1Test(argc, argv); &#125; else if (!strcasecmp(argv[2], "util")) &#123; return utilTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "endianconv")) &#123; return endianconvTest(argc, argv); &#125; else if (!strcasecmp(argv[2], "crc64")) &#123; return crc64Test(argc, argv); &#125; else if (!strcasecmp(argv[2], "zmalloc")) &#123; return zmalloc_test(argc, argv); &#125; return -1; /* test not found */ &#125;#endif 当宏定义 REDIS_TEST 存在，并且参数合适的情况下启动测试程序，argv[0] 肯定是指 redis 服务器喽，那 argv[1] 的值如果是 test，而 argv[2] 的值是 ziplist，那么会调用 ziplist 的测试函数 ziplistTest，如果 argv[2] 的值是 zmalloc，那么会调用测试函数 zmalloc_test，为啥这里函数名命名规范不统一呢？挠头。 程序环境初始化12345678910 /* We need to initialize our libraries, and the server configuration. */#ifdef INIT_SETPROCTITLE_REPLACEMENT spt_init(argc, argv);#endif setlocale(LC_COLLATE,""); tzset(); /* Populates 'timezone' global. */ zmalloc_set_oom_handler(redisOutOfMemoryHandler); srand(time(NULL)^getpid()); gettimeofday(&amp;tv,NULL); crc64_init(); 当 INIT_SETPROCTITLE_REPLACEMENT 这个宏存在的时候，调用 spt_init 函数来为设置程序标题做准备 setlocale() 用来设置地点信息，这一句应该是设置成依赖操作系统的地点信息，比如中国，韩国等等 tzset() 设置时区，这里可能影响到程序运行后，调整时区是否对程序产生影响 srand(time(NULL)^getpid()); 初始化随机种子 gettimeofday(&amp;tv,NULL); 这里用到了函数开头定义的一个变量 tv，用来获取当前时间 crc64_init(); 循环冗余校验初始化，crc 神奇的存在 初始化配置信息123456789uint8_t hashseed[16];getRandomBytes(hashseed,sizeof(hashseed));dictSetHashFunctionSeed(hashseed);server.sentinel_mode = checkForSentinelMode(argc,argv);initServerConfig();ACLInit(); /* The ACL subsystem must be initialized ASAP because the basic networking code and client creation depends on it. */moduleInitModulesSystem();tlsInit(); 定一个16字节的空间用来存放哈希种子 随机获取一段16字节数据作为种子 将刚刚获取的种子数据设置到hash函数中 分析命令行参数，判断是否是哨兵模式 初始化服务器配置 ACL 初始化，不用管它具体是什么，进入下一层时自然会看到 初始化模块系统 tls 初始化，存疑，好奇的话进去看看也可以，好吧，原来是 ssl 那一套，够喝一壶的 存储参数信息123456/* Store the executable path and arguments in a safe place in order * to be able to restart the server later. */server.executable = getAbsolutePath(argv[0]);server.exec_argv = zmalloc(sizeof(char*)*(argc+1));server.exec_argv[argc] = NULL;for (j = 0; j &lt; argc; j++) server.exec_argv[j] = zstrdup(argv[j]); 这一小节比较简单，注释写的也很清楚，就是将命令行参数存储起来，方便重启 redis 服务 根据参数确定启动方式123456789101112131415/* We need to init sentinel right now as parsing the configuration file * in sentinel mode will have the effect of populating the sentinel * data structures with master nodes to monitor. */if (server.sentinel_mode) &#123; initSentinelConfig(); initSentinel();&#125;/* Check if we need to start in redis-check-rdb/aof mode. We just execute * the program main. However the program is part of the Redis executable * so that we can easily execute an RDB check on loading errors. */if (strstr(argv[0],"redis-check-rdb") != NULL) redis_check_rdb_main(argc,argv,NULL);else if (strstr(argv[0],"redis-check-aof") != NULL) redis_check_aof_main(argc,argv); 当启用哨兵模式的时候初始化额外的配置，啥是哨兵，现在还不用知道啊，从字面上来看就好了，反正知道命令行里如果指定了哨兵模式就要额外初始化一点东西。 下面这两个参数有点意思，简单扩展下，rdb 和 aof 是 redis 的两种数据落地的持久化方式，这里有意思的地方是判断了 argv[0] 这个参数，一般 argv[0] 是程序的名字，这个是固定不变的，而 redis 这里将程序名字作为参数来判断，也就是说你把可执行程序换个名字运行，它的行为就会发生变化。 处理并加载命令行参数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465if (argc &gt;= 2) &#123; j = 1; /* First option to parse in argv[] */ sds options = sdsempty(); char *configfile = NULL; /* Handle special options --help and --version */ if (strcmp(argv[1], "-v") == 0 || strcmp(argv[1], "--version") == 0) version(); if (strcmp(argv[1], "--help") == 0 || strcmp(argv[1], "-h") == 0) usage(); if (strcmp(argv[1], "--test-memory") == 0) &#123; if (argc == 3) &#123; memtest(atoi(argv[2]),50); exit(0); &#125; else &#123; fprintf(stderr,"Please specify the amount of memory to test in megabytes.\n"); fprintf(stderr,"Example: ./redis-server --test-memory 4096\n\n"); exit(1); &#125; &#125; /* First argument is the config file name? */ if (argv[j][0] != '-' || argv[j][1] != '-') &#123; configfile = argv[j]; server.configfile = getAbsolutePath(configfile); /* Replace the config file in server.exec_argv with * its absolute path. */ zfree(server.exec_argv[j]); server.exec_argv[j] = zstrdup(server.configfile); j++; &#125; /* All the other options are parsed and conceptually appended to the * configuration file. For instance --port 6380 will generate the * string "port 6380\n" to be parsed after the actual file name * is parsed, if any. */ while(j != argc) &#123; if (argv[j][0] == '-' &amp;&amp; argv[j][1] == '-') &#123; /* Option name */ if (!strcmp(argv[j], "--check-rdb")) &#123; /* Argument has no options, need to skip for parsing. */ j++; continue; &#125; if (sdslen(options)) options = sdscat(options,"\n"); options = sdscat(options,argv[j]+2); options = sdscat(options," "); &#125; else &#123; /* Option argument */ options = sdscatrepr(options,argv[j],strlen(argv[j])); options = sdscat(options," "); &#125; j++; &#125; if (server.sentinel_mode &amp;&amp; configfile &amp;&amp; *configfile == '-') &#123; serverLog(LL_WARNING, "Sentinel config from STDIN not allowed."); serverLog(LL_WARNING, "Sentinel needs config file on disk to save state. Exiting..."); exit(1); &#125; resetServerSaveParams(); loadServerConfig(configfile,options); sdsfree(options);&#125; 这段内容很长，但是核心的内容不多，前一部分是判断特殊参数，用来显示程序使用方法，启动内存测试等等，中间部分是分析命令行参数保存到字符串中，最后几行是读取服务器配置文件，并使用字符串中的参数选项覆盖文件中的部分配置。 打印启动和警告信息1234567891011121314serverLog(LL_WARNING, "oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo");serverLog(LL_WARNING, "Redis version=%s, bits=%d, commit=%s, modified=%d, pid=%d, just started", REDIS_VERSION, (sizeof(long) == 8) ? 64 : 32, redisGitSHA1(), strtol(redisGitDirty(),NULL,10) &gt; 0, (int)getpid());if (argc == 1) &#123; serverLog(LL_WARNING, "Warning: no config file specified, using the default config. In order to specify a config file use %s /path/to/%s.conf", argv[0], server.sentinel_mode ? "sentinel" : "redis");&#125; else &#123; serverLog(LL_WARNING, "Configuration loaded");&#125; 打印 redis 服务器启动信息，比如版本号，pid，警告信息等等，没有实际修改数据。 守护模式和初始化123456789server.supervised = redisIsSupervised(server.supervised_mode);int background = server.daemonize &amp;&amp; !server.supervised;if (background) daemonize();initServer();if (background || server.pidfile) createPidFile();redisSetProcTitle(argv[0]);redisAsciiArt();checkTcpBacklogSettings(); 根据守护进程配置和是否受监督来决定是否作为守护进程，什么是受监督，到现在还不知道，但是本着不懂装懂的方式看代码，可以认为我们懂了，后面自然还会有解释的地方。 接着就调用了 initServer(); 函数，这个初始化函数内容是比较长的，之前版本中很多 mian 函数中的内容都移到了这里面，初始化完成后创建 Pid 文件，设置进程名字，显示 redis 的Logo，检查一些配置，这个 backlog 参数之前面试的时候还被问到过，好奇的话可以提前了解一下。 哨兵模式判断启动并加载持久化数据1234567891011121314151617181920212223242526272829303132333435363738 if (!server.sentinel_mode) &#123; /* Things not needed when running in Sentinel mode. */ serverLog(LL_WARNING,"Server initialized");#ifdef __linux__ linuxMemoryWarnings();#endif moduleLoadFromQueue(); ACLLoadUsersAtStartup(); InitServerLast(); loadDataFromDisk(); if (server.cluster_enabled) &#123; if (verifyClusterConfigWithData() == C_ERR) &#123; serverLog(LL_WARNING, "You can't have keys in a DB different than DB 0 when in " "Cluster mode. Exiting."); exit(1); &#125; &#125; if (server.ipfd_count &gt; 0 || server.tlsfd_count &gt; 0) serverLog(LL_NOTICE,"Ready to accept connections"); if (server.sofd &gt; 0) serverLog(LL_NOTICE,"The server is now ready to accept connections at %s", server.unixsocket); if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123; if (!server.masterhost) &#123; redisCommunicateSystemd("STATUS=Ready to accept connections\n"); redisCommunicateSystemd("READY=1\n"); &#125; else &#123; redisCommunicateSystemd("STATUS=Waiting for MASTER &lt;-&gt; REPLICA sync\n"); &#125; &#125;&#125; else &#123; InitServerLast(); sentinelIsRunning(); if (server.supervised_mode == SUPERVISED_SYSTEMD) &#123; redisCommunicateSystemd("STATUS=Ready to accept connections\n"); redisCommunicateSystemd("READY=1\n"); &#125;&#125; 这段代码看起来像是再做一些通知提醒，其中比较重要的几个函数是moduleLoadFromQueue()、 InitServerLast() 和 loadDataFromDisk() ，第一个函数是加载模块的，第二个函数是在模块加载完成之后才能初始化的部分内容，最后一个是从磁盘加载数据到内存，这也是 redis 支持持久化的必要保证。 打印内存警告并启动事件监听123456789/* Warning the user about suspicious maxmemory setting. */if (server.maxmemory &gt; 0 &amp;&amp; server.maxmemory &lt; 1024*1024) &#123; serverLog(LL_WARNING,"WARNING: You specified a maxmemory value that is less than 1MB (current value is %llu bytes). Are you sure this is what you really want?", server.maxmemory);&#125;redisSetCpuAffinity(server.server_cpulist);aeMain(server.el);aeDeleteEventLoop(server.el);return 0; 看到这段代码我们就来到了 main 函数结尾的部分，redisSetCpuAffinity() 是要做些和 CPU 相关的设置或配置，aeMain() 是主逻辑，对于提供服务的程序来说里面大概率是一个死循环，再满足指定的条件下才会打断退出，而 aeDeleteEventLoop() 就是循环结束时清理事件的操作，到此为止 main 函数就执行完啦。 彩蛋这个 main 函数的代码中有一个神奇的用法不知道大家有没有发现，就是下面这句话： 123serverLog(LL_WARNING, "You can't have keys in a DB different than DB 0 when in " "Cluster mode. Exiting."); 是不是看起来有些奇怪，不用管这个函数的定义是怎样的，可以告诉大家这个函数的定义类似于 printf 函数，只不过在最前面加了一个整型参数，那么调用这个函数时传了几个参数呢？3个？2个？，这个地方很神奇的会把两个字符串拼接到一起，类似于下面的写法： 12serverLog(LL_WARNING, "You can't have keys in a DB different than DB 0 when in Cluster mode. Exiting."); 这样的字符串不仅可以分成两行，实际上可以分成任意行，最后都会拼接在一起，是不是很神奇。 总结 j 这个变量在 redis 的源码中经常出现，应该是作者的行为习惯吧，有些人爱用 i，而这个作者 antirez 爱用 j。 不能一口吃个胖子，看代码也是一样，不能期望一次性把所有的内容都看懂，一段时间后自己的代码都看不懂了，跟别说别人写的了。 redis 代码中频繁使用 server 这个变量，从 main 函数分析中也能看到，这个是个全局变量，代表了整个 redis 服务器程序数据。 不懂装懂或者说不求甚解是熟悉代码整体结构的一项优秀品质，这时候只要看个大概就可以了，真到熟悉细节的时候才是需要钻研的时候。 代码风格完全统一还是比较难实现的，从一个 main 函数中也可以看到，大部分函数是驼峰命名法，还要少量的下划线命名和帕斯卡命名。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 你微笑的模样，提醒着我不要躲藏，坚持原来的方向，哪怕最后遍体鳞伤，困难只会让坚持的人越来越强，共勉~ 2020-8-15 23:48:53]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>源码</tag>
        <tag>C</tag>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB调试指北-启动GDB并查看说明信息]]></title>
    <url>%2Fblog%2F2020%2F08%2F01%2FGDB%E8%B0%83%E8%AF%95%E6%8C%87%E5%8C%97-%E5%90%AF%E5%8A%A8GDB%E5%B9%B6%E6%9F%A5%E7%9C%8B%E8%AF%B4%E6%98%8E%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言学习 gdb 使用是一个漫长的过程，先了解一下 gdb 的启动方式和基础信息的查看方法，能够帮助我们更全面的认知这个工具。gdb 是一个交互式命令行程序，在使用 gdb 调试的时候不断的在命令行内输入命令，然后 gdb 程序就会给出反馈信息，这在很大程序上可以帮助我们调试程序问题。 gdb 版本查看gdb 的安装教程网络上有很多，这里就不提供安装步骤了，可以直接通过命令行，也可以从源码安装，找个教程一步步操作就行了，安装完之后使用 which 命令查看一下程序安装的位置： 12albert@home-pc:~$ which gdb/usr/bin/gdb 确认 gdb 已经安装后我们再看一下程序版本，我用的是 Ubuntu 16.04 版本中匹配的 gdb 程序，版本稍微有些低，据说 9.x 版本中对 Python 支持的非常好，调试的时候查看变量更加方便了，这些神奇的特性我们暂时还用不到，先简单了解下就好： 12345678910111213141516albert@home-pc:~$ gdb --versionGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word".albert@home-pc:~$ gdb 启动直接启动gdb 作为一个程序和其他的程序启动方式是一样的，直接敲入 gdb 命令回车就可以了： 12345678910111213141516albert@home-pc:~$ gdbGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word".(gdb) 看着是不是有些眼熟，这就是刚刚查 gdb 版本时看到的那段文字，只不过这段文字结束时不是返回到命令行，而是显现出了 (gdb) 的字样，我们暂时把它叫做 gdb 命令行，这就是我们与 gdb 程序进行交互的主要途径了。 去掉版本信息启动上面启动 gdb 时出现的这段文字很长，有时候反复调试程序时看到这段文字有点烦，想把它去掉怎么办？非常简单，在启动时加上 -q 参数就可以了。 12albert@home-pc:~$ gdb -q(gdb) 怎么样，这次上面那段文字不见了，直接就进入 gdb 命令行了吧。 gdb 信息查看其实刚刚被我们嫌弃的那段文字，里面记录了不少信息，其中还展示了 show copying、show warranty、show configuration 等多个命令，我们可以简单尝试下这些命令有什么作用。 show copying输入 show copying 命令展示的是一份比较长的版本许可证说明，我省略了中间的部分，如果想看的话可以自己输入命令试一下，GPL v3 的许可证看起来很熟悉吧。 12345678910111213141516albert@home-pc:~$ gdb -q(gdb) show copying GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. &lt;http://fsf.org/&gt; Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed....... Later license versions may give you additional or differentpermissions. However, no additional obligations are imposed on anyauthor or copyright holder as a result of your choosing to follow alater version. show warranty命令 show warranty 输出的内容相比之前的命令就短很多了，是一份免责声明，序号从15开始，接着版本许可证的序号往下写的。 1234567891011121314151617181920212223242526272829303132(gdb) show warranty 15. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BYAPPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHTHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTYOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULARPURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAMIS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OFALL NECESSARY SERVICING, REPAIR OR CORRECTION. 16. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITINGWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYSTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANYGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THEUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OFDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRDPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OFSUCH DAMAGES. 17. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability providedabove cannot be given local legal effect according to their terms,reviewing courts shall apply local law that most closely approximatesan absolute waiver of all civil liability in connection with theProgram, unless a warranty or assumption of liability accompanies acopy of the Program in return for a fee. show configuration最后一个 show configuration 展示的 gdb 的一下配置信息，比如 --with-system-gdbinit=/etc/gdb/gdbinit 在进阶版的 gdb 调试技巧中应该经常用到，先有个印象就行。 12345678910111213141516171819(gdb) show configurationThis GDB was configured as follows: configure --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-auto-load-dir=$debugdir:$datadir/auto-load --with-auto-load-safe-path=$debugdir:$datadir/auto-load --with-expat --with-gdb-datadir=/usr/share/gdb (relocatable) --with-jit-reader-dir=/usr/lib/gdb (relocatable) --without-libunwind-ia64 --with-lzma --with-python=/usr (relocatable) --without-guile --with-separate-debug-dir=/usr/lib/debug (relocatable) --with-system-gdbinit=/etc/gdb/gdbinit --with-babeltrace("Relocatable" means the directory can be moved with the GDB installationtree, and GDB will still find it.)(gdb) apropos其实在 gdb 启动说明中还展示了 apropos 这个命令，可以用这个命令来显示与指定词语相关的命令，比如 apropos print 就是查询所有描述中带有 print 的命令，可以执行测试一下： 1234567891011121314151617(gdb) apropos printagent-printf -- Agent-printf "printf format string"alias -- Define a new command that is an alias of an existing commandbacktrace -- Print backtrace of all stack framesbt -- Print backtrace of all stack framescall -- Call a function in the programcommands -- Set commands to be executed when a breakpoint is hitcompile code -- Compilecompile print -- Evaluate EXPR by using the compiler and print resultdisable pretty-printer -- GDB command to disable the specified pretty-printer...info type-printers -- GDB command to list all registered type-printersinfo vector -- Print the status of the vector unitinspect -- Print value of expression EXPmaintenance agent-printf -- Translate an expression into remote agent bytecode for evaluation and display the bytecodesmaintenance btrace packet-history -- Print the raw branch tracing data---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit--- help查询具体的命令可以使用 help 子命令，比如查看 bt 这个查看调用栈帧的命令就可以使用 help bt，输入后回车可以得到这个命令的描述信息。 123456(gdb) help btPrint backtrace of all stack frames, or innermost COUNT frames.With a negative argument, print outermost -COUNT frames.Use of the 'full' qualifier also prints the values of the local variables.Use of the 'no-filters' qualifier prohibits frame filters from executingon this backtrace. gdb工作作为一款调试利器，可以使用的命令是在是太多了，除了这些还有很多命令等着我们去发现，今天的内容仅仅作为入门必备先简单了解一下。 总结 gdb 是一个交互式的命令行调试工具，通过不断执行命令，展示调试信息帮助我们调试程序 当启动 gdb 这个工具后，命令行会变成 (gdb)的形式，等着我们输入命令开始调试使用 gdb 作为一个强大的 GNU 工具，文档比较全，如果觉的文档枯燥，也可以跟着我的总结来熟悉一下基础用法。 开源环境下软件的版权信息和免责声明写的都比较完整，其中有很多描述值得我们学习。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 而世之奇伟、瑰怪，非常之观，常在于险远，而人之所罕至焉，故非有志者不能至也~ 2020-8-5 23:17:04]]></content>
      <categories>
        <category>GDB</category>
      </categories>
      <tags>
        <tag>GDB</tag>
        <tag>manual</tag>
        <tag>调试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白眼中的docker究竟是个什么东西]]></title>
    <url>%2Fblog%2F2020%2F07%2F28%2F%E5%B0%8F%E7%99%BD%E7%9C%BC%E4%B8%AD%E7%9A%84docker%E7%A9%B6%E7%AB%9F%E6%98%AF%E4%B8%AA%E4%BB%80%E4%B9%88%E4%B8%9C%E8%A5%BF%2F</url>
    <content type="text"><![CDATA[前言Docker，中文名：容器引擎，别名：小鲸鱼，生于2013年3月20日，有一个法裔美籍的母亲 Solumon Hykes，父亲是谁，不得而知。虽然只有7岁，但是在 Hello World 平行宇宙中也算进入了青壮年吧，正在飞速的发展着。 Docker 这个动物选的比较有意思，是一只蓝色的鲸鱼，作为地球上最大的动物，用它来代表容器再合适不过了。不过有谁知道为什么编程技术总是和动物挂钩啊？比如 Linux 的企鹅，Python 的大蛇，Hadoop 的大象等等，有知道的小伙伴还请告知一下。 俗话说的好，“程序不逛动物园，肯定技术有点悬”，经常看到网上有人推荐编程学习方法，先学学基础，然后再看几本儿动物书就可以了，看来这些封面上的动物已经深入人心了。 先把这些动物放到一边，来看看这个 docker 究竟是什么，之前我也不知道它是什么，甚至到了现在我也不能准确的说出它是什么，我只是以一个小白的身份来学习和使用，并且把一些弄懂的知识点总结起来，方便日后查找。 关于docker的疑问如果你之前看到 docker 时会有下面这些疑问，可以跟着文章梳理了解一下，如果你对这些问题的答案早已烂熟于胸，那么可以简单浏览下，帮我挑挑毛病，也是帮助想学习的同学们： docker 最近很火啊，它到底能用来做什么？ docker 和虚拟机好像啊，难道就是轻量虚拟机吗？它们两者还有其他的区别吗？ docker 教程里有 ubuntu 上安装 docker，还有 docker 上安装 linux，什么鬼，到底谁装谁啊？ docker 真的这么牛吗？那开发项目必须得用上它啊，显得高端大气上档次！ docker 宣称构建一次，处处运行，那它应该能跨平台吧？ 作为小白我也是带着这些疑问开始慢慢了解 docker 的，特别是那个 ubuntu 上装 docker，docker 上还能装 ubuntu，都给我整蒙圈了，通过不断学习才渐渐弄清了其中的原因。 疑问探索解答docker 是什么关于 docker 我们来看下常见的介绍： Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器或 Windows 机器上，也可以实现虚拟化，容器是完全使用沙箱机制，相互之间不会有任何接口。 从这一段话中我们可以得到以下几个知识点： 它是一个容器引擎 可以用来打包应用 可以发布到 linux 或者 windows 上 可以实现虚拟化 采用沙箱机制，相互隔离 结合查到的资料来了解一下 docker，首先它是一个容器引擎，引擎这个词经常出现，什么游戏引擎，物理引擎，发动机引擎等等，每种引擎其实就是负责一种核心工作的模块或事物，通过封装来简化工作流程，降低工作难度，而 docker 作为容器引擎的作用当然就是生产容器了。 它的目的就是隔离应用，在隔离空间内部署自己独特的环境，需要了解的是它并不是一项新的技术，而是利用了 Linux 核心中的 cgroups 和 namespace 等资源分脱机制来进行隔离，这种被称为容器的进程独立于宿主和其它的隔离的进程，是很早就存在的技术，只不过经过 docker 封装之后使用起来更加方便了。 再说打包应用，这也是 docker 迅速火起来的一个原因，因为环境部署是在太费时费力了，之前在服务器配置一个应用的运行环境，要安装各种软件，Java/Tomcat/MySQL等等。安装和配置这些东西非常麻烦，并且还存在各种版本，而当我们换另一台同样操作系统的服务器还要再配置一遍，有没有办法这些配置直接拷贝过来呢？其实这就是 docker 要做的事情，将应用与运行环境打包到一起，直接在 docker 中运行一个容器就好了，你所依赖的环境直接就装好了。 前面提到 docker 是利用了 linux 内核的一些特性，那么 windows 可以运行吗？如果你查询早期一点的资料会得到不可以的答案，或者说即使在 windows 上运行 docker，也是在中间加了一层 linux 虚拟机。而如今已经 2020 年了，windows 上可以直接安装 docker for windows 来启动提供 docker 服务，而 docker for mac 也使得 docker 运行在 mac 上不再困难，windows 很早就和 docker 进行了合作，最新的 win10 上启动 docker 甚至可以切换内核为 linux 或者 windows，很神奇吧。 说到这里你应该对 docker 有了一个简单的了解，其中有一点很重要，它和宿主机是共享内核的，这是解答上面很多疑惑的钥匙，至于虚拟化，隔离这些都很容易理解了，而这些概念在虚拟机上常常出现，所以很容易把它俩弄混。 容器与虚拟机自从 docker 出现，容器和虚拟机的对比就没有停过，这些对比常常从启动时间、资源占用、隔离性，操作便利性等方面来进行比较，可以用搜索引擎搜一下，大概就是下面这个样子： 特性 Docker容器 VM虚拟机 启动速速 秒级 分钟级 性能 接近原生 明显弱于原生 硬盘使用 相对较小，可以自由分配 创建时分配，易造成浪费 系统支持量 支持上千个 一般几十个 造成这种差异的原因是什么？还是前面说的 docker 和宿主机是共享内核，而虚拟机是自己创建了一整套系统，虽然隔离性更强，但是也造成了资源的浪费和效率的降低。 一直想找一个例子来形象地对比一下虚拟机和容器，我强行编一个吧，比如你是一个财富自由的人，准备回老家养养牛种种菜，包个鱼塘钓钓鱼，顺便再养一窝小白兔，但是小白兔会吃你种的蔬菜，牛偶尔也会踩到小白兔，这时怎么办，把它们隔开呗。 作为一个钱花不完的人，你准备造几个“小地球”，然后把饲养的动物和种植的植物都放到各自的“地球”中放养，每个小地球都是一个密封的环境，里面有自己的太阳、月亮、空气、河流、山川等等，这种方法当然可以，只是成本有些高，一旦建立了这个小地球，它所占有的资源就定下来了，基本上与大地球隔离，但是它还要依赖大地球，还要建立在地球上，可以类比下虚拟机。 因为建造小地球太费时费力了，所以你改了策略，这次不创建完全密闭的环境了，我直接造个篱笆就可以了，阳光、空气、河流我还是使用大地球的，只是在篱笆里我进行定制，做一些鱼塘、蔬菜大棚等等。由于建造篱笆非常省事，我可以批量生产，有需要了我可以直接拿来几个，放在地上就可以使用了，并且不同动物以及植物之间都有篱笆挡着，不会出现相互影响的问题了，这就有点像容器了。 虽然有些牵强，但是这个例子还是可以帮助我们了解容器和虚拟机的区别，实际上容器与虚拟机并不是对立的关系，有时为了防止容器无限制的占用物理机资源，还会现在物理机上运行虚拟机，然后在虚拟机里运行 docker，他们两者只是不同需求下的不同选择而已。 操作系统和容器到底谁安装谁前面说过 ubuntu 上装 docker，docker 上还能装 ubuntu 这个问题困扰了我好久，实际上 windows 可以装虚拟机，而虚拟机中有可以装 windows 这没什么好奇怪的，这里的 dokcker 指的就是 docker 引擎，或者认为是 docker 服务器。 它们确实可以相互安装，但情况是不同的，首先说 ubuntu 上装 docker，docker 说白了还是一种软件，本质上和你在电脑上装个QQ也没有多大差别，只不过这个软件有点特殊，通过它还能下载、安装别的环境，这么说它看起来有点像应用商店了，不过他虽然提供仓库，但是不仅仅是仓库，本质上它就是一种帮助你搭建环境的软件。 再来看看 docker 上装 ubuntu，还记得之前说过的一个重点吗？ docker 上安装的环境与宿主机共享内核，这就决定了他不能安装完整的系统，不管是 ubuntu、CentOS 还是 RedHat，它所安装的系统仅仅包含运行库和工具链，内核还是用宿主机的，相当于在 docker 中给内核套了一个新的壳子而已。 这下应该清楚了，ubuntu 上装 docker 就是在 ubuntu 上装了一个容器软件， docker 上装 ubuntu 就是在 docker 容器中给宿主内核套上了一个新系统的壳子，使其满足应用软件的环境，配备应用软件可使用的工具链。 这么厉害的容器项目中一定要用吗相信这种问题就是不了解容器也可以回答，肯定不是都要用啊，没有什么技术是只有优点没有缺点吧，凡是技术总有其适合的领域和场景，一味的追求最新的技术不一定符合所要开发的项目。 docker 也没有传说的那么神，它也有着这样那样的问题，比如一直津津乐道的资源伸缩机制，不像虚拟机那样创建时便规定了资源大小，即使不使用也占用着，而 docker 可以直接使用宿主资源，避免了很多浪费。但是反过来想，虚拟机规定了资源的多少，如果不够用了只影响它自己，而 docker 如果一个环境出了问题，它可以把整个物理机的资源耗完，影响机器上的所有服务。 另外，docker 建议只部署无状态的服务，它们不应该承载任何交易数据，所有数据应该保存在数据库服务器中，器随时可以停止、或者删除。当容器被删除掉，容器里的数据将会丢失，即使你要把 docker 数据放在主机来存储，它依然不能保证不丢数据，具体的细节我也在学习，有这方面经验的朋友可以发表一下见解。 docker的跨平台先来看看跨平台的概念： 跨平台概念是软件开发中一个重要的概念，即不依赖于操作系统，也不依赖硬件环境。一个操作系统下开发的应用，放到另一个操作系统下依然可以运行。 首先要弄明白你说的跨平台指的是 docker 跨平台，还是它里边的应用使用 docker 就能跨平台了，从定义来说 docker 这个容器软件应该算是跨平台的，毕竟 Linux、 Windows、 Mac 都有了 docker 的安装包，那么他里面的镜像运行之后的容器能跨平台吗？这还要看具体的应用，docker 没有让一个非跨平台软件变成跨平台软件的能力。 关于这一点你还要牢记前面说的，docker 中的环境与宿主机共享内核，你创建了一个自己编写的exe程序的镜像，拿到安装了 docker 的 ubuntu 机器上显然是无法成功运行的。 基础知识点docker的缺点docker 的卖点是让你摆脱配置环境的困扰，但真实情况是你打包镜像的机器和系统版本，最好和你要运行的目标机器和系统版本一致，另外 docker 环境最好也一样，忽然感觉它没有那么神奇了，这不还是要求版本吗？ 试想一下，你用一个高版本的 docker 服务打包，其中使用了一些新特性，然后放到低版本的 docker 服务下怎么能保证成功运行，机器配置也是一样，之前看到过一个问题 “尝试在具有 4.19 或更高内核的 Linux 系统上运行 docker official centos:6 或 centos:5 容器，当尝试启动它时，你会发现内核和程序不兼容”。 这样看来，docker 只适合在相同环境下批量复制，使得实现自动化测试和持续的集成很方便，但还是有些问题需要注意的： docker 是基于64位系统环境的，32位环境下无法使用 隔离性相比 KVM 之类的虚拟化方案还是有些欠缺 容器随着用户进程的停止而销毁，其中的日志、打点等用户数据不便收集 网络管理相对简单，主要是基于 namespace 隔离 容器的 root 和宿主机 root 等同，这使得容器容易受到攻击 … docker 的组成前面一直在说初学 docker 时的疑问，接下来看看 docker 究竟都包括哪些内容，docker 这个容器引擎实际上是一个客户端/服务器应用程序，客户端负责与守护的服务进程进行对话，而服务进程负责构建、运行和分发 docker 容器。docker 客户端和服务进程可以在同一系统上运行，也可以进行远程访问，通过网络接口使用 RESTful API 进行通信。 使用 docker 时常常要接触三个概念：镜像（Image）、容器（Container）和仓库（Repository）。简单来说镜像就是我们的想要打包的程序机加上程序运行环境，打包出来的一个文件，相当于程序安装包。当镜像运行起来我们就得到了容器，镜像与容器的关系就类似于类和对象的关系。仓库就是存放镜像的地方，与代码的仓库 Github 很像，docker 镜像也有一个常用的仓库叫 Docker Hub，方便人们直接下载镜像来运行。 运行一个镜像Hello world 常常被拿来新知识的入门和开头，今天我们用这个例子来做一下收尾吧，首先你得有 docker 环境，说人话就是你得装了 docker 软件，之前不是一直说 docker 就是一个软件吗，你想用它当然得安装了，这类教程很多，假设你已经安装完了， 我们在一台 CentOS 上操作使用一下 docker，查询下系统版本和 docker 版本： 123456789[root@remote-os ~]# lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.7.1908 (Core)Release: 7.7.1908Codename: Core[root@remote-os ~]# docker --versionDocker version 19.03.7, build 7141c199a2[root@remote-os ~]# 首先下载 hello world 镜像，使用 docker image pull hello-world 命令 123456[root@remote-os ~]# docker image pull hello-worldUsing default tag: latestlatest: Pulling from library/hello-worldDigest: sha256:49a1c8800c94df04e9658809b006fd8a686cab8028d33cfba2cc049724254202Status: Image is up to date for hello-world:latestdocker.io/library/hello-world:latest 查看本地镜，使用 docker image ls 命令，发现 hell-world 镜像已经在本地了 12345678[root@remote-os ~]# docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest adafef2e596e 3 weeks ago 73.9MBregistry latest 708bc6af7e5e 6 months ago 25.8MBhello-world latest bf756fb1ae65 7 months ago 13.3kBwurstmeister/zookeeper latest 3f43f72cb283 18 months ago 510MBhyper/docker-registry-web latest 0db5683824d8 3 years ago 599MB[root@remote-os ~]# 下载完成之后直接使用 docker container run hello-world 命令运行就可以了，这个镜像运行打印完直接就退出了 12345678910111213141516171819202122[root@remote-os ~]# docker container run hello-worldHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 如果不想退出，运行的镜像应该是能提供某种服务的，比如前面一直说的 ubuntu，它可以在 docker 中运行起来，直接使用 docker container run -it ubuntu bash 命令就行，这里为什么我们不先下载呢？实际上如果你指定的镜像在本地没有的话会自动下载，不需要手动下载完再运行。 1234567891011[root@remote-os ~]# docker container run -it ubuntu bashroot@0577050677ac:/# cat /etc/issueUbuntu 20.04 LTS \n \lroot@0ecfed0920aa:/# lsbin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin srv sys tmp usr varroot@0ecfed0920aa:/# ll home/total 8drwxr-xr-x 2 root root 4096 Apr 15 11:09 ./drwxr-xr-x 1 root root 4096 Jul 31 16:23 ../root@0ecfed0920aa:/# 可以看到上面的操作，我们又进入了 ubuntu 系统，成功运行了镜像，现在得到了一个容器，可以通过 docker container ls 命令查看，还可以通过 docker container rm [containerID] 命令来删除容器。 总结 docker 软件可以运行在windows、linux 和 mac 上 docker 容器与宿主机共享一个系统内核，如果依赖内核版本的应用最好保证物理机系统版本一致 docker 容器与虚拟机并不是对立的，有时候会放在一起配合使用 docker 有自己的镜像仓库，可以直接下载安装，使用起来相当方便，因为网络原因，如果想快速搭建最好提前准备好镜像文件 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 海纳百川有容乃大，壁立千仞无欲则刚~ 2020-8-1 00:31:35]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GDB调试指北大全]]></title>
    <url>%2Fblog%2F2020%2F07%2F18%2FGDB%E8%B0%83%E8%AF%95%E6%8C%87%E5%8C%97%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[前言GDB 作为程序调试利器，是一个受通用公共许可证（GPL）保护的自由软件，全称是 GNU Debugger，又常常被称为 GNU symbolic debugger 或者 GNU project debugger，能够帮助开发者调试程序，分析应用程序运行过程。目前支持调试 C、 C++、 D、 Go、 Objective-C、 Fortran、 Java、 OpenCL C、 Pascal、 assembly、 Modula-2、 Ada 等多种编程语言。 GDB能做什么GDB 是调试程序的强大武器，能够帮助开发者找出程序出现BUG的原因，但是不要指望它能自己查问题，它仅仅是一个工具，可以帮助我们查找问题原因，常常被用来做以下事情： 分析程序崩溃的原因 查找程序表现出错误行为的原因 找到一些从源码上难以发现的逻辑错误 GDB调试步骤 使用 g++ 附加 -g 参数编译程序，g++ -g mainpro.cpp -o mainpro 使用 gdb 程序来启动调试我们自己构建的程序，gdb mainpro 使用 run、break、print 等命令调试程序 使用 quit 命令退出程序 GDB调试示例 编写示例程序代码，保存到文件 mainpro.cpp 中 1234567891011#include &lt;iostream&gt;int main()&#123; int a = 110, b = 119, c; c = a + b; std::cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; std::endl; return 0;&#125; 使用 g++ 附加 -g 参数编译程序 123albert@home-pc:~$ g++ -g mainpro.cpp -o mainproalbert@home-pc:~$ lsmainpro mainpro.cpp 使用 gdb 命令来启动调试 1234567891011121314151617albert@home-pc:~$ gdb mainproGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1Copyright (C) 2016 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-linux-gnu".Type "show configuration" for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type "help".Type "apropos word" to search for commands related to "word"...Reading symbols from mainpro...done.(gdb) 使用 break 8 在第8行打断点，使用 run 命令启动程序，使用 print c 打印程序变量 12345678910(gdb) break 8Breakpoint 1 at 0x4008b7: file mainpro.cpp, line 8.(gdb) runStarting program: /home/albert/mainproBreakpoint 1, main () at mainpro.cpp:88 std::cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; std::endl;(gdb) print c$1 = 239(gdb) 使用 quit 命令退出程序 1234567(gdb) quitA debugging session is active. Inferior 1 [process 227] will be killed.Quit anyway? (y or n) yalbert@home-pc:~$ GDB启动参数与命令列表（持续更新）GDB 众多的启动参数和命令提供了强大的调试功能，每一条都可以展开得到很多知识，这些知识的学习是一个持续的过程，短时间无法消化和吸收，所以准备总结一个系列，从最简单的命令开始总结，持续更新学习下去，文章链接不定期更新。 GDB命令 备注 参考文章 gdb、gdb -q GDB启动、查看说明 [GDB调试指北-启动GDB与查看说明] gdb pro、gdb pro 123 启动调试程序、调试正在运行的程序 [GDB调试指北-使用GDB启动调试] directory new-path 查看调试源码 GDB调试指北-查找丢失源码文件 set substitute from-path to-path 查看调试源码 GDB调试指北-查找丢失源码文件 总结 GDB 调试技巧更多的是工具本身的功能，所谓“重剑无锋，大巧不工”，熟练利用这个工具才能发挥最大的威力 知识的学习时一个持续的过程，只有不断的学习和总结才能不断进步，而不要被那些花里胡哨的外表所迷惑 有些知识学着学着就通了，前几天看到 printf 这个函数，很疑惑为什么末尾要加个 f，猜想它是格式化 format 的意思 经过查证果然如此，此时距离第一次在 C 语言中学习 printf 函数已经过去了10年 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 大漠孤烟直，长河落日圆~ 2020-7-18 20:09:58]]></content>
      <categories>
        <category>GDB</category>
      </categories>
      <tags>
        <tag>GDB</tag>
        <tag>manual</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb调试解决找不到源代码的问题]]></title>
    <url>%2Fblog%2F2020%2F07%2F13%2Fgdb%E8%B0%83%E8%AF%95%E8%A7%A3%E5%86%B3%E6%89%BE%E4%B8%8D%E5%88%B0%E6%BA%90%E4%BB%A3%E7%A0%81%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言通过 gdb 启动程序，打好断点运行，开始调试输入 list 命令，结果发现找不到源代码，是不是很糟心，让我们来看看怎么解决这种情况。 先來说明我们要处理的情况，调试程序找不到源代码首先你得有源代码，如果编译完程序你把源代码删了，或者单独把执行程序拷贝到一个没有源代码的机器上，那么拜拜吧您嘞，这种情况不是本文能解决的。 如果你确实有源代码，正常编译源代码并且加入了 -g 选项，编译完之后没有改变源代码位置，那么调试的时候基本都会找到源代码，所以这种情况也不在我们的讨论范围之内。 分析到现在就剩下一种情况，程序编译完成之后我移动了代码的位置。实际工作中可能不会这么无聊，故意改变目录位置让调试程序找不到，但是工作中常常会出现发布机编译完代码要在开发机调试的情况，两台机器上的代码时一样的，但是源代码的位置可能放置的不同，那么在个人开发机上调试这样的程序就会找不到源代码，这也就是我们要解决的问题。 找到源代码的必要性其实在我看来找不到源代码的问题没有那么严重，编译程序里记录了文件名，行号等信息，可以在调试的时候对照着本地的源代码进行“盲调”，这种“盲调”的操作之前可没少干，因为线上环境中没有源代码，我只能一边对照着 gdb 调试输出的行号，一边对照本地的源代码进行程序分析，通过这种方法也解决了不少问题。 虽然看着源代码调试没有那么必要，但是如果可以看见那肯定是更好了，所以本文还是列举出最常见的处理方法，解决一下本来有代码，但因为目录不匹配无法正常调试的问题。 涉及到的命令下面几个命令是 gdb 命令，注意要放到和 gdb 交互命令行输入才可以，别管会不会，先混个脸熟，以后要经常用的： show dir dir 目录 set dir 目录1:目录2:目录3 dir pwd cd 目录 set substitute-path from-path to-path gdb怎样找源代码有时候很奇怪，代码明明就在那里，gdb 你睁开眼睛行不行，为什么你就是找不到呢？其实 gdb 也很苦的好不好，一直帮你查问题还要忍受着你每天的埋怨，到底是什么原因导致 gdb 对眼前的代码视而不见呢？ 其实 gdb 查找代码也要遵循一定的规则，不能每次都全盘扫描吧，那不是得给它累死。举个例子吧，我们在安装一些软件，特别是一些命令行工具的时候，总是有一步要求你把工具或软件所在目录添加到环境变量中，这个变量的名字叫做 Path。 这个 Path 其实就是电脑上众多软件所在目录的集合，当你直接使用软件的程序时，会优先从 Path 这个集合中的目录下去找，成功找到就会直接调用，否则提醒你软件不存在。 源代码目录集合而在 gdb 的调试过程中也有这样一个目录集合，我暂且称它为 SourcePathSet，后面就用这个名字了，因为还要涉及到多种查找目录，请注意区分。 gdb 在查找源码的时候首先在 SourcePathSet 中所包含的目录下找，如果找不到就会提示查找失败了，也就是这篇文章所提到的问题。 源代码文件程序在编译的过程中会记录源文件的名字和路径，这个路径可能是绝对路径，比如 /mnt/d/main.cpp，也可能是相对路径 ../main.cpp ，究竟是哪一种取决于编译时使用的参数。 我们以绝对路径为例，比如文件名为 /mnt/d/main.cpp，我们可以把它拆分成包含路径和不包含路径两种形式：/mnt/d/main.cpp 和 main.cpp，当 SourcePathSet 中包含一个路径叫 /mnt/e时， gdb 搜索的路径包括以下几种： /mnt/d/main.cpp /mnt/e/mnt/d/main.cpp /mnt/e/main.cpp 当源文件是相对路径 ../main.cpp 的时候，那么搜索的路径就变成了下面两个： /mnt/e/../main.cpp /mnt/e/main.cpp 说到这里你可能就明白了，当 gdb 找不到源文件的时候，修改 SourcePathSet 就可以了，把想让它搜索的路径添加到 SourcePathSet，如果符合它的搜索规则，那么就可以找到了。 目录集合的默认值SourcePathSet 在 gdb 启动后开始生效，默认值并不是空，而是 $cdir:$cwd，这又是什么鬼？其中的 $cdir 叫做编译目录，是代码在编译时记录到程序中的，$cwd 表示当前的调试目录，可以通过 cd 命令来修改，要注意这个 cd 修改的是 gdb 会话中的当前目录，不会影响启动 gdb 前文件系统中的目录位置。 假设 $cdir 的值是 /usr，cwd 的值是 /home/albert，我们又添加了 /mnt/e 到 SourcePathSet 中，那么此时 SourcePathSet 的值为 /mnt/e:$cdir:$cwd，如果源文件的是 /mnt/d/main.cpp，查找的目录就会出现以下几种： /mnt/d/main.cpp /mnt/e/mnt/d/main.cpp /usr/mnt/d/main.cpp /home/albert/mnt/d/main.cpp /mnt/e/main.cpp /usr/main.cpp /home/albert/main.cpp 查看各种目录先做一下准备工作，编写一段简单代码，另存文件名为 main.cpp，保存在目录 /mnt/d/cpp 下： 12345678910111213#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 1; int b = 2; int c = a + b; cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl; return 0;&#125; 切换到目录 /mnt/d下， 查看 cpp 目录下文件并使用 g++ 编译，编译完成后将文件 mian.cpp 移动到 /mnt 目录下： 123456789albert@home-pc:/mnt/d$ ls cpp/main.cppalbert@home-pc:/mnt/d$ g++ /mnt/d/cpp/main.cpp -g -o mainalbert@home-pc:/mnt/d$ ls mainmainalbert@home-pc:/mnt/d$ sudo mv cpp/main.cpp ../[sudo] password for albert:albert@home-pc:/mnt/d$ ls ../c d e f main.cpp 启动 gdb 调试程序并打好断点，输入 run 运行发现，断点被触发，但是显示出 No such file or directory.，说明没有找到源代码文件。 123456789albert@home-pc:/mnt/d$ gdb -q mainReading symbols from main...done.(gdb) b 8Breakpoint 1 at 0x4008ac: file /mnt/d/cpp/main.cpp, line 8.(gdb) runStarting program: /mnt/d/mainBreakpoint 1, main () at /mnt/d/cpp/main.cpp:88 /mnt/d/cpp/main.cpp: No such file or directory. 查看源代码文件名和编译目录直接在 gdb 命令行中输入 info source 回车就可以了 12345678(gdb) info sourceCurrent source file is /mnt/d/cpp/main.cppCompilation directory is /mnt/dSource language is c++.Producer is GNU C++ 5.4.0 20160609 -mtune=generic -march=x86-64 -g -fstack-protector-strong.Compiled with DWARF 2 debugging format.Does not include preprocessor macro info.(gdb) 通过这个命令发现，源代码文件是 /mnt/d/cpp/main.cpp，编译目录是 /mnt/d 查看源代码搜索目录在 gdb 环境下输入 show dir 命令就可以显示 SourcePathSet 这个集合中都有哪些目录，由于还没有设置过现在还是默认值 $cdir:$cwd 123(gdb) show dirSource directories searched: $cdir:$cwd(gdb) 查看当前目录查看当前目录就比较简单了，直接 pwd 就搞定了 123(gdb) pwdWorking directory /mnt/d.(gdb) 我们“如愿以偿”的让 gdb 找不到代码了，从现在的环境来看，$cdir 和 $cwd 相同都是 /mnt/d，所以此时搜索的目录只有： /mnt/d/cpp/main.cpp /mnt/d/mnt/d/cpp/main.cpp /mnt/d/main.cpp 而代码被我们移动到了/mnt/main.cpp，gdb 自然就找不到了，后面来看看具体怎么处理这种情况。 具体示例说了这么多原理的东西，如果弄明白了这些很容易找到解决问题的办法，下面写一个完整点的例子，来感受一些具体怎么修复这个问题，新建三个文件 mainpro.cpp、mymath.h、mymath.cpp，目录结构和内容如下： 1234567albert@home-pc:/mnt/d$ tree /mnt/d/mainpro//mnt/d/mainpro/|-- core| `-- mainpro.cpp`-- kit |-- mymath.cpp `-- mymath.h 123456789101112131415//mainpro.cpp#include "../kit/mymath.h"#include &lt;iostream&gt;using namespace std;int main()&#123; int a = 1, b = 2; mymath* m = new mymath(); int c = m-&gt;add(a, b); cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl; return 0;&#125; 123456//mymath.hclass mymath&#123;public: int add(int a, int b);&#125;; 12345678//mymath.cpp#include "mymath.h"int mymath::add(int a, int b)&#123; int c = a + b; return c;&#125; 在 /mnt/d/mainpro 目录下编译代码，然后将代码文件所在目录 core 和 kit 拷贝到 /mnt/e/newpro 目录下，将可执行文件拷贝到 /home/albert 目录下。 123456789101112131415albert@home-pc:/mnt/d/mainpro$ g++ /mnt/d/mainpro/core/mainpro.cpp /mnt/d/mainpro/kit/mymath.cpp -g -o mainproalbert@home-pc:/mnt/d/mainpro$ tree.|-- core| `-- mainpro.cpp|-- kit| |-- mymath.cpp| `-- mymath.h`-- mainpro2 directories, 4 filesalbert@home-pc:/mnt/d/mainpro$ mkdir /mnt/e/newproalbert@home-pc:/mnt/d/mainpro$ sudo mv core/ /mnt/e/newpro/albert@home-pc:/mnt/d/mainpro$ sudo mv kit/ /mnt/e/newpro/albert@home-pc:/mnt/d/mainpro$ mv mainpro /home/albert/ 在 /home/albert 目录下启动 gdb 开始调试，先在 main 函数打断点，查询源文件路径和编译目录等信息； 1234567891011121314151617181920212223albert@home-pc:~$ gdb -q mainproReading symbols from mainpro...done.(gdb) b mainBreakpoint 1 at 0x4008de: file /mnt/d/mainpro/core/mainpro.cpp, line 7.(gdb) runStarting program: /home/albert/mainproBreakpoint 1, main () at /mnt/d/mainpro/core/mainpro.cpp:77 /mnt/d/mainpro/core/mainpro.cpp: No such file or directory.(gdb) info sourceCurrent source file is /mnt/d/mainpro/core/mainpro.cppCompilation directory is /mnt/d/mainproSource language is c++.Producer is GNU C++ 5.4.0 20160609 -mtune=generic -march=x86-64 -g -fstack-protector-strong.Compiled with DWARF 2 debugging format.Does not include preprocessor macro info.(gdb) list2 in /mnt/d/mainpro/core/mainpro.cpp(gdb) pwdWorking directory /home/albert.(gdb) show dirSource directories searched: $cdir:$cwd(gdb) 果然找不到源代码了，从上面的调试信息来看，可以得到以下信息： 源代码文件为 /mnt/d/mainpro/core/mainpro.cpp 程序编译目录为 /mnt/d/mainpro 当前目录为 /home/albert 而源代码查找列表中只有 $cdir:$cwd，说明只包含 /mnt/d/mainpro 和 /home/albert，那么查找的目录有： /mnt/d/mainpro/core/mainpro.cpp /mnt/d/mainpro/mnt/d/mainpro/core/mainpro.cpp /home/albert/mnt/d/mainpro/core/mainpro.cpp /mnt/d/mainpro/mainpro.cpp /home/albert/mainpro.cpp 这些目录显然找不到源代码文件了，因为文件已经被我移动到 /mnt/e/newpro/ 目录下了，也就是 /mnt/e/newpro/core/mainpro.cpp，下面来尝试一些解决方法。 使用 dir 命令解决刚才说了源代码查找集合 SourcePathSet 中只有 $cdir:$cwd，我们可以自己加一个嘛，比如像下面这样： 1234567891011121314(gdb) dir /mnt/e/newpro/core/Source directories searched: /mnt/e/newpro/core:$cdir:$cwd(gdb) list2 #include &lt;iostream&gt;3 using namespace std;45 int main()6 &#123;7 int a = 1, b = 2;8 mymath* m = new mymath();910 int c = m-&gt;add(a, b);11 cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;(gdb) 这样就可以找到了，我们接着在 add 函数上下个断点，继续执行 12345678910111213141516(gdb) b mymath::addBreakpoint 2 at 0x4009a6: file /mnt/d/mainpro/kit/mymath.cpp, line 6.(gdb) cContinuing.Breakpoint 2, mymath::add (this=0x613c20, a=1, b=2) at /mnt/d/mainpro/kit/mymath.cpp:66 /mnt/d/mainpro/kit/mymath.cpp: No such file or directory.(gdb) list1 in /mnt/d/mainpro/kit/mymath.cpp(gdb) info sourceCurrent source file is /mnt/d/mainpro/kit/mymath.cppSource language is c++.Producer is GNU C++ 5.4.0 20160609 -mtune=generic -march=x86-64 -g -fstack-protector-strong.Compiled with DWARF 2 debugging format.Does not include preprocessor macro info.(gdb) 结果发现又找不到文件 /mnt/d/mainpro/kit/mymath.cpp 了，因为和之前不是一个文件，这个文件在其他的目录下，所以还要使用 dir 命令，把新的目录加到源代码查找集合 SourcePathSet 中： 1234567891011121314(gdb) dir /mnt/e/newpro/kit/Source directories searched: /mnt/e/newpro/kit:/mnt/e/newpro/core:$cdir:$cwd(gdb) list1 #include "../kit/mymath.h"2 #include &lt;iostream&gt;3 using namespace std;45 int main()6 &#123;7 int a = 1, b = 2;8 mymath* m = new mymath();910 int c = m-&gt;add(a, b);(gdb) 这次又能成功找到了，可是如果有好多个文件要调试，难道要把所有的目录都加进去吗？其实可以有简便方法的，在启动 gdb的时候可以指定搜索的源代码路径，这些路径都会被加到到源代码查找集合 SourcePathSet 中，具体操作如下，先退出gdb，然后重新加参数启动如下： 12345albert@home-pc:~$ gdb -q mainpro `find /mnt/e/newpro/ -type d -printf '-d %p '`Reading symbols from mainpro...done.(gdb) show dirSource directories searched: /mnt/e/newpro/kit:/mnt/e/newpro/core:/mnt/e/newpro:$cdir:$cwd(gdb) 其实这条命令的本来面目是 gdb -q mainpro -d xxxxx，只不过这组合了 find 命令以后使用起来更加方便了，可以把指定目录下的子目录全都添加到参数中 使用 cd 命令解决如果是临时调试倒是用不到上面设置启动参数那么麻烦，因为变量 $cwd 也在搜索集合中，既然在编译时记录的源文件被改变了位置，那么我们调整我们的当前位置，让代码出现搜索路径中，还是上面的这个例子： 123456789101112131415161718192021222324252627albert@home-pc:~$ pwd/home/albertalbert@home-pc:~$ gdb -q mainproReading symbols from mainpro...done.(gdb) b mainBreakpoint 1 at 0x4008de: file /mnt/d/mainpro/core/mainpro.cpp, line 7.(gdb) rStarting program: /home/albert/mainproBreakpoint 1, main () at /mnt/d/mainpro/core/mainpro.cpp:77 /mnt/d/mainpro/core/mainpro.cpp: No such file or directory.(gdb) list2 in /mnt/d/mainpro/core/mainpro.cpp(gdb) cd /mnt/e/newpro/core/Working directory /mnt/e/newpro/core.(gdb) list2 #include &lt;iostream&gt;3 using namespace std;45 int main()6 &#123;7 int a = 1, b = 2;8 mymath* m = new mymath();910 int c = m-&gt;add(a, b);11 cout &lt;&lt; "c = " &lt;&lt; c &lt;&lt; endl;(gdb) 上面的操作通过 cd /mnt/e/newpro/core/ 命令直接进入了源代码目录，当然就找到了，但是这还是会有点问题，当碰到需要调试好几个文件的时候就需要使用 cd 命令跳来跳去，要想一劳永逸，请看下面这个方法。 使用 set substitute-path 命令解决我们移动源代码的时候往往会整个目录移动，或者说开发机和发布机上面的代码文件组织结构是一样，只是所在的磁盘位置是不一样的，所以如果可以设置用一个路径替换原代码文件的路径就好了， set substitute-path from-path to-path 这个命令就可以达到想要的目的，这个命令还可以简写成 set substitute from-path to-path，比如还是前面的例子，源代码从 /mnt/d/mainrpo 目录整体移动到了 /mnt/e/newpro 目录，调试时找不到源代码可以使用 set substitute /mnt/d/mainrpo /mnt/e/newpro 命令来指定替换目录，这样就可以找到源代码啦，下面来测试一下： 1234567891011121314151617181920212223242526272829303132333435363738albert@home-pc:~$ gdb -q mainproReading symbols from mainpro...done.(gdb) set substitute-path /mnt/d/mainrpo /mnt/e/newpro(gdb) b mainBreakpoint 1 at 0x4008de: file /mnt/d/mainpro/core/mainpro.cpp, line 7.(gdb) runStarting program: /home/albert/mainproBreakpoint 1, main () at /mnt/d/mainpro/core/mainpro.cpp:77 /mnt/d/mainpro/core/mainpro.cpp: No such file or directory.(gdb) cd /mnt/e/newpro/Working directory /mnt/e/newpro.(gdb) list2 /mnt/d/mainpro/core/mainpro.cpp: No such file or directory.(gdb) set substitute-path /mnt/d/mainpro /mnt/e/newpro(gdb) list 01 #include "../kit/mymath.h"2 #include &lt;iostream&gt;3 using namespace std;45 int main()6 &#123;7 int a = 1, b = 2;8 mymath* m = new mymath();910 int c = m-&gt;add(a, b);(gdb) info sourceCurrent source file is /mnt/d/mainpro/core/mainpro.cppCompilation directory is /mnt/d/mainproLocated in /mnt/e/newpro/core/mainpro.cppContains 14 lines.Source language is c++.Producer is GNU C++ 5.4.0 20160609 -mtune=generic -march=x86-64 -g -fstack-protector-strong.Compiled with DWARF 2 debugging format.Does not include preprocessor macro info.(gdb) pwdWorking directory /home/albert.(gdb) 通过调试信息 Located in /mnt/e/newpro/core/mainpro.cpp 可以看到，果然在新的位置找到了源代码。 总结 调试的时候找不到源码有多种解决方法，需要根据实际情况选择最合适的解决方案。 编译时使用绝对路径时，推荐使用 set substitute-path from-path to-path 的方式。 编译时使用相对路径时，使用 set substitute from-path to-path 或者 dir new-path 都可以。 对于临时查找一个问题，单独调试某一个文件时使用 cd 命令就可以搞定了。 直接在 gdb 环境输入 dir 命令回车确认，可以重置 dir 目录 或者 set dir 目录 命令修改过的源代码搜索目录集合。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当人的才华不足以撑起个人的欲望时就会感到焦虑，当面对不利的情况和事件却又无力改变时就会感到愤怒，而弱肉强食一直都是生活的本质，惟有强大才是解决这一切负面情绪的良药~ 2020-7-18 15:36:53]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>source</tag>
        <tag>gdb</tag>
        <tag>dir</tag>
        <tag>path</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell脚本中获取命令运行结果、特殊变量使用、条件判断等常用操作]]></title>
    <url>%2Fblog%2F2020%2F07%2F07%2FShell%E8%84%9A%E6%9C%AC%E4%B8%AD%E8%8E%B7%E5%8F%96%E5%91%BD%E4%BB%A4%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C%E3%80%81%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD%E7%AD%89%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言最近在处理一个 Python 局部变量的作用域问题时发现有些奇怪，想起了之前常写的 Lua 脚本，于是想写个函数测试一下，结果发现短短的十几行代码出现了多个错误，这可是我写了近三年的代码啊，才放下半年就记不清了，所以知识这个东西还是要不断“温故”，今天要总结的 Shell 脚本命令也是，基本属于一看就会，一写不对的状态，所以还是要把常用的操作总结到一起，方便查找和复习。 获取命令执行结果脚本中常常要获取一些命令的执行结果，比如当前目录 pwd、当前时间 date 等等，如果在控制台时直接输入后回车就能看到结果，但是在 Shell 脚本中却不能这样做，常见的有以下两种方式。 使用反引号 `command `来执行命令反引号就是键盘上 Tab 键上方的那个按键对应的符号，常写 Markdown 的小伙伴知道这个符号就是包裹代码块的那个符号，在 Shell 脚本中被用来执行命令得到结果，举个简单的例子 1234567#!/bin/bashresult=dateecho $resultresult=`date`echo $result 将上述命令保存到文件 cmd.sh 中运行 ./cmd.sh 得到结果： 123$ ./cmd.shdateTue Jul 7 23:48:03 CST 2020 从运行结果可以看出，如果不加反引号，我们常用的这些命令会被当成普通的字符串处理。 使用括号组合 $(command) 来执行命令除了上面的反引号，使用美元符和小括号组合也可以在 Shell 脚本中运行命令，使用同样的例子测试 1234567#!/bin/bashresult=`date`echo $resultresult=$(date)echo $result 保存到文件 cmd.sh 中运行 ./cmd.sh 得到结果： 123$ ./cmd.shTue Jul 7 23:53:27 CST 2020Tue Jul 7 23:53:27 CST 2020 对比可以看出两种方式在这个命令下运行结果是一样的。 两种方式的区别虽然上述两种方式都可以在 Shell 脚本中得到命令运行的结果，但是有一点是不一样的，那就是反引号执行命令不支持嵌套，不能实现反引号中再出现反引号，而 $(command)的方式是支持嵌套的，关于这一点可以看下面这个例子。 12$ echo $(ls $(pwd))cmd.sh 分析一下这个命令 echo $(ls $(pwd))，最里面的命令是 $(pwd)先执行得到当前目录，然后执行命令 $(ls 当前目录)得到目录下的文件，再通过 echo 命令把这个结果输出，就得到了 cmd.sh 这个文件名，因为我这个目录下只有这一个文件。 系统的命令使用反引号的方式改写就不生效了。 12$ echo `ls `pwd``cmd.shpwd 我们还是仿照上面嵌套来写，但是 echo 后面的内容其实被分成了3部分，一个ls命令，一个pwd字符串、一个空命令，这样就能解释运行结果 cmd.shpwd了。 对照着结果我们就可以知道了， $(command)的方式更加强大，可以支持命令的嵌套，应用更广泛一点，而反引号的方式跟多出现在之前的脚本中。 特殊变量使用从学习语言的第一天起就记住了变量名中只能有数字、字母、下划线，并且数字不能打头（Shell中只能字母开头），但是在 Shell 脚本中有一些特殊的变量，包含各种奇奇怪怪的符号。 $0 $1 $2 …这些是运行 Shell 脚本时传递给脚本的命令行参数。命令行参数用 $n 表示，$0表示当前脚本的文件名，$1 表示第一个参数，$2 表示第二个参数，依次类推，可以类比 Windows 下的 %0、%1、%2… $$当前 Shell 脚本的进程ID。如果在命令行执行得到的是当前 bash 的进程ID，如果放到脚本中，得到的是脚本的进程ID。 $?可以获取上一个命令执行后的返回结果。 $传递给脚本的命令行参数的个数。 $*传递给脚本的命令行参数的所有参数。 $@传递给脚本的命令行参数的所有参数，与 $* 稍有不同。 测试写个脚本测试一下，新建 cmdargs.sh 文件，编写下面代码： 123456789#!/bin/bashecho \$0 is $0echo \$1 is $1echo \$2 is $2echo \$$ is $$echo \$# is $#echo \$* is $*echo \$@ is $@ 先执行 ./cmdargs.sh 脚本， 然后输出 $? 脚本的退出状态，运行结果如下: 1234567891011$ ./cmdargs.sh I love my daughter$0 is ./cmdargs.sh$1 is I$2 is love$$ is 197$# is 4$* is I love my daughter$@ is I love my daughter$ echo $?0 $* 和 $@ 的区别对照这个源码和输出结果，这些特殊变量应该可以分清楚了，其中 $* 和 $@ 都是把所有内容都列出来了，但它俩还是有点区别的，当这两个变量都被双引号包裹时，通过 for 循环会得到不同结果，写个脚本 cmdargs2.sh 试一下 12345678910111213#!/bin/bashecho "test for \"\$*\""for var in "$*"do echo "$var"doneecho "test for \"\$@\""for var in "$@"do echo "$var"done 运行结果如下, &quot;$*&quot;把所有的参数当成了一个整体，而 &quot;$@&quot; 把各个参数都拆分开了，可以通过循环依次打印出来。 12345678$ ./cmdargs2.sh I love my daughtertest for "$*"I love my daughtertest for "$@"Ilovemydaughter 条件判断说起条件判断第一反应就是 if 了，在 Shell 脚本中也有 if 语句，同样是条件判断的中坚力量，先来看看 if 语句的写法： if 语句格式1234567if [ -d $filename ]; then echo "this is a directory."elif [ -a $filename ]; then echo "the file is exist."else echo "the file is not exist."fi 直接提供一个最复杂的情况，如果不需要 elif 或者 else 分支，直接删掉就可以，但是 if、then、fi 这些都是必须的，并且中括号里面的表达式与中括号之间都要有空格，如果挨着写会报错的。 中括号 [] 的作用一度认为 if 条件语句就是这样写，中括号 [] 应该是语法的一部分，但是查询后发现这居然是一个命令，和 ls，pwd 一样是一个可以执行命令，放在 if 条件判断时基本等同于 test 命令。 1234$ which [/usr/bin/[$ which test/usr/bin/test 看着这个查询结果感觉神奇吧，此外还有一个 [[]] 双中括号的操作，这个就不是命令了，而是 Shell 的一个关键字，比 [] 要强大的多。 具体条件Shell 脚本最常见的条件就是文件判断，数字判断和字符串判断了，接下来列举一下这些判断的常见写法。 文件判断 命令 含义 -a $filename 文件存在时为真 -d $filename 文件名对应的是目录时为真 -s $filename 文件非空时为真 -r $filename 文件可读时为真 -w $filename 文件可写时为真 -x $filename 文件可执行时为真 数字判断 命令 含义 n1 -eq n2 n1等于n2时为真 n1 -ne n2 n1不等n2时为真 n1 -gt n2 n1大于n2时为真 n1 -lt n2 n1小于n2时为真 n1 -ge n2 n1大于等于n2时为真 n1 -le n2 n1小于等于n2时为真 字符串判断 命令 含义 -n str1 str1字符串不为空串时值为真 -z str1 str1字符串为空串时值为真 str1 == str2 str1与str2相等时为真 str1 != str2 str1与str2不等时为真 str1 &gt; str2 按字典序str1排在str2后面时为真 str1 &lt; str2 按字典序str1排在str2前面时为真 数字判断特殊写法 命令 含义 ((&quot;$n1&quot; == &quot;$n2&quot;)) n1等于n2时为真 ((&quot;$n1&quot; != &quot;$n2&quot;)) n1不等n2时为真 ((&quot;$n1&quot; &gt; &quot;$n2&quot;)) n1大于n2时为真 ((&quot;$n1&quot; &lt; &quot;$n2&quot;)) n1小于n2时为真 ((&quot;$n1&quot; &gt;= &quot;$n2&quot;)) n1大于等于n2时为真 ((&quot;$n1&quot; &lt;= &quot;$n2&quot;)) n1小于等于n2时为真 逻辑关系运算符 命令 含义 -a 与操作，用于[] 和 test 操作符 -o 或操作，用于[] 和 test 操作符 ！ 取反操作，用于[] 、 test 操作符 和 [[]] 关键字 &amp;&amp; 与操作，用于[[]] 关键字 \ \ 或操作，用于[[]] 关键字 这些逻辑写法千奇百怪的，写两个例子就慢慢就慢慢理解了，比如判断一个字符串不为空，并且这个字符串指定的目录还存在就可以写成 123if [ -n "$1" -a -d "$1" ]; then echo $1 directory is existfi 使用双小括号来比较数值变量，写在双小括号中的变量前面可以不加 $ 符号，还有诸多特权等着你去发现 12345num1=$1num2=$2if (( num1 &gt; num2)); then echo num1 \&gt; num2fi 总之在学习这些条件比较的时候踩了不少坑，有很多情况都没有注意到，不过慢慢也适应了这种语法，但还是免不了会出现一个小问题，这里提供一个 Shell 语法检查的在线网站 《shellcheck》，将要检查的脚本放到页面上检测，会给出详细的错误信息，当然也有命令版本，可以自己到对应的 github 页面上下载哦~ 总结 Shell 脚本中获取命令的执行结果，可以通过反引号`command`，或者小括号 $(command) 的方式得到 Shell 脚本中有一系列 $ 开头的变量，用好他们是脚本和函数传递参数的关键 Shell 脚本中的条件判断对于初学者来说很头大，有许多注意的点要记住，判断形式也多种多样 脚本中有单引号、双引号、反引号，简单来记就是单引号中原样输出，双引号中变量求值后输出，反引号中只能写需要执行的命令 脚本中还要中括号、双中括号、小括号、双小括号等，上面都提到过，可以自己练习下，具体的细节怕是要单独总结了，放到一起太多了 脚本的中的分号起到语句结束的作用，如果有换行就不需要分号了，比如 if 条件后面的 then 如果换行，那么 then 前面的分号可以省略 再记住一个坑，脚本赋值等号两端不能有空格，脚本判断等号两端必须有空格 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有些局，选择不入便立于不败之地，选择介入，即使曾身经百战，也恐难全身而退，更不要谈什么收益了~ 2020-7-11 00:30:00]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>if</tag>
        <tag>Shell</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[根证书的应用和信任基础]]></title>
    <url>%2Fblog%2F2020%2F07%2F06%2F%E6%A0%B9%E8%AF%81%E4%B9%A6%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E4%BF%A1%E4%BB%BB%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[前言人生在世总要相信点什么，信亲人、信朋友、信你面前的陌生人，即便这些你都不信，也要信自己吧，假如连自己都不信了，那在地球上恐怕很难生存了。 我们把钱存入银行，因为我们相信当我们需要用钱时可以通过银行卡把钱取出来；我们拿着车票准时来到候车大厅，因为我们相信除非特殊情况，我们买的那趟车绝不会提前丢下我们而去；遇到纠纷我们会选择打官司，因为我们相信法官最后会给我们公正的判决结果。 生活中我们信任自己的经验，信任自己的亲人朋友，并依赖这些信任来做许多事情，这些信任是我们正常生活学习的前提，缺了这些我们将寸步难行。而在网络中我们同样需要信任，这些信任是筑造网络社会的基石。 有些信任是有条件的，比如银行贷款时不能通过空口白话就借来白花花的银子，而抵押物是贷款银行为了相信你附加的筹码；有些信任是无条件的，比如前面一篇总结 《认证、HTTPS、证书的基本含义》中提到的根证书，我们必须无条件信任，否则我们将置身于网络猜疑的海洋之中，无法正常利用网络带给我们的便利。 信任链我们常听说 HTTPS 更加安全，它是通过非对称加密技术，让我们可以在不确定的网络环境中可以确认对方的身份，安全传输密钥，但这一切都是有前提的，你得相信你的操作环境是安全的，你没有被人监控，你的电脑没有被人控制，你的数据没有被人篡改，抛开环境谈安全都是耍流氓~ 好了，我们可以回顾一下，要想验证一个网站的身份，我们需要得到网站的公钥，如果可以解开网站拿私钥加密的消息，我们就证明了网站的身份，而网站的公钥不能由网站直接发给我们，需要找权威机构给它证明，相当于找了个担保人。 权威机构会用自己的私钥把网站的信息和公钥合在一起生成证书，当我们访问网站时首先得到这个证书，然后用权威机构的公钥来解开证书内容，得到网站的信息和网站的公钥，然后进行信息比对和公钥解密来认证身份，这时我们需要思考，权威机构的公钥从哪里来？ 权威机构可以找更加权威的机构按照相同的方式给它做证书，这样一环一环的就走下去，形成了信任链，然后就无穷无尽了，一个权威机构给另一个权威机构证明，我可以玩到天荒地老，到底什么时候是个头啊，其实我们可以人为的确定一个，那就是根证书，他不需要找别的人给它证明，如果一个网站证书最终信任链顶端是有效的根证书，那么网站身份被确认。 根证书接下来看看根证书在哪呢？它内置在我们的浏览器（Firefox）和操作系统中，我们需要无条件的信任，从理论上讲没办法判断根证书的真假，它是自证清白的。这里需要注意，根证书不止有一个，它可以有很多个，“根”只是说明信任链到此为止，整条信任链上的节点都是“可信”的。所以说还是不要随意安装根证书，因为有了它就可以在你的电脑为所欲为。 说到这里有些人会想，根证书内置在操作系统和浏览器（Firefox）中，如果我下载一个被恶意修改的浏览器岂不是危险了，这种担心是有必要的，所以请尽量在正规网站下载，可是怎么证明哪些网站是正规网站呢？可以使用系统自带的根证书判断。 如果我的系统是盗版系统，根证书被人改过，那不是更危险了，事实确实如此，算了吧，还是暂时不相信网络了，我去买个系统光盘吧（不知道现在还有没有人用光盘装系统），可是卖你光盘的人能保证光盘的内容不被篡改吗？你说那不能，因为他是微软高级经理的小舅子，应该不能卖盗版碟吧。 即使光盘不是盗版的，但是制作光盘的内容有没有人动过手脚呢？这些我们还是无法确认，我们能做的只是尽可能的在正规渠道购买正版系统，这种情况遇到证书被篡改的情况很小，然后就无条件相信这个系统了，这就是我文章开头说的，我么总要信点什么，试想如果盖茨在 Windows 操作系统的证书中留有后门，你又能做些什么呢，所以还是不要纠结了，既然用就在正常使用的前提下信任它。 应用及分析说是应用，实际上我只是想吐槽而已，在吐槽之前我们应该了解，证书可以跟各大证书机构（也就是各种CA）来买，也可以自己生成，可能有人会想了，自己生成挺好啊，不用花钱谁还买啊？可是刚刚说过了，跟CA买的证书都是操作系统内置证书认证过的，自己生成的证书操作系统和浏览器可不认，那怎么办呢？ 干脆自己安装个根证书，自己给自己认证得了，用户岂是你想让安装就安装的，别说，还真是这样，只要你说的情况很危急，必须安装，那么大多数的小白用户是会自动安装的，这时你想到了谁？ 不知道大家想到了谁，反正我是想到了建行网银证书和令人“可歌可气”的12306，接下来简单扒一扒他们两个的故事… 建行网银证书最先接触的证书就是建行网银证书，我的第一代网银盾用了将近10年，去年才刚刚升级成2代，可以说真的是太稳定了，不知道做网银的产品经理是谁，你简直就是程序员的福音，在2020年的今天打开建行的官网，首页倒是好看了许多，但是有些内容，比如证书安装、U盾介绍的页面还是原来丑丑的样子。 之前办理U盾时还花钱，根本都不懂啊，使用U盾必须装证书啊，不装就不安全啊，现在回想起来，和我说这话的人可能根本就不懂什么是安全，什么是不安全，反正装就是了，每次付款都要启动建行验证程序，这也是我手动安装过的次数最多的证书，是它开启了我网上购物的里程。 已经2020年了，打开建行的官网依旧提示我正在使用不安全的连接，使用网银依旧让我自己安装证书，可能作为一个大银行，官方网站迟迟不启用 HTTPS，使用网银盾坚持要用户自己安装证书，应该不仅仅是证书价格的问题，可能还有什么其他的原因。 神奇的12306毕业后直接在12306买票的次数就少了，现在一般使用 APP 来解决，前一阵发现12306居然不要求自己安装证书了，仔细一查原来从2017年开始，12306官网就购买了 DigiCert Inc 认证的证书，确实是一个进步的boy，终于舍得花点钱买证书了，作为一个巨大型的网站，它方便了人们购票的方式，是值得歌颂和称赞的，但是每次购票前还要安装烦人的证书，确实挺令人生气的。 原来“根证书”3个红字显示在页面正中间，确实起到了提醒的作用，挺扎眼的，不过那已经一去不复返了，我再放个图，大家一起回顾一下。 ESET SSL Filter CA最后放一个例子，让你感受下根证书的威力，ESET是总部位于斯洛伐克布拉迪斯拉发的一家世界知名的电脑安全软件公司，主要做杀毒软件，前不久复习 HTTPS 知识的时候发现，我访问各大网站的证书全都变成了 ESET SSL Filter CA，这是什么鬼，难道 ESET SSL Filter CA 是个特别大的证书机构？ 当时还没有意识到是杀毒软件的证书，以为大家都是买的这家证书，后来发现不太对，百度、谷歌、GitHub、Stack Overflow，怎么都是一样的证书，继续深究才发现被“窃听”了。 我们知道使用 HTTPS 通信因为使用了非对称加密，没有私钥是无法窃听加密内容的，但是这款杀毒软件做到了，它有一个HTTPS 内容过滤的功能，做了我的电脑和各大网站的中间人，按理说 HTTPS 是可以检测出中间人的，但是这款软件在电脑中安装了根证书，所有浏览器认为它是合法的，理论上可以窃听你所有内容，甚至为所欲为。 总结 信任不仅是人类社会的基石，在网络世界同样重要 证书之间的层层信任构成了信任链，而根证书是不需要被其他人证明的 不要随意安装来历不明的根证书，那样可能会使的电脑更容易遭受到攻击 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 若衣食无忧，谁甘愿拼搏！努力鞭策自己无非是为了挣得可以选择生活的权利~ 2020-7-5 23:44:41]]></content>
      <categories>
        <category>加密与认证</category>
      </categories>
      <tags>
        <tag>认证</tag>
        <tag>非对称加密</tag>
        <tag>HTTPS</tag>
        <tag>根证书</tag>
        <tag>中间人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[码龄10年工作6年的搬砖小哥，最常访问的学习网站都在这里了]]></title>
    <url>%2Fblog%2F2020%2F06%2F18%2F%E7%A0%81%E9%BE%8410%E5%B9%B4%E5%B7%A5%E4%BD%9C6%E5%B9%B4%E7%9A%84%E6%90%AC%E7%A0%96%E5%B0%8F%E5%93%A5%EF%BC%8C%E6%9C%80%E5%B8%B8%E8%AE%BF%E9%97%AE%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%E9%83%BD%E5%9C%A8%E8%BF%99%E9%87%8C%E4%BA%86%2F</url>
    <content type="text"><![CDATA[前言这完全是一篇水文，主要看别人分享的文章很有收藏价值，所以也想试着总结一下这种类型的文章，不过之前确实用过一些比较好的网站，有些网站是查找问题时找到的，但是解决完问题就找不到了很可惜，所以我养成了收藏网址的习惯，感觉有用就会分门别类的添加到书签中，再次遇到之前解决过的问题就先在书签里搜一下，有时候会加快解决问题的进度，下面这幅图是我浏览器书签中的一部分。 网络技术飞速发展到今天，越来越多的功能被搬到了“云”上，导致原来需要在本地安装的多种开发环境完全不需要搭建了，如果是临时使用完全可以在浏览器中实现，比如对于程序猿来说不可或缺的编程开发环境，已经出现很多在线编译和运行的网站，再比如原来被称作 PS大神 的设计者们必须要在电脑上安装 Photoshop 这个庞然大物，可是现在你可以发现很多在线 PS 的软件，处理简单功能分分钟搞定，这是我截取的网页上 在线PS软件 的一部分，足以以假乱真。 好了，开始进入正题了，作为一个天天写代码的搬砖小伙，每天都在敲敲敲，不是在敲代码就是在敲代码的路上，还有一种可能就是在学习如何敲代码，那么这样的榆木脑袋每天都会访问哪些学习网站呢？接下来我把最常访问的一些网站列举一下，有可能后续会更新，但我要是犯懒就算了。 网站列表接下来会分成几个大类来列举一个我最常用的一些网站，每个人的喜好不同，但是仔细看看，或许有些你会感兴趣哦！ 一、文档项目如果写一个功能有现成的轮子给我用就好了，其实网络上有很多现成的轮子，我们要善于利用别人的成果转换为自己进步的阶梯。 1、cppreference &gt;&gt; https://en.cppreference.com/w/ 首推这个网站其实是有点偏心的，因为每天都在写C++，所以还是首先就想到了这个网站，这个网站中可以查到已经发布的各个C++标准的库函数，特性、头文件等等，对于不确定的函数返回值、新标准的特性、函数的常见用法都可以在这个网站找到，这个网站还有中文版的，学习C++的小伙伴可以常来逛逛。 2、 GitHub &gt;&gt; https://github.com/ 被广大程序员调侃成“全球最大的同性交友网站”怎么能不上榜，GitHub 这个网站就算你不常用但也会常听到吧，上面充满了全世界精英团队编写的轮子，有趣的是这样一个最支持开源的网站居然被微软的这个最大的闭源厂商受够了，不过现在还是发展的越来越好了。你可以在上面阅读一些开源代码，看看那些明星产品究竟是怎样实现的，真正为我所用。 二、数据仓库程序发展离不开数据存储，数据是支撑程序发展的基石，现在的数据库已经不是当初的关系数据满天下了，各种各样的数据库类型被发明了出来，列数据库、文档数据库、键值数据库等等，真的是太多了。 1、 Redis &gt;&gt; https://redis.io/commands 非关系型数据库中最火的一个了吧，在认真学习之前一度认为它是一个新产品，后来才知道 Redis 其实在 2009 年就已经诞生了，作为一款键值型的内存数据库，现在被广泛引用于各个领域，而 Redis 的官方文档是需要不断去翻阅的，最近发布了 Redis 6.0，引入了网络多线程，以后的面试题可能要留神了。 2、 MySQL &gt;&gt; https://dev.mysql.com/doc/refman/8.0/en/ 虽然 NoSQL 数据库在各个领域兴起，但是现在还是关系数据库占据着主导地位，MySQL 就是关系数据库中的明星产品了，自从被 ORACLE 收购以后也在不断发展，最近版本从5.7一跃直接到8.0，据说MySQL 8 要比 MySQL 5.7 快 2 倍，还带来了大量的改进和更快的性能！感兴趣的可以查阅一下 MySQL 的文档，它的文档格式特别棒，看着就让人赏心悦目。 3、 墨天轮 &gt;&gt; https://www.modb.pro/dbRank 墨天轮上聚集了很多数据库爱好者，是一个新兴的数据库技术交流平台，一直渴望成为一个专业的技术社区，高效便捷、开放互助、乐于分享，能够承载我们数据人的学习和成长，促进整个行业的发展和创新，在这个网站上我们可以看到各大数据库排行，了解数据库相关的最新发展和方向。 三、工具集合文章开头也提到了，如今很多工具都搬到了线上，这样既节省了电脑空间，也免去了安装和配置工具的麻烦，只要不是IDE的重度依赖者，使用在线工具还是很方便的。 1、 在线工具 &gt;&gt; https://tool.lu/ 这个网站提供了众多的在线工具，每次一用到时间戳转换或者URL编码等操作，我肯定会第一时间打开这个页面，因为本地调 API 太麻烦了，有时还需要搭建环境，在网站上找到对应的工具直接操作就可以了，还带有实时刷新的功能，完全没必要自己在本地写代码。 2、AlbertWorld &gt;&gt; http://www.008ct.top 这个网站收录了很多有用的网址，不仅仅是工具，文档、教程、数据、资源包括方方面面，其中包括很多讲解原理的知识和有用的素材，很像一个小小的杂货铺，偶尔上新哦！ 四、疑难解答解决问题是程序员每天都要面临的功课，而程序员要解决的问题往往是没见过的，如果一个程序猿天天值只处理那么几个相同的问题，那么他已经走上了被淘汰的道路，查找问题原因，给出解决方案，祝贺你，你今天又进步了。 1、 CSDN &gt;&gt; https://www.csdn.net/ 用了这么久的 CSDN 一直不知道全称是什么？前几天才查了一下全称是 Chinese Software Developer Network，立意很深远的样子，不过确实是一个不错的网站，从去年开始大面积调整，原来的广告真是惨不忍睹，改版后现在好多了，工作中很多解决方案都出自这个网站，之前在论坛里没少逛，解答问题的同时，自己的知识也得到了巩固。 2、 StackOverflow &gt;&gt; https://stackoverflow.com/ 一个和 GitHub 比肩的网站，一个专门解决程序猿问题的网站，你要坚信，作为一个普通的程序搬砖工，你遇到的问题别人也遇到过，所以遇到问题来这个网站查一查，有时问题瞬间就被解决了，特别是一些专业的工具仅仅报了一个错误代码，通过搜索引擎很难定位具体问题，但是在这个网站上的前辈已经为你趟好路了。 五、进阶刷题程序猿就是一个活到老学到老的职业（如果35岁被淘汰就不用学了），必须时刻保证自己的学习状态，更新自己的知识储备，刷题成为了一项锻炼脑力的活动，因为很多公司特别是大公司都会要求算法达到一定的水平，所以没事多刷刷题，不要让自己的大脑锈住了。 1、力扣 &gt;&gt; https://leetcode-cn.com/problemset/all/ 这个网站貌似有很多名字，现在显示的是力扣，之前是在全球服注册的，后来莫名其妙的有注册了一次，变成了家门口的版本，这上面有很多算法题，一段时间没看居然还加上了面试题，不过它搞的那个竞赛挺有意思的，作为长期的两题选手，看着高手们10分钟做完4题，犹如神仙打架一般。 2、 POJ &gt;&gt; http://poj.org/ 这个 Online Judge 有些历史了，不过一直保持着更新，ACM 竞赛时也尝试在这里刷过题，和 LeetCode 比起来这里的题似乎更难一些，如果想挑战更高难度，不妨来这里试一下。 六、教程案例当我们想学习一门新技术的时候，很渴望得到一份简单明了的教程，实际上很多技术的官方网站文档都非常完整，但是对于初学者来说理解起来会有些难度，这时候可以看一些边学边做的教程，在不断尝试中学习知识。 1、廖雪峰官网 &gt;&gt; https://www.liaoxuefeng.com/wiki/1016959663602400 廖雪峰此乃神人也，看看我截取的这篇教程的访问量你就清楚了，前两年我看到这篇文章的时候访问量才几亿，跟着教程完整的学了一遍，现在访问量已经400多亿了，受欢迎程序难以想象，廖大神写得教程浅显易懂，非常适合初学者，从头来一步步的就学会了，想当初我跟着他学爬虫把他的文章都爬了，哈哈~2、 菜鸟教程 &gt;&gt; https://www.runoob.com/ 同样是一个接地气的教程网站，谁刚开始学的时候不是一只菜鸟呢，这个网站教程很多，只要你想学总能找到你喜欢的那款，并且在讲解时会有例子和函数参数说明，非常适合初学者。 总结 总有小伙伴调侃说：收藏从未停止，学习从未开始，其实收藏是一个好苗头，只有想学才有可能去学 如果仔细看了这些网站，你会发现有些网站的设计让人真的很舒服，临时补充一个 https://git-scm.com/ 真正应了那句话，比你优秀的人比你还努力，你的产品都那么强了，网站居然还那么好看，还让不让人活了~ ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 我们没有生活在和平的年代，只是生活在了和平的国度，想开点，珍惜眼前的一切，灾难都会过去，我们还有一双手去争夺属于自己的未来。 2020-6-20 00:16:49]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>收集</tag>
        <tag>网站</tag>
        <tag>working</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++中的std::lower_bound()和std::upper_bound()函数]]></title>
    <url>%2Fblog%2F2020%2F06%2F15%2FC-%E4%B8%AD%E7%9A%84std-lower-bound-%E5%92%8Cstd-upper-bound-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言问题是躲不掉的，该来的总会来，这不是代码中又遇到了 std::upper_bound() 函数，再来学习一遍好了，在我的印象中每次看到这 lower_bound 和 upper_bound 两个函数都有些别扭，凡是见到他们必须查一遍，因为我记得它们两个函数的作用不对称，这一点记得很清楚，但是它们两个函数查找的细节却记不住，这次总结一下，强化记忆，下次回忆起来应该会快一点。 函数定义今天看到这两个函数时挠挠头又打开了搜索引擎，看到文章里写到 std::lower_bound() 是返回大于等于 value 值的位置，而 std::upper_bound() 是返回第一个大于 value 值的位置，第一反应真是瞎写，怎么俩都是大于，肯定应该是一个大于一个小于啊，这样才“合理”嘛！ 但是当看到多个文章中采用相同的说法时，刚刚还“坚定”的想法开始动摇，然后开始查C++标准文档，一遍遍读着那有些拗口的文字: std::lower_bound returns an iterator pointing to the first element in the range [first, last) that is not less than (i.e. greater or equal to) value, or last if no such element is found. std::upper_bound returns an iterator pointing to the first element in the range [first, last) that is greater than value, or last if no such element is found. 这些标准文档上的文字印证了刚刚查询到的结果，两个函数返回的结果都是迭代器，std::lower_bound() 是在区间内找到第一个大于等于 value 的值的位置并返回，如果没找到就返回 end() 位置。而 std::upper_bound() 是找到第一个大于 value 值的位置并返回，如果找不到同样返回 end() 位置。 两个函数都提到了大于操作，而没有涉及到小于操作，这就是我前面提到的不对称，也是我感觉不合理的地方，但是当尝试使用了几次这两个函数之后，我发现这两个函数的设计的恰到好处，这样的设计很方便我们来做一些具体的操作。 实际例子首先说明这两个函数内部使用了二分查找，所以必须用在有序的区间上，满足有序的结构中有两个常见的面孔：std::map 和 std::set，他们本身就是有序的，所以提供了 std::map::lower_bound() 和 std::set::lower_bound() 这种类似的成员函数，但是原理都是一样的，我们可以弄明白一个，另外类似的函数就都清楚了。 自己设计如果你看了这两个函数的具体含义也和我一样不太理解为什么这样设计，可以思考一下接下来这个需求，找出数组内所有值为2和3的元素，图例如下： 对于一个有序数组，我们在实现 lower_bound() 函数和 upper_bound() 函数时可以让它返回指定的位置来确定取值区间，第①种情况就是标准函数库的实现方式，而第②种和第③种就是我第一印象中感觉应该对称的样子，这样看起来也没什么问题，下面具体来分析下后两种设计有哪些不好的地方。 具体分析假如我们采用第②种实现方式，那么实现打印元素2和3的代码要写成下面这样： 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; std::vector&lt;int&gt; v&#123;1,1,2,2,3,3,3,5,7,8&#125;; std::vector&lt;int&gt;::const_iterator itorLower = std::lower_bound(v.begin(), v.end(), 2); std::vector&lt;int&gt;::const_iterator itorUpper = std::upper_bound(v.begin(), v.end(), 3); while(true) &#123; std::cout &lt;&lt; *itorLower &lt;&lt; std::endl; if (itorLower == itorUpper) break; ++itorLower; &#125; return 0;&#125; 代码看起来还可以，打印完元素后判断到达了结尾直接跳出循环，但是如果要是数组中不包含元素2和3呢，那么也会打印出一个元素，还有可能导致程序崩溃。 如果我们采用第③种实现方式，那么实现打印元素2和3的代码要写成下面这样： 12345678910111213141516#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; std::vector&lt;int&gt; v&#123;1,1,2,2,3,3,3,5,7,8&#125;; std::vector&lt;int&gt;::const_iterator itorLower = std::lower_bound(v.begin(), v.end(), 2); std::vector&lt;int&gt;::const_iterator itorUpper = std::upper_bound(v.begin(), v.end(), 3); for(++itorLower; itorLower != itorUpper; ++itorLower) &#123; std::cout &lt;&lt; *itorLower &lt;&lt; std::endl; &#125; return 0;&#125; 这代码看起来简洁了很多，但是在循环开始前需要先调用 ++itorLower，因为第一个元素并不是需要找到的元素，所以要先跳过它，这样看来确实多做了一步操作，一开始就让 itorLow 指向第一个2就好了呀。 最终版本当你尝试几种实现方式就会发现，还是标准库提供的这种方式使用起来更加方便，虽然采取的不是对称的方式，但是统一了存在查找元素和不存在查找元素的的情况，写出的代码也比较简洁，没有多余的步骤，代码如下： 123456789101112131415#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; std::vector&lt;int&gt; v&#123;1,1,2,2,3,3,3,5,7,8&#125;; auto itorUpper = std::upper_bound(v.begin(), v.end(), 3); for(auto itorLower = std::lower_bound(v.begin(), v.end(), 2); itorLower != itorUpper; ++itorLower) &#123; std::cout &lt;&lt; *itorLower &lt;&lt; std::endl; &#125; return 0;&#125; 总结 有些函数的实现方式和我们想象的并不一样，但是我们可以通过熟练使用来了解它为什么这样设计 对称结构虽然是很美的，但是非对称的结构在编程中常常出现，同样有其美丽所在 遇到类似的问题可以动笔画一画，列举出各种情况会有利于你做出正确的判断 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 有时会很焦虑，看到优秀的人比你还努力时总让人感到急迫，但是一味的忧患是无意义的，脚下迈出的每一步才是真真正正的前进，不要去忧虑可能根本就不会发生的事情，那样你会轻松许多 2020-6-26 23:21:40]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>lower_bound</tag>
        <tag>upper_bound</tag>
        <tag>查找</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认证、HTTPS、证书的基本含义]]></title>
    <url>%2Fblog%2F2020%2F06%2F14%2F%E8%AE%A4%E8%AF%81%E3%80%81HTTPS%E3%80%81%E8%AF%81%E4%B9%A6%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%90%AB%E4%B9%89%2F</url>
    <content type="text"><![CDATA[前言通过前面的总结 《对称加密、非对称加密、公钥、私钥究竟是个啥？》，我们基本了解了对称加密和非对称加密的概念和特点，考虑到效率和安全性，一般可以使用非对称加密来传递对称加密所需密钥，之后就采取对称加密通信了，这样可以大大提高数据发送的的效率。 其实密码技术除了应用在加密领域外还有很多其他的用途，比如验证数据的完整性、用来做认证、提供一些不可否认的证据等，这些应用也常常出现在我们的日常生活中，比如很多官方网站在提供软件下载链接的同时，还附带一个验证的字符串，实际上很多就是md5码或者hash码，这些就是供下载的人来验证完整性的，防止被其他人篡改。 我们下载完软件之后，使用工具来将软件转化成一串字符，听起来很神奇，实际上就是计算一下软件的md5码或hash码，然后和网站上的标注的信息进行对比，如果一致那么软件就是完整的。我曾经就遇到过一次，当时做的游戏发包，同事给我发了测试包，安装之后无法正常使用，检查包的大小与发送端的一样，后来使用检测工具计算发现md5是不同的，原因可能是发送包的时候电脑卡死过，导致最后发来数据包与原始数据产生了差异。 但是你有没有想过，这个软件虽然是完整的，通过md5计算发现也没有被其他人篡改，那么怎么证明你下载的网站真的是官网呢？万一官网也是伪造的呢？如果有人伪造了官网，又将上面的软件进行修改重新计算md5然后上传到自己伪造的界面上，你要怎么来识别呢？ 可能有的人会想到看网址啊，taobao.com 就一定是淘宝的网站吗？这个域名是可以伪造的，所以要验证网站上东西是真的，那么首先要验证你看到的网站是真的，这就涉及到了认证身份，接下来可以简单了解下什么是认证。 认证其实密钥不仅仅可以用来加密，还可以用来认证的，那么什么是认证？认证是一种信用保证形式，表示对一种事物或一个人的信任和认可，比如常见的毕业证书、结婚证书都是对人一段经历或一种关系的证明和认可。最简单的网站登录是基于密码的验证，这实际上就是利用对称加密的认证。 网站保存了你的用户信息和密码，下次再登录的时候输入密码后，网站会用你输入的密码和之前保存的密码进行对比，如果密码相同则认证成功，成功的证明了“你”就是“你”，而非对称加密同样可以用来做认证。 在非对称加密的实现中，私钥是只有自己保存的，而私钥加密的内容可以使用公钥解开，如果一份加密数据可以用 Jerry的公钥解开，那么我们就可以认为这份数据是 Jerry 发出的，因为只有 Jerry 自己有私钥，所以可以通过这种方式来进行认证。 而在网络上想要认证一个网站的身份，确认它不是钓鱼网站，第一个映入脑海中的就是 https，一般提到 https 都会说它是加密的、安全的，是 http 的升级版，但是 https 的安全不仅仅体现在加密上，还有它的认证功能，可以使你免受钓鱼网站的侵害。 HTTPS简单了解下 HTTPS，一般来说网络模型常说的有OSI七层模型和五层模型，HTTPS 的诞生并没有增加模型的层数，HTTP 是建立在 TCP基础上的应用层协议，而 HTTPS 是在 TCP 和 HTTP 之间的会话层中加了一些特殊操作，使原来明文传输的内容，在会话层这一步进行加密，并且可以对数据来源进行认证。 提到 HTTPS 就不得不说 SSL 和 TSL， SSL 是应用在 HTTP上的一个协议加密层，最初是由网络大佬网景公司（Netscape）研发，后来升级为 TSL，简单的理解就是 HTTP + SSL/TSL = HTTPS。 随着网络安全逐渐得到大家的认识，一些主流网站基本都都将访问方式改成了 https，支持 https 的网站在浏览器的地址栏中通常有一把小锁，点开会提醒你访问的是安全的连接，如果你访问的连接疑似被人篡改或者仿冒，那么这把小锁会被斜线划掉，提醒你网站危险请谨慎访问。 那么 https 是怎么判断出来哪些网站是安全的，哪些网站是仿冒的呢？毕竟有些网址都很像甚至可以做成一模一样的，这就用到了非对称加密的认证功能，当我用 A 的公钥可以解密一段消息，那么就可以证明消息是 A 发来的，https 的认证功能正是利用了这个特点。 当访问一个网站的时候，网站先给我发一个用它的私钥加密的数据，然后我用它的公钥来解密，如果解密成功就说明我访问的网站是正常的，可以继续访问，如果解密失败则很有可能是虚假或者仿冒的网站，应该仔细辨别一下了。 这里会有一个问题，我怎么才能得到网站的公钥呢？之前说过密钥配送问题，直接由网站发给我肯定不行，中间可能被篡改，也有可能一个虚假网站把它自己的公钥发给我了，我用假的公钥验证对应的假的私钥也是成功的，这样就起不到认证的效果了，必须给他找个证明人才行，这就要用到我们下面要说的证书了。 证书证书是用来证明一件事情或东西的，刚才说网站的公钥不能它自己来发，这样不能证明它的身份，我们可以找一个权威的机构给它认证一下，我每次从权威机构获得网站的证书，从这个证书中取得公钥，如果用这个公钥可以解开网站私钥加密的内容，那么就可以认证它的身份了。 这里提到的证书就是网站所有者找权威机构申请的，权威机构把网站信息、有效时期、对应的公钥、序列号消息等数据存储到证书中，当我们需要能某个网站的公钥时，去证书中取就可以了，这里的证书有点像营业执照了，由权威机构发布，用来证明你的身份。 但是权威机构的证书怎么发给我呢？我们有理由认为网络是不安全的，那证书如果直接通过网络发给我同样是不安全的，还有一个问题就是网站虽然找了一个权威机构，但是我认为它不够权威怎么办？这时这个权威机构可以找一个更权威的机构证明自己，让更权威的机构给自己颁发一个证书，这样就形成了证书链。 就像现实生活中我要找个人来干活，因为工期比较紧所以找的人要求踏实，必须能老老实实把活干完，不能半路撂挑子，张三过来应聘，正在我犹豫时，走过来一个叫李四的人说张三没问题，但是我还是不能确定，因为李四我也不熟悉，然后李四居然把我爸叫来了，我爸和我说李四这个人特别诚实，从来不说谎，这时一条证书链就形成了，李四为张三证明，我爸为李四证明，那谁来证明我爸说的是真的呢？不需要的，我无条件相信他。 这在计算机的证书链中就是根证书，根证书不需要别人来证明，你只能无条件相信它，它是整个信息链的源头，通常内置在操作系统或者浏览器中，关于根证书还要一些好玩的故事和一些变态的应用，下次再说吧，准备睡觉了~ 总结 密码技术除了应用在加密领域，还可以用来验证数据的完整性、用来做认证、提供一些不可否认的证据 HTTPS 不仅可以用来加密通信内容，还可以用来验证网站的真实性 正规的支持 HTTPS的网站在访问时会地址栏会有一把安全的小锁头，但是有些不出现小锁头的网站并不一定都是非法的 HTTP 网站是没有小锁头的，因为有些数据不需要加密，毕竟绝大多数的 HTTPS 证书是要钱的，有很多网站由于经费问题还未投身于 HTTPS 的怀抱 根证书通常内置的操作系统或者浏览器中，是证书链的源头，你必须无条件的信任他。 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 当我们拼劲全力到达自己的终点时，可能会看到同行的人正在你的终点线前伸伸懒腰准备出发，然后一骑绝尘消失在你震惊的目光中，但是这不是我们放弃努力的理由，因为如果你不努力，你甚至连他们的背影也看不到~ 2020-6-14 23:20:21]]></content>
      <categories>
        <category>加密与认证</category>
      </categories>
      <tags>
        <tag>加密</tag>
        <tag>认证</tag>
        <tag>非对称加密</tag>
        <tag>HTTPS</tag>
        <tag>证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对称加密、非对称加密、公钥、私钥究竟是个啥？]]></title>
    <url>%2Fblog%2F2020%2F06%2F07%2F%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E3%80%81%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86%E3%80%81%E5%85%AC%E9%92%A5%E3%80%81%E7%A7%81%E9%92%A5%E7%A9%B6%E7%AB%9F%E6%98%AF%E4%B8%AA%E5%95%A5%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[前言进入正题之前先唠叨几句，不久前听到一个名词叫——费曼学习法，核心思想就是用通俗的话语把复杂道理或技术讲清楚，如果你已经明白了这个方法的含义，那么我好像离成功又进了一步。其实这个方法一直在尝试使用，但是没想到它居然有个“洋气”的名字。 由于之前学习时接触了加密、验证、HTTPS、证书等知识，感觉挺有意思的，最近也用到了一些这方面的内容，所以决定把这些概念重新梳理一下，免得一段时间不复习又还给书本了。本打算写一篇总结把这些概念整理到一起，但是初步想了一下很难实现，涉及到的概念实在太多了，所有还是决定分成几次来写吧。 分开写就比较随便了，写到哪完全看心情，不过我还是力图用最简单的描述来讲清楚问题，抛开具体的实现细节（其实我也不懂），梳理流程和概念性的知识，想了解具体的实现细节还是找专业的书籍去补充吧，我曾经看了一遍《图解密码技术》，过了这么久整本书我就记得两个词——异或、求余，再比如使用公钥和私钥来完成非对称加密，就是利用了两个大质数 (p,q) 乘积 (n) 难以逆向求解，这些太细节的东西很难展开一点点讲清楚。 最理想的状态是把学习知识当成是看故事书，阅读完一个个情节就吸收的差不多了，而不是把这些知识当成武功秘籍，然后一点点参悟，最后觉得枯燥而放弃，所以为了知识解惑，也算是将学习成果做个笔记，我们开始从最基础的知识学起。 对称加密对称加密一般指：加密和解密使用的是同一个密钥的加密方式。就像防盗门的钥匙一样，可以用钥匙把门锁上，也可以用这同一把钥匙再把门打开。 对称加密示例至于密钥怎么使用要看具体的加密算法了，可以举一个简单的例子，比如有下面这样一句话： I like cat 我想把它发给一个好朋友，但是又不想被别人看到，万一有其他人一眼看到，那我的喜好就暴露了（那有怎样呢？），这时我们可以把这句话改的稍微隐晦一点，我可以和好友约定一个密码，假设是 1，然后我把原来这句话的每个非空白字母都替换一下，按字母表顺序使用后一个字母替换前一个字母，比如用字母 b 替换字母 a，那么这句话就变成了： J mjlf dbu 这时就不怕被别人一眼看穿消息内容了，没有意义的字符串是比较难记的，但是当我的好友收到这句话时，使用我们约定的密码 1 就知道字母顺序变换了1位，所以他再将将字母反向替换回来就能够将文字还原。 这个例子很简单，但可以说明对称加密的关键，就是加密解密使用同一个密钥，例子中的 1 就是这个密钥，它可以让解密者知道，还原信息时需要反向移动1位即可，消息发送流程如下： 1234graph LRA[I like cat] -- 用1加密 --&gt; B((J mjlf dbu))B((J mjlf dbu))-- 发送给好友 --&gt; C((J mjlf dbu))C((J mjlf dbu))-- 用1解密 --&gt; D[I like cat] 对称加密的问题刚才的例子已经说了对称加密的流程，但是有一个问题需要解决，这个密码 1 我要怎么告诉我的好友呢？直接发消息被别人看到怎么办，打电话也有可能被别人窃听啊！ 密钥配送这就涉及到了一个密钥配送的问题，如果想让对方解密就需要把密码发过去，但是密钥有可能被其他人窃取，这样秘密就不再是秘密了，可能你会想即使密码被别人窃取了也不要紧，因为他根本不知道怎么用。 请不要做这种假设，简单的情况没有密码都能破解，更何况在密码和数据都被窃取的情况下呢，另外在密码领域我们建议使用完全公开的密码算法，这样的算法经过时间的检验才能被用于加密，千万不要独创一套自认为很安全的加密算法，单靠隐藏算法的细节来达到加密的目的是很危险的。 发送密钥可能被窃取，不发送密钥对方无法解密，这个加密的密钥配送问题是使用对称加密必须要解决的，而下面要说的这种非对称就不同了，可以将一把密钥直接发送给对方，即使被窃取也没有关系。 非对称加密看这个名字就知道它有点“针对”那个叫做对称加密的小伙伴，从定义上来说对称加密指的是加密和解密使用相同的密钥（为啥不叫同钥加密咧），而非对称加密指的是加密和解密过程使用不同的密钥来进行。 乍一听好像有点不可思议啊，怎么滴，难道还能两把不同的钥匙开一把锁？确实可以！这有点像中学物理里面的两个开关控制一个灯泡。在一个漆黑的楼梯两端，分别有一个开关，控制着楼梯上方的一个灯泡，上楼前先打开楼梯下面的开关，然后上楼后关掉楼梯上面的开关，而下楼时进行相反的操作，先打开楼梯上面的开关，然后下楼后默认楼梯下面的开关，找了张电路图，感兴趣可以再分析一下。 不过非对称加密和这种双掷开关不完全相同，使用开关时可以在同一端打开或关闭，但是非对称加密时，只能在一端加密，然后在另一端解密，同一端是不能同时加密和解密的。 公钥与私钥具体地，非对称加密指的是根据特殊规则生成两把密钥 A 和 B，分别叫做公钥和私钥。私钥自己保留，公钥则分发给自己的小伙伴用来用来和自己通信，理论上生成的两把密钥选择哪一把作为私钥都可以，但是出于效率和安全等方面的要求，公钥和私钥再生成时会给出特殊的条件，所以在实际使用过程中，两者通常是不会互换的。 非对称加密的示例使用公钥和私钥怎样完成非对称加密呢？下面来看一个具体的场景，比如有 Tom 、Jerry 、Spike 三个小伙伴，有一天 Jerry 想给 Tom 发点小秘密，又不想让 Spike 发现，首先他想到的是对称加密，先和 Tom 约定一个密码，再给 Tom 发送加密消息，但是想到前几天，自己和 Tom 的消息被 Spike 破解了，因为两个人发送密钥和加密消息的过程都被窃听了，如果这次的消息再被窃听到怎么办？ 后来Jerry想起Tom曾经自己生成了一对公钥和私钥，然后把公钥发给了自己和 Spike，那这样就可以使用非对称加密了，Jerry 使用 Tom 给的公钥把要发送的小秘密进行加密，然后发送给了 Tom。这时 Spike 果然在窃听，但是窃听到的消息使用了 Tom 的公钥进行了加密，只有 Tom 拥有解开这条消息的私钥，而 Spike 虽然拥有 Tom 的公钥也是解不开的。 1234graph LRA[I like cat] -- Jerry用Tom公钥加密 --&gt; B((密文))B((密文))-- 发送给Tom --&gt; C((密文))C((密文))-- Tom用自己的私钥解密 --&gt; D[I like cat] 怎么判断解开上面的描述中出现了“解开”一词，这两个字在我刚开始学习加密这些知识的时候困扰了我好久，查了好多讲解也没弄明白，什么叫能解开，什么叫解不开。它不像现实生活中的事物那么形象，比如把电视打开，那么电视就出现图像了，把锁解开门就能打开了。在数据加密的过程中，数据本质上是一堆二进制数据，加密之后还是一堆二进制数据，解密时使用密钥进行特定的运算就会得到解密后的二进制数据，怎么判定这些“解开”的数据是否是原数据呢？ 后来在不断的学习过程中，接触了一些开源的非对称加密算法实现，比如常用的 RSA，基础的函数包括公钥加密、私钥解密、私钥加密、公钥解密等，当你在解密时将密文和密钥传入解密函数进行特定的运算，计算过程和计算结果必须满足特定的条件，这些条件是算法保证的，如果有条件不满足那么解密失败，这就是上面所提到的解不开。 非对称加密的问题之前提到对称加密时，密钥配送问题是一个难题，因为网络上发送密钥很容易被截获，无法保证密钥不被窃取。很多情况下又不能面对面的传递密钥，而非对称加密的出现解决了这个问题，因为公钥是可以被任何人知道的，所以网络上发送公钥就不怕被窃取，但是如果例子中，Jerry 收到的 Tom 的公钥实际上在途中被 Spike 替换了怎么办？ 这就又引入了一个问题——中间人攻击，形象的来表述就是有第三方 Spike 侵入了原本两个人 Tom 和 Jerry 的通信中，Spike 对 Tom 时把自己伪装成 Jerry，和 Jerry 沟通时又将自己伪装成 Tom，这样原本两个人的沟通信息全都被第三方窃取了，这个问题的根本就是获取公钥不可信，不过证书中心可以解决这个问题，后面我们再继续深入了解，这里就不展开了。 对称加密和非对称加密对比 加密类型 常见算法 加密处理速度 遇到的问题 解决办法 对称加密 DES、AES 快 密钥配送问题 面对面交换或者使用非对称加密传送秘密 非对称加密 RSA、DSA 慢 中间人攻击问题 通过证书中心来解决中间人攻击 总结 虽然想写的尽可能的通俗易懂，但还是不免会引入一些令人犯困的概念，一开始记住就好，后面理解了不会觉得枯燥 还在不断尝试表达方式，总结中融入了一些我当时学习时的想法和疑惑，我遇到的这些问题应该很多人也遇到过 使用对称加密方法，速度快，效率高，但是会面临密钥配送的问题 非对称加密虽然很巧妙的，但是效率较低，所以一般的用法是使用非对称加密来传送简短的对称加密密钥，然后再使用对称加密的方式传送数据 为了更好的加密你的数据，应使用公开的加密算法，他们都是经过时间考验的，单靠隐藏加密细节来加密时很危险的 ==&gt;&gt; 反爬链接，请勿点击，原地爆炸，概不负责！&lt;&lt;== 见过了外面的大千世界，便不再甘心留在原地，而这种不甘心恰恰就是动力~]]></content>
      <categories>
        <category>加密与认证</category>
      </categories>
      <tags>
        <tag>公钥</tag>
        <tag>私钥</tag>
        <tag>加密</tag>
        <tag>认证</tag>
        <tag>非对称加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git在回退版本时HEAD\~和HEAD^的作用和区别]]></title>
    <url>%2Fblog%2F2020%2F05%2F30%2Fgit%E5%9C%A8%E5%9B%9E%E9%80%80%E7%89%88%E6%9C%AC%E6%97%B6HEAD%E2%80%A6-%E5%92%8CHEAD-%E7%9A%84%E4%BD%9C%E7%94%A8%E5%92%8C%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言今天总结一个小知识点，虽然不难，但是对新手有很强的迷惑性，了解一下也挺好。我们在使用 Git 回退到版本的时候，可能见过这种写法 git reset --hard HEAD~，有时候也会遇到这种写法 git reset --hard HEAD^，这两个语句都是将代码库还原到上一个版本，但是只差了一个符号，他们究竟有什么区别呢？这里先给出结论：HEAD~ 和 HEAD^ 含义不同，功能一样！ HEADHEAD 这个词在 git 使用过程中经常出现，作用很像是数据结构中指向二叉树根节点root的指针。有个 root 指针我们就可以对二叉树进行任意操作，它是二叉树的根基。而 git 中的 HEAD 概念也类似一个指针，它指向是当前分支的“头”，通过这个头节点可以追寻到当前分支之前的所有提交记录。 git 的提交记录之间的关系很像一棵树，或者说是一张图，通过当前的提交记录指向上一次提交记录串联起来，形成一个头结构，而在 git 中我们常常说的切换分支，只不过是 git 客户端帮你把要操作的那条路径的头节点，存储到了 HEAD 文件中。 HEAD 在 git 版本控制中代表头节点，也就是分支的最后一次提交，同时也是一个文件，通常在版本库中 repository/.git/HEAD，其中保存的一般是 ref: refs/heads/master 这种分支的名字，而本质上就是指向一次提交的 hash 值，一般长成这个样子 ce11d9be5cc7007995b607fb12285a43cd03154b。 HEAD~ 和 HEAD^在 HEAD 后面加 ^ 或者 ~ 其实就是以 HEAD 为基准，来表示之前的版本，因为 HEAD 被认为是当前分支的最新版本，那么 HEAD~ 和 HEAD^ 都是指次新版本，也就是倒数第二个版本，HEAD~~ 和 HEAD^^ 都是指次次新版本，也就是倒数第三个版本，以此类推。 这个说法在之前的总结 《git checkout/git reset/git revert/git restore常用回退操作》 中提到过，但是并未展开说，今天就来测试一下。 HEAD 后面 ~ 和 ^ 的区别其实 HEAD~ 和 HEAD^ 的作用是相同的，这两者的区别出现在重复使用或者加数字的情况，下面来分情况说明一下。 HEAD~ 和 HEAD^后面都加1加上参数1之后变成了 HEAD~1 和 HEAD^1，其实这就是他们本来的面貌，在参数为 1 的情况下可以省略，HEAD~1 表示回退一步，参数1表示后退的步数，默认推到第一个父提交上，而HEAD^1表示后退一步，直接后退到第n个父提交上，数字1表示是第一个父提交。 这里引入一个父提交的概念，也就是在最新提交之前的最近的提交我称它为父提交，但是父提交会有两个吗？实际上会的，直接的父提交可能会有很多，分支合并是产生父提交的一种常见原因，两个分支合并到一起时，这两个分支的原 HEAD 都会成为合并后最新提交的父提交。 理解了这个概念，我们发现虽然数字是一样的，但是含义却不相同，HEAD~1 中指的是后退的步数，HEAD^1指的是退到第几个父提交上。 HEAD~ 和 HEAD^后面都加0这是一种比较特殊的情况， 加上参数0之后变成了 HEAD~0 和 HEAD^0，其实他们指向的节点没有改变，还是代表了 HEAD，只要了解这种情况就行了，我还没有见过谁这样写过。 HEAD~ 和 HEAD^后面都加大于1的数字这时就会发现两者的不同了，比如我们把数字都定为2，那么 HEAD~2 代表后退两步，每一步都后退到第一个父提交上，而 HEAD^2 代表后退一步，这一步退到第二个父提交上，如果没有第二个父提交就会报出以下错误： fatal: ambiguous argument ‘HEAD^2’: unknown revision or path not in the working tree.Use ‘–’ to separate paths from revisions, like this:‘git […] – […]’ 具体示例上面说了几种加数字的情况，如果是第一次接触可能还是不太明白，没关系，我可以实际操作一下，看个具体的例子就明白了。 准备工作下面是一个测试代码库的分支结构，一共有 dev1、dev2、dev3、dev4 四个分支，最终合并到 dev1 分支，提交记录如下： 123456789101112131415161718192021albert@home-pc MINGW64 /d/gitstart (dev1)$ git alllog* ce11d9b (HEAD -&gt; dev1) Merge branch 'dev3' into dev1|\| * e330eac (dev3) update at dev3 - 3| * 7ab3c98 Merge branch 'dev4' into dev3| |\| | * c8795e8 (dev4) update at dev4 - 2| | * 155d3db update at dev4 - 1| * | ccdf16a update at dev3 - 2| * | 9f08bb0 update at dev3 - 1| |/* | f82b57b update at dev1 - 3* | dcdcb87 Merge branch 'dev2' into dev1|\ \| * | 32d6213 (dev2) update at dev2 - 2| * | ca4db4a update at dev2 - 1| |/| * d8d80b7 update readme at dev2* | 034ccb6 update readme at dev1 - 2* | d58fedc update readme at dev1 - 1 也许有颜色标记会看得更清楚一些，所以截个图放在这： 刚看这种图的时候要注意一点，记录列表中的先后关系不代表提交时间的先后，如果习惯于看SVN的记录以后，很容易在看日志信息时加上时间因素，但是这个时间因素在 git 查看记录时变得不再明显，比如上面记录中的 e330eac 在图形上要比 f82b57b 更接近 HEAD 提交 ce11d9b，但是因为处在不同的分支上，在合并之前他俩的修改时间还真不一定是哪个更早一些。 树形记录在 git 的提交记录图上，我们可以确定当前提交的父提交（所依赖的提交）是哪一个或者哪几个，但是不能确定任意两个提交的时间先后，为了能更清楚的看清分支提交的依赖关系，还是看下面这个树形图更方便一些。 1234567891011121314151617181920graph TB ce11d9b--&gt;f82b57b; f82b57b--&gt;dcdcb87; dcdcb87--&gt;034ccb6; 034ccb6--&gt;d58fedc; d58fedc--&gt;dev1; dcdcb87--&gt;32d6213; 32d6213--&gt;ca4db4a; ca4db4a--&gt;dev2; ce11d9b--&gt;e330eac; e330eac--&gt;7ab3c98; 7ab3c98--&gt;ccdf16a; ccdf16a--&gt;9f08bb0; 9f08bb0--&gt;dev3; 7ab3c98--&gt;c8795e8; c8795e8--&gt;155d3db; 155d3db--&gt;dev4; 查看命令在验证 HEAD~ 和 HEAD^ 之前我们先学习一个命令 git rev-parse HEAD 这个命令可以显示出 HEAD 对应的提交的 hash 值，加上 --short 参数就可以显示出长度为7位的短 hash，用起来比较方便，测试如下： 1234567albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse HEADce11d9be5cc7007995b607fb12285a43cd03154balbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEADce11d9b 开始测试下面可以用 git rev-parse --short 命令来测试 HEAD 后面跟不同参数时对应的提交是哪一个了，测试如下： HEAD~、HEAD^、HEAD~1、HEAD^1123456789101112131415albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~f82b57balbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^f82b57balbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~1f82b57balbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^1f82b57b 测试后发现，这四种写法结果是一样的，都是指向 HEAD 的第一个父提交，这和我们前面说的观点一致。 HEAD~~、HEAD^^、HEAD~2、HEAD^2123456789101112131415albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~~dcdcb87albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^^dcdcb87albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~2dcdcb87albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^2e330eac 这次我们发现，前三个表示方法是一样的，指向同一个提交记录，但是最后一个与他们不同，这时根据前面提到定义来看就行了，HEAD~~ 实际上是 HEAD~1~1的简写，而~ 后的数字就是指的后退的步数，所以 HEAD~~ 等价于 HEAD~2，属于一种合并计算。 HEAD^^ 是 HEAD^1^1 的简写，而 ^ 后面的数字表示后退一步到第几个父提交上，因为数字是1，所以 HEAD^^ 表示退一步到第一个父提交上，在退一步到第一个父提交上，这时与 HEAD~~ 的作用是相同的。 HEAD^2 就有些不同了，它表示后退一步到第二个父提交上，所以对照树形图是第二排的第二个节点。 ~ 和 ^ 混合使用看了上面的例子对于 ~ 和 ^ 的使用应该有些明白了，它俩其实可以组合使用的，比如想退到第5排、第2个节点上，也就是 ca4db4a 上，简单来看需要第一步到第一个父提交上，在退一步到第一个父提交上，然后退一步到第二个父提交上，最后退一步到第一个父提交上。 那么我们根据需求可以写成 HEAD^1^1^2^1，测试一下看看 hash 是否正确： 123albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^1^1^2^1ca4db4a 测试发现没有问题，其实还可以合并啊，我们知道1是可以省略的，所以可以简写成 HEAD^^^2^，另外多个 ^ 还可以写成 ~n 的形式，所以这个节点还可以表示成 HEAD~2^2^的样子，测试如下，结果是一样的。 1234567albert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD^^^2^ca4db4aalbert@home-pc MINGW64 /d/gitstart (dev1)$ git rev-parse --short HEAD~2^2^ca4db4a 关于 git reset 的一点思考刚学习 git reset 的命令时一直认为是一个回退命令，其实学习一段时间之后发现，这个命令其实很符合它的名字，就是一个重置(reset)命令，通过 git reset 命令可以修改 HEAD 指向不同的提交，这个提交甚至都不必是当前分支上的某次提交，测试后发现，只要是版本库中合法提交都可以使用这个命令进行设置，相应的版本库的内容也会发生对应的变化，从这一点来看，它真的太强大了，它可以使你正在开发的 dev 分支瞬间变成 master 分支。 总结 HEAD~ 后面加数字表示后退的步数，每次后退都默认退到第一个父提交上，HEAD~2 表示连退两步。 HEAD^ 后面加数字表示只退一步，但是这一步后退到数字表示的父提交上，HEAD^2 表示退一步到第二个父提交上。 git 在查看多分支提交记录时，日志的先后顺序不代表提交时间的先后顺序。 git reset 命令是一个重置 HEAD 的命令，可以指挥版本库指向任何一个合法提交。 俗话说：人不犯我，我不犯人；可俗话又说：先下手为强，后下手遭殃！俗话说：宁为玉碎，不为瓦全；可俗话又说：留得青山在，不怕没柴烧！…其实只要你变成了那个成功的“俗话”，你说的就是金科玉律，警世哲理！ 2020-5-31 14:51:49]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>版本控制</tag>
        <tag>Git</tag>
        <tag>HEAD</tag>
        <tag>reset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置Beyond Compare 4作为git mergetool来解决git merge命令导致的文件冲突]]></title>
    <url>%2Fblog%2F2020%2F05%2F21%2F%E9%85%8D%E7%BD%AEBeyond-Compare-4%E4%BD%9C%E4%B8%BAgit-mergetool%E6%9D%A5%E8%A7%A3%E5%86%B3git-merge%E5%91%BD%E4%BB%A4%E5%AF%BC%E8%87%B4%E7%9A%84%E6%96%87%E4%BB%B6%E5%86%B2%E7%AA%81%2F</url>
    <content type="text"><![CDATA[前言使用 git merge 命令合并代码的时候可能会产生文件冲突，产生这种冲突的根本原因是文件的同一处同时被多次修改，这种同时修改常体现的不同分支上，当多个分支修改了同一处代码，再合并代码的时候就会产生冲突，因为 git 程序也不知道我们想要保留哪一份修改，这时就需要我们手动修改产生冲突的文件。 当冲突内容很少的时候我们可以打开文本编辑器，找到 &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;、=========== 和 &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 这三行字符包裹的内容就是需要解决冲突的部分，但是当冲突内容特别多时我们还是习惯于通过可视化的工具来处理，Beyond Compare 就是这样一款工具，可以用来比较不同的文本文件、表格文件，还可以比较文件夹内容，之前用着比较习惯，所以在处理 git 冲突的时候也想使用这个工具来做，通过查找技术文档发现了下面的方法。 解决方案鉴于大家都比较急，查找问题时想要直接找到答案，所以我这里直接说明配置步骤，送给不求甚解的小伙伴，也方便今后我可以直接找到，不过配置之前还是要先看一下前提。 前提 在 Windows 上安装了 git 客户端，可以执行 git 命令（废话！没装 git 怎么产生冲突的） 安装了 Beyond Compare 4 这个软件，下载链接很多，自己找一个吧，实在找不到，那就放弃吧（找我要） 配置首先找到 Beyond Compare 的安装路径，比如我的软件安装路径是 D:\mybc4\BComp.exe，然后在 git 命令行客户端中执行下面命令： 1234git config --global merge.tool bc4git config --global mergetool.bc4.cmd "\"D:\\mybc4\\BComp.exe\" \"\$LOCAL\" \"\$REMOTE\" \"\$BASE\" \"\$MERGED\""git config --global mergetool.bc4.trustExitCode truegit config --global mergetool.keepBackup false 至此，git mergetool 就配置完了，当下次冲突的时候，直接使用 git mergetool 命令就可以调用 Beyond Compare 解决冲突文件了，但是你不好奇，这些设置命令都是什么意思吗？为什么执行完这些命令就能调用 Beyond Compare 4 这个软件了，如果你感兴趣可以接下往下看一看。 Beyond Compare这是一款强大的比较工具，前面提到它可以比较文本、比较表格、比较文件夹，但是它的能力不仅限于此，它甚至可以比较MP3、比较图片、比较注册表，我们的目的是调用它的比较功能，但是前提是这款软件允许你调用，如果它不给你提供接口，你就是想调用也得绕上八百个圈才可以。 这一点我们可以查询文档确定，文档是安装软件时自带的，名字为 BCompare.chm，如果找不到，安利你一个叫做 Everything 的软件，装上它以后，电脑中的一切东西都能搜索找到。 这个文档应该很容易找到的，与软件的可执行文件在同一目录，其实我们使用的比较工具应该是 BCompare.exe，但是为什么在配置 git mergetool 的是后用的是 BComp.exe 呢？这一点文档中有写： BCompare.exe: This is the main application. Only one copy will run at a time, regardless of how many windows you have open. If you launch a second copy it will tell the existing copy to start a comparison and exit immediately.BComp.exe: This is a Win32 GUI program. If launched from a version control system, it should work just fine. If launched from a console window, the console (or batch file) will not wait for it. 文档是英文的，但是比较容易理解，总的来说 BCompare.exe 是主程序，BComp.exe 用在版本控制工具中更加优秀，至于文档中提到的主程序只能启动一个副本的说明，我试了一下并不是这样的，但是这不是重点，根据文档建议，我们应该调用 BComp.exe 程序。 关于调用参数，文档中对于每种形式的比较也给出了说明，我们这里只列举两个文件和四个文件这两种参数，两个文件作为参数时常用来对比，我直接使用主程序对比文件就是这种形式，参数格式为 BCompare.exe &quot;C:\Left File.ext&quot; &quot;C:\Right File.ext&quot;，但是使用时我常把文件直接拖拽到软件上进行比较。四个文件作为参数时常用来处理文件冲突，参数类型为 BCompare.exe C:\Left.ext C:\Right.ext C:\Center.ext C:\Output.ext，参数中文件的名字表明处理时的位置和作用，看下面这个图就明白了。 从红框圈定的位置就可以发现和文件的对应关系了，最下面是最终的输出文件，也是我们可以手动修改的文件。 文件冲突及处理产生冲突先看一下 git 仓库的原始情况 12345678910111213141516albert@home-pc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@home-pc MINGW64 /d/gitstart (dev)$ lsREADME.mdalbert@home-pc MINGW64 /d/gitstart (dev)$ cat README.mdlearn git branch commandm2test checkout 在此基础上新建两个分支 dev1 和 dev2 12345678910111213albert@home-pc MINGW64 /d/gitstart (dev)$ git checkout -b dev1Switched to a new branch 'dev1'albert@home-pc MINGW64 /d/gitstart (dev1)$ git checkout -b dev2Switched to a new branch 'dev2'albert@home-pc MINGW64 /d/gitstart (dev2)$ git branch | grep dev dev dev1* dev2 在 dev2 分支上修改 README.md 文件后提交 12345678910111213141516171819albert@home-pc MINGW64 /d/gitstart (dev2)$ echo "this is dev2 test"&gt;&gt;README.mdalbert@home-pc MINGW64 /d/gitstart (dev2)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@home-pc MINGW64 /d/gitstart (dev2)$ git commit -m"update readme at dev2"[dev2 d8d80b7] update readme at dev2 1 file changed, 1 insertion(+)albert@home-pc MINGW64 /d/gitstart (dev2)$ cat README.mdlearn git branch commandm2test checkoutthis is dev2 test 切换回 dev1 分支修改 README.md 文件后提交 1234567891011121314151617181920212223albert@home-pc MINGW64 /d/gitstart (dev2)$ git checkout dev1Switched to branch 'dev1'albert@home-pc MINGW64 /d/gitstart (dev1)$ echo "this is dev1 test"&gt;&gt;README.mdalbert@home-pc MINGW64 /d/gitstart (dev1)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directorygit com -albert@home-pc MINGW64 /d/gitstart (dev1)$ git commit -m"update readme at dev1"[dev1 3136341] update readme at dev1 1 file changed, 1 insertion(+)albert@home-pc MINGW64 /d/gitstart (dev1)$ cat README.mdlearn git branch commandm2test checkoutthis is dev1 test 这时在 dev1 分支上合并 dev2 分支上的修改就会产生冲突 1234567891011121314151617181920albert@home-pc MINGW64 /d/gitstart (dev1)$ git merge dev2Auto-merging README.mdCONFLICT (content): Merge conflict in README.mdAutomatic merge failed; fix conflicts and then commit the result.albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ cat README.mdlearn git branch commandm2test checkout&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADthis is dev1 test=======this is dev2 test&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev2albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ lsREADME.md 冲突产生了，文档中同一位置被两个分支修改后合并导致的，内容里出现了 &lt;&lt;&lt;、===、&gt;&gt;&gt;，包裹的内容被分成了两部分，上面一部分是当前分支修改的，下面一部分是从 dev2 分支合并过来的，还要注意虽然产生了产生了冲突，但是目录中并没有产生其他多余的文件。 解决冲突这样的冲突比较简单，我们只要使用文本工具删除不想要的内容，保存后 git add README.md，然后再 git commit 就完成了冲突的解决，但是因为配置了 git mergetool，我们可以用它来解决冲突，直接在命令行敲命令 git mergetool 就可以: 12345678albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ git mergetoolMerging:README.mdNormal merge conflict for 'README.md': &#123;local&#125;: modified file &#123;remote&#125;: modified file 这时光标不会退出，一闪一闪并且打开 BComp.exe 工具，截图如下： 这时如果你打开 git 库所在目录会发现除了 README.md 还多了下面4个文件： 12345README.mdREADME_BACKUP_584.mdREADME_BASE_584.mdREADME_LOCAL_584.mdREADME_REMOTE_584.md 按照自己的实际情况修改最下面的文件，然后点击箭头所指的保存按钮，关闭 Beyond Compare，查询一下仓库状态 123456789101112albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ git statusOn branch dev1All conflicts fixed but you are still merging. (use "git commit" to conclude merge)Changes to be committed: modified: README.mdalbert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ lsREADME.md 不但冲突文件没有了，还给我们自动执行 git add README.md 命令，我们只需要执行 git commit 就解决完了冲突。 123456789101112albert@home-pc MINGW64 /d/gitstart (dev1|MERGING)$ git commit[dev1 b348ae6] Merge branch 'dev2' into dev1albert@home-pc MINGW64 /d/gitstart (dev1)$ git adog* b348ae6 (HEAD -&gt; dev1) Merge branch 'dev2' into dev1|\| * d8d80b7 (dev2) update readme at dev2* | 3136341 update readme at dev1|/* 5f4181e (origin/dev, dev) add comments 工具配置的参数含义回过头来再看看 git mergetool 的4句配置到底有什么用 1234git config --global merge.tool bc4git config --global mergetool.bc4.cmd "\"D:\\mybc4\\BComp.exe\" \"\$LOCAL\" \"\$REMOTE\" \"\$BASE\" \"\$MERGED\""git config --global mergetool.bc4.trustExitCode truegit config --global mergetool.keepBackup false git config首先你需要知道 git config 的作用，就是用来配置 git 的，加上了 --global 表示调整全局 git 配置，不加的话就是调整当前库的 git 配置。windows上的全局配置一般在 C:\Users\用户名\.gitconfig，如果你之前用过 git，一般会执行过 git config --global user.name xxx 对吧，这些命令都是来调整 git 配置的，打开这个 .gitconfig 你会看到 123456789101112131415[user] name = albert email = albert@163.com[core] autocrlf = true[alias] st = status adog = &quot;log --all --decorate --oneline --graph&quot;[merge] tool = bc4[mergetool &quot;bc4&quot;] cmd = \&quot;D:\\mybc4\\BComp.exe\&quot; \&quot;$LOCAL\&quot; \&quot;$REMOTE\&quot; \&quot;$BASE\&quot; \&quot;$MERGED\&quot; trustExitCode = true[mergetool] keepBackup = false 看看最后几行就是我们添加的4项配置，只不过到文件中变成了键值对的形式，经过测试后发现，这些属性最少两级，比如 user.name 、core.autocrlf，最多三级比如 mergetool.bc4.cmd、 mergetool.bc4.trustExitCode，如果级数再多会怎么办，你可以试试 git config --global a.b.c.d.e test，它最终也会被拆成三级如下 12[a &quot;b.c.d&quot;] e = test git mergetool这个需要查一下官方文档了，git mergetool --help 就能打开git官方文档，文档写得真不错，排版格式看着就很舒服。 文档提到添加 --tool-help 选项可以列举可以的合并工具，展示如下 12345678910111213141516171819202122232425262728293031323334353637albert@home-pc MINGW64 /d/gitstart (dev1)$ git mergetool --tool-help'git mergetool --tool=&lt;tool&gt;' may be set to one of the following: vimdiff vimdiff2 vimdiff3 user-defined: bc4.cmd "D:\Program Files\Beyond Compare 4\BComp.exe" "$LOCAL" "$REMOTE" "$BASE" "$MERGED"The following tools are valid, but not currently available: araxis bc bc3 codecompare deltawalker diffmerge diffuse ecmerge emerge examdiff guiffy gvimdiff gvimdiff2 gvimdiff3 kdiff3 meld opendiff p4merge smerge tkdiff tortoisemerge winmerge xxdiffSome of the tools listed above only work in a windowedenvironment. If run in a terminal-only session, they will fail. 这一查才发现，原来 git mergetool 支持的工具有这么多，不过下面这些我都没安装，用一下上面列举的3个，试试 git mergetool --tool=vimdiff，果然打开了一个界面 幸亏不如 Beyond Compare 好用，不然我不是白配置了，不过这些工具确实方便，都不需要配置，只要安装了参数中指定一下就可以用了，比如这个 bc3，我猜它是 Beyond Compare 3，只不过我安装的是 Beyond Compare 4 这个版本。 这些内置工具使用的前提是已经安装了，并且安装软件的目录放在了环境变量 Path 中，如果没有放在这个变量中需要通过 mergetool.&lt;tool&gt;.path 参数来配置，比如我把 Beyond Compare 3 安装在了 D 盘根目录，就可以设置 git config --global mergetool.bc3.path &quot;D:\\&quot;。 我们在可用工具中没有找到 Beyond Compare 4 为什么我们可以用呢？因为 git mergetool 命令还支持自定义合并解决冲突的工具，只要指定 mergetool.&lt;tool&gt;.cmd 就可以调用了，就像 git mergetool --tool-help 查询结果中提到的 user-defined: bc4.cmd &quot;D:\Program Files\Beyond Compare 4\BComp.exe&quot; &quot;$LOCAL&quot; &quot;$REMOTE&quot; &quot;$BASE&quot; &quot;$MERGED&quot;，git mergetool 把 bc4 作为了一个等同于内置合并工具的软件。 再来看看这4句配置的含义： 1234git config --global merge.tool bc4git config --global mergetool.bc4.cmd "\"D:\\mybc4\\BComp.exe\" \"\$LOCAL\" \"\$REMOTE\" \"\$BASE\" \"\$MERGED\""git config --global mergetool.bc4.trustExitCode truegit config --global mergetool.keepBackup false 第一句 git config --global merge.tool bc4 是说把 git mergetool 的默认工具配置成 bc4，如果不指定默认工具在使用时就需要写成 git mergetool --tool=bc4 或者 git mergetool -t bc4 了，可是 bc4 是我们自己起的名字，根本就没有这个名字啊，接着往下看。 第二句 git config --global mergetool.bc4.cmd &quot;\&quot;D:\\mybc4\\BComp.exe\&quot; \&quot;\$LOCAL\&quot; \&quot;\$REMOTE\&quot; \&quot;\$BASE\&quot; \&quot;\$MERGED\&quot;&quot; 指定了工具 bc4 的调用路径和参数，后面的这4个参数都是 git mergetool 命令提供的，依次代表本地修改，被合并分支修改，两端未修改前版本文件，最终合并导出的文本文件。 第三句 git config --global mergetool.bc4.trustExitCode true， 设置为 true 表示信任软件的返回码，并依据返回码确定合并是否成功，如果设置成 false 就会在合并完成后问你是否解决完冲突，设置成 true 会方便很多。 第四句 git config --global mergetool.keepBackup false， 是指定在合并完成后删除备份文件 *.orig，这个文件会在调用 git mergetool 是产生 *.orig 备份文件，成功合并后自动删除就可以了。 思考至此终于弄明白这个 git mergetool 是怎么工作的了，但是想这样一个问题，这个 &lt;tool&gt;.cmd 一定得调用冲突解决工具吗？如果你从头看到这里应该会明白，这里只是给用户提供了一个调用自定义工具的方式，至于你调用什么它是不关心的，你完全可以在 git mergetool 的时候让电脑关机，这些都是可以的，在你明白了原理以后，一切都变得简单了。 总结 Beyond Compare 是一款强大的比较工具，合理的使用可以有效的提升工作效率 git mergetool 内置了很多可以使用的合并工具，并且支持调用自定义的合并工具 git 的官方文档写得真的挺详细，有时间可以多看一看，你会发现很多有意思的功能 急于解决问题时可以不求甚解，解决问题后最好可以明白其中的缘由，这其实就是一种进步 尽管科技很发达，但有些人一旦分开可能真的就是一生不见了]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>工具</tag>
        <tag>mergetool</tag>
        <tag>文件冲突</tag>
        <tag>bc4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用c++filt命令还原C++编译后的函数名]]></title>
    <url>%2Fblog%2F2020%2F05%2F16%2F%E4%BD%BF%E7%94%A8c-filt%E5%91%BD%E4%BB%A4%E8%BF%98%E5%8E%9FC-%E7%BC%96%E8%AF%91%E5%90%8E%E7%9A%84%E5%87%BD%E6%95%B0%E5%90%8D%2F</url>
    <content type="text"><![CDATA[前言这个命令功能单一，但是非常强大，可以用来还原C++编译后的函数名，为什么C++的函数名需要单独的命令来还原，因为他们看起来都是这样 _ZNK4Json5ValueixEPKc、这样 _Z41__static_initialization_and_destruction_0ii 或者这样的 _ZN6apsara5pangu15ScopedChunkInfoINS0_12RafChunkInfoEED1Ev，仅通过这一串字母很难知道原函数的名字是什么，参数类型就更难分析了，实际上C++在编译函数时有一套命名函数的规则，每种参数使用什么字母表示都是有约定的，但是通过学习这些约定来还原函数太麻烦了，还好有人编写了 c++filt 命令可以让我们直接得到编译前的函数名，真好…… C++编译后的函数名C++ 编译后的函数名字非常古怪，相比而言 C 语言编译后的函数看起来就正常许多了，extern &quot;C&quot;、函数重载、name mangling 这些知识点都与 C++ 这个奇怪的函数名有些关系，extern &quot;C&quot; 的作用简而言之就是告诉编译器和链接器被“我”修饰的变量和函数需要按照 C 语言方式进行编译和链接，这样做是由于 C++ 支持函数重载，而 C 语言不支持，结果导致函数被 C++ 编译后在符号库中的名字和被 C语言编译后的名字是不一样的，程序编译和连接就会出现问题，此类问题一般出现在 C++ 代码调用 C 语言写的库函数的时候。 而 name mangling 就是实现 C++ 函数重载的一种技术或者叫做方式，要求同名的 C++ 函数参数个数不同或参数类型不同，如果只有返回值类型不同，那么两个函数被认为是相同的函数，无法成功通过编译。接下来我们就来看几个例子，看看 C++ 编译后的函数名有什么变化。 C++和C语言编译后的函数名对比我们来写一段相同的代码，分别使用 gcc 和 g++ 进行编译，从代码到可执行文件需要经历“预处理、编译、汇编、链接”4个步骤，接下来为了看到编译后函数名的不同，我们只进行前两步，生成汇编代码，再来比较不同。 gcc编译simple.c文件123456789101112131415// simple.cint myadd(int a, int b)&#123; return a + b;&#125;int main()&#123; int a = 110; int b = 119; int c = myadd(a, b); return 0;&#125; gcc simple.c -S 生成汇编代码文件simple.s内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 .file "simple.c" .text .globl myadd .type myadd, @functionmyadd:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl %edi, -4(%rbp) movl %esi, -8(%rbp) movl -4(%rbp), %edx movl -8(%rbp), %eax addl %edx, %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size myadd, .-myadd .globl main .type main, @functionmain:.LFB1: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 subq $16, %rsp movl $110, -12(%rbp) movl $119, -8(%rbp) movl -8(%rbp), %edx movl -12(%rbp), %eax movl %edx, %esi movl %eax, %edi call myadd movl %eax, -4(%rbp) movl $0, %eax leave .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE1: .size main, .-main .ident "GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609" .section .note.GNU-stack,"",@progbits g++编译simple.cpp文件123456789101112131415// simple.cppint myadd(int a, int b)&#123; return a + b;&#125;int main()&#123; int a = 110; int b = 119; int c = myadd(a, b); return 0;&#125; g++ simple.cpp -S 生成汇编代码文件simple.s内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 .file "simple.cpp" .text .globl _Z5myaddii .type _Z5myaddii, @function_Z5myaddii:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl %edi, -20(%rbp) movl %esi, -24(%rbp) movl $0, -4(%rbp) movl -20(%rbp), %edx movl -24(%rbp), %eax addl %edx, %eax movl %eax, -4(%rbp) movl -4(%rbp), %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size _Z5myaddii, .-_Z5myaddii .globl main .type main, @functionmain:.LFB1: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 subq $16, %rsp movl $110, -12(%rbp) movl $119, -8(%rbp) movl $0, -4(%rbp) movl -8(%rbp), %edx movl -12(%rbp), %eax movl %edx, %esi movl %eax, %edi call _Z5myaddii movl %eax, -4(%rbp) movl $0, %eax leave .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE1: .size main, .-main .ident "GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609" .section .note.GNU-stack,"",@progbits 虽然只有几行代码，可是生成汇编文件之后变成了50多行，我们只需要关注 myadd() 这个函数编译之后变成了什么就可以了，汇编代码虽然不好读，但是查找一个函数名应该没问题的，对照着上面的代码我们发现，myadd() 这个函数通过 gcc 编译之后的函数名还是 myadd，而通过 g++ 编译之后的函数名变成了 _Z5myaddii，可以明显感觉到最后的两个字母 i 代表的是参数 int，使用 c++filt 命令还原如下： 12$ c++filt _Z5myaddiimyadd(int, int) C++函数重载编译后的函数名对比我们还是在刚才的代码的基础上增加一个参数类型不同的 myadd 函数，修改后的代码如下： 1234567891011121314151617int myadd(int a, int b)&#123; return a + b;&#125;float myadd(float a, float b)&#123; return a + b;&#125;int main()&#123; int c = myadd(110, 119); float d = myadd(52.0f, 13.14f); return 0;&#125; g++ simple.cpp -S 生成汇编代码文件simple.s内容为： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879 .file "simple.cpp" .text .globl _Z5myaddii .type _Z5myaddii, @function_Z5myaddii:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl %edi, -4(%rbp) movl %esi, -8(%rbp) movl -4(%rbp), %edx movl -8(%rbp), %eax addl %edx, %eax popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE0: .size _Z5myaddii, .-_Z5myaddii .globl _Z5myaddff .type _Z5myaddff, @function_Z5myaddff:.LFB1: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movss %xmm0, -4(%rbp) movss %xmm1, -8(%rbp) movss -4(%rbp), %xmm0 addss -8(%rbp), %xmm0 popq %rbp .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE1: .size _Z5myaddff, .-_Z5myaddff .globl main .type main, @functionmain:.LFB2: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 subq $16, %rsp movl $119, %esi movl $110, %edi call _Z5myaddii movl %eax, -8(%rbp) movss .LC0(%rip), %xmm1 movss .LC1(%rip), %xmm0 call _Z5myaddff movd %xmm0, %eax movl %eax, -4(%rbp) movl $0, %eax leave .cfi_def_cfa 7, 8 ret .cfi_endproc.LFE2: .size main, .-main .section .rodata .align 4.LC0: .long 1095908721 .align 4.LC1: .long 1112539136 .ident "GCC: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609" .section .note.GNU-stack,"",@progbits 这次一共3个函数，生成的汇编代码更长，但是我们一眼就能看见汇编代码中包含 _Z5myaddii 和 _Z5myaddff 两个函数，这就是函数重载的产物，两个参数类型不同的同名函数编译之后生成了不同的名字，_Z5myaddff 函数末尾的两个 f 应该指的就是参数类型 float。 使用c++filt定位问题示例c++filt的作用就是还原函数名字，它可以帮我们查找动态链接库中缺少的函数，还原崩溃堆栈中一大串的函数名字母等等，下面来看一个崩溃堆栈的例子，代码内容尽量简写，只为了说明问题，现实情况可能要复杂的多。 首先定义一个打印函数堆栈的函数，参考之前的总结《linux环境下C++代码打印函数堆栈调用情况》，代码如下： 123456789101112131415161718192021222324252627282930#include &lt;execinfo.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;signal.h&gt;#include &lt;iostream&gt;void show_stack(int nSignal)&#123; static const int MAX_STACK_FRAMES = 12; void *pStack[MAX_STACK_FRAMES]; static char szStackInfo[1024 * MAX_STACK_FRAMES]; char ** pStackList = NULL; int frames = backtrace(pStack, MAX_STACK_FRAMES); pStackList = backtrace_symbols(pStack, frames); if (NULL == pStackList) return; strcpy(szStackInfo, "stack traceback:\n"); for (int i = 0; i &lt; frames; ++i) &#123; if (NULL == pStackList[i]) break; strncat(szStackInfo, pStackList[i], 1024); strcat(szStackInfo, "\n"); &#125; std::cout &lt;&lt; szStackInfo; // 输出到控制台，也可以打印到日志文件中&#125; 再写一段隐藏着崩溃问题的代码： 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;string&gt;class CTest&#123;public: const std::string&amp; get_string() &#123;return s;&#125; void set_string(const std::string&amp; str) &#123;s = str;&#125;private: std::string s;&#125;;void foo(float z)&#123; int *p = nullptr; *p = 110; std::cout &lt;&lt; z;&#125;void test(std::string str)&#123; CTest* pTest = new CTest(); pTest-&gt;set_string("20200517"); const std::string&amp; s = pTest-&gt;get_string(); delete pTest; std::cout &lt;&lt; str &lt;&lt; std::endl; if (s == "20200517") foo(13.14);&#125;void func(int a, int b)&#123; std::string s = std::to_string(a) + std::to_string(b); test(s);&#125;int main()&#123; signal(SIGSEGV, show_stack); func(250, 520); return 0;&#125; 编译运行，果然崩溃了： 12345678910111213$ g++ simple.cpp --std=c++11$ ./a.outstack traceback:./a.out() [0x401aff]/lib/x86_64-linux-gnu/libc.so.6(+0x354b0) [0x7fd5f98b54b0]/lib/x86_64-linux-gnu/libc.so.6(+0x16eff6) [0x7fd5f99eeff6]/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE7compareEPKc+0x3a) [0x7fd5f9f9145a]./a.out() [0x4022b6]./a.out() [0x401d30]./a.out() [0x401e27]./a.out() [0x401ed8]/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0) [0x7fd5f98a0830]./a.out() [0x4019f9] 这时崩溃的堆栈中发现了一个特别长的函数 _ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE7compareEPKc，使用 c++filt 命令来还原函数： 12$ c++filt _ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE7compareEPKcstd::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;::compare(char const*) const 从函数名来看是一个与字符串相关的 compare 函数，查看代码发现是 s == &quot;20200517&quot; 这一句的问题，所以说能确切的知道函数名对我们查找问题来说还是挺有帮助的。 总结 c++filt 命令可以还原 C++ 为实现函数重载采用 name mangling 搞出来的奇奇怪怪的函数名 注册信号回调函数方式：signal(SIGSEGV, show_stack);，SIGSEGV代表无效的内存引用 注意 C 语言和 C++ 在编译后函数命名方式的不同，C 语言不支持严格意义的重载，C++支持 阳光、空气、水，这些真的是好东西，当你真的快要失去它们才意识的到的话就有些晚了…]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>工具</tag>
        <tag>c++filt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[汇编指令入门级整理]]></title>
    <url>%2Fblog%2F2020%2F05%2F09%2F%E6%B1%87%E7%BC%96%E6%8C%87%E4%BB%A4%E5%85%A5%E9%97%A8%E7%BA%A7%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言我们大都是被高级语言惯坏了的一代，源源不断的新特性正在逐步添加到各类高级语言之中，汇编作为最接近机器指令的低级语言，已经很少被直接拿来写程序了，不过我还真的遇到了一个，那是之前的一个同事，因为在写代码时遇到了成员函数权限及可见性的问题，导致他无法正确调用想执行的函数，结果他就开始在 C++ 代码里嵌入汇编了，绕过了种种限制终于如愿以偿，但是读代码的我们傻眼了… 因为项目是跨平台的，代码推送的 Linux 上编译的时候他才发现，汇编代码的语法在 Linux 和 Windows 上居然是不一样的，结果他又用一个判断平台的宏定义“完美”的解决了，最终这些代码肯定是重写了啊，因为可读性太差了，最近在学习左值、右值、左引用和右引用的时候，总是有人用程序编译生成的中间汇编代码来解释问题，看得我迷迷糊糊，所以决定熟悉一下简单的汇编指令，边学习边记录，方便今后忘记了可以直接拿来复习。 什么是汇编语言汇编语言是最接近机器语言的编程语言，引用百科中的一段话解释为： 汇编语言（assembly language）是一种用于电子计算机、微处理器、微控制器或其他可编程器件的低级语言，亦称为符号语言。在汇编语言中，用助记符代替机器指令的操作码，用地址符号或标号代替指令或操作数的地址。汇编语言又被称为第二代计算机语言。 汇编语言产生的原因对于绝大多数人来说，二进制程序是不可读的，当然有能人可以读，比如第一代程序员，但这类人快灭绝了，直接看二进制不容易看出来究竟做了什么事情，比如最简单的加法指令二进制表示为 00000011，如果它混在一大串01字符串中就很难把它找出来，所以汇编语言主要就是为了解决二进制编码的可读性问题。 汇编与二进制的关系换句话来说，汇编语言就是把给机器看的二进制编码翻译成人话，汇编指令是机器指令的助记符，与机器指令是一一对应的关系，是一种便于阅读和记忆的书写格式。有效地解决了机器指令编写程序难度大的问题，并且使用编译器，可以很方便的把汇编程序转译成机器指令程序，比如之前提到的 00000011 加法指令，对应的汇编指令是 ADD，在调用汇编器时就会把 ADD 翻译成 00000011。 寄存器说到汇编指令不得不提到寄存器，寄存器本身是用来存数据的，因为 CPU 本身只负责逻辑运算，数据需要单独储存在其他的地方，但是对于不熟悉寄存器的人来说会有疑惑，数据不是存在硬盘上吗？或者说数据不是存在内存中吗？这些想法都没错，那么寄存器是用来做什么的呢？ 寄存器作用其实硬盘、内存都是用来存储数据的，但是 CPU 的运算速度远高于内存的读写速度，更不用说从硬盘上取数据了，所以为了避免被拖慢速度影响效率，CPU 都自带一级缓存和二级缓存，一些 CPU 甚至增加了三级缓存，从这些缓存中读写数据要比内存快很多，但是还是无法使用飞速运转的 CPU，所以才会有寄存器的存在。 寄存器不是后来增加的，在最初的计算中就已经设计出来，相比而言，多级缓存出现的更晚一些，通常那些最频繁读写的数据都会被放在寄存器里面，CPU 优先读写寄存器，再通过寄存器、缓存跟内存来交换数据，达到缓冲的目的，因为可以通过名称访问寄存器，这样访问速度是最快的，因此也被称为零级缓存。 存取速度比较通过上面的叙述我们可以知道存取速度从高到低分别是: 寄存器 &gt; 1级缓存 &gt; 2级缓存 &gt; 3级缓存 &gt; 内存 &gt; 硬盘，关于它们的存取速度，举个例子很容易就能明白了，比如我们做菜（CPU工作）时，取手中（寄存器）正拿着的肉和蔬菜肯定是最快的，如果没有就需要把案板上（1级缓存）处理好的菜拿过来，如果案板上没有就在更远一点的洗菜池（2级缓存）中找一找，还没找到的话就要到冰箱（3级缓存）中看一看了，这时发现家里真没有，那去楼下的菜店（内存）去买点吧，转了一圈发现没有想要的，最后还是开车去农贸市场（硬盘）买吧。 通过上面这个例子应该能明白它们的速度关系了，既然缓存这么快，为什么不用缓存代替内存，或者将2、3级缓存都换成1级缓存呢？这里边有一个成本问题，速度越快对应着价格越高，如果你买过机械硬盘和固态硬盘应该很容易就理解了。 寄存器分类常用的 x86 CPU 寄存器有8个：EAX 、EBX、ECX、EDX、EDI、ESI、EBP、ESP，据说现在寄存器总数已经超过100个了，等我找到相关资料再来补充，上面这几个寄存器是最常用的，这些名字也常常出现在汇编的代码中。 我们常说的32位、64位 CPU 是指数据总线的宽度或根数，而寄存器是暂存数据和中间结果的单元，因此寄存器的位数也就是处理数据的长度与数据总线的根数是相同的，所以32位 CPU 对应的寄存器也应该是32位的。 常用寄存器用途上面提到大8个寄存器都有其特定的用途，我们以32位 CPU 为例简单说明下这些寄存器的作用，整理如下表： 寄存器 含义 用途 包含寄存器 EAX 累加(Accumulator)寄存器 常用于乘、除法和函数返回值 AX(AH、AL) EBX 基址(Base)寄存器 常做内存数据的指针, 或者说常以它为基址来访问内存. BX(BH、BL) ECX 计数器(Counter)寄存器 常做字符串和循环操作中的计数器 CX(CH、CL) EDX 数据(Data)寄存器 常用于乘、除法和 I/O 指针 DX(DH、DL) ESI 来源索引(Source Index)寄存器 常做内存数据指针和源字符串指针 SI EDI 目的索引(Destination Index)寄存器 常做内存数据指针和目的字符串指针 DI ESP 堆栈指针(Stack Point)寄存器 只做堆栈的栈顶指针; 不能用于算术运算与数据传送 SP EBP 基址指针(Base Point)寄存器 只做堆栈指针, 可以访问堆栈内任意地址, 经常用于中转 ESP 中的数据, 也常以它为基址来访问堆栈; 不能用于算术运算与数据传送 BP 寄存器EAX、AX、AH、AL的关系在上面的图标中每个常用寄存器后面还有其他的名字，它们是同一个寄存器不同用法下的不同名字，比如在32位 CPU 上，EAX是32位的寄存器，而AX是EAX的低16位，AH是AX的高8位，而AL是AX的低8位，它们的对照关系如下: 1234500000000 00000000 00000000 00000000|===============EAX===============|---4个字节 |======AX=======|---2个字节 |==AH===|-----------1个字节 |===AL==|---1个字节 汇编语言指令终于说到汇编常用指令了，因为 linux 和 windows 下的汇编语法是有些不同的，所以下面我们先通过 windows 下的汇编指令来简单学习一下，后续再来比较两者的不同。 数据传送指令 指令 名称 示例 备注 MOV 传送指令 MOV dest, src 将数据从src移动到dest PUSH 进栈指令 PUSH src 把源操作数src压入堆栈 POP 出栈指令 POP desc 从栈顶弹出字数据到dest 算术运算指令 指令 名称 示例 备注 ADD 加法指令 ADD dest, src 在dest基础上加src SUB 减法指令 SUB dest, src 在dest基础上减src INC 加1指令 INC dest 在dest基础上加1 DEC 减1指令 DEC dest 在dest基础上减1 逻辑运算指令 指令 名称 示例 备注 NOT 取反运算指令 NOT dest 把操作数dest按位取反 AND 与运算指令 AND dest, src 把dest和src进行与运算之后送回dest OR 或运算指令 OR dest, src 把dest和src进行或运算之后送回dest XOR 异或运算 XOR dest, src 把dest和src进行异或运算之后送回dest 循环控制指令 指令 名称 示例 备注 LOOP 计数循环指令 LOOP label 使ECX的值减1，当ECX的值不为0的时候跳转至label，否则执行LOOP之后的语句 转移指令 指令 名称 示例 备注 JMP 无条件转移指令 JMP lable 无条件地转移到标号为label的位置 CALL 过程调用指令 CALL labal 直接调用label JE 条件转移指令 JE lable zf =1 时跳转到标号为label的位置 JNE 条件转移指令 JNE lable zf=0 时跳转到标号为label的位置 linux 和 windows 下汇编的区别前面说到 linux 和 windows 下的汇编语法是不同的，其实两种语法的不同和系统不同没有绝对的关系，一般在 linux 上会使用 gcc/g++ 编译器，而在 windows 上会使用微软的 cl 也就是 MSBUILD，所以产生不同的代码是因为编译器不同，gcc 下采用的是AT&amp;T的汇编语法格式，MSBUILD 采用的是Intel汇编语法格式。 差异 Intel AT&amp;T 引用寄存器名字 eax %eax 赋值操作数顺序 mov dest, src movl src, dest 寄存器、立即数指令前缀 mov ebx, 0xd00d movl $0xd00d, %ebx 寄存器间接寻址 [eax] (%eax) 数据类型大小 操作码后加后缀字母，“l” 32位，“w” 16位，“b” 8位（mov dx, word ptr [eax]） 操作数前面加dword ptr， word ptr，byte ptr的格式 （movb %bl %al） 总结 汇编指令是机器指令的助记符，与机器指令是一一对应的 AT&amp;T的汇编语法格式和Intel汇编语法格式的是不同的 常用寄存器：EAX 、EBX、ECX、EDX、EDI、ESI、EBP、ESP 存取速度从高到低分别是: 寄存器 &gt; 1级缓存 &gt; 2级缓存 &gt; 3级缓存 &gt; 内存 &gt; 硬盘 常用的汇编指令：mov、je、jmp、call、add、sub、inc、dec、and、or 如今的每分每秒都是人生，不要总想着将自然发生的事情拖到预定的时刻才进行~]]></content>
      <categories>
        <category>ASM</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>汇编</tag>
        <tag>windows</tag>
        <tag>asm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11在左值引用的基础上增加右值引用]]></title>
    <url>%2Fblog%2F2020%2F05%2F05%2FC-11%E5%9C%A8%E5%B7%A6%E5%80%BC%E5%BC%95%E7%94%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E4%B8%8A%E5%A2%9E%E5%8A%A0%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言右值引用这个词是最开始是学习 easylogging++ 这个日志开源项目的时候遇到的，当时遇到 &amp;&amp; 这样的写法先是一愣，还有这种写法？难道是引用的地址？结果查询资料才明白这叫做右值引用。 右值引用的出现其实右值引用是在 C++11 时增加的新内容，在此之前，引用是没有左值和右值之分的，只存在一种引用，也就是后来 C++11 标准中的左值引用，而右值引用的提出主要是为了解决之前左值引用出现的一些尴尬的问题。 左值和右值说到右值引用需要先了解下左值和右值，这也是我自己学习的过程，之前在 《简单聊聊C/C++中的左值和右值》 这篇笔记中总结过，可以简单理解左值就是放在 = 左边，可以取到地址，可以被赋值的表达式，而右值通常是放在 = 右侧，不能取地址，只能被当成一个“值”的表达式。 右值引用的作用右值引用的出现并不是为了取代左值引用，也不是和左值引用形成对立，而是充分利用右值内容来减少对象构造和析构操作，以达到提高程序代码效率的目的。 也就是说增加右值引用这个特性是为了提高效率，之前的总结中也提到过，在 C++11 中还引入了 std::move() 函数，并用这个函数改写了 std::remove_if() 函数，这就是提高效率的例子。 使用 std::move() 函数意味着放弃所有权，对于一个左值，如果我们明确放弃对其资源的所有权，则可以通过 std::move() 来将其转为右值引用，放弃所有权的这个操作不一定都是方便的，比如 std::auto_ptr 这个第一代的智能指针，就是因为转移了所有权，使用起来不太方便，才在最新标准中被废弃的。但如果你明确要转移所有权，并且合理使用，有时可以有效的提高程序效率。 引用类型的对比在学习使用右值引用之前先复习一下左值引用，对比学习更有利于我们的记忆。 左值引用1234int i = 22;int&amp; j = i;j = 11; 上面这几行代码就是最常见左值引用的例子，变量 j 引用了变量 i 的存储位置，修改变量 j 就修改了变量 i 的值，但是如果引用一个值会怎么样呢？比如下面这行代码： 1int&amp; j = 22; 编译这行代码会得到一个编译错误： error: invalid initialization of non-const reference of type ‘int&amp;’ from an rvalue of type ‘int’ int&amp; j = 22; 像上面这种问题，可以使用常量引用来解决。 常量引用针对上面的编译错误，改成常量引用就可以通过编译了，就像这样： 1const int&amp; j = 22; 使用常量引用来引用数字常量22，可以编译通过是因为内存上产生了临时变量保存了22这个数据，这个临时变量是可以进行取地址操作的，因此变量 j 引用的其实是这个临时变量，相当于下面的这两句： 12const int temp = 22;const int &amp;j = temp; 看到这里我们发现常量引用可以解决引用常量的问题，那么为什么非得新增一个右值引用呢？那是因为使用常引用后，我们只能通过引用来读取数据，无法去修改数据，这在很多情况下是很不方便的。 右值引用常量引用可以使用右值引用来改写，改写之后可以正常编译，并且还可以进行修改： 1int&amp;&amp; j = 22; 这句代码有两个需要注意的点，第一是右值引用是 C++11 中才增加的，所以需要增加 --std=c++11 这个编译选项才能正常编译，第二是右值引用的两个地址符需要连着写成 &amp;&amp;, 如果中间有空格写成 &amp; &amp; 会被认为是引用的引用而导致编译错误，这是不符合语法的。 右值引用的示例前面对引用类型进行了对比，但是还没有发现右值引用的好处，接下来用一个例子来展示一下增加右值引用之前的写法，和使用右值引用的写法，通过对比来了解一下右值引用究竟有什么好处。 我们来实现一个自定义缓冲区，先使用最常见的方法来实现拷贝构造函数和拷贝赋值函数，简单实现如下，功能不太完整，但是可以说明右值引用的作用： 常量引用实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;class CBuffer&#123;public: // 构造函数 CBuffer(int size = 1024): m_size(size) &#123; cout &lt;&lt; "CBuffer(int)" &lt;&lt; endl; m_buffer = new char[size]; &#125; // 析构函数 ~CBuffer() &#123; cout &lt;&lt; "~CBuffer()" &lt;&lt; endl; delete[] m_buffer; m_buffer = nullptr; m_size = 0; &#125; // 拷贝构造 CBuffer(const CBuffer &amp;origin): m_size(origin.m_size) &#123; cout &lt;&lt; "CBuffer(const CBuffer&amp;)" &lt;&lt; endl; m_buffer = new char[origin.m_size]; memcpy(m_buffer, origin.m_buffer, m_size); &#125; // 赋值重载 CBuffer&amp; operator=(const CBuffer &amp;origin) &#123; cout &lt;&lt; "operator=(const CBuffer&amp;)" &lt;&lt; endl; if (this == &amp;origin) return *this; delete[] m_buffer; m_size = origin.m_size; m_buffer = new char[origin.m_size]; memcpy(m_buffer, origin.m_buffer, m_size); return *this; &#125; int get_size() &#123; return m_size; &#125; static CBuffer gen_buffer(const int size) &#123; CBuffer temp_buffer(size); return temp_buffer; &#125;private: char *m_buffer; int m_size;&#125;;int main()&#123; CBuffer b1; CBuffer b2(b1); cout &lt;&lt; "b1.size = " &lt;&lt; b1.get_size() &lt;&lt; endl; cout &lt;&lt; "b2.size = " &lt;&lt; b2.get_size() &lt;&lt; endl; b2 = CBuffer::gen_buffer(100); return 0;&#125; 运行结果是： CBuffer(int)CBuffer(const CBuffer&amp;)b1.size = 1024b2.size = 1024CBuffer(int)operator=(const CBuffer&amp;)~CBuffer()~CBuffer()~CBuffer() 这个例子不具有实用性，只为了说明问题，CBuffer 这个类定义为了拷贝构造函数并且重载了 = 运算符，两个函数参数均使用常量引用的类型，这就是一般的写法。 但是这样实现有一个问题，因为参数是常量引用，所以没办法修改原对象的值，我们看到拷贝构造和赋值重载两个函数中都有申请空间和拷贝的操作，这种操作在操作内存较大的对象是比较耗时，所以应该尽量避免，我们想到可以使用新对象的指针指向旧对象指针来解决，这样就不用拷贝了，可是这样修改会导致两个对象指向同一块内存，这个问题需要解决。 改为左值引用实现报错如果两个对象指向同一块内存，那么对象在析构的时候就会将一块内存释放两次导致奔溃，这时考虑在拷贝构造或者赋值重载时，将原来对象的指针设置成空就可以了，但是参数是常量没有办法修改啊，那我们将 const 关键字去掉试试，将两个函数改成这样： 1234567891011121314151617181920212223// 拷贝构造CBuffer(CBuffer &amp;origin): m_size(origin.m_size)&#123; cout &lt;&lt; "CBuffer(CBuffer&amp;)" &lt;&lt; endl; m_buffer = origin.m_buffer; origin.m_buffer = nullptr; origin.m_size = 0;&#125;// 赋值重载CBuffer&amp; operator=(CBuffer &amp;origin)&#123; cout &lt;&lt; "operator=(CBuffer&amp;)" &lt;&lt; endl; if (this == &amp;origin) return *this; m_buffer = origin.m_buffer; origin.m_buffer = nullptr; origin.m_size = 0; return *this;&#125; 看起来没有什么问题，但是编译的时候会报错： error: invalid initialization of non-const reference of type ‘CBuffer&amp;’ from an rvalue of type ‘CBuffer’ b2 = CBuffer::gen_buffer(100); ^note: initializing argument 1 of ‘CBuffer&amp; CBuffer::operator=(CBuffer&amp;)’ CBuffer&amp; operator=(CBuffer &amp;origin) 这个错误是什么意思呢？其实说的就是在调用 CBuffer::gen_buffer(100); 函数时，会产生一个临时对象，这个临时对象在赋值给 b2 是会调用CBuffer&amp; operator=(CBuffer &amp;origin) 函数，但是这个函数的参数是一个左值引用类型，而临时对象是一个右值，无法绑定到左值引用上，所以报错了。 还有拷贝构造函数也是有相同的问题，当写出类似 b2 = CBuffer(CBuffer(1000)) 类型会产生临时对象的语句时，同样会因为左值引用不能绑定到右值上而报错，这时候就要请出右值引用了。 改为右值引用实现对于赋值重载函数，我们使用右值引用将其改写为: 12345678910111213// 赋值重载CBuffer&amp; operator=(CBuffer &amp;&amp;origin)&#123; cout &lt;&lt; "operator=(CBuffer&amp;&amp;)" &lt;&lt; endl; if (this == &amp;origin) return *this; m_buffer = origin.m_buffer; origin.m_buffer = nullptr; origin.m_size = 0; return *this;&#125; 这时可以正常通过编译，并且只是修改了指针的指向，并没有申请和拷贝另外一份内存。 std::move() 函数如果我们将拷贝构造函数的参数也改成右值引用的形式： 123456789// 拷贝构造CBuffer(CBuffer &amp;&amp;origin): m_size(origin.m_size)&#123; cout &lt;&lt; "CBuffer(CBuffer&amp;)" &lt;&lt; endl; m_buffer = origin.m_buffer; origin.m_buffer = nullptr; origin.m_size = 0;&#125; 编译时就会发现编译错误： error: use of deleted function ‘constexpr CBuffer::CBuffer(const CBuffer&amp;)’ CBuffer b2(b1); ^note: ‘constexpr CBuffer::CBuffer(const CBuffer&amp;)’ is implicitly declared as deleted because ‘CBuffer’ declares a move constructor or move assignment operator class CBuffer 其本质问题就是主函数中 CBuffer b2(b1); 这一句引起的，因为变量 b1 是一个左值，但是拷贝构造函数接受的是右值引用，所以类型不匹配导致了编译错误，这时可以使用 std::move() 函数改成这条语句为 CBuffer b2(std::move(b1)); 就可以正常编译运行了，运行结果为： CBuffer(int)CBuffer(CBuffer&amp;)b1.size = 0b2.size = 1024CBuffer(int)operator=(CBuffer&amp;&amp;)~CBuffer()~CBuffer()~CBuffer() 查看运行结果会发现 b1.size = 0，因为 b1 调用了 std::move() 函数，转移了资源的所有权，内部已经被“掏空”了，所以在明确所有权转移之后，不要再直接使用变量 b1 了。 万能引用听到这个名字就感觉很厉害，什么是万能引用，其实就是可以同时接受左值和右值的引用类型，但是这种完能引用只能发生在推导的情况下，下面给出了一个例子： 12345678910111213141516#include &lt;iostream&gt;using namespace std;template&lt;typename T&gt;void func(T&amp;&amp; val)&#123; cout &lt;&lt; val &lt;&lt; endl;&#125;int main()&#123; int year = 2020; func(year); func(2020); return 0;&#125; 这段代码中 T&amp;&amp; val 就是万能引用，因为是在模板中，类型需要推导，如果是在普通函数中 T&amp;&amp; val 这个形式就是右值引用。 左值引用和右值引用判定的函数文中多次提到左值和右值，可能刚学习这块内容的小伙伴会有些懵，其实 C++ 中提供了判定左值引用和右值引用的函数，头文件为 &lt;type_traits&gt;，函数名为 is_reference、 is_rvalue_reference、 is_lvalue_reference，看名字就可以知道他们的用途，看下面的例子就更清楚了。 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;type_traits&gt;using namespace std;int main()&#123; int i = 22; int&amp; j = i; int&amp;&amp; k = 11; cout &lt;&lt; "i is_reference: " &lt;&lt; is_reference&lt;decltype(i)&gt;::value &lt;&lt; endl; cout &lt;&lt; "i is_lvalue_reference: " &lt;&lt; is_lvalue_reference&lt;decltype(i)&gt;::value &lt;&lt; endl; cout &lt;&lt; "i is_rvalue_reference: " &lt;&lt; is_rvalue_reference&lt;decltype(i)&gt;::value &lt;&lt; endl; cout &lt;&lt; "j is_reference: " &lt;&lt; is_reference&lt;decltype(j)&gt;::value &lt;&lt; endl; cout &lt;&lt; "j is_lvalue_reference: " &lt;&lt; is_lvalue_reference&lt;decltype(j)&gt;::value &lt;&lt; endl; cout &lt;&lt; "j is_rvalue_reference: " &lt;&lt; is_rvalue_reference&lt;decltype(j)&gt;::value &lt;&lt; endl; cout &lt;&lt; "k is_reference: " &lt;&lt; is_reference&lt;decltype(k)&gt;::value &lt;&lt; endl; cout &lt;&lt; "k is_lvalue_reference: " &lt;&lt; is_lvalue_reference&lt;decltype(k)&gt;::value &lt;&lt; endl; cout &lt;&lt; "k is_rvalue_reference: " &lt;&lt; is_rvalue_reference&lt;decltype(k)&gt;::value &lt;&lt; endl; return 0;&#125; 运行结果如下，满足返回1，否则返回0： i is_reference: 0i is_lvalue_reference: 0i is_rvalue_reference: 0j is_reference: 1j is_lvalue_reference: 1j is_rvalue_reference: 0k is_reference: 1k is_lvalue_reference: 0k is_rvalue_reference: 1 总结 右值引用的写法为 T&amp;&amp; val，两个地址符要挨在一起，在模板中被称为万能引用 注意左值引用和右值引用的使用区别，其实本质都是为了减少无效的拷贝 std::move() 函数会转移对象的所有权，转移操作之后将左值转为右值引用，原对象不可再直接使用 可以使用 is_reference、 is_rvalue_reference、 is_lvalue_reference 来判断引用类型 陪伴是最长情的告白，等待是最极致的思念五一离家返工了，心里有些不是滋味，为了家出来奋斗却将“家”抛在了身后，珍惜眼前人吧~]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>左值</tag>
        <tag>右值</tag>
        <tag>左值引用</tag>
        <tag>右值引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单聊聊C/C++中的左值和右值]]></title>
    <url>%2Fblog%2F2020%2F04%2F24%2F%E7%AE%80%E5%8D%95%E8%81%8A%E8%81%8AC-C-%E4%B8%AD%E7%9A%84%E5%B7%A6%E5%80%BC%E5%92%8C%E5%8F%B3%E5%80%BC%2F</url>
    <content type="text"><![CDATA[前言为什么标题要写成简单聊聊，而不是写成什么“C++中左值与右值详解”或者现在很流行的“惊了！看了这一篇左值与右值讲解，他吊打了面试官”，其实带有详解这个词是需要勇气的，最起码要融会贯通之后才敢这么说吧，本来是学习右值引用的，结果涉及到了左值和右值，然后去了解他们历史发现也是有些混乱，操作中又经常涉及到运算符优先级，真是越学越乱了。 问题索性也把右值引用放一边，从头来看看这个左值和右值，其实我跟这两个词一点都不熟，最多就是在编译报错的提示框中看到他们，当然有时候也会看到他们的英文名字 lvalue 和 rvalue，这时候一般就是编译器开始抱怨了，说我写了什么它不能理解的东西，其实嘛，我自己都没完全理解，从现在开始边学边总结了，先展示一个常见报错： 1error: lvalue required as increment operand 这是什么意思，这么绕嘴，左值需要作为增长操作数，请说人话：自增操作需要一个可以赋值的变量作为操作数，需要变量就直说嘛，为什么要左值、右值的把人都绕蒙了。 历史渊源这个世界一直是在变化的，可能之前你一直引以为豪的经验大楼，转眼之间就会倾塌。关于左值和右值的历史，普遍的观点是最初来源于 C 语言，后来被引入到了 C++，但是关于左值和右值的含义和实现却在一直改变和完善，对于它的历史讲解发现一篇总结的比较好的文章 《C/C++ 左值和右值, L-value和R-value》。 这是2012年的一篇文章，文中给出了历史说明依据，最后还举了一些例子来说明 C 和 C++ 关于左值实现的不同，但是实际操作后你会发现，时间的车轮早已向前行进了一大截，文中提到的那些不同，在最新的 gcc 和 g++ 编译器上早已变得相同，文中提到的反例现在看来几乎没有意义了。 简单梳理下，左值的定义最早出现在 《The C Programming Language 》一书中，指的是引用一个对象，放在赋值表达式 = 左边的值。 后来在新的 C 语言标准中提到左值是赋值表达式 = 左边的值或者需要被改变的值，而等号的右边的值被称为右值。左值更好的表达为可以定位的值，而右值是一种表达数据的值，基于这个表述 L-value 可以理解为 locator value，代表可寻址，而 R-value 可以理解为 read value，代表可读取。 不过以上的新解，完全是人们为了理解左值、右值赋予的新含义，从历史发展来看，一开始左值和右值完全就是通过等号的左边和右边来命名的，只不过随着标准的完善和语言的发展、更替，虽然两个名字保留了下来，但是它们的含义却在逐步发生改变，与最初诞生时的 = 左右两边的值这个含义相比，已经相差很多了。 认识左值和右值关于左值右值有几条规则和特点，先列举在这里，后面可以跟随例子慢慢体会： 左值和右值都是指的表达式，比如 int a = 1 中的 a 是左值，++a 是左值, func() 也可能是左值，而 a+1 是右值， 110 也是一个右值。 左值可以放在 = 的左边，右值只能放在 = 的右边，这其中隐含的意思就是左值也能放在 = 的右边，但是右值不能放在 = 的左边。 左值可以取地址，代表着内存中某个位置，可以存储数据，右值仅仅是一个值，不能取地址，或者它看起来是一个变量，但它是临时的无法取地址，例如一个函数的非引用的值返回。 以上规则从定义来看一点也不严谨，比如一个常量定义是可以赋值，后面就不行了，它也可以取地址，但是不能赋值的它到底是左值还是右值，这点其实不用纠结，心里知道这个情况就可以了。 再比如一个普通变量，它原本是一个左值，当用它给其他变量赋值的时候，它又化身为一个右值，这时它也可以取地址，好像与上面的说法相违背了，但是仔细想想真的是这样吗？它只是临时化身为右值，其实是一个左值，所以才可以取地址的。 其实你如果不做学术研究、不斤斤计较，那么完全可以把能够赋值的表达式作为左值，然后把左值以外的表达式看成右值，如果你不熟悉解左值和右值可能根本不会影响你平时的工作和学习，但是了解它有助于我们深入理解一些内置运算符和程序执行过程，以及在出现编译错误的时候及时定位问题。 具体的示例最简单的赋值语句1int age = 18; 这个赋值语句很简单，= 作为分界线，左边的 age 是左值，可以被赋值，可以取地址，它其实就是一个表达式，代表一个可以存储整数的内存地址；右边的 18 也是一个表达式，明显只能作为右值，不能取地址。 118 = age; 这个语句在编译时会提示下面的错误： 1error: lvalue required as left operand of assignment 错误提示显示：赋值语句的左边需要一个左值，显然 18 不能作为左值，它不代表任何内存地址，不能被改变。 如果程序中的表达式都这么简单就不需要纠结了，接着我们往下看一些复杂点的例子。 自增自减运算1++age++; 第一眼看到这个表达式，你感觉它会怎样运算，编译一下，你会发现编译失败了，错误如下： error: lvalue required as increment operand 加个括号试试： 1++(age++) 编译之后会出现相同的错误： error: lvalue required as increment operand 再换一种加括号的方式再编译一次： 1(++age)++ 这次成功编译了，并且输出值之后发现 age 变量增加了两次。 先不考虑左值右值的问题，我们可以从这个例子中发现自增运算的优先级，后置自增 age++ 的优先级要高于前置自增 ++age 的优先级。 现在回过头来看看之前的编译错误，为什么我们加括号改变运算顺序之后就可以正常执行了呢？这其实和自增运算的实现有关。 前置自增前置自增的一般实现，是直接修改原对象，在原对象上实现自增，然后将原对象以引用方式返回： 12345UPInt&amp; UPInt::operator++()&#123; *this += 1; // 原对象自增 return *this; // 返回原对象&#125; 这里一直操作的是原对象，返回的也是原对象的引用，所以前置自增表达式的结果是左值，它引用的是原对象之前所占用的内存。 后置自增后置自增的一般实现，是先将原对象的数据存储到临时变量中，接着在原对象上实现自增，然后将临时变量以只读的方式返回： 123456const UPInt UPInt::operator++(int)&#123; UPInt oldValue = *this; // 将原对象赋值给临时变量 ++(*this); // 原对象自增 return oldValue; // 返回临时变量&#125; 这里返回的是临时变量，在函数返回后就被销毁了，无法对其取地址，所以后置自增表达式的结果是右值，不能对其进行赋值。 所以表达式 ++age++; 先进行后置自增，然后再进行前置自增就报出编译错误了，因为不能修改右值，也不能对右值进行自增操作。 自增表达式赋值前面说到前置自增表达式是一个左值，那能不能对其赋值呢？当然可以！试试下面的语句： 1++age = 20; 这条语句是可以正常通过编译的，并且执行之后 age 变量的值为 20。 函数表达式函数可以作为左值吗？带着这个疑问我们看一下这个赋值语句： 1func() = 6; 可能有些同学会有疑问，这是正常的语句吗？其实它是可以正常的，只要 func() 是一个左值就可以，怎么才能让他成为一个左值呢，想想刚才的前置自增运算可能会给你启发，要想让他成为左值，它必须代表一个内存地址，写成下面这样就可以了。 1234567891011int g;int&amp; func()&#123; return g;&#125;int main()&#123; func() = 100;&#125; 函数 func() 返回的是全局变量 g 的引用，变量 g 是一个可取地址的左值，所以 func() 表达式也是一个左值，对其赋值后就改变了全局变量 g 的值。 那么我们注意到这里 func() 函数返回的是全局变量的引用，如果是局部变量会怎么样呢？ 12345678910int&amp; func()&#123; int i = 101; return i;&#125;int main()&#123; func() = 100;&#125; 上面的代码编译没有错误，但是会产生一个警告，提示返回了局部变量的引用: 1warning: reference to local variable ‘i’ returned [-Wreturn-local-addr] 运行之后可就惨了，直接显示段错误： 1Segmentation fault (core dumped) 改为局部变量之后，func() 函数虽然返回了一个值，但是这个值是一个临时值，函数返回之后该值被销毁，对应的内存空间也不属于它了，所以在最后赋值的时候才会出现段错误，就和我们访问非法内存是产生的错误时一样的。 总结 可以被赋值的表达式是左值，左值可以取地址。 右值应该是一个表示值的表达式，不是左值的表达式都可以看成是右值 后置自增操作符的优先级要高于前置自增操作符，它们是按照从右向左结合的 关于左值和右值的知识点还有很多，后续想到了再补充，我也是边学边总结，如果有错误也欢迎小伙伴们及时指出，我会及时改正的 时刻静下来想想当初为什么出发，不要在现实的汪洋中偏离航向]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>左值</tag>
        <tag>右值</tag>
        <tag>lvalue</tag>
        <tag>rvalue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（九）：替换带有等号=的字符串的子串]]></title>
    <url>%2Fblog%2F2020%2F04%2F18%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E6%9B%BF%E6%8D%A2%E5%B8%A6%E6%9C%89%E7%AD%89%E5%8F%B7-%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[前言今天写这篇记录要解决的问题来源于最近一名读者的提问，之前写过一篇名为《.bat批处理（六）：替换字符串中匹配的子串》的总结文章，结果有读者在评论区提问说，如果想要替换的子串中包含等号 =，那么就无法替换了，问有没有什么办法可以解决。遇到这个问题的第一感觉应该挺好处理的吧，如果批处理程序在替换操作中认为等号 = 比较特殊，那就加个转义字符应该就可以了，但事实却证明这种想法有些天真了。 在尝试多次失败之后，我意识到事情远没有想象的那么简单，开始在网上寻找解决方案，结果有些让人意外，绝大多数人都说这是 SET 命令的执行规则决定的，无法实现这种需求。当要替换的子串中包含 = 时，第一个 = 就会被认为是替换语法中的 =，进而导致无法得到正确的结果，即使是使用转义字符都无法完成正确替换，加入的转义字符会影响匹配，导致替换失败。还有一些人建议用其他工具来完整这种需求，比如记事本的替换功能 O(∩_∩)O。 遇到的问题看了上面的叙述，可能有些小伙伴对我所说的问题还没有太直观的认识，接下来我们举个例子来说一下这个问题究竟是怎样产生的。 0x00 带有 = 的字符串首先需要被替换的字符串中要包含等号，我们来定义一个这样的变量： 1set STR=abcdo=ocar12a=ajdjko=ot 变量的名字是 STR，变量的值是 abcdo=ocar12a=ajdjko=ot，其中包含了三个 =。 0x01 带有 = 的想要被替换的子串确定一下我们想要替换的子串 o=o，假如我们想把它替换成字母 A，按照一般的替换规则X:Y=Z，在 X 串中寻找到 Y 串之后把它替换成 Z 串，实现的代码如下： 1234567@echo offset STR=abcdo=ocar12a=ajdjko=otset RESULT=%STR:o=o=A%echo %RESULT%pause &gt; nul 运行之后的结果是： abcdo=A=o=Acar12a=ajdjko=A=o=At 和我们想法不一样，我们本来想把 o=o 替换成 A，但是从结果来看应该是把 o 替换成了 o=A，原因就是我们选择的被替换中的子串 o=o 包含一个 =，而这个 = 被当成了替换语法 X:Y=Z 中的 =，所以就不对了。 0x02 尝试用转义字符来处理很多语言中都有转义字符，比如 Markdown 语法中的反斜杠 \，在 Markdown 语法中被星号 * 包裹的文字是倾斜的，但是如果想正常的输出一个 * 怎么办呢？就需要在 * 前面加一个反斜杠 \，变成 \*，这样 * 原本的倾斜文字的作用就被转义了，变成了一个普通的输出字符。 在批处理中也有转义字符的概念，它就是 ^，我们知道在批处理中 &gt;、| 等符号都是有特殊用处的，所以不能简单的输出，比如 echo &gt; 是无法输出一个大于号的，要写成 echo ^&gt; 才能正常输出一个 &gt; 符号。 我们就利用这个转义字符来告诉替换命令，被替换的子串中的 = 是一个普通字符，不能作为替换规则的一部分，所以被替换的子串写成了 o^=o，我们实现下面的代码，看看能不能达到目的： 1234567@echo offset STR=abcdo=ocar12a=ajdjko=otset RESULT=%STR:o^=o=A%echo %RESULT%pause &gt; nul 运行之后结果如下： abcdo=ocar12a=ajdjko=ot 与替换前对比发现没有任何变化，看来转义字符的想法没能帮助我们解决问题，还是想想其他的办法吧。 稳扎稳打的解决方案既然 = 这么特殊，我们就先想办法干掉等号，直接替换的方式不好使，我们可以一个字符一个字符的判断啊，虽然麻烦一点，但是解决问题才是最重要的。 既然要一个个的字符去判断，就需要遍历原字符串，最简单的可以使用字符串分割啊，语法为 原串:~偏移,长度 就可以了，如果不太清楚可以参考一下 《.bat批处理（三）：变量声明、设置、拼接、截取》，截取第一个字符的语法是 原串:~0,1， 截取第二个字符的语法是 原串:~1,1，以此类推。 具体的思路就是我们先判断第一个字符，如果是 = 就进行替换，如果不是 = 就放到结果字符串里，然后继续判断第二个字符进行操作，最后所有的字符处理一遍就完成了替换。 需要使用 goto 语句来写一个循环，代码逻辑比较简单，就是遍历所有字符，是 = 就替换，不是 = 就保留，假设我们先把 = 替换成 #，实现的代码如下： 123456789101112131415161718@echo offset STR=abcdo=ocar12a=ajdjko=otset CURSTR=%STR%set RESULT=:nextif "%CURSTR%" equ "" goto endset a=%CURSTR:~0,1%if "%a%" equ "=" (set RESULT=%RESULT%#) else (set RESULT=%RESULT%%a%)set CURSTR=%CURSTR:~1%goto next:endecho source string is %STR%echo result string is %RESULT%pause &gt; nul :next 是循环的入口，每次截取第一个字符，判断是 = 就在结果中拼接 # 字符，相当于完成了替换，如果字符不是 = ，就将字符直接拼接到结果中，操作之后将原串的第一个字符删除形成新的原串，然后再判断第一个字符，以此类推，直到原串为空，运行结果如下： source string is abcdo=ocar12a=ajdjko=otresult string is abcdo#ocar12a#ajdjko#ot 最终方案事情到了这里好像还没完，在实际操作中有些情况不是替换一个 =，往往是替换的内容中包含 =，上面将 = 替换成 # 不具有通用型，如果是一开始的请求，将 o=o替换成 A 就不能这样写了，就应该是每次判断3个字符了，写起来有些麻烦，批处理中没有获得字符串长度的函数，需要自己实现一个，如果是100个字符的被替换串，那代码就很难写了。 既然 = 都能被我们替换掉，肯定有办法实现上面我们这种将 o=o替换成 A 的要求，下面我们就列举一种通用的处理方法。 0x00 首先将 = 替换成一个原串中不可能出现的字符或者序列这步替换可能最后需要还原的，所以要求我们替换成的目标序列不能在原串中出现，比如我们上面把 = 替换成了 #， 如果原串中有 # 就会弄混了，不能确定是原来字符串中就存在的 #，还是由 = 变成的 #。 这个序列我们可以定义的变态一点，比如把 = 替换成 ###i#am#happy###，我们把它记作 α。 0x01 用这个不能出现序列替换我们之前要查找替换子串中的 =我们之前要查找替换的子串是 o=o，那么替换之后形成 o###i#am#happy###o，我们把它记作 β。 0x02 将第1步结束获得的替换结果作为原串，将其中的 β 替换成 A其实就是把第1步替换完结果作为原串，把其中的 o###i#am#happy###o 也就是原来的 o=o 替换成 A。 0x03 将第3步结果的子串作为原串，将其中的 α 替换为 =这一步就是处理那些虽然是 =，但是这个 = 不是我要替换的结果子串中的，所以要还原 代码实现步骤梳理清楚了，下面来写代码，按照步骤一步步写就可以了： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@echo offrem 第一步set CORESTR=###i#am#happy###set STR=abcdo=ocar12a=ajdjko=otset CURSTR=%STR%set RESULT1=:next1if "%CURSTR%" equ "" goto end1set a=%CURSTR:~0,1%if "%a%" equ "=" (set RESULT1=%RESULT1%%CORESTR%) else (set RESULT1=%RESULT1%%a%)set CURSTR=%CURSTR:~1%goto next1:end1echo source1 string is %STR%echo result1 string is %RESULT1%pause &gt; nulrem 第 2 步set CORESTR=###i#am#happy###set STR=o=oset CURSTR=%STR%set RESULT2=:next2if "%CURSTR%" equ "" goto end2set a=%CURSTR:~0,1%if "%a%" equ "=" (set RESULT2=%RESULT2%%CORESTR%) else (set RESULT2=%RESULT2%%a%)set CURSTR=%CURSTR:~1%goto next2:end2echo source2 string is %STR%echo result2 string is %RESULT2%pause &gt; nulrem 第3步，需要开启延迟变量setlocal ENABLEDELAYEDEXPANSIONset RESULT3=!RESULT1:%RESULT2%=A!echo result3 string is %RESULT3%pause &gt; nulrem 第4步set RESULT4=!RESULT3:%CORESTR%==!echo finally result is %RESULT4% 运行之后的结果为： source1 string is abcdo=ocar12a=ajdjko=otresult1 string is abcdo###i#am#happy###ocar12a###i#am#happy###ajdjko###i#am#happy###otsource2 string is o=oresult2 string is o###i#am#happy###oresult3 string is abcdAcar12a###i#am#happy###ajdjkAtfinally result is abcdAcar12a=ajdjkAt 这次终于替换成功了，o=o 被成功替换成了字母 A，代码中用到了延迟变量，主要是为了实现被替换字符串是变量的情况，不清楚延迟变量的用法可以简单查询一下，至此文章开头提出的问题我们就成功解决了，虽然路途有些坎坷。 总结 批处理程序中的 = 比较特殊，使用常规的 X:Y=Z 的语法不能替换包含 = 的子串 遇到上述情况可以将字符串切割，采用逐个字符比较的方式，将 = 替换成其他字符再进行后续操作 有时候也不必非得使用批处理来替换包含 = 的字符串，随便一个文本工具，比如记事本都可以文本进行替换 如果非得用命令解决，也可以使用从 linux 的 sed 命令移植到 windows 的 sed.exe 程序来很方便的进行替换 使用 sed 命令的语法是 echo abcdo=ocar12a=ajdjko=ot | sed -e &quot;s/o=o/A/g&quot;，一步就可以完成了文章开头的需求了 如果你暂时没有 sed.exe 程序，可以点击这个链接 sed.exe程序 下载，若不是在同一目录使用，记得将命令目录添加到环境变量中 时间慢慢地磨去了年少轻狂，也渐渐地沉淀了冷暖自知。]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++11中的时间库std::chrono（引发关于时间的思考）]]></title>
    <url>%2Fblog%2F2020%2F04%2F08%2FC-11%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4%E5%BA%93std-chrono%2F</url>
    <content type="text"><![CDATA[时间都去哪了？还没好好感受年轻就… 前言时间是宝贵的，我们无时无刻不在和时间打交道，这个任务明天下班前截止，你点的外卖还有5分钟才能送到，那个程序已经运行了整整48个小时，既然时间和我们联系这么紧密，我们总要定义一些术语来描述它，像前面说到的明天下班前、5分钟、48个小时都是对时间的描述，程序代码构建的程序世界也需要定义一些术语来描述时间。 今天要总结学习的是 std::chrono 库，它是 C++11 标准时从 boost 库中引入的，其实在 C++ 中还有一种 C 语言风格的时间管理体系，像我们常见的函数 time()、clock()、localtime()、mktime() 和常见的类型 tm、time_t、clock_t 都是 C 语言风格的时间管理体系。 std::chrono 这个库之前接触的不多，C++20 标准都出了，C++11 引入的这个库还没怎么用过，整天和 time()、 localtime()、 tm 打交道，最近工作中换了项目，代码中出现了 std::chrono 的使用，是时候好好学习总结一下了。 chrono 的概况 头文件 #include &lt;chrono&gt; 命名空间 std::chrono 这个库从 C++11 引入标准之后，每个版本都有所修改，不过核心内容变化不是太大，他定义了三种主要类型，分别是 durations、clocks 和 time points，以及围绕这些类型的一些工具函数和衍生的定义。 chrono 的核心内容duration这个模板类用来表示时间间隔，我们知道时间的基本单位是秒，这个类的对象所表示的时间间隔也是以秒为单位的，它的定义如下： 12template&lt;class Rep, class Period = std::ratio&lt;1&gt;&gt;class duration; Rep 表示一种数值类型，用来描述周期 Period 的数值类型，比如可以是 int、float 等，而 Period 的类型是 std::ratio，同样是一个模板类，实际表示的是一个有理数，像100、0、1/1000（千分之一）等等。 在 std 这个命名空间下有很多已经定义好的有理数，可以举几个常见的头文件 &lt;ratio&gt; 中的例子: 12345678nano std::ratio&lt;1, 1000000000&gt; // 十亿分之一micro std::ratio&lt;1, 1000000&gt; // 百万分之一milli std::ratio&lt;1, 1000&gt; // 千分之一centi std::ratio&lt;1, 100&gt; // 百分之一deci std::ratio&lt;1, 10&gt; // 十分之一deca std::ratio&lt;10, 1&gt; // 十hecto std::ratio&lt;100, 1&gt; // 百kilo std::ratio&lt;1000, 1&gt; // 千 比如我们想定义一个整数类型的100秒的时间间隔类型可以使用： 1typedef std::chrono::duration&lt;int, std::ratio&lt;100,1&gt;&gt; my_duration_type; 当然也可以简写成： 1typedef std::chrono::duration&lt;int, std::hecto&gt; my_duration_type; 如果我们想定义一个整数类型1分钟的时间间隔类型可以写成： 1typedef std::chrono::duration&lt;int, std::ratio&lt;60,1&gt;&gt; my_minute_type; 因为这种时、分、秒的时间表示在代码逻辑中很常用，所有在 std::chrono 命名空间下已经定义好了一些时间间隔类型: 123456std::chrono::nanoseconds duration&lt;/*signed integer type of at least 64 bits*/, std::nano&gt;std::chrono::microseconds duration&lt;/*signed integer type of at least 55 bits*/, std::micro&gt;std::chrono::milliseconds duration&lt;/*signed integer type of at least 45 bits*/, std::milli&gt;std::chrono::seconds duration&lt;/*signed integer type of at least 35 bits*/&gt;std::chrono::minutes duration&lt;/*signed integer type of at least 29 bits*/, std::ratio&lt;60&gt;&gt;std::chrono::hours duration&lt;/*signed integer type of at least 23 bits*/, std::ratio&lt;3600&gt;&gt; 另外还有一个很重要的成员函数 count()，用来获得指定的时间间隔对象中包含多少个时间周期，接下来可以写个例子理解一下，我们用 duration 这个模板类来表示一下5分钟和12小时，看看他应该怎么使用，对于5分钟你可以看成是 5 个 1 分钟或者 1 个 5 分钟，或者更变态你可以看成 2.5 个 2 分钟，而 12 小时一般会看成是 12个 1 小时，你当成 0.5 个 1 天也是可以的： 123456789101112131415161718192021222324#include &lt;chrono&gt;#include &lt;iostream&gt;int main()&#123; // 以下为5分钟表达 std::chrono::minutes minute1&#123;5&#125;; // 5个1分钟 std::chrono::duration&lt;int, std::ratio&lt;5*60, 1&gt;&gt; minute2&#123;1&#125;; // 1个5分钟 std::chrono::duration&lt;double, std::ratio&lt;2*60, 1&gt;&gt; minute3&#123;2.5&#125;; // 2.5个2分钟 std::cout &lt;&lt; "minutes1 duration has " &lt;&lt; minute1.count() &lt;&lt; " ticks\n" &lt;&lt; "minutes2 duration has " &lt;&lt; minute2.count() &lt;&lt; " ticks\n" &lt;&lt; "minutes3 duration has " &lt;&lt; minute3.count() &lt;&lt; " ticks\n"; // 一下为12小时表达 std::chrono::hours hours1&#123;12&#125;; // 12个1小时 std::chrono::duration&lt;double, std::ratio&lt;60*60*24, 1&gt;&gt; hours2&#123;0.5&#125;; // 0.5个1天 std::cout &lt;&lt; "hours1 duration has " &lt;&lt; hours1.count() &lt;&lt; " ticks\n" &lt;&lt; "hours2 duration has " &lt;&lt; hours2.count() &lt;&lt; " ticks\n"; // 使用 std::chrono::duration_cast&lt;T&gt; 将分钟间隔转化成标准秒间隔 std::cout &lt;&lt; "minutes1 duration has " &lt;&lt; std::chrono::duration_cast&lt;std::chrono::seconds&gt;(minute1).count() &lt;&lt; " seconds\n";&#125; 上述代码中还使用了 std::chrono::duration_cast&lt;T&gt;() 函数，用于各种时间间隔的换算，运行结果如下： 123456minutes1 duration has 5 ticksminutes2 duration has 1 ticksminutes3 duration has 2.5 tickshours1 duration has 12 tickshours2 duration has 0.5 ticksminutes1 duration has 300 seconds clock从名字可以看出这个类叫做时钟，时钟是用来看时间和计时的，常用的两个类是 system_clock 和 steady_clock，在 C++20 标准中又加入了多种内容，现在我们先来看看这两个常用类。 从这一部分开始类的定义让人有些迷糊，其实 clock 引用了 std::chrono::duration 和后面要说的 std::chrono::time_point， 而 std::chrono::time_point 又引用了 std::chrono::duration 和现在要讲的 std::chrono::system_clock、 std::chrono::steady_clock，如果只看定义很容易被绕晕，所以还是先做个练习实验一下。 system_clock这个类被称为系统内时钟，当修改系统时钟时可能会改变其单调递增的性质，静态成员函数有 now()、to_time_t()、from_time_t() 三个，关于它的单调性被修改举个例子，一般认为时间一直是递增的，但是当你现在调用一次函数 now()，然后把时间往过去调1天，然后再调用 now() 函数，就会发现新得到的时间“变小”了。 也因为这样它会受到 NTP（Network Time Protocol，网络时间协议）的影响，但是不会受时区和夏令时的影响（其实很多国家早就废除夏令时了）。 下面写个例子练习一下，例子中使用了 now()、to_time_t()、from_time_t() 三个函数，不清楚的时候可以对照一下： 12345678910111213141516171819202122232425262728293031#include &lt;chrono&gt;#include &lt;iostream&gt;int main()&#123; std::chrono::duration&lt;int, std::ratio&lt;60*60*24&gt; &gt; one_day(1); // 根据时钟得到现在时间 std::chrono::system_clock::time_point today = std::chrono::system_clock::now(); std::time_t time_t_today = std::chrono::system_clock::to_time_t(today); std::cout &lt;&lt; "now time stamp is " &lt;&lt; time_t_today &lt;&lt; std::endl; std::cout &lt;&lt; "now time is " &lt;&lt; ctime(&amp;time_t_today) &lt;&lt; std::endl; // 看看明天的时间 std::chrono::system_clock::time_point tomorrow = today + one_day; std::time_t time_t_tomorrow = std::chrono::system_clock::to_time_t(tomorrow); std::cout &lt;&lt; "tomorrow time stamp is " &lt;&lt; time_t_tomorrow &lt;&lt; std::endl; std::cout &lt;&lt; "tomorrow time is " &lt;&lt; ctime(&amp;time_t_tomorrow) &lt;&lt; std::endl; // 计算下个小时时间 std::chrono::system_clock::time_point next_hour = today + std::chrono::hours(1); std::time_t time_t_next_hour = std::chrono::system_clock::to_time_t(next_hour); std::chrono::system_clock::time_point next_hour2 = std::chrono::system_clock::from_time_t(time_t_next_hour); std::time_t time_t_next_hour2 = std::chrono::system_clock::to_time_t(next_hour2); std::cout &lt;&lt; "tomorrow time stamp is " &lt;&lt; time_t_next_hour2 &lt;&lt; std::endl; std::cout &lt;&lt; "tomorrow time is " &lt;&lt; ctime(&amp;time_t_next_hour2) &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 12345678now time stamp is 1586662332now time is Sun Apr 12 11:32:12 2020tomorrow time stamp is 1586748732tomorrow time is Mon Apr 13 11:32:12 2020tomorrow time stamp is 1586665932tomorrow time is Sun Apr 12 12:32:12 2020 steady_clock这是一个单调时钟，一旦启动之后就与系统时间没有关系了，完全根据物理是时间向前移动，成员函数只有一个 now()，通常可以用来计时，使用方法与 system_clock 相比简单许多，下面写个小例子。 123456789101112131415161718#include &lt;chrono&gt;#include &lt;iostream&gt;int main()&#123; // 先记录程序运行时间 std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); volatile int nDstVal, nSrcVal; for (int i = 0; i &lt; 1000000000; ++i) nDstVal = nSrcVal; // 做差值计算耗时 std::chrono::duration&lt;double&gt; duration_cost = std::chrono::duration_cast&lt; std::chrono::duration&lt;double&gt; &gt;(std::chrono::steady_clock::now() - start); std::cout &lt;&lt; "total cost " &lt;&lt; duration_cost.count() &lt;&lt; " seconds." &lt;&lt; std::endl; return 0;&#125; 运行结果如下：1total cost 1.9424 seconds. time point这个类与 duration 类似，同样是模板类，表示具体的时间点，比如今天 18:00 开饭，明天上午 10:00 发版本，今年 5 月 1 日可能因为疫情不让出去玩了，像这些具体的时间点可以使用 std::chrono::time_point 来表达，它的定义如下： 12template&lt;class Clock, class Duration = typename Clock::duration&gt;class time_point; 首先这个类是在 std::chrono 这个命名空间下，但是你会经常看到以下这种写法： 12std::chrono::system_clock::time_point today = std::chrono::system_clock::now();std::chrono::steady_clock::time_point start = std::chrono::steady_clock::now(); 好像 time_point 又在 std::chrono::system_clock 和 std::chrono::steady_clock 范围内，实际上这两个范围内的 time_point 引用的是 std::chrono::time point，看看 std::chrono::system_clock 的定义能明白一些。 123456789101112class system_clock &#123;public: using rep = /*see description*/ ; using period = ratio&lt;/*unspecified*/, /*unspecified*/ &gt;; using duration = chrono::duration&lt;rep, period&gt;; using time_point = chrono::time_point&lt;system_clock&gt;; static constexpr bool is_steady = /*unspecified*/ ; static time_point now() noexcept; // Map to C API static time_t to_time_t (const time_point&amp; t) noexcept; static time_point from_time_t(time_t t) noexcept;&#125;; 对照上面的定义可以知道，std::chrono::system_clock::time_point 实际上 std::chrono::time_point&lt;system_clock&gt;，这几个时间类的定义相互引用，看到这一部分的时候一定不要烦躁，一步步推导分析其中的关系。 time_point 这个类有一个成员函数 time_since_epoch() 用来获得 1970-01-01 00:00:00 到 time_point 时间经过的 duration, 返回的 duration 的单位取决于 timepoint 定义时的 duraion 的单位，不过你也可以得到 duration 之后使用 std::chrono::duration_cast&lt;T&gt;() 函数来转化。 12345678910111213141516171819202122232425262728293031#include &lt;chrono&gt;#include &lt;iostream&gt;int main()&#123; // 获得epoch 和 now 的时间点 std::chrono::time_point&lt;std::chrono::system_clock&gt; epoch = std::chrono::time_point&lt;std::chrono::system_clock&gt;&#123;&#125;; std::chrono::time_point&lt;std::chrono::system_clock&gt; now = std::chrono::system_clock::now(); // 显示时间点对应的日期和时间 time_t epoch_time = std::chrono::system_clock::to_time_t(epoch); std::cout &lt;&lt; "epoch: " &lt;&lt; std::ctime(&amp;epoch_time); time_t today_time = std::chrono::system_clock::to_time_t(now); std::cout &lt;&lt; "today: " &lt;&lt; std::ctime(&amp;today_time); // 显示duration的值 std::cout &lt;&lt; "seconds since epoch: " &lt;&lt; std::chrono::duration_cast&lt;std::chrono::seconds&gt;(epoch.time_since_epoch()).count() &lt;&lt; std::endl; std::cout &lt;&lt; "today, ticks since epoch: " &lt;&lt; now.time_since_epoch().count() &lt;&lt; std::endl; std::cout &lt;&lt; "today, hours since epoch: " &lt;&lt; std::chrono::duration_cast&lt;std::chrono::hours&gt;(now.time_since_epoch()).count() &lt;&lt; std::endl; return 0;&#125; 运行结果如下： 12345epoch: Thu Jan 1 08:00:00 1970today: Sun Apr 12 12:30:04 2020seconds since epoch: 0today, ticks since epoch: 1586665804624992500today, hours since epoch: 440740 从运行结果来看，epoch 的时间点是 Thu Jan 1 08:00:00 1970，为什么不是 1970-01-01 00:00:00 呢？那是因为我们在东8区，格林威治时间为1970-01-01 00:00:00 的时候，我们的时间就是 Thu Jan 1 08:00:00 1970，这样看来 std::ctime() 这个函数考虑了时区的影响，相同的代码如果在韩国同时运行得到的可能就是 epoch: Thu Jan 1 09:00:00 1970。 关于时间的思考思考一个问题，时间是不是一种不变的量，或者换一种说法，它是不是一种均匀的量。如果了解过《三体》中的部分章节，你就会发现时间总在被任意改变着。但是在现实生活中好像时间就是一个标准，我们认为它是一成不变的，总是感觉今天的1天和昨天的24小时在时间上是等同的，今年的这一年和去年的365天是等同的，但其实你了解一下闰年、闰秒、夏令时就会发现，前面提到的这些未必等同。 日常生活中对时间的描述只是为了理解和阐明一些事物，我们把太阳升到头顶叫做中午，把地球自转一圈叫做一天24小时，把地球围绕太阳公转一圈叫做1年365天，但是地球自转不是那么均匀的，也就是说每转一圈占用的绝对时间是不一样的，我们现在使用的时钟通常是滴答滴答一秒秒的走着，如果地球自转一圈的时间不是完全相同的，那么建立在这个滴答上的一切时间都是不准确的。 什么是建立在滴答滴答上的时间，我们以滴答一次作为1秒来计算，那么1分钟是60秒，也就是滴答60次，1小时是60分钟，滴答3600次，一天是24小时，滴答86400次，滴答的次数是均匀的，但是自转和公转是不均匀的，那么两个时间就对不上了，所以出现了闰秒、闰年等方法来调整时间，使得我们用来描述生活的时间和周围的环境现象可以一致，不然大约几千年以后就会出现中午12点天上出现月亮的奇观，那时的人们在史书中会发现我们这个时代中午12点挂在天上的是太阳，简直太玄幻。 有没有一种计时可以描述这种不均匀的自转呢？其实我们伟大的古人早已经发明出来了，你一定听说过日晷这种计时工具，它是观测日影记时的仪器，主要是根据日影的在日晷面上的位置，以指定当时的时辰或刻数，是我国古代较为普遍使用的计时仪器。为什么它没有时间不一致的问题？因为它本身就是不均匀的，它是根据自然现象来规定生活中每天的时间的，其实对照现在来说就是每个时辰的滴答数实际上是不一样的。 日晷这种不均匀的计时其实是为了适应天文现象，方便人们的生产生活，所以说现在地球自转一圈是一天，但不一定是86400秒，地球公转一圈是一年，但不一定是365天，后来人们使用电子设备计时，按道理来说应该非常准确，但是因为地球自转、公转的速率都不稳定，这种差距渐渐地会给生活带来困扰，于是又发明了一个折中的协调世界时，会在适当的时候闰秒、闰天，以弥补这种差距。假如你买了一个绝对精准的不联网的电子计时器，但是几年之后你就会发现你的计时器肯定和大家使用的标准时间不一致了。 其实还有一种基于特定铯原子的振荡周期来确定的国际原子时，主要是在时间精度要求较高的航天、通讯、电子等领域，为了保持系统的连续性而使用的，在日常生活中基本不会使用，但是这个时间是相对恒定的，不会去计较天文现象，每一秒都“准确”的流逝着。 时间函数思考现在回过头来再来看这些时间函数，是不是感觉有点不一样了，比如 time(NULL) 这个函数，它返回的是从 1970-01-01 00:00:00 到现在时间的秒数，回忆一下上面关于时间的思考，这个秒数真的是准确的吗？其实你如果理解了上面的内容就能得出结论，它肯定和国际原子时是有出入的。 再考虑下闰秒的影响，假如你实现了一个函数，第一次执行是在0点执行，执行之后你设置了一个86400秒的倒计时，也就是1天的时间，到第二天0点的时候正好又执行，你又设置了一个86400秒的倒计时，但今天正好是闰秒的日子，也就是今天会比昨天多1秒，那么今天的时间到23:59:59的时候就经过了86400秒，也就是说在23:59:59的时候就会执行你写的函数，如果碰到秒杀就尴尬了… 一般的程序开发不用太考虑闰秒的影响，但是如果这一秒的误差出现的宇宙飞船的飞行中，可能会导致几十公里的误差，所以程序员们一定要理解闰秒的可能带来的问题，评估自己所写的代码需不需要处理这种情况。曾经的一次闰秒直接导致了芬兰航空系统的瘫痪，所以一些大型项目还是会提前很长时间就把即将到来的闰秒处理写入到自己的系统中，以应对它带来的危险。 当你认为时间不会倒流的时候，它确实就发生了。我们一般假定时间不会倒流，但是如果你过分依赖这个特性，可能就会导致一些问题，这种情况常常出现设定了自动校准时间的电脑上，电脑的时间走快了，然后到达一定的差距后会触发校准程序，这时就会出现“时间倒流”的现象，比如 time(NULL) 这种依赖于电脑时间的函数，在这种情况下函数返回值就会变小，出现不单调性。 总结 关于时间的操作真的太多了，我居然发现一种名为 operator&quot;&quot;h 的操作符，与数字连用表示小时，有兴趣的话可以自己扩展学习一下。 durations、clocks 和 time points 三种有关时间操作的定义相互之间是有引用的，需要理清其中的关系。 需要了解闰秒、闰年、天文时、原子时、协调时产生的原因，这样就可以做到熟悉原理，心里不慌。 在测试的例子中出现了时区的概念，其实是人们为了生产生活主动创造出来以适应自然现象的。 这里抛出一个疑问，我之前刚接触时晕乎了很久，后来渐渐才明白，有些时间函数的说明中会提到与时区无关，比如 time(NULL)、还有今天学习的 system_clock，但是当我修改电脑时区的时候会发现，这些函数的返回值会发生突变，大家有探究过其中的原因吗？ 我们都是追逐时间奔跑的蝼蚁，改变世界的同时也被时间改变着。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>C++11</tag>
        <tag>chrono</tag>
        <tag>时间</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10通过带命令行的安全模式清除顽固的广告弹窗文件]]></title>
    <url>%2Fblog%2F2020%2F04%2F02%2FWin10%E9%80%9A%E8%BF%87%E5%B8%A6%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E6%B8%85%E9%99%A4%E9%A1%BD%E5%9B%BA%E7%9A%84%E5%B9%BF%E5%91%8A%E5%BC%B9%E7%AA%97%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言最近电脑开机后偶尔会出现一个弹窗，这种广告弹窗见的多了也就麻木了，本来也没放在心上，随手一关就准备去做其他事情了，但是点击关闭按钮后这个广告弹窗居然还弹出了二次确认框，想想也忍了，毕竟广告商做半天就是为了让你多看几眼，当我用鼠标的光标接近这个确认按钮时，确认框消失了，整个广告页面居然还在！ 一开始我还以为自己手滑点错了，后来试了3、4次之后发现，这个二次确认框从出来到消失不到1秒钟，以我的手速试了这么多次就没点到，这次暴脾气上来了，再也不忍了，我必须找到你是哪个软件的广告页，然后把你干掉！ 战斗经历本以为找到软件直接卸载就完事了，没想到碰上硬茬了，这个软件大有来头，真不是随随便便能搞定的。 查找广告来源这一步比较简单，这个打开的广告页在任务栏上有个图标，鼠标光标放到图标上会显示缩略图，就像下面这样： 在图标上单击右键，然后在弹出菜单中将光标移到最上面的选项继续单击右键，这时会在弹出一个菜单，如果一次不行就多试几次： 这时点击菜单上的属性按钮会弹出这个广告页对应程序的属性页面： 开始删除程序让我找到你了吧，目录是 D:\Program Files (x86)\MyDrivers\DriverGenius\ksoft，看来是驱动精灵软件带来的广告页，直接进入目录删除： 尴尬！提示“你需要提供管理员权限才能删除此文件”，点击“继续”按钮试试： 依旧不行，提示“你需要权限执行此操作”、“你需要Administrators提供的权限才能对此文件进行更改”，真是有点诡异，一个普通软件居然还要管理员权限才能删。 找管理员帮忙没办法了，提权吧！我把沉睡的管理员账号的都开启了，再试一次还是不行，我可是 Administrator 啊，在这个电脑中还有什么是我不能干的吗？ 微软：“你能干什么你说了不算，我说了算！”。 右键单击软件查看属性是不是只读了呢？没有啊！这时我还没意识到它究竟有多难缠，以为简简单单设置几个属性就能把它删掉，于是一拍脑袋决定，这种情况下一般需要修改权限啊，然后在属性面板中点击了“安全”选项卡： 然后点击“高级按钮”，弹出了很多教程中都给出的界面，长成下面这个样子： 这时要点击“更改”开始修改权限了，接着神奇的一幕发生了，当前界面一闪没有弹出修改界面，而原来的“更改”两个字也变成灰色不能再使用了。 有点慌了啊，试试命令行吧，一个 del 强制删除试试，丝毫未动，删除请求被拒绝了： 再试试别的文件，删除一个失败，再试一个又失败，最后发现这个文件夹中，我连个日志文件都删不掉，不仅发出了灵魂拷问，我真的是管理员吗？我的 Administrator 不会是假的吧？打开文件夹左看看、右看看没有发现什么可疑的地方，忽然我发现文件夹外面一层有个齿轮，难道被当成系统配置了，这是什么骚操作？ 看来我的电脑已经被这个软件给控制住了，一个做驱动的，在操作系统启动时早早的被唤醒，牢牢的控制住了局面，设置了一道道钩子，将可能影响到的它生存的途径全部堵死，这可能就是我修改权限时，界面闪了一下就再也修改不了的原因吧。 之前还碰到过一个软件比较坑，也是不让删除，在任务管理器强行关闭时会提示拒绝访问，最后发现一个名称比较相似的服务一直在启动着，然后尝试关闭这个服务，结果有趣的事情发生了，只要我点禁用服务一刷新，它又会重启，不知道还有哪个匿名的进程在默默的帮助它。 进入安全模式没办法了，本来想快速解决战斗，改改权限后直接删除了，哪里想到它这么顽强，既然是涉及到了驱动，谁知道你在系统启动时搞了什么鬼，我就在另一个世界把你搞掉吧，从安全模式启动，让你原来的小算盘只能在硬盘里乖乖的躺着了，说到这怎么有一种从四维空间看三维世界的感觉。 关于怎么进入安全模式的命令行，之前在XP和Win7上好像是开机就可以选，在Win10上开机没有看到，之前也没操作过，不过网络上有大量的教程，我发现其中有两个比较有意思的，一个是要我用U盘做系统盘，然后假装给电脑做系统，在配置界面打开命令行进行设置然后重启，这有点太麻烦了吧。 还有一种更好玩就是让你在电脑启动的时候直接按电源键关机，反复尝试2-3次等电脑感觉到自己异常了，就能看到安全模式的选项了，这就好比让你借梯子你借不到，就在家里放了把火，结果借来了消防队的云梯，有可能损失惨重啊。 这里说一个我感觉最简单的方法吧，使用 Win+R 组合键，调出 windows 运行窗口，然后输入 shutdown /r /o，回车等着电脑重启就可以了。 接着电脑会出现下面这个画面，选择其中的“疑难解答”选项： 然后界面变化进入下面展示的“高级选项”界面，选择其中的“启动设置”选项： 最后在启动设置的详细界面上选择“重启”按钮，短暂运行之后，电脑上开始出现下面的选项： 这时就可以选择进入系统的模式了，使用 F1~F9 来进行选择，F4 就是进入安全模式，不加载多余的驱动，F6 是带命令提示符的安全模式： 我们可以按键盘上的 F6 选择带命令提示符的安全模式，然后界面上就出现了下面这个“黑框框”： 彻底删除文件有了黑框框就可以删除文件，先通过 cd 命令进入待删除文件所在目录，然后使用 del 命令删除文件： 12345C:\Windows\System32&gt;d:D:\&gt;cd "Program Files (x86)\MyDrivers\DriverGenius\ksoft"D:\Program Files (x86)\MyDrivers\DriverGenius\ksoft&gt;del /f znb.exe 没有任何报错，世界都安静了，输入 shutdown /r /t 0 重启电脑，正常进入操作系统，这时就会发现刚刚统治了我的电脑的可执行程序，已经被我干掉不存在了。 总结 这些顽固广告真是厉害，一般删除文件的办法还真删不掉它们。 不光普通方法删不掉，连一些“流氓卫士”的文件粉碎功能都拿它们没办法。 不过电脑毕竟在用户手中，总有一些非常规办法可以干掉这些不正常的文件。 修理电脑时没有什么是重启电脑不能解决的，如果真的有，那就请你重装系统。 别放弃，坚持朝着目标一步一步的走就好了~]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>del</tag>
        <tag>Win10</tag>
        <tag>exe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git stash帮你在切换分支前暂存不想提交的修改]]></title>
    <url>%2Fblog%2F2020%2F03%2F25%2Fgit-stash%E5%B8%AE%E4%BD%A0%E5%9C%A8%E5%88%87%E6%8D%A2%E5%88%86%E6%94%AF%E5%89%8D%E6%9A%82%E5%AD%98%E4%B8%8D%E6%83%B3%E6%8F%90%E4%BA%A4%E7%9A%84%E4%BF%AE%E6%94%B9%2F</url>
    <content type="text"><![CDATA[前言偶然间发现这个命令，正好解决了最近遇到的问题，使用 Git 管理代码时有这样一种场景，你正在分支 branch2 上开发新功能，突然刚刚提交测试的 branch1 分支上报了严重的BUG，需要尽快修改，这时候就需要切换到 branch1 分支上去修复BUG，但是你刚刚在分支 branch2 修改的文件还没有提交，接下来该怎么办？ 如果本地的修改正好到达一个比较完整的阶段，可以直接提交，然后切换分支改BUG，那再好不过了。可是发生这种情况的时候往往是函数写了一半，或者功能大致写完但是还没来得及测试，这样的代码你敢提交吗？我感觉最好还是不要提交吧，那么如果这时候切换分支会有什么后果呢？一般会遇到两种情况：第一种是你在 branch2 分支上所做的修改与 branch1 上做过的修改不冲突，这时切换分支会将本地修改带到 branch1 分支，如果冲突了就是第二种情况，git checkout branch1 命令会被拒绝，当然你可以添加 -f 参数强行切换分支是能成功切换的，代价就是你会丢掉本地的所有修改。 git stash上面提到的切换分支时遇到的两种情况一般都不是我们想要的，之前说过“在 Git 中没有真正的方法来做任何事情，这就是它的妙处！”，但是关于切换分支有这样一个建议，那就是在切换分支时尽量保证你的工作区和暂存区是干净的，而 git stash 命令就是用来做这件事的。 当我们遇到这种状况，本地的修改我不能提交，不想带到新切换的分支，更不想直接丢掉，只想把他们暂存到一个地方，等我切换完分支修改好BUG，再切换回来迎接他们。使用 SVN 想保存本地修改可以使用 patch，而使用 Git 想要解决这种情况更加方便，那就是利用 git stash 命令。 使用方法这个命令的使用方法非常简单，最常用的 git stash push 和 git stash pop 就能应付大部分情况了，其中 push 这个单词还可以省略，使用起来可以说是相当方便了，接下来尝试一下具体用法。 本地有修改时切换分支的两种情况之前提到过这两种情况，一种是将当前分支修改带到要切换的分支，另一种是切换会导致冲突，本次切换操作被拒绝，下面具体操作一下。 将当前分支修改带到要切换的分支首先以 dev 分支为基础新建 feature 分支 12345678910albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ git checkout -b featureSwitched to a new branch 'feature' 在 feature 分支上修改文件，再切换回 dev 分支，可以正常切换，git status 查看状态，发现修改的文件被带到了 dev 分支上 123456789101112131415161718192021222324252627282930albert@homepc MINGW64 /d/gitstart (feature)$ echo "test checkout"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (feature)$ git checkout devSwitched to branch 'dev'M README.mdYour branch is up to date with 'origin/dev'.albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a") 这里要注意一点，在切换到 dev 分支的时候，有一行 M README.md的内容，表示这个文件在切换过来的时候就是修改的。 如果你想要的效果就是这样，就可以直接提交了，比如修改了代码发现分支弄错了，可以这样带着修改的内容切换分支，假设就是这种情况，我们直接在 dev 分支提交修改。 123456789101112131415albert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git commit -m"add comments"[dev 5f4181e] add comments 1 file changed, 1 insertion(+)gitalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree clean 切换分支操作被拒绝上面一种情况，在 feature 分支的修改被带到 dev 分支提交，我们在此基础上切换回 feature 分支看一下： 12345678albert@homepc MINGW64 /d/gitstart (dev)$ git checkout featureSwitched to branch 'feature'albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featurenothing to commit, working tree clean 发现此时 feature 分支上没有任何修改了，我们再改一次，然后切换到 dev 分支上试试： 12345678910111213141516171819albert@homepc MINGW64 /d/gitstart (feature)$ echo "second try"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (feature)$ git checkout deverror: Your local changes to the following files would be overwritten by checkout: README.mdPlease commit your changes or stash them before you switch branches.Aborting 看到了吧，切换分支的操作被拒绝了，原因是这次切换可能导致本地的修改被覆盖，你可以在切换分支前尝试 commit 你的修改或者 stash 你的修改，等等，这里出现了 stash 这个单词，其实之前我都没注意到，不是修改好提交了就是直接加 -f 参数放弃了所做的修改，没想到还有这样神奇 stash 命令帮我渡过难关。 stash 一般操作接下来展示一下 git stash 最常用的操作，也就是标题中提到的——在切换分支前暂存不想提交的修改，继续在上面的环境下操作，现在 feature 分支上修改了 README.md 文件，切换到 dev 分支时因为可能产生冲突而被拒绝，我们先来看一下文件状态。 12345678910111213141516171819202122albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (feature)$ git diffwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directorydiff --git a/README.md b/README.mdindex 76a124a..4c2bfb8 100644--- a/README.md+++ b/README.md@@ -1,2 +1,3 @@ learn git branch command m2+second try 存储临时修改对比显示我们增加了一行，然后执行 git stash 命令，再查看一下文件状态： 12345678910albert@homepc MINGW64 /d/gitstart (feature)$ git stashwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directorySaved working directory and index state WIP on feature: 6ae97d0 Revert "modify README 1"albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featurenothing to commit, working tree clean 执行完 git stash 之后我们发现，刚才的修改不见了，本地状态提示为 nothing to commit, working tree clean，这时我们再来切换分支： 12345678910111213albert@homepc MINGW64 /d/gitstart (feature)$ git checkout devSwitched to branch 'dev'Your branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree clean 这次切换就没有被拒绝，成功的切换到了 dev 分支，你可以在 dev 分支上进行想要的操作，全部操作完成后再切换回 feature 分支，我们这里就不操作了，直接切回 feature 分支查看一下状态： 12345678albert@homepc MINGW64 /d/gitstart (dev)$ git checkout featureSwitched to branch 'feature'albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featurenothing to commit, working tree clean 还原临时修改我们看到工作区很干净，这时如果想还原刚才在 feature 分支的修改，可以使用 git stash pop 命令，我们执行一下然后查看状态： 1234567891011121314151617181920albert@homepc MINGW64 /d/gitstart (feature)$ git stash popOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")Dropped refs/stash@&#123;0&#125; (bd500adb74d57a3d916a89ff2cd4536cf4eaf6ae)albert@homepc MINGW64 /d/gitstart (feature)$ git statusOn branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a") 刚刚被临时暂存的修改又恢复了，我们可以在 feature 分支上继续愉快地开发了。 stash 进阶操作作为这么神奇的命令，stash 肯定不止这么一点点用法，接下来再列举几个较为常用的参数组合： 查看临时存储的所有条目 git stash list当你使用几次 git stash 命令之后就会发现，这个命令有点像建立还原点，所以暂存命令不止可以用一次，当使用多次暂存命令之后就会形成一个暂存列表，这时可以使用 git stash list 命令查看所有的暂存操作，执行命令后大概就是下面的样子： 1234albert@homepc MINGW64 /d/gitstart (feature)$ git stash liststash@&#123;0&#125;: WIP on feature: 84dfd79 add test filestash@&#123;1&#125;: WIP on feature: 6ae97d0 Revert "modify README 1" 这个列表的构成很像一个栈，stash@{0} 是栈顶元素，stash@{1} 是栈顶下面的一个元素，当使用 git stash 命令时会把新的临时存储信息压入栈顶，原来的信息向栈底移动，当使用 git stash pop 命令的时候又会把栈顶的元素弹出，恢复到工作区和暂存区。 临时存储未追踪的新文件 git stash -u在开发过程中新添加的文件不属于任何一个分支，在不冲突的文件情况下也可以在切换分支的时候带到新的分支，默认在使用 git stash 命令的时候不会把这些文件临时存起来，如果想要存起来加上 -u 参数就可以了，执行之后你会发现这个新加的文件在工作区中消失了。 临时存储被忽略的文件 git stash -a被忽略的文件在默认情况下也不会被 git stash 命令存储，想要临时存储这部分文件只要使用 -a 参数就可以了，这样不仅会把忽略的文件临时存储，连未追踪的文件也存储了起来。 stash 操作的标号前面的 git stash list 命令也提到了，使用 git stash 命令的结果会形成一个栈形式的列表，其中 stash@{n} 就是每次临时存储对应的标号，针对于这些标号的操作也有很多，如果不加这些标号默认使用 stash@{0} ，也就是栈顶元素。 查看临时修改的具体内容 git stash show stash@{0}这个查询过程和查询提交日志的形式有点像，主要展示了某次临时存储时改了哪些内容： 123456789albert@homepc MINGW64 /d/gitstart (feature)$ git stash show stash@&#123;0&#125; test.txt | 1 + 1 file changed, 1 insertion(+)albert@homepc MINGW64 /d/gitstart (feature)$ git stash show stash@&#123;1&#125; README.md | 1 + 1 file changed, 1 insertion(+) 恢复指定标号的临时修改 git stash apply stash@{0}在恢复临时存储的修改时不仅可以使用 git stash pop 命令来恢复栈顶那一次修改，也可以按照标号恢复指定的某次修改，测试如下： 123456789101112131415161718192021albert@homepc MINGW64 /d/gitstart (feature)$ git stash apply stash@&#123;1&#125;On branch featureChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (feature)$ git stash liststash@&#123;0&#125;: WIP on feature: 84dfd79 add test filestash@&#123;1&#125;: WIP on feature: 6ae97d0 Revert "modify README 1"albert@homepc MINGW64 /d/gitstart (feature)$ git stash apply stash@&#123;1&#125;error: Your local changes to the following files would be overwritten by merge: README.mdPlease commit your changes or stash them before you merge.Aborting 这个命令在执行后，指定标号的修改会被恢复到工作区和暂存区，但是临时存储的列表不会被删除，这时可以尝试再次恢复相同标号的修改到工作区，你会发现本次操作因为修改了相同的文件而被拒绝。 刪除指定标号的临时存储的修改 git stash drop stash@{0}可以在临时存储列表中删除指定标号的一些修改，可以测试一下看看效果： 123456789101112albert@homepc MINGW64 /d/gitstart (feature)$ git stash liststash@&#123;0&#125;: WIP on feature: 84dfd79 add test filestash@&#123;1&#125;: WIP on feature: 6ae97d0 Revert "modify README 1"albert@homepc MINGW64 /d/gitstart (feature)$ git stash drop stash@&#123;1&#125;Dropped stash@&#123;1&#125; (8408e56305fabcd82c1d05db18e177c89c47c5ac)albert@homepc MINGW64 /d/gitstart (feature)$ git stash liststash@&#123;0&#125;: WIP on feature: 84dfd79 add test file 利用临时存储的修改内容新建分支 git stash branch &lt;branchname&gt; [&lt;stash&gt;]一般这种情况就是使用过多次 git stash push 命令，而本地分支还修改了其他内容，直接恢复之前的修改不太合适，所以利用这个命令新建一个分支，分支的内容以指定的存储标号 &lt;stash&gt; 对应的提交 commit-id 为基础，然后应用 &lt;stash&gt; 的修改，实际上就是新建了一个对应 &lt;stash&gt; 的分支，继续之前未完成的工作，&lt;stash&gt; 默认为 stash@{0}，测试如下： 12345678910111213141516albert@homepc MINGW64 /d/gitstart (feature)$ git stash branch feature1Switched to a new branch 'feature1'M README.mdOn branch feature1Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.md modified: test.txtno changes added to commit (use "git add" and/or "git commit -a")Dropped refs/stash@&#123;0&#125; (2922b3eeeda44c98316453b93fcf07c1fcfffca4)albert@homepc MINGW64 /d/gitstart (feature1)$ git stash list 这个操作会消耗掉对应 &lt;stash&gt; 标号的临时存储的内容，将这些内容从存储列表中移除。 stash 的注意事项这个命令不仅可以在同一分支上存储和还原，也可用于不同分支之间，这时就可以有一个应用，当我们发现在错误的分支上开发了代码，可以先 git stash push 将这些修改临时存储起来， 然后切换到正确的分支，再执行 git stash pop 命令将刚才的修改引用到现在的分支上。 git stash push 命令默认是存储工作区和暂存区的修改内容的，但是 git stash pop 命令在还原是默认将所有的修改还原到工作区，如果想还原到对应的暂存区，需要加额外的参数，像这样 git stash pop --index。 总结 这个命令挺有用的，在合作开发的时候经常碰到临时问题需要处理，切换分支暂存一下很方便 感觉这个命令其实和 commit 也很像的，在操作过程中你会发现，它也有自己的 hash-id，但是不会放到 commit 列表中 这个命令参数也有好多个，不过记住常用的就可以面对大多数情况了，简单列举下 git stash push 会将当前本地的修改临时保存起来，push 可以省略 git stash list 查看当前stash push操作的记录 git stash pop 取出最近一次修改，并应用到本地 git stash apply stash@{n} 应用 stash@{n} 对应的修改，但是不删除这条记录 git stash show stash@{n} 展示 stash@{n} 对应的修改的实际修改内容]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>checkout</tag>
        <tag>stash</tag>
        <tag>merge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[没想到C++中的std::remove_if()函数历史还挺悠久]]></title>
    <url>%2Fblog%2F2020%2F03%2F19%2F%E6%B2%A1%E6%83%B3%E5%88%B0C-%E4%B8%AD%E7%9A%84std-remove-if-%E5%87%BD%E6%95%B0%E5%8E%86%E5%8F%B2%E8%BF%98%E6%8C%BA%E6%82%A0%E4%B9%85%2F</url>
    <content type="text"><![CDATA[前言看到 remove 这个单词的第一反应是什么意思？我的第一感觉是删除、去掉的意思，就像一个程序员看到 string 就会说是字符串，而不会说它是线、或者细绳的意思，可是C++里居然有个函数叫 std::remove()，调用完这个函数什么也没删除，这我就奇怪了，打开有道词典查询一下： 不查不要紧，一查吓一跳，以下是词典给出的三个释义： vt. 移动，迁移；开除；调动 vi. 移动，迁移；搬家 n. 移动；距离；搬家 及物动词、不及物动词、名词给出的含义都是移动，只有一个开除的意思和删除有点像，难道我穿越了？我之前一直以为它是删除的意思啊，很多函数还是用它命名的呢！ 赶紧翻翻其他的字典，给高中的英语老师打个电话问问，最终还是在一些释义中找到了删除的意思，还有一些用作删除的例句，有趣的是在有道词典上，所有的单词解释都和移动有关，所有的例句都是和删除有关。 remove_if的历史为什么要查单词的 remove 的意思，当然是被它坑过了，本来想从 std::vector&lt;T&gt; 中删除指定的元素，考虑到迭代器失效的问题，放弃了循环遍历的复杂处理，选择直接使用算法函数 std::remove_if()来进行删除，之前对于 std::remove() 和 std::remove_if() 有过简单的了解，不过记忆还是出现了偏差。 一直记得 std::remove() 函数调用之后需要再使用 erase() 函数处理下，忘记了 std::remove_if() 函数也要做相同的处理，于是在出现问题的时候一度怀疑这个函数的功能发生了变更，开始找这个函数历史迭代的版本，这里推荐一个网站 C++标准函数查询 - std::remove_if()，用来查询函数的定义、所在头文件和使用方法非常方便。 文档中有这样两句： 1) Removes all elements that are equal to value, using operator== to compare them.3) Removes all elements for which predicate p returns true. 解释函数作用时用到的单词都是 remove ，你说神不神奇，这里应该都是取的移动的意思。 这两句话对应的函数声明应该是： 12345template&lt; class ForwardIt, class T &gt;ForwardIt remove( ForwardIt first, ForwardIt last, const T&amp; value ); // (until C++20)template&lt; class ForwardIt, class UnaryPredicate &gt;ForwardIt remove_if( ForwardIt first, ForwardIt last, UnaryPredicate p ); // (until C++20) 这两个函数后面都有相同的说明—— (until C++20) ，意思大概就是说这两个函数一直到 C++20 版本都存在，在我的印象中 std::remove_if() 函数比较新，最起码得比 std::remove() 函数年轻几岁，可是他们到底是哪个版本添加到c++标准的的呢？中途的功能有没有发生变更，继续回忆！ 第一次看到这两个函数应该是在看《Effective STL》这本书的时候，大概是5年前了，正好这个本书就放在手边，赶紧翻目录查一下，打开对应章节发现其中确实提到了删除 std::vector&lt;T&gt; 中的元素时，在调用了这两个函数之后都需要再调用 erase() 函数对待删除的元素进行擦除。 看看书的出版时间是2013年，难道是 C++11 的标准加上的，不对，看一下翻译者写得序，落款时间2003年，不能是 C++03 的标准吧？不过这是一本翻译书籍，再看看原作者 Scott Meyers 写的前言，落款时间2001年，好吧，看来这两个函数肯定在 C++98的版本中就已经存在了，我有点惊呆了，这确实颠覆了我的记忆和认知。 造成这种认知错误主要有两方面原因，第一方面就是受到了开发环境的限制，从一开始学习的时候Turob C 2.0、VC++ 6.0、VS2005、VS2008、VS2010就很少接触 C++11 的知识，Dev-C++ 和 Code::Blocks 也是在特定的情况下使用，没有过多的研究，结果在刚开始工作的时候开发工具居然是VS2003，这个版本我之前都没听说过，还好一步步升级到了08、13、17版本。 第二方面就是这两个函数常常与 Lambda 表达式，auto 关键字一起用，这都是 C++11 里才有的，让人感觉好像这个 std::remove_if() 函数也是 C++11 版本中的内容，造成了错觉。总来说还是用的少，不熟悉，以后多看多练就好了。 remove_if的实现要想更深入的学习 std::remove_if() 函数， 那这个函数实现的细节有必要了解一下，这有助于我们理解函数的使用方法，下面给出两个版本可能的实现方式，也许下面的实现与你查到的不一样，但是思想是相通的，有些实现细节中使用了 std::find_if() 函数，这里没有列举这个版本，下面这两个版本的代码更容易让人明白，它究竟做了哪些事情。 123456789101112131415// C++98 版本template &lt;class ForwardIterator, class UnaryPredicate&gt; ForwardIterator remove_if (ForwardIterator first, ForwardIterator last, UnaryPredicate pred)&#123; ForwardIterator result = first; while (first!=last) &#123; if (!pred(*first)) &#123; *result = *first; ++result; &#125; ++first; &#125; return result;&#125; 123456789101112131415// C++11 版本template &lt;class ForwardIterator, class UnaryPredicate&gt; ForwardIterator remove_if (ForwardIterator first, ForwardIterator last, UnaryPredicate pred)&#123; ForwardIterator result = first; while (first!=last) &#123; if (!pred(*first)) &#123; *result = std::move(*first); ++result; &#125; ++first; &#125; return result;&#125; 对比两段代码有没有发现区别——只改了半行代码，将赋值语句中的 *first 在 C++11 版本中替换成了 std::move(*first)，这只能发生在 C++11 之后，因为 std::move() 函数是 C++11 才加入的。 这代码乍一看挺唬人的，其实仔细分析一下还挺简单的，只是这些符号看起来有些生疏，其实可以把 ForwardIterator 看成一个指针类型，UnaryPredicate 是一个函数类型，我们改写一下: 12345678910111213int* remove_if (int* first, int* last, func_type func)&#123; int* result = first; for (;first!=last;++first) &#123; if (!func(*first)) &#123; *result = *first; ++result; &#125; &#125; return result;&#125; 这代码是不是就比较接地气了，想象一下，一个是包含10个元素的数组，让你删除其中的偶数怎么做？其实就是遍历一遍数组，从开始位置到结束位置逐个判断，如果不是偶数就不进行操作，如果是偶数就把当前的偶数向前移动到结果指针上就好了，结果指针向后移动准备接受下一个奇数，这个判断是不是偶数的函数就是上面代码中的 func()。 最后结果指针 result 停在有效元素后面一个位置上，这个位置到结尾指针 last 的位置上的元素都应该被删除，这就是为什么常常将 std::remove_if() 函数的返回值作为 erase() 函数的第一个参数，而将 last 指针作为 erase() 函数的第二个参数，实际作用就是将这些位置上的元素擦除，从头擦到尾，达到真正删除的目的。 具体使用说了这么多，接下来看看具体怎么用，我们将 std::remove_if() 函数和 erase() 函数分开使用，主要看一下调用 std::remove_if() 函数之后的 vector 中元素的值是怎么变的。 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;bool isEven(int n) // 是否是偶数&#123; return n % 2 == 0;&#125;int main()&#123; std::vector&lt;int&gt; vecTest; for (int i = 0; i &lt; 10; ++i) vecTest.push_back(i); for (int i = 0; i &lt; vecTest.size(); ++i) std::cout &lt;&lt; vecTest[i] &lt;&lt; " "; std::cout &lt;&lt; std::endl; // 移动元素 std::vector&lt;int&gt;::iterator itor = std::remove_if(vecTest.begin(), vecTest.end(), isEven); // 查看移动后的变化 for (int i = 0; i &lt; vecTest.size(); ++i) std::cout &lt;&lt; vecTest[i] &lt;&lt; " "; std::cout &lt;&lt; std::endl; // 删除元素 vecTest.erase(itor, vecTest.end()); for (int i = 0; i &lt; vecTest.size(); ++i) std::cout &lt;&lt; vecTest[i] &lt;&lt; " "; return 0;&#125; 运行结果为： 0 1 2 3 4 5 6 7 8 91 3 5 7 9 5 6 7 8 91 3 5 7 9 从结果可以看出，第二步调用 std::remove_if() 函数之后，vector 中的元素个数并没有减少，只是将后面不需要删除的元素移动到了 vector 的前面，从第二行结果来看，调用 std::remove_if() 函数之后返回的结果 itor 指向5，所以擦除从5所在位置到结尾的元素就达到了我们的目的。 这段代码在 C++98、C++11、C++14 环境下都可以编译运行，在这里推荐一个在线编译器 C++ Shell，可以测试各个版本编译器下运行结果，界面简洁明了，方便测试。 上面的代码其实写得有些啰嗦，如果使用 C++11 语法之后，可以简写为： 1， 运行结果： 0 1 2 3 4 5 6 7 8 91 3 5 7 9 总结 对于模糊的知识要花时间复习，避免临时用到的时候手忙脚乱出问题 对于一些心存疑虑的函数可以看一下具体的实现，知道实现的细节可以让我们更加清楚程序都做了哪些事情 对于新的技术标准可以不精通，但是必须花一些时间进行了解，比如新的 C++ 标准 对于违反常识的代码，先不要否定，即使在你的运行环境中报错，说不定人家是新语法呢？ 曾经看到一段在类的定义时初始化非静态变量的代码，一度认为编译不过，但后来发现在 C++11 中运行的很好]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>历史</tag>
        <tag>删除</tag>
        <tag>remove</tag>
        <tag>remove_if</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python操作Excel工作簿(\*.xlsx)]]></title>
    <url>%2Fblog%2F2020%2F03%2F14%2FPython%E6%93%8D%E4%BD%9CExcel%E5%B7%A5%E4%BD%9C%E7%B0%BF%2F</url>
    <content type="text"><![CDATA[前言Excel 作为流行的个人计算机数据处理软件，混迹于各个领域，在程序员这里也是常常被处理的对象，可以处理 Excel 格式文件的 Python 库还是挺多的，比如 xlrd、xlwt、xlutils、openpyxl、xlwings 等等，但是每个库处理 Excel 的方式不同，有些库在处理时还会有一些局限性。 接下来对比一下几个库的不同，然后主要记录一下 xlwings 这个库的使用，目前这是个人感觉使用起来比较方便的一个库了，其他的几个库在使用过程中总是有这样或那样的问题，不过在特定情况下使用也是挺不错的。 EXCEL文件Excel 被称为电子表格，其实际可以保存的格式分为很多种，但是“Excel 工作簿(*.xlsx)”和“Excel 97-2003 工作簿(*.xls)”是其中比较常用的两种，可以认为 .xls 格式的表格是 03版Excel 之前常用的格式，而 .xlsx 是 03版之后，一般指 07版Excel 之后常用的格式。 一般的 Excel 程序对于上述的两种格式都可以打开编辑，也可以相互转化存储，不过还是建议在没有特殊要求的情况下使用新版本的格式，一方面新的稳定版本可能会修复之前的一些BUG，同时也会带来进行一些优化。 我也是在写这篇总结之前才发现，一个空的 .xlsx 格式的文件大小有 7KB，而一个空的 .xls 格式的文件大小有 24KB，当我分别写入一个相同的汉字后，两个文件大小变成了 10KB 和 30KB，差距还是不小的，还有一个问题就是在将 .xlsx 格式的文件另存为 .xls 格式时还会有兼容性提示，提醒用户有些设置可能会丢失，所以能选新版本还是尽量用新版本吧。 测试环境因为很多应用程序是不断迭代的，相对应的 Python 库也是不断迭代的，这里尽可能的给出版本号，不同的版本可能会有不同的问题： 操作系统: Windows 10 随意版 Python: 3.75 xlrd: 1.2.0 xlwt: 1.3.0 xlutils: 2.0.0 openpyxl: 3.0.3 xlwings: 0.18.0 以上各个程序库使用之前自行安装就行，安装方法就不赘述了，不过可以提供一个可以快速安装镜像源，使用 pip install -i https://pypi.doubanio.com/simple 库名 可以尽可能解决下载安装缓慢的问题。 Excel具体操作关于使用 Python 具体操作 Excel 的方法可以分为三组，配合使用 xlrd、xlwt、xlutils 操作作为第一组，使用库 openpyxl 作为第二组，而 xlwings 作为第三组，这篇总结重点总结 xlwings 的使用，其他两组简单了解。 xlrd、xlwt、xlutils这一组操作 Excel 的库名字很形象，一个读、一个写、一个小工具，凑到一起就可以对 Excel 肆意妄为了，下面做个小练习，打开一个 Excel 文件然后修改第一个单元格的值，再另存为一个新文件，代码如下: 123456789101112131415import xlrdimport xlwtimport xlutils.copydef save_as_new_file(file_name, new_file_name): # 打开Excel文件 rb = xlrd.open_workbook(file_name) # 创建一个可写入的副本 wb = xlutils.copy.copy(rb) # 获得第一个sheet页签 ws = wb.get_sheet(0) # 第一个单元格写入测试值 ws.write(0, 0, 'test value') # 另存为一个新文件 wb.save(new_file_name) 上述代码无论是操作 .xlsx 文件还是操作 .xls 文件都不会报错，但是另存为的 .xlsx 格式的文件会打不开，同时你会发现正常存储的 .xls 文件打开后格式全都没了，怎么办，改个参数试试，将打开文件的代码修改如下： 1rb = xlrd.open_workbook(file_name, formatting_info=True) 其中参数 formatting_info=True 就表示打开Excel时保留原有的格式，但是这是相对于 .xls 格式的文件，对于 .xlsx 格式的文件直接跑出异常 raise NotImplementedError(&quot;formatting_info=True not yet implemented&quot;)，就因为处理不了 .xlsx 格式的文件，我暂时没有使用这几个库操作 Excel。 还有一点，这几个库操作单元格时，行和列的索引是从0开始的。 openpyxl首先说这个库主要用来操作 .xlsx 格式的文件，对于 .xls 格式的文件无法打开，会报 openpyxl does not support the old .xls file format 这样的错误，但是可以存储成这样的格式，再次打开时会有格式不匹配的警告，但是基础的数据还在，所以还是优先用来操作 .xls 格式的文件吧。 写一个新文件的常见用法： 1234567891011121314151617181920from openpyxl import Workbookfrom openpyxl import load_workbookfrom openpyxl.styles import Font, Fill, Alignment, PatternFilldef write_new_excel(file_name): # 创建一个excel文档 wb = Workbook() # 获得当前激活的sheet对象 ws = wb.active # 给A2单元格赋值 ws['A2'] = 'This is A2 cell' # 一行添加多列数据 ws.append([1, 2, 'hello']) # 添加新的sheet ws = wb.create_sheet(title='NewInfo',index=0) # 设置单元格的值 ws['A1'] = 'This is new sheet' # 保存excel wb.save(file_name) 读取和改写一个原有文件的常见用法： 12345678910111213141516171819202122232425262728def read_update_excel(file_name): # 加载Excel表 wb = load_workbook(file_name) # 打印sheet数量 print('sheet count:', len(wb.sheetnames)) # 打印所有sheet名字 print('sheet name list:', wb.sheetnames) # 获取第一个sheet对象 ws = wb[wb.sheetnames[0]] # 打印sheet表行数和列数 print('rows count:', ws.max_row, 'cols count:', ws.max_column) # 更新单元格A1的内容 ws['A1'] = 'this is A1' # 在第二行位置插入一行 ws.insert_rows(2) # 删除第五行 ws.delete_rows(5) # 获取单元格对象，对应B2单元格 cell = ws.cell(2,2) # 设置单元格内容 cell.value = 'this is B2' # 修改字体格式为粗体 cell.font = Font(bold=True) # 修改单元格格式 cell.fill = PatternFill("solid", fgColor="F0CDCD") # 保存原文件或另存一个文件 wb.save(file_name) 使用这个库遇到的情况，存储带有样式的数据没有发现问题，但是当加入一个计算公式后，另存为一个文件时明显文件尺寸变小了，但是数据和公式没有发现有问题。 有资料说处理速度真的很慢，因为我处理的文件比较小，但是没有发现这方面的问题，还有一个问题就是说Excel中的宏全部丢失，这个测试的时候确实是丢了，只不过这个好像和文件格式有关，要想保存宏需要存储为 .xlsm 格式，但是 openpyxl 使用来操作 .xlsx 文件的，存储时会导致宏丢失，强行存储为 .xlsm 格式会导致最终的文件打不开。 还有一点，这个库操作单元格时，行和列的索引是从1开始的。 xlwings这个库在操作的首先要创建一个 App，通过这个创建出来的 App 对象来操作 Excel，非常像把 Excel 的各种操作 api 封装到一起，然后通过这个 App 对象来调用，如果在创建 App 的时候不设置隐藏参数，是会正常打开 Excel 程序的。 使用 xlwings 的基本方式：1234567891011import xlwings as xw# 设置Excel程序不可见app = xw.App(visible=False, add_book=False)# 通过 app 操作 Excel文件# app.bala bala bala .....# app.bala bala bala .....# 优雅的退出app.quit() 创建一个新的 Excel 文件并写入数据：12345678910111213141516def write_new_excel(app, file_name): # 创建新的 Excel 表 wb = app.books.add() # 获取当前活动的sheet ws = wb.sheets.active # 初始化二维区域的值 arr_data = [[1, 2, 3], [4, 5, 6], [7, 8, 'end']] # 设置到新建的Excel中 ws.range('A1:B3').value=arr_data # 设置单独一个单元格的值 ws.range('A4').value='this is A4' # 设置单独一个单元格的值 ws[3,1].value='this is B4' # 保存Excel文件 wb.save(file_name) wb.close() 需要注意的是通过行索引和列索引修改单元格时，起始索引是0。 读入已有 Excel 表格并修改1234567891011121314151617181920212223242526272829303132333435363738394041def read_update_excel(app, file_name): # 加载已有的表格 load_wb = app.books.open(file_name) # 获取Excel表中第一个sheet load_ws = load_wb.sheets[0] # 打印sheet的名字 print(load_ws.name) # 根据sheet名字获取sheet对象 load_ws = load_wb.sheets[load_ws.name] # 获取当前活动的sheet load_ws = load_wb.sheets.active # 获取存在数据的行数和列数 rows = load_ws.api.UsedRange.Rows.count cols = load_ws.api.UsedRange.Columns.count print('rows count:', rows, 'cols count:', cols) # 修改指定单元格数据（A1单元格） load_ws[0,0].value='this is A1' # 有空行或空列时获取准确的行列数量 print(load_ws.used_range.shape) # 从A1单元格开始扩展到非空行空列，最后的行数和列数 print((load_ws.range('A1').expand().last_cell.row, load_ws.range('A1').expand().last_cell.column)) # 从A1单元格开始扩展到非空行空列，最后的行数和列数 print((load_ws.range('A1').expand().last_cell.row, load_ws.range('A1').expand().last_cell.column)) # 从A1单元格开始扩展到非空行空列，最后形状 print(load_ws.range(1,1).expand().shape) # 从A1单元格开始扩展到非空行空列，最后的行数和列数 print((load_ws.range('A1').expand('table').rows.count, load_ws.range('A1').expand('table').columns.count)) # 保存修改后的Excel load_wb.save(file_name) load_wb.close() Excel 增加删除行和列1234567891011121314151617181920def insert_delete_rowscols(app, file_name): # 加载已有的表格 load_wb = app.books.open(file_name) # 获取当前活动的sheet load_ws = load_wb.sheets.active # 从第2行开始插入4行，也就是说2-5行变成新插入的空行 load_ws.api.rows('2:5').insert # 删除第6行和第7行 load_ws.api.rows('6:7').delete # 插入一个单元格，实际测试效果是B列从B2开始向下移动，B2为新添加的单元格 load_ws.range('B2').api.insert # 插入新的一列 load_ws.api.columns('B').insert # 删除一列 load_ws.api.columns('C').delete # 保存修改后的Excel load_wb.save(file_name) load_wb.close() 单元格宽高查询设置与合并12345678910111213141516171819202122232425262728293031def cell_operation(app, file_name): # 加载已有的表格 load_wb = app.books.open(FILE_PATH_ROOT + file_name) # 获取当前活动的sheet load_ws = load_wb.sheets.active # 合并单元格 load_ws.range('A2:A3').api.merge #获取单元格 cell = xw.Range('B2') # 打印单元格所在的行和列 print("row is:", cell.row, "col is:", cell.column) # 打印当前格子的高度和宽度 print("cell.width:", cell.width, "cell.height:", cell.height) # 设置当前格子的高度和宽度 cell.row_height = 32 cell.column_width = 64 # 指定单元格的高度和宽度自适应 cell.columns.autofit() cell.rows.autofit() # 再次打印当前格子的高度和宽度 print("cell.width:", cell.width, "cell.height:", cell.height) # 保存修改后的Excel load_wb.save(file_name) load_wb.close() 几个库支持情况对比虽然前面写了这么多方法，但是遇到一个实际的问题时还是会犹豫，到底用哪种方式呢？下面做一个简单的对比，只是根据我做的实验来简单对比，如果有不准确甚至是错误的地方，欢迎大家指出来，我会尽快改正的。 情景/库 xlrd、xlwt、xlutils openpyxl xlwings 读取.xls 可以带有样式读取 不支持 可以读取 保存.xls 可以带有样式保存 可以保存，但是提示文件扩展名不匹配，可以看到原始数据 可以保存，但是提示文件扩展名不匹配，可以看到原始数据 读取.xlsx 可以读取，但没有样式 可以带有样式读取 可以带有样式读取 保存.xlsx 保存后打不开 可以带有样式保存 可以带有样式保存 读取.xlsm 可以读取，但没有样式和宏 可以读取，但没有宏 可以读取包含宏的表格 保存.xlsm 保存后打不开，存成 .xls 格式宏丢失 保存后打不开，存成 .xls想 格式宏丢失 存储后宏还在 增删行和列 没有直接方法 支持 支持 另存后大小 .xls 文件没有变化 .xlsx 文件会变小 .xls、.xlsx 文件没有变化 使用建议 只操作.xls文件可以考虑 只操作.xlsx文件可以考虑，不能带有宏 一个比较好的选择，使用时感觉速度稍微有点慢 总结 Excel 表格程序经过版本的更替发生了很大的变化，出现了相同内容时 .xls 比 .xlsx 格式的文件大很多的情况 基于上一点考虑，如果能使用的新版的表格，那么就放弃旧的格式的吧 还有一个神奇的情况，一个带有少量数据的 .xlsx 格式的表格要比一个空表格还要小，这是什么情况，暂时没弄明白怎么回事，求知道的大神告知一二]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Excel</tag>
        <tag>xlrd</tag>
        <tag>xlutils</tag>
        <tag>xlwings</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git checkout/git reset/git revert/git restore常用回退操作]]></title>
    <url>%2Fblog%2F2020%2F03%2F03%2Fgit-checkout-git-reset-git-revert-git-restore%E5%B8%B8%E7%94%A8%E5%9B%9E%E9%80%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[在 Git 中没有真正的方法来做任何事情，这就是它的妙处！ 前言经常会听到别人说，如果时光可以倒流，我将会如何如何，可是现阶段的科技还达不到时光倒流的目的，或许在《三体》世界的四维裂缝里可以试一下。现实的世界中找不到后悔药，但是在代码的世界里却可以轻松实现，错误的BUG修改、砍掉的做了一半的功能都可以轻松回退，不留一丝痕迹，回滚之后一切又可以重新开始了。 代码回退大型编程项目的开发往往伴随着版本工具的使用，其实引入代码版本控制工具，有一部分原因也是为了方便回退，回退操作每天都发生的，只是有时是我们显式的操作，有时却自然而然的进行着，我们切换着分支很可能就是从开发版本回退到一个稳定版本，我们查询日志，实际上是在记忆上回退我们整个的开发过程，找寻其中的问题和修改的内容。 Git管理下的各种文件状态Git的使用中，由于一个文件存在好几种状态的变化，所以处理起回退要分情况进行，有些各式各样的命令最终分析起来其实作用是一样的。 说起Git常常会提到工作区、暂存区、版本库的概念，这是很通用的说法，其实工作区一般就是指我们能看到的文件、本地操作文件所在的目录，我们正常编写的代码文件、管理的资源文件都是在工作区里操作，这里的文件也不是全都平等的，又细分为受版本控制的文件和不受版本控制的文件。 提到暂存区就和index文件建立起了联系，工作区的新文件和已经修改的受版本控制的文件，使用 git add file_name 就可以加到暂存区，相当于登记报个名，以后提交到版本库的时候会把这些登记的文件都带上，实际上执行了 git add 命令的文件都生成了对应的 object 对象，放在.git/objects目录下，状态变成了 staged， 当提交到版本库时，分支会引用这些对象。 版本库就是文件修改的目的地了，最终的修改会提交到版本库，这时提交的文件状态变成 committed，其实也是一种 unmodified 状态，一路走来，版本库中记录了你的每一次提交，可以追溯你每一次修改的内容。 其实还有一个远程仓库的概念，一般确定本地仓库的修改没有问题了，或者要将本地代码远程备份时，可以将自己修改的分支推送到远程仓库，因为有时候我们也想回退已经推送到远程仓库的修改，所以这里先提一下远程仓库。 总结起来一个文件的状态通常可以分为： 不受版本控制的 untracked 状态 受版本控制并且已修改的 modified 状态 受版本控制已修改并提交到暂存区的 staged 状态 从暂存区已经提交到本地仓库的 committed 状态 提交到本地仓库未修改或者从远程仓库克隆下来的 unmodified 状态 Git回退命令上面提到了在 Git 这个版本控制工具下文件的各种状态，其实回退操作就是通过命令实现这些文件状态的“倒退”，进而达到回退操作的目的，下面一起先来了解下这些可以实现回退的命令。 git checkout这个命令又出现了，上次是总结 git branch 分支操作的时候，git checkout 可以用来新建或者切换分支，这次总结回退版本的命令，git checkout 也可以用来回退文件版本，很神奇吧。 其实这个命令的作用就是它单词的本义——检出，他的常用操作也取自这个意思，比如 git checkout branch_name 切换分支操作，实际上就是把指定分支在仓库中对应的所有文件检出来覆盖当前工作区，最终表现就是切换了分支。 而针对于文件的检出可以使用 git checkout -- file_name，当不指定 commit id 就是将暂存区的内容恢复到工作区，也就可以达到回退本地修改的作用。 不过，这个身兼数职的 git checkout 命令现在可以轻松一些了，从 Git 2.23 版本开始引入了两个新的命令： git switch 用来切换分支，git restore用来还原工作区的文件，这个后面还会提到。 git revertrevert 这个词的意思是：归还，复原，回退，它和后面即将提到的 restore 在意思上简直无法区分，为了区别他们两个这里可以把 git revert 看成归还的意思，对某次提交执行 git revert 命令就是对这次修改执行一个归还操作，其实就是反向再修改一次。 要理解 git revert 就要从反向修改的含义来看，当我们再一个文件中添加一行内容，并提交到版本库后，产生一个提交id——commit-id-a，如果这时使用 git revert commit-id-a 命令，就相当于在工作区中的那个文件将刚在新加的一行内容删除掉，然后再进行一个提交。 注意，这个操作是会改变分支记录的，因为产生了新的提交。 git restore这个命令是 Git 2.23 版本之后新加的，用来分担之前 git checkout 命令的功能，作用就是用暂存区或者版本库中的文件覆盖本地文件的修改可以达到回退修改的目的，同时也可以使用版本库中的文件覆盖暂存区的文件，达到回退git add 命令的目的。 注意，这个操作是不会影响分支记录的，就是相当于之前的 git checkout 命令重新检出一份文件来覆盖本地的修改。 git resetreset 重新设置的意思，其实就是用来设置分支的头部指向，当进行了一系列的提交之后，忽然发现最近的几次提交有问题，想从提交记录中删除，这是就会用到 git reset 命令，这个命令后面跟 commit id，表示当前分支回退到这个 commit id 对应的状态，之后的日志记录被删除，工作区中的文件状态根据参数的不同会恢复到不同的状态。 --soft: 被回退的那些版本的修改会被放在暂存区，可以再次提交。 --mixed: 默认选项，被回退的那些版本的修改会放在工作目录，可以先加到暂存区，然后再提交。 --hard: 被回退的那些版本的修改会直接舍弃，好像它们没有来过一样。 这样来看，git set 命令好像是用来回退版本的，但是如果使用 git rest HEAD file_name 命令就可以将一个文件回退到 HEAD 指向版本所对应的状态，其实就是当前版本库中的状态，也就相当于还原了本地的修改。 git rm临时插播的命令，本来删除不能算是回退，但是如果它和某些命令反着来就是一种回退，比如对一个新文件使用 git add newfile_name 命令，然后再使用 git rm --cached newfile_name 就可以将这个文件从暂存区移除掉，但是在工作区里没有消失，如果不加 --cached 参数，就会从工作区和版本库暂存区同时删除，相当于执行了 rm newfile_name 和 git add new_file 两条命令。 具体回退操作说了这么多肯定有点懵，特别是一个相同的需求可以使用很多命令来实现的时候，接下来看一些具体需求，整个测试过程用上一篇总结《git branch常用分支操作》使用的 git 仓库来进行，远程地址是 git@gitee.com:myname/gitstart.git，下面测试开始，我们看一下这些情况怎么进行还原： 初始状态12345678910111213141516171819202122albert@homepc MINGW64 /d/gitstart (dev)$ lsREADME.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master remotes/origin/dev remotes/origin/masteralbert@homepc MINGW64 /d/gitstart (dev)$ git branch -vv* dev 3226b63 [origin/dev] add readme file master 3226b63 [origin/master] add readme file 还原00：工作区中未加到暂存区和版本库的文件，还原今天所做的修改实话实说，办不到，没有加到过暂存区就没有被追踪，它的任何修改是没有办法回退的，可是使用 Ctrl+Z 碰碰运气，没准就退回到了你想要的状态。 还原01：工作区中未加到暂存区和版本库的文件，执行了 git add 操作这种情况可以使用git rm --cached newfile、git restore --staged newfile 或者 git reset HEAD newfile 命令，使用后两个命令的时候不能是版本库的第一个文件。 git rm1234567891011121314151617181920212223242526272829303132albert@homepc MINGW64 /d/gitstart (dev)$ echo "test data"&gt;new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git add new.txtwarning: LF will be replaced by CRLF in new.txt.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) new file: new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git rm --cached new.txtrm 'new.txt'albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) new.txtnothing added to commit but untracked files present (use "git add" to track) git restore12345678910111213141516171819202122232425262728albert@homepc MINGW64 /d/gitstart (dev)$ git add new.txtwarning: LF will be replaced by CRLF in new.txt.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) new file: new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git restore --staged new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) new.txtnothing added to commit but untracked files present (use "git add" to track) git reset12345678910111213141516171819202122232425262728albert@homepc MINGW64 /d/gitstart (dev)$ git add new.txtwarning: LF will be replaced by CRLF in new.txt.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) new file: new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git reset HEAD new.txtalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) new.txtnothing added to commit but untracked files present (use "git add" to track) 还原02：版本库中的文件，修改或删除后未执行 git add 操作我们直接修改 README.md 文件吧，删除刚才添加的未受版本管理的 new.txt，在 README.md 文件中添加内容，然后试着还原，这种情况常常出现在修改一个功能还未提交，但是先不要求修改了，可以直接还原。 这种情况可以使用git restore file_name、git checkout -- file_name 或者 git reset --hard HEAD 命令，最后的git reset 命令带有 --hard 参数不能再加文件目录，只能将工作区全还原。 git restore123456789101112131415161718192021222324albert@homepc MINGW64 /d/gitstart (dev)$ echo "new line"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git restore README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree clean git checkout123456789101112131415161718192021222324albert@homepc MINGW64 /d/gitstart (dev)$ echo "new line"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git checkout -- README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree clean git reset1234567891011121314151617181920212223242526272829albert@homepc MINGW64 /d/gitstart (dev)$ echo "new line"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git reset --hard HEAD README.mdfatal: Cannot do hard reset with paths.albert@homepc MINGW64 /d/gitstart (dev)$ git reset --hard HEADHEAD is now at 3226b63 add readme filealbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree clean 还原03：版本库中的文件，修改或删除后执行了 git add 操作使用了 git add 命令之后，文件的改变就放到了暂存区，这种情况可以使用git restore --staged file_name 或者 git reset HEAD file_name 命令。 git restore执行 git restore --staged file_name 实际上是使用版本库中的文件覆盖暂存区中的数据，执行结束后文件状态变成了 &lt;还原02&gt; 中的情况。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ echo "test add"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) modified: README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git restore --staged README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a") git resetgit reset 命令如果加上 --hard 参数不能再加文件目录，只能将工作区全还原，如果不加默认参数为 --mixed，执行之后修改的文件状态变成了 &lt;还原02&gt; 中的情况。 12345678910111213141516171819202122232425262728293031323334353637383940414243albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes to be committed: (use "git restore --staged &lt;file&gt;..." to unstage) modified: README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git reset HEAD README.mdUnstaged changes after reset:M README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a") 还原04：版本库中的文件，修改或删除后执行了 git add、git commit 操作git commit 命令一旦执行了之后就形成了“历史”，我们叫做提交日志，要想回退就得有篡改历史的能力，很幸运 Git 给了我们这种能力，其实提交之后我们可以把本地文件反向修改，然后再提交一次，但是我们说的还原，一般都是只倒退，既然是错误的提交，我们就像把这段“历史”抹去，这时就要用到 git reset HEAD^ 命令。 执行这个命令之后，刚刚的提交记录就被抹掉了，文件状态就回到了 &lt;还原02&gt; 的情况，如果加上参数 --soft 就会回到 &lt;还原03&gt; 的情况，如果加上参数 --hard ，就不能添加 file_name 这个文件名，然后整个工作区倒退到上一次修改之前，其他两种参数 --mixed 和 --soft 就可以指定添加名字。 这里的 HEAD^ 表示最新版本的前一版，也就是倒数第二版本，可以类推，HEAD^^ 表示倒数第三版本，HEAD^^^ 表示倒数第四版本。 另外还有另一种写法 HEAD~1 表示最新版本的前一版，也就是倒数第二版本，HEAD~2 表示倒数第三版本，HEAD~3 表示倒数第四版本。 其中 ^ 和 ~ 的含义并不相同，涉及到合并分支的概念，有兴趣的话可以多了解下，这里就不展开了，继续还原当前这种情况，我们选择 git reset HEAD^ 命令，先提交看下： 12345678910111213141516171819202122232425262728293031323334353637383940414243albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git commit -m"modify readme 1"[dev 8a40f22] modify readme 1 1 file changed, 1 insertion(+)albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ git log -2commit 8a40f229881da037ff99070fa205d7819ba9f51b (HEAD -&gt; dev)Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 15:46:32 2020 +0800 modify readme 1commit 3226b63185a16398a02d5eaea47c95309ba49588 (origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme file 然后再还原试试： 123456789101112131415161718192021222324albert@homepc MINGW64 /d/gitstart (dev)$ git reset HEAD^Unstaged changes after reset:M README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git log -2commit 3226b63185a16398a02d5eaea47c95309ba49588 (HEAD -&gt; dev, origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme file 怎么样，历史被我们抹除了，需要注意的是，如果想还原“历史”，那么 git set 命令后面不能跟文件名，也就是说必须整个还原到上一版本，否则就相当于将单个文件简单反向修改添加到暂存区，而之前对文件的修改保留在本地，文件的日志并没有回退，具体的文件状态还得你自己操作感受一下。 还原05：版本库中的文件，修改或删除后执行了 git add、git commit、git push 操作这种情况就是还原远程仓库的日志记录了，实际上操作步骤先按照 &lt;还原04&gt; 来处理，然后将本地分支情况推送到远程分支即可。 我们先把刚才的修改提交，然后推送到远程分支，使用 git status 可以看到本地分支已经领先远程分支了(Your branch is ahead of ‘origin/dev’ by 1 commit.)， git push 操作之后两个分支同步了。 1234567891011121314151617181920212223242526272829303132albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is ahead of 'origin/dev' by 1 commit. (use "git push" to publish your local commits)nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ git logcommit a5b6c18db71a0487f6316f5db4304a99984f2ab3 (HEAD -&gt; dev)Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 15:51:56 2020 +0800 modify readme 1commit 3226b63185a16398a02d5eaea47c95309ba49588 (origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme filealbert@homepc MINGW64 /d/gitstart (dev)$ git pushWarning: Permanently added the ECDSA host key for IP address '180.97.125.228' to the list of known hosts.Enumerating objects: 5, done.Counting objects: 100% (5/5), done.Writing objects: 100% (3/3), 286 bytes | 286.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)remote: Powered by GITEE.COM [GNK-3.8]To gitee.com:myname/gitstart.git 3226b63..a5b6c18 dev -&gt; dev 这时通过远程仓库的管理软件，你可以看到远程分支已经有了最新的提交，然后我们可以参考 &lt;还原04&gt; 的情况，先将本地日志还原，再推送到远程仓库。 1234567891011121314151617181920212223242526272829303132333435albert@homepc MINGW64 /d/gitstart (dev)$ git reset HEAD^Unstaged changes after reset:M README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is behind 'origin/dev' by 1 commit, and can be fast-forwarded. (use "git pull" to update your local branch)Changes not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git restore &lt;file&gt;..." to discard changes in working directory) modified: README.mdno changes added to commit (use "git add" and/or "git commit -a")albert@homepc MINGW64 /d/gitstart (dev)$ git logcommit 3226b63185a16398a02d5eaea47c95309ba49588 (HEAD -&gt; dev, origin/master, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme filealbert@homepc MINGW64 /d/gitstart (dev)$ git pushTo gitee.com:myname/gitstart.git ! [rejected] dev -&gt; dev (non-fast-forward)error: failed to push some refs to 'git@gitee.com:myname/gitstart.git'hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details. 和想象的不太一样的，这种情况是远程仓库的记录领先，无法直接推送，此时可以添加 -f 参数，用本地提交记录覆盖远程分支记录： 123456albert@homepc MINGW64 /d/gitstart (dev)$ git push -fTotal 0 (delta 0), reused 0 (delta 0)remote: Powered by GITEE.COM [GNK-3.8]To gitee.com:myname/gitstart.git + a5b6c18...3226b63 dev -&gt; dev (forced update) 这次再查询远程分支记录，发现也被回退了，目的达成。 还原06：两次git commit 之后产生两条日志，只还原第一次提交这种情况其实发生了两次修改和两次提交，和 &lt;还原05&gt; 情况不同的是要还原的提交不是最后一次，如果使用 git reset 命令必然将最后一次修改也还原了，虽然不能直接完成，但是给我们提供了解决问题的思路： 第一种方法：直接使用 git reset HEAD^^ 命令还原两次提交，然后在工作区将文件按第二次修改再改一次进行提交，这种方法适用于想要抹除第一次提交历史的情况。 第二种方法：如果你不在意提交历史，只是想还原第一次修改，那么可以使用 git revert HEAD^ 命令来反向修改那一次变化，修改之后会自动添加到暂存区，等待提交。 先来修改提交两次，产生两次记录： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758albert@homepc MINGW64 /d/gitstart (dev)$ git statusOn branch devYour branch is up to date with 'origin/dev'.nothing to commit, working tree cleanalbert@homepc MINGW64 /d/gitstart (dev)$ echo "m1"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git commit -m"modify README 1"[dev e570df1] modify README 1 1 file changed, 1 insertion(+)albert@homepc MINGW64 /d/gitstart (dev)$ echo "m2"&gt;&gt;README.mdalbert@homepc MINGW64 /d/gitstart (dev)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (dev)$ git commit -m"modify README 2"[dev 140547f] modify README 2 1 file changed, 1 insertion(+)gialbert@homepc MINGW64 /d/gitstart (dev)$ git logcommit 140547f8d0b10d9a388beaf2ce522c38c878a839 (HEAD -&gt; dev)Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:26:17 2020 +0800 modify README 2commit e570df134b39ee7424bc8c48c1067e72c3fb9637Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:26:07 2020 +0800 modify README 1commit 3226b63185a16398a02d5eaea47c95309ba49588 (origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme filealbert@homepc MINGW64 /d/gitstart (dev)$ cat README.mdlearn git branch commandm1m2 然后使用 git revert HEAD^ 还原第一次修改记录： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647albert@homepc MINGW64 /d/gitstart (dev)$ git revert HEAD^Auto-merging README.mdCONFLICT (content): Merge conflict in README.mderror: could not revert e570df1... modify README 1hint: after resolving the conflicts, mark the corrected pathshint: with 'git add &lt;paths&gt;' or 'git rm &lt;paths&gt;'hint: and commit the result with 'git commit'albert@homepc MINGW64 /d/gitstart (dev|REVERTING)$ vi README.mdalbert@homepc MINGW64 /d/gitstart (dev|REVERTING)$ git add README.mdalbert@homepc MINGW64 /d/gitstart (dev|REVERTING)$ git commit[dev 6ae97d0] Revert "modify README 1" 1 file changed, 1 deletion(-)albert@homepc MINGW64 /d/gitstart (dev)$ git logcommit 6ae97d0e136abc1ed241854298037ca9d1c4460c (HEAD -&gt; dev)Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:31:50 2020 +0800 Revert "modify README 1" This reverts commit e570df134b39ee7424bc8c48c1067e72c3fb9637.commit 140547f8d0b10d9a388beaf2ce522c38c878a839Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:26:17 2020 +0800 modify README 2commit e570df134b39ee7424bc8c48c1067e72c3fb9637Author: albert &lt;qianxuan101@163.com&gt;Date: Sat Mar 7 16:26:07 2020 +0800 modify README 1commit 3226b63185a16398a02d5eaea47c95309ba49588 (origin/master, origin/dev, release, master)Author: albert &lt;qianxuan101@163.com&gt;Date: Wed Feb 26 00:36:35 2020 +0800 add readme file 因为修改了同一个文件，还原的时候还产生了冲突，解决冲突之后才提交，看日志发现这是一条新的记录，在实际操作的过程中可能会发生比这还要麻烦的场景，多练就好了。 常用集合使用 Git 进行版本管理时，遇到的回退情况远不止这么多，这只是我目前常见的，之后遇到还会补充，每种情况我们其实不止有一种解决方式，接下来对于每种情况给一个我个人常用的处理方式，因为 git checkout 的作用被逐渐拆分成更具体的 git switch 和 git restore，我们尽量选择功能明确的命令： 还原00：工作区中未加到暂存区和版本库的文件，还原今天所做的修改 尝试下Ctrl+z吧，不行就找找自动保存的缓存文件，看看能不能找到之前版本 还原01：工作区中未加到暂存区和版本库的文件，执行了 git add 操作 直接使用 git restore --staged file_name 命令，如果版本不支持则使用 git rm --cached file_name 还原02：版本库中的文件，修改或删除后未执行 git add 操作 直接使用 git restore file_name 命令，如果版本不支持则使用 git checkout -- file_name 还原03：版本库中的文件，修改或删除后执行了 git add 操作 直接使用 git restore --staged file_name 命令，按 &lt;还原02&gt; 情况处理 还原04：版本库中的文件，修改或删除后执行了 git add、git commit 操作 直接使用 git reset HEAD^ 命令，按 &lt;还原02&gt; 情况处理，或者使用 git reset --soft HEAD^ 命令，按 &lt;还原03&gt; 情况处理 还原05：版本库中的文件，修改或删除后执行了 git add、git commit、git push 操作 先按照 &lt;还原04&gt; 情况处理，然后使用 git push -f 命令 还原06：两次git commit 之后产生两条日志，只还原第一次提交 使用 git revert HEAD^ 命令，解决冲突后提交，revert 后面跟具体的 commit id 也可以。 总结 参考这些具体的例子你会发现，很多操作选择在使用 git status 之后都有列举 所以说 git status 是一个可以提示你做选择的强大帮手，不知所措时可以试试它 Git 2.23版本之后学会用 git switch 和 git restore 命令，因为之前 git checkout 背负了太多了 最后放一幅图吧，只画了主要的，没有画出全部情况，否则会很乱，可以对照着练习一下]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>checkout</tag>
        <tag>reset</tag>
        <tag>revert</tag>
        <tag>restore</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git branch常用分支操作]]></title>
    <url>%2Fblog%2F2020%2F02%2F25%2Fgit-branch%E5%B8%B8%E7%94%A8%E5%88%86%E6%94%AF%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[前言最近代码的版本控制工具由SVN换成了Git，只用管理个人项目常用的灵魂三步git add、git commit、git push看来是行不通了，之前虽然也一直在用 git，但是用法很有限，主要集中在前面提到的三步，所以为了更好的工作，我决定还是好好总结一下。 分支在Git的操作里有着很重要的地位，代表了不同的开发线路，创建一个分支，也就多了一个索引文件，相比于SVN分支拷贝全部文件来说来方便的多，所以Git使得按功能分支的开发模式变得非常简单，在开发过程中常常需要对分支进行操作。 远程仓库本来就几个分支，操作上也没有太麻烦，但是加入了远程仓库以后，事情变得复杂起来。有了远程仓库一般意味着代码开发需要多人合作了，这时候常常会产生冲突，分支合并时也变得不那么容易了。 远程仓库其实也很好理解，就是放在远处用来保存代码资源的一个仓库，其实和本地的代码库没有什么区别，这个远程仓库主要是为了把大家修改的代码都合并到一起，给大家提供一个统一的目标点。 远程仓库究竟有多远，常见的代码托管平台：github、gitlab、码云都可以提供远程仓库，如果你在月球上放置一台可以联网的代码仓库服务器，那么距离就是38.4万千米，但是远程仓库也可以很近，你也可以把本机电脑的D盘里的代码仓库作为E盘的代码仓库的远程仓库，或许远程仓库可能只和你隔了一个文件夹。 由于网络的原因，github 和 gitlab 访问常常很慢，所以为了做练习测试推送，我在码云创建了一个仓库 gitstart，它的地址大概是这个样子：git@gitee.com:myname/gitstart.git，创建的方法一搜一大把，上面提到的几个托管平台，在哪创建都可以，一定要记住地址，因为后面还要用到。 建立联系本地创建文件夹并进入12345678albert@homepc MINGW64 /d$ mkdir gitstartalbert@homepc MINGW64 /d$ cd gitstart/albert@homepc MINGW64 /d/gitstart$ 这里的文件夹名字可以和远程仓库不同，但是为了看起来方便对应，还是取相同的名字好一点。 初始化仓库123456albert@homepc MINGW64 /d/gitstart$ git initInitialized empty Git repository in D:/gitstart/.git/albert@homepc MINGW64 /d/gitstart (master)$ 临时插播好奇心（不在流程中）目前这个状态有点意思，初始化完之后，(master) 这个字符串表示当前是在 master分支，查一下日志看看： 123456albert@homepc MINGW64 /d/gitstart (master)$ git logfatal: your current branch 'master' does not have any commits yetalbert@homepc MINGW64 /d/gitstart (master)$ 提示也是正确的，说 master分支没有任何提交，但是我们查询一下分支看看： 12345albert@homepc MINGW64 /d/gitstart (master)$ git branch -aalbert@homepc MINGW64 /d/gitstart (master)$ 居然是空的，没有分支，查询 .git\HEAD 文件发现里面有一行 ref: refs/heads/master，说明当前分支时 master，但是为什么查询分支没有结果呢？ 打开 .git\refs\heads 目录，发现这个文件夹下根本没有 master文件，其实想想也对，Git 中的分支其实对应着 commit id，现在什么都没有提交，master 也就找不到 commit id，所以就是有 master 文件，里面也不知道写什么。 查询远程仓库12345albert@homepc MINGW64 /d/gitstart (master)$ git remote -valbert@homepc MINGW64 /d/gitstart (master)$ 依旧什么内容都没有，说明还没有和远程仓库建立联系。 与远程仓库建立对应关系12345678910albert@homepc MINGW64 /d/gitstart (master)$ git remote add origin git@gitee.com:myname/gitstart.gitalbert@homepc MINGW64 /d/gitstart (master)$ git remote -vorigin git@gitee.com:myname/gitstart.git (fetch)origin git@gitee.com:myname/gitstart.git (push)albert@homepc MINGW64 /d/gitstart (master)$ 这一步需要注意，origin看起来就是一个远程仓库的别名，代表着 git@gitee.com:myname/gitstart.git 这个代码仓库，刚刚提到过，这个远程仓库也可以是本地的，所以你添加git remote add origin d:/test 也是可以的，就表明 gitstart 的远程仓库是本地的 test 仓库。 第一个分支刚刚说过，现在本地库的状态有些特殊，实际上刚刚在码云上创建的 git@gitee.com:myname/gitstart.git 库也很特殊，他们都没有真正的分支，这时只要我们成功提交一次，创建一个commit id，就相当于初始化了master分支。 添加README文件12345678910111213albert@homepc MINGW64 /d/gitstart (master)$ echo "learn git branch command"&gt;README.mdalbert@homepc MINGW64 /d/gitstart (master)$ git add README.mdwarning: LF will be replaced by CRLF in README.md.The file will have its original line endings in your working directoryalbert@homepc MINGW64 /d/gitstart (master)$ git commit -m"add readme file"[master (root-commit) 3226b63] add readme file 1 file changed, 1 insertion(+) create mode 100644 README.md 查询当前分支123albert@homepc MINGW64 /d/gitstart (master)$ git branch -a* master 这次可以是出现了，分支为 master，前面的 * 表示为当前分支。 将分支推送到远程仓库12345678910albert@homepc MINGW64 /d/gitstart (master)$ git push -u origin masterEnumerating objects: 3, done.Counting objects: 100% (3/3), done.Writing objects: 100% (3/3), 248 bytes | 248.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)remote: Powered by GITEE.COM [GNK-3.8]To gitee.com:myname/gitstart.git * [new branch] master -&gt; masterBranch 'master' set up to track remote branch 'master' from 'origin'. 至此，本地仓库和远程仓库就建立了联系，下面可以开始学习 Git 分支命令了。 分支操作新建分支新建分支可以使用 git branch branch_name 命令，以下就是一个创建名为 release 分支的命令： 12albert@homepc MINGW64 /d/gitstart (master)$ git branch release 也可以使用 git checkout -b branch_name 来创建一个新分支，创建完会自动切换到新分支： 123456albert@homepc MINGW64 /d/gitstart (master)$ git checkout -b devSwitched to a new branch 'dev'albert@homepc MINGW64 /d/gitstart (dev)$ 切换分支这是一个很奇怪的命令，命令格式为 git checkout branch_name，总感觉 checkout 子命令包揽了不属于自己的工作，如果在git branch的基础上加一个参数会更合理的一点，但这和切换分支的实际含义可能还有关系，切换分支其实就是修改HEAD文件中的 commit id，而没有真正的发生切换。 12345678910albert@homepc MINGW64 /d/gitstart (dev)$ git checkout releaseSwitched to branch 'release'albert@homepc MINGW64 /d/gitstart (release)$ git checkout devSwitched to branch 'dev'albert@homepc MINGW64 /d/gitstart (dev)$ 查看本地分支像刚才我们创建的 release 分支和 dev 分支都是在本地创建的，这样的分支通过 git branch 命令就可以查看 12345albert@homepc MINGW64 /d/gitstart (dev)$ git branch* dev master release 这样就列举了本地的所有分支，在当前分支名字 dev 前面哈还有一个 * 作为标记 查看远程分支只要在上面的命令基础上加上 -r 参数就行了 123albert@homepc MINGW64 /d/gitstart (dev)$ git branch -r origin/master 查询到的分支只有 origin/master 一个，这个分支是一开始我们进行第一次提交产生 master 分支之后，通过 git push -u origin master 推送到远程仓库的，所以现在只有一个。 查看所有分支所有分支包括本地分支和远程分支，将 -r 参数换成 -a 参数就可以了 123456albert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master release remotes/origin/master 将本地分支推送到远程仓库其实之前已经操作过了，可以试着复习一下，git push -u origin branch_name，其实这是一个简写，-u 可以写成 --set-upstream 表示设置上游分支，其实就是和远程仓库的分支建立联系。 branch_name 也是 local_branch_name:remote_branch_name的一种简写，冒号前表示本地分支，冒号后面表示远程分支，如果只写一个就表示两个分支名相同，远程仓库中如果没有这个分支就会新建一个。 也就是说 git push -u origin dev 和 git push--set-upstream origin dev:dev 是一样的，下面来试一下，然后查看一下分支： 1234567891011121314151617albert@homepc MINGW64 /d/gitstart (dev)$ git push -u origin devTotal 0 (delta 0), reused 0 (delta 0)remote: Powered by GITEE.COM [GNK-3.8]remote: Create a pull request for 'dev' on Gitee by visiting:remote: https://gitee.com/myname/gitstart/pull/new/myname:dev...myname:masterTo gitee.com:myname/gitstart.git * [new branch] dev -&gt; devBranch 'dev' set up to track remote branch 'dev' from 'origin'.albert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master release remotes/origin/dev remotes/origin/master 冒号前后的米名字是不是一定相同呢？完全没有必要，我们可以让本地的 release 分支对应远程的 master 分支，只不过这样怪怪的，但是操作上完全可以的。 12345678albert@homepc MINGW64 /d/gitstart (dev)$ git checkout releaseSwitched to branch 'release'albert@homepc MINGW64 /d/gitstart (release)$ git push -u origin release:masterEverything up-to-dateBranch 'release' set up to track remote branch 'master' from 'origin'. 查看本地分支与远程分支对应关系这个也是刚刚知道的，可以使用 git branch -vv 命令，注意是两个 v: 12345albert@homepc MINGW64 /d/gitstart (release)$ git branch -vv dev 3226b63 [origin/dev] add readme file master 3226b63 [origin/master] add readme file* release 3226b63 [origin/master] add readme file 执行这个命令之后可以看出，本地的 master 和 release 分支都对应着远程的 master 分支 删除本地分支我们先复习一下新建分支，然后把它推送到远程仓库，再使用 git branch -d branch_name 命令进行删除 12345678910111213141516171819202122albert@homepc MINGW64 /d/gitstart (release)$ git checkout -b feature_testSwitched to a new branch 'feature_test'albert@homepc MINGW64 /d/gitstart (feature_test)$ git push origin feature_testTotal 0 (delta 0), reused 0 (delta 0) remote: Powered by GITEE.COM [GNK-3.8]remote: Create a pull request for 'feature_test' on Gitee by visiting:remote: https://gitee.com/myname/gitstart/pull/new/myname:feature_test...myname:masterTo gitee.com:myname/gitstart.git * [new branch] feature_test -&gt; feature_testalbert@homepc MINGW64 /d/gitstart (feature_test)$ git branch -a dev* feature_test master release remotes/origin/dev remotes/origin/feature_test remotes/origin/master 开始删除分支，删除之前记得切换到别的分支，否则删除不成功 1234567891011121314151617albert@homepc MINGW64 /d/gitstart (feature_test)$ git checkout devSwitched to branch 'dev'Your branch is up to date with 'origin/dev'.albert@homepc MINGW64 /d/gitstart (dev)$ git branch -d feature_testDeleted branch feature_test (was 3226b63).albert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master release remotes/origin/dev remotes/origin/feature_test remotes/origin/master 删除远程分支通过上面的操作我们发现只删除了本地的分支，远程的分支还在，要想删除远程分支，需要使用 git push origin --delete branch_name 命令 12345678910111213albert@homepc MINGW64 /d/gitstart (dev)$ git push origin --delete feature_testremote: Powered by GITEE.COM [GNK-3.8]To gitee.com:myname/gitstart.git - [deleted] feature_testalbert@homepc MINGW64 /d/gitstart (dev)$ git branch -a* dev master release remotes/origin/dev remotes/origin/master 这次再查看时发现远程分支也被删掉了。 获取远程主分支到本地其实 Git 的克隆命令默认就是把远程仓库的主分支下载到本地，我们可以使用 git clone 远程地址 本地文件夹 命令来克隆一个仓库，如果本地文件夹省略，则默认新建一个与仓库名相同的文件夹： 1234567891011121314151617albert@homepc MINGW64 /d$ git clone https://gitee.com/myname/gitstart.git gitstartcopyCloning into 'gitstartcopy'...remote: Enumerating objects: 3, done.remote: Counting objects: 100% (3/3), done.remote: Total 3 (delta 0), reused 0 (delta 0)Unpacking objects: 100% (3/3), done.albert@homepc MINGW64 /d$ cd gitstartcopy/albert@homepc MINGW64 /d/gitstartcopy (master)$ git branch -a* master remotes/origin/HEAD -&gt; origin/master remotes/origin/dev remotes/origin/master 获取远程其他分支到本地从上面命令执行后的结果来看，当前本地仓库中只有 master 分支，其他的分支都是在远程仓库上，这时可以用 git checkout branch_name 命令来下载远程分支： 1234567891011121314151617albert@homepc MINGW64 /d/gitstartcopy (master)$ git checkout devSwitched to a new branch 'dev'Branch 'dev' set up to track remote branch 'dev' from 'origin'.albert@homepc MINGW64 /d/gitstartcopy (dev)$ git branch -a* dev master remotes/origin/HEAD -&gt; origin/master remotes/origin/dev remotes/origin/masteralbert@homepc MINGW64 /d/gitstartcopy (dev)$ git branch -vv* dev 3226b63 [origin/dev] add readme file master 3226b63 [origin/master] add readme file 看到这里可能会疑惑了，git checkout branch_name 不是切换分支的命令吗？实际上当 branch_name 分支在本地不存在而远程仓库存在时，这个命令与 git checkout -b &lt;branch&gt; --track &lt;remote&gt;/&lt;branch&gt; 含义相同，会在本地新建一个分支，并与远程分支建立联系。 常用集合 新建分支：git checkout -b branch_name 切换分支：git checkout branch_name 查看分支：git branch -a 删除分支：git branch -d branch_name 推送分支到远程：git push origin branch_name 删除远程的分支：git push origin --delete branch_name 拉取远程分支到本地：git checkout branch_name 查询分支的对应关系：git branch -vv 总结 以上这些命令都是在本地测试过的，可能考虑的不太全面，不过没关系，以后的分支操作还会补充到这里。 这些命令在有些特殊的情况下使用可能会遇到问题，如果大家发现了问题请及时指出，我会尽快修改的。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>branch</tag>
        <tag>checkout</tag>
        <tag>push</tag>
        <tag>remote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挥一挥衣袖，开始一段新的旅程]]></title>
    <url>%2Fblog%2F2020%2F02%2F16%2F%E6%8C%A5%E4%B8%80%E6%8C%A5%E8%A1%A3%E8%A2%96%EF%BC%8C%E5%BC%80%E5%A7%8B%E4%B8%80%E6%AE%B5%E6%96%B0%E7%9A%84%E6%97%85%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[悄悄的我走了，正如我悄悄的来；我挥一挥衣袖，不带走一片云彩。 前言钱钟书老先生在《围城》中说道:“天下只有两种人。比如一串葡萄到手，一种人挑最好的先吃，另一种人把最好的留到最后吃。照例第一种人应该乐观，因为他每吃一颗都是吃剩的葡萄里最好的；第二种人应该悲观，因为他每吃一颗都是吃剩的葡萄里最坏的。不过事实却适得其反，缘故是第二种人还有希望，第一种人只有回忆”。 而我在反思自己时却发现，无法将自身完全归于这两种人的一类，如果非要选一种，我更像是老先生提到的第二种人，总喜欢把最好的留到最后。按理说这样的人应该总是向前充满希望的，但是我却热衷于收集回忆，记录生活中的点点滴滴，认真记下生活中的每一笔支出，写下人生中一次次感动… 其实一开始我并没有这方面的爱好，不知从何时起，儿时的记忆渐渐和梦境中的画面杂糅在了一起，有些事情已经分不清是之前确实发生的，还是曾经在梦境中悄悄的来到过，所以慢慢地我养成了这样的习惯，记录生活中一切想要被回忆的事情，期待着有一天能与对此感兴趣的人一起分享这些点点滴滴。 缘由技术博客中很少写自己的生活，这一次其实也和技术有关，在一个工作岗位上勤勤恳恳工作六年之后，今年终于鼓足勇气决定出来找找新的机会，确定了新的目标之后回头看看这六年收获了很多，同时在这段找工作的经历中也学会了不少东西，新的工作基本定下来了，现在总结一下工作以来的经历以及面试中遇到的问题，方便后续复盘时能有个参照。 懵懵懂懂的6年前走出校门从大四学期便开始走出校门，一切按照教学大纲进行着，大四这年是企业实训，我们被安排到一家机构进行学习，学习结束已经13年的深冬，我们一群小伙伴作为花朵开始走出曾经的温室。 其实从实训的后半段我们已经开始在北京各大高校“流窜”，参加了很多不请自来的校招，进行了一轮轮的笔试，但很少有人从中得到满意的工作机会，我也尝试过几次，但是感觉自己真的很渺小，经过努力得到了一个大厂的面试邀请，我怀着激动的心情前去面试，走的时候还换上了自己都觉得怪异的正装，好在面试官并不在意我的这份不自然，完全投入身心开始进行面试。 很幸运我通过了一面，但是在去参加二面的路上我才发现后背已经被汗水浸湿了，之前也参加过几次面试，但是这一次确实是让我身心俱疲，从中也渐渐体会到了不同公司之间的技术差距，我一心想加入这个团体，但是二面的结果又把我拉回了现实，二面的过程很糟糕，有一道题我至今还记得，那就是关于数据库的连接，但是回答的很模糊，由于自己的知识储备不足，我怀疑自己当时连题目都没有弄懂。 走进社会大厂失利后，开始寻找其他的机会，毕竟工作是现在的第一要务，放弃了保研机会一心想着早点参加工作，如果连工作都找不到岂不是让人笑话了，时间不久便通过了几家面试，其中有意向可以试试的有两家，一家是做偏硬件的软件，另一家就是做游戏开发的，工资待遇差不多，相比较而言第一家要高一些，但是当时沉迷于Dota的我经过“深思熟虑”之后，委婉的谢绝了第一家的邀请，进入了这家游戏公司，也就是后来我工作了6年的公司。 开始工作我的人生很幸运，我一直这样觉得，在这里我碰到了我工作的中的第一位导师，都说师傅领进门，修行在个人，那么他就是我进入社会环境的那个师傅，但是他比我也大不了几岁，我更愿意称呼他为兄长。 时光荏苒，岁月如梭，即便转眼已经过去了6年多，但是工作第一天他让我修改的第一个BUG我至今还记得，那是一把“罪恶坑的钥匙”，BUG具体的细节并不重要，而是他处理的方式让我记忆犹新。 开发环境配置好之后，兄长便指给我一个BUG，让我尝试修改，便是那个“罪恶坑的钥匙”，第二天他就过来询问BUG的修改情况，我告诉他我的修改思路A，他说可以这样改，但是这种修改方式可能会给后面带来一些不利于扩展的问题，然后在他的电脑上给我看了他建议的修改思路B，然后让我按照这个思路去修改，我比较之后确实思路B更好一些。问题的关键是这个BUG他已经想好了修改方案，并且尝试修改过，他指定让我修改完全是为了帮我熟悉问题的处理方式而非完成工作。 之后也和一些其他领导沟通过工作，但是能这样带我入门的兄长就只此一个，其他人大多是就是完成工作即可，很少有人再想教我的什么东西了。我是幸运的，在我懵懂的年纪碰上了这样一位领路人，之后我们在项目组之间分分合合，但始终工作在同一个屋檐下。 勤勤恳恳的6年初入职场刚刚参见工作，一切都显得那么新鲜，经常会有这样的感叹：原来游戏中的这个功能是这样实现的！开始的时候对于工作的状态还是有点不适应，印象最深的就是下午的时候总是昏昏沉沉的，当时可以用“熬”这个字来形容，但是随着后面工作内容的铺开，大脑在紧张的处理这些问题时，犯困的毛病就改掉了。 工作之前写的项目很多是个人完成的，就是几个人合伙做一个项目，基本上也不太大，所有的代码也都很了解，但是刚接触这个游戏项目时感觉它太大了，所有的代码只能不断的搜索才能找到，仿照已有的功能开发了两个新功能之后，渐渐的找到了感觉。 很长一段时间之后再回过头来看自己的代码时会发出这样的感叹：这段代码是我写的吗？现在整个流程我已经清楚了，但是当时写这段代码的时候是怎么找到这里的。其实一开始写代码完全是照葫芦画瓢，很多语句不知道什么意思，但是功能类似，这样写完就可以用了。 当时还有一个情况就是开发环境是没有网络的，有问题不能上网去查，好在分配给我的没有太复杂的功能，依照原来的系统都可以完成，并且我喜欢做笔记，常用的那些代码实现都让我记在了本子里，有些还记了不止一遍，这些笔记我至今还留着，现在看起来显得过于幼稚，但却是我工作以来的痕迹。 渐入佳境工作一年以后，对整个游戏已经比较熟悉了，可以独自完成很多功能，稳定下来的游戏也逐渐对接多个平台开始蓬勃发展，那时的我真的是干劲十足，每天像打了鸡血一样，作为服务器开发的我开始偶尔“插手”客户端的开发工作。 期间还养成了每天读书的习惯，其实这个习惯的养成是被动的，原因是分配给我的电脑比较卡，我提过几次但是一直没有换的机会，每天早上开机至少得10分钟左右才能正常顺畅的工作，所以后来我一般会早来一会，开机这期间我就会把旁边同事的书拿过来看看，后来同事的书看完了，我就买一些相同类型的技术书籍来看，再后来开始扩展知识面，买一些流行技术的书，这个习惯就一直保留了下来，一直到现在每年都会看几本技术书籍，有些是不朽的经典，有些是新进的技术。 其实很多书的内容我只是有大概的印象，具体的内容早就忘记了，偶有几本书感觉有意思会回过头来再次翻看，每次看都会有不同的感受，我喜欢在纸质书上做笔记，想到什么就写什么，有些章节会被我划的很乱。在我看书的时候总有同事问我，你看那么多书都记得吗？都学会了吗？这时我常常会自嘲一般的回答：“看着玩而已，早都不记得了”。 实际上记不记得重要吗？今天吃了有营养的东西，明天依旧会饿，你会因为明天还会吃饭就放弃今天的美食吗？我想不会的吧，我感觉看书也是一样，我今天看了明白了一些事情，或者读到一个故事感动了很久，明天忘了就忘了，毕竟我曾经学会过，曾经也感动过。这些东西会消失的无影无踪吗？我想也不会的吧，吃过的美食总会有一部分营养进入了我们的细胞，成为了肉体的一部分，而曾经读过的书会忘得一干二净吗？当然不会，那些使我们印象深刻的文字总会在未来的某个深刻，在我们的脑子中再次迸发出来。 再入蛮荒天下没有不散的筵席，参加工作时就参与开发的这个项目终于到了最后的维护阶段，这个阶段距离我刚进公司时已经过去了2年半的时间，此时原项目不再进行新的开发只进行必要的维护，原项目组的人也被分成了两部分，现在的有两个新项目，一个是相同技术栈不同玩法的端游项目，一个是紧追潮流的手游项目。我当时想去做手游，最终也确实分到了手游组，就是从这时起，我与之前的兄长分到了不同的项目。 事实证明这个公司向手游进军的项目确实是一条蛮荒的道路，整个技术链遇到了前所未有的挑战，我们一步步探索着前进的道路，试图越过一个个技术的深坑，而真实情况却是多少次我都陷在了里面。 在这个项目组我遇到了很多新的伙伴，有的乐观、有的开放、有的乐于奉献、有的精益求精，在这我看到了相同而又不同的服务器程序，之前的程序被改的面目全非，我又得重新适应，面对全新的客户端也有太多的新知识需要学习，每天必须打起十二分精神来应对工作。 一次次的否定自我，一次次的推到重建，在项目的紧要关头，升级引擎、重建UI、优化逻辑，最终还是把这款游戏送上了线，但事实却如昙花一般，一闪而过，失败了，我们没有做出成功的产品，仅仅是做了一次失败的尝试，此时距离进入这个项目组过了1年半的时间。 并入源头手游组的失败尝试使我有了到外面大世界看看的想法，就在这时，和我们一同开始的另一个项目组已经完成了一次轮回，很明显他们成功了，作为同时开始的两个项目，一成一败的比较对于我们的打击很大，而那个成功的项目组也进入维护阶段，领导决定合并两个项目组继续完成手游的开发，这使我又打消了出门找工作的念头。 两个项目虽然一成一败，但是各有优势，因为最终做的是手游，所以原来的手游组有技术优势，而另一个成功的项目有成功的游戏内容，两者一合并应该很快就能出一个产品，更重要的是，这两拨人有很多都是曾经的好友，好友联手打造一个游戏也是很有意思的事情。 可是理想很丰满，现实很骨感，事实证明做出一款游戏是多么的不容易，虽然两部分好友合并到一起没有什么磨合的问题，但是游戏内容的一次次修改不断冲击着之前制定的开发计划，整个开发计划不断修改，时间节点在不断修改的需求面前显得那么渺小，常常被无情的践踏。 终于看到胜利的曙光了，在不断调整了2年之后，游戏迎来了上线的的一天，之后开始根据线上反馈进行调整，在我看来游戏开发到这已经基本完成，虽然达不到爆款的要求，但终究是一款中规中矩的游戏，没有大的问题，也没有太闪光的点，我在这的修行也要告一段落了。 离开这里的一个导火索是游戏内容一次大的调整，本来现阶段不可能大面积修改功能了，可是在计划中还是出现了太多看不懂的修改内容，因为之前有了完成游戏就离开的想法，看到这里仿佛又要开启一个新游戏了，我也就没有再留下的必要了，是时候到外面的世界去看看了。 信心满满的6年后外出求索19年底，在第一家也是唯一一家公司呆了6年之后，我开始外出面试了，从一开始的信心满满到后面的发奋图强，我逐渐认识到了，我必须出来闯闯了，我在一个安逸的地方待了太久，虽然每天都在学习，但事实上优秀的人比你还要努力，以下简单介绍下面试过程，对于需要掌握的知识进行一个梳理，便于日常复习警醒自己，大概面了几家，以下按一面时间先后排列以下T、D、Y、W、Z、H：，全部以字母代替就不列出公司名了，有兴趣的可以进一步交流下。 T公司 很抱歉一开始把公司名看错了，当时还在想一家旅游公司怎么还做游戏，但是毕竟是第一家在招聘APP上给我发面试邀请的公司，怎么也要去看看，后来了解了一下这是一家主营棋牌类的公司，面试当天早早就来到了这家公司，顺便再楼下吃个了饭，面试开始先填个表格，内容跟查户口一样，我只填了其中的必要信息，接着做笔试题，包括后面几家面试，这是我唯一做的一套笔试题，内容不难，可能就是一个简单了解。 我还没写完面试官就来了，我看他特别像我初中的化学老师，整个过程很轻松，聊聊笔试题、曾经的项目，面试官还介绍了他们公司的情况，他表示了对我的肯定，问我有没有兴趣转Golang语言，我内心是拒绝的，其实嘴上也拒绝了，因为我一直使用C++，之后又是其他一个组的负责人来面试，他们使用的Python，整个过程依旧轻松加愉悦，还向他请教了分布式服务器的知识，最终因为我不想放弃C++而结束，他问原因是什么？我开玩笑说：可能是情怀吧！ 涉及到笔试面试部分内容，列举在此主要为了重温复盘，如果你想做游戏开发也可以看看这些知识： 不同类型数据内存占用大小 估算PC机上1秒钟可能执行的空的for循环次数 linux下常用搜索文件命令 常用的设计模式 字符串翻转 扑克牌中挑最长顺子 回旋排列矩阵 分布式服务器设计 linux中的lvs 服务器横向扩展 从头实现一个服务期 websocket 玩家背包怎样设计 D公司 这个公司完全是抱着学习的心态去的，因为公司本身很大并且不是做游戏的，来这家公司完全是因为他们的技术总监在招聘APP给我发了面试邀请，我本来觉得不合适，人家说可以来试试，抱着学习的态度我就去了，为了这次面试还看了好几个调度算法，最终也没用上。 本来10点半的面试，7点多我就出发了，期间地铁还坐反了，还好出门早，来到西二旗发现手机都没有信号，出门都骑不了自行车，走了很久才找到一辆，开锁出发一气呵成，9点多就到公司楼下了，旁边便利店买了个菜团子，对于干吃的我来说太大了，10点左右进入公司，大公司就是不一样，进门登记后还要贴一个签，这是怕我乱跑啊。 面试不久后进行，来了一个小哥哥，年纪应该不大，很沉稳的样子，带了一台笔记本电脑，这个好像是标配，提倡无纸化办公吧，我的一切反馈他都会记录在上面，整个过程对于他来说应该是轻松的，但是对于我来说有些窘迫，整个过程对我的评价就是，很多东西用的很熟，但是对于原理掌握的还不够，算是没有达到他们的要求，这也在我的意料之中，毕竟就是来学习的。涉及到的面试内容大概有如下问题： 开源项目源码的阅读情况 动态库加载路径 编译的过程 线程崩溃为什么会导致进程挂掉？一定会挂掉吗？ 加权最短路径 打印过程中出现中断 中断信号怎么处理 怎么理解多态 编译时多态和运行时多态 模板和基础类型的效率比较 gdb调试 为什么先构造基类 析构函数的调用顺序 非阻塞的write什么时候返回 如果连不上服务器会有那些情况 注意wireshark的使用 listen的backlog参数 Y公司 这个公司有自己成熟的产品线，涉及到卡牌、MMORPG等等，同样是在招聘APP上收到面试邀请，但这次招人的是一个SLG游戏组，整个给人的感觉无论是公司的氛围还是项目的情况与我当前公司很像，一共来公司面了两次，第一次两个技术Leader分别进行面试，然后又和HR聊了一下，技术面主要围绕曾经的项目，后来第二次面试跟游戏制作人聊了一下，感觉和之前的公司更像了，当时就打了退堂鼓，最终婉言谢绝了这家公司的Offer，面试主要技术内容： 技能设计 redis缓存 指针用法 二分法思想 项目熟练度 W公司 这个公司是游戏开发中的大厂了，首先是在招聘APP上，HR和我沟通之后要去了简历想要看看，后来收到了面试电话确定了面试时间，面试当天也是早早的来到了公司，这天在周围没有找到吃饭的地方，要饿着肚子了，等待了一会被HR小姐姐带去二楼等了半小时，后来她跑过来告诉我位置错了，确实有点尴尬。 之后我被带去了正确的位置，然后开始了面试过程，面试官是一个小哥哥，整个面试的过程感觉表现的不是很好，有些问题回答的不太完整，但是却从中学习到了很多东西，临走时问了几个面试问题的正解，并且冒昧的问了小哥哥的工作年限，得知才比我大两岁就已经在游戏大厂当主程之后，深感我们之间的差距还很大，同时也激起了我努力学习的意志。 面试后好几天也没有消息，本来我感觉这次面试可能失败了，但是几天后我又收到了该公司的二面邀请，收到邀请时挺高兴的，当时还有另外几家面试，之前已经约好了时间，所以这次二面不得不向后推了几天，因为是第二次去，路线熟悉了很多，又是早早来到公司，本来以为还是技术面，但是交流几个问题之后发现问的都是之前的项目，和人员之间的沟通的问题，后来对方主动说明他是项目负责人，整个聊天过程比较轻松，谈过之后让回去等消息。 第二天收到HR视频面试的邀请，本来想约晚一点回家好好面试，但是因为HR小姐姐还有其他安排，我只得将面试时间提前，在公司旁边找了个安静的地方进行视频面试，主要聊了一下目前的薪资待遇以及项目情况，能够入职的时间等等，整个过程很愉快，并且得知其实是一个工作室在招聘，我问了一些相关的问题，面试结束回到家我仔细考虑了这个机会，第二天又找该项目的负责人了解了项目的详细情况，觉得这是一个很好的学习机会，与目前的工作内容有很强的互补性，可以试一试。 Gitflow使用方法 gcc编译过程 extern和static的作用 多态、虚函数、多继承虚函数 大根堆创建和插入 排序找出接近当前数的较大数 迭代器的理解、迭代器都是指针吗？ 字符编码、unicode、utf8 指针数组、数组指针、函数指针 引用和指针的区别 网络4次挥手、为什么要4次？ 函数阻塞是否占用资源–挂起不占用 Z公司 这个公司不是游戏公司，近两年异常火爆，有专门的游戏部门，但是我面试的职位不是游戏岗位，而是时下非常火的中台岗位，其实是在尝试新的领域。起初是猎头在招聘APP上要了我的简历，然后接到了公司HR小姐姐的电话，约定了面试时间，相互加了微信，面试之前和HR小姐姐交流了不少，知道公司技术面大概有3面，因为心里没底，抱着学习的态度考虑能过两面就行，如果实在太难能过一面也行，作为一个求知者，知道各个公司都需要哪些知识也就有了学习的目标。 可现实总是太残酷，这个面试我算通过了半面，什么叫半面，由于我的“出色”表现，我感觉正常的一轮面试并没有结束就被礼貌的请出来了，因为几个问题之后我也感觉出来了，我之前学的技术和他们开发思想差的有点多，所以出于礼貌，面试官也没说什么，还给出了一些建议，人真的不错，大公司的涵养还是有的。 你开发的最满意的系统 –道具系统 map和hashmap的区别 stl的使用 vector的扩容，是不是线程安全的？ 遍历删除vector元素，迭代器失效 有没有用过redis的有序集合 redis中hash插入的时间复杂度 设计一个红包系统 –评价为原始的面向对象方式，有些过时 给出建议这个红包系统必须要考虑redis、分布式、容灾、备份 建议如果想转向互联网需要准备的很多，可以先看下现有的解决方案 H公司 这个公司的面试机会是猎头推荐的，主要做战争题材的游戏比较多，现在也有卡牌和休闲，本来约的面试时间比较早，但是由于个人原因回了次老家，结果这个面试不得不向后推了，面试当天来到公司，前台居然一个人都没有，之后电话联系到面试官，首先表达了之前改约的歉意开始了面试过程。 面试主要围绕之前的项目进行，对具体的系统实现问的很详细，通过对细节的了解，对我之前的工作内容有很多不理解，感觉有很多内容不符合他的认知，整个过程倒还轻松，没有太多的技术问题，总体感觉不是一路人，很可能走不到一起。 聊了大概一小时，换HR继续聊，还是问了之前项目、期望薪资以及入职时间等等，确定了是卡牌组再招人，问了一些当前公司情况之后，按照流程回去等消息，但个人觉得可能不太合适。 面向对象要求比较高，C+Class的方式不被认可 着重问了一个游戏系统的实现方式（押镖） 面相对象设计技能系统 强调游戏充值实现的重要性，以及可能出现的多种情况 认为只有DBA才有权利修改数据库结构 准备离开出去面试一圈基本确定了新的工作，是时候离开了，先跟带自己入门的兄长道个别，我们两个聊了很久，对于我离开去学习新知识，兄长表示支持，他不仅是我工作上的领路人，同时脾气特别好，平时处理问题也很妥当，一直是我学习的榜样。 紧接着便向老大提出了离职，准备年前离职后去新公司入职，而老大的意思是再等等，年前太仓促了，先看看现在公司的情况，年后回来如果还想走再办离职吧，考虑到还有一段时间的就要放假了，为了更好的完成了交接工作，我答应了老大的请求。 即将离开过年期间考虑了好久，还是准备出去闯一闯，今年春节的新型冠状病毒疫情非常严重，很多公司都推迟了上班时间，虽然2号之后就回来上班了，但是很多同事由于封路的原因都还没回来，离职手续也一直没有办成，年后又找老大聊了一次，毕竟工作了6年，虽然不舍，但是确实该离开了，期待下周的情况能好一些，能顺利办完手续开始新的旅程。 挥挥手再出发 更新于2020年2月15日22:45:51 挥手告别事情办得比较顺利，经过前期的准备，周一便完成了工作交接，上传了交接文档，周二开始办理离职手续，由于新型冠状病毒疫情的原因，公司依旧没有什么人，好在办理离职的人员都在，签字、签字、再签字，成功在下午拿到离职证明，不过唯一遗憾的是，工牌和门禁卡同时上交了，不能给我留个纪念了，毕竟是在身上装了6年的工牌，6年了几乎没有离开过…… 因为很多同事也没来，加之疫情的严重性，散伙饭并没有吃成，前一天下班的时候专门去旁边的簋街转了一圈，发现除了几家仅有的外卖之外，都是黑着灯的，这可是簋街啊，是让人们可以排队等到凌晨2点的簋街，现在居然这样冷冷清清的，找不到吃饭的地方，散伙饭只能作罢。 下班前跟仅有的几个来上班的好友道了别，当然其中还有我那位可敬的兄长，当所有人都在关心你飞的高不高时，只有朋友关心你累不累，兄长就是这样的朋友，临走了还关心地问我社保能不能接上，只因为我之前和他提过一次担心社保断缴的问题。因为很多人还没来上班，剩下的关系好的小伙伴，在我晚上回家后，通过微信开始了与他们的远程云分别。 还看今朝告别了过去的工作，自然要步入新的旅程，为尽量避免人员接触，新的公司在周五为我在线办理了入职手续，两位帮忙办理入职的新同事真的非常友好，整个流程遇到不懂的都会及时解答，这让我非常期待下周一正式工作后的生活，新的旅程即将开始，又要在一个地方生根发芽了~]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用__declspec(dllexport)和__declspec(dllimport)在Windows平台编写和使用DLL的小例子]]></title>
    <url>%2Fblog%2F2020%2F02%2F05%2F%E5%88%A9%E7%94%A8-declspec-dllexport-%E5%92%8C-declspec-dllimport-%E5%9C%A8Windows%E5%B9%B3%E5%8F%B0%E7%BC%96%E5%86%99%E5%92%8C%E4%BD%BF%E7%94%A8DLL%E7%9A%84%E5%B0%8F%E4%BE%8B%E5%AD%90%2F</url>
    <content type="text"><![CDATA[前言关于 __declspec(dllexport) 和 __declspec(dllimport) 这两个关键字在上大学期间就没见过几次面，直到毕业后在公司项目的代码中又遇到过几次，每次也是绕着走，生怕和它产生什么联系，只知道它和动态链接库 DLL 有关，但是当前这个项目中几乎没有用到自己写的动态链接库，所以我也就心安理得的躲了它这么久。 最近看一些开源项目的源码时又发现了这两个关键字，此时凭借自己掌握的知识和学习方法再来看这两个关键字，发现也没有什么值得害怕的地方，其实简单来说就是 __declspec(dllexport) 是用来说明指定类和函数需要从 DLL 中导出的，而 __declspec(dllimport) 是用来说明指定的类和函数是从DLL中导入的。 说明 __declspec(dllexport) 和 __declspec(dllimport) 只在 Windows 平台才有，用来说明类或函数的导出和导入。 在 Linux 平台上源文件中的所有函数都有一个的visibility属性，默认导出。如果要隐藏所有函数导出，则需要在GCC编译指令中加入 -fvisibility=hidden 参数。 生成 dll 的同时还会生成对应的 lib 文件，一般是一些索引信息，记录了 dll 中函数的入口和位置，这在之前还真的不知道，原来一直以为 lib 只是静态库文件呢。 疑问 为什么要导入导出，直接把代码拿过来一起编译不好吗？ 想要一起编译前提是你得有源代码，如果人家就给你一个动态库或者静态库，你想把源代码放到一起编译的愿望根本实现不了。 为什么要分为静态库和动态库？搞这么麻烦，还要导入导出。 这具体的就要查查他们两者的优缺点了，每种事务的产生必要有其产生的原因，比如静态库，很可能就是一个程序员今天在A工程写了一个读取文件的类，过一段时间又在B工程写了一个读取文件的类，代码都差不多，不久又在C工程中直接把代码复制过来改一改又写了一份，这时想到干脆了写个“静态库”这种东西吧，相同的代码直接封装到库中，哪个工程需要就直接拿过来编译，也不需要再复制代码了。 又比如动态库，前面的静态库解决了代码重复开发和维护的问题，但是读取文件的静态库中的代码在A、B、C三个工程中都存在一份，导致每个可执行程序都很大，可不可以共用一份呢？结果又发明了动态库，在编译时只指定函数的入口地址，运行时才加载动态库，这样就使得可执行程序体积大大缩小。 以上内容纯粹我个人想像的，真正发明静态库和动态库是由于什么原因，大家可以自行去了解… 动态库要比静态库好吗？ 个人感觉合适的才是最好的，不存在动态库要比静态库好的说法，最起码不是全都好，动态库的代码是在可执行程序运行时才载入内存的，在编译过程中仅简单的引用，因此代码体积较小，但是运行时要去加载库会花费一定的时间，执行速度相对会慢一些，总的来说静态库是牺牲了空间换时间，而动态库是牺牲了时间换空间。 .h（头文件） .lib（库文件） .dll（动态链接库文件） 之间的联系和区别 .h 文件是编译时需要的， .lib 是链接时需要的， .dll 是运行时需要的。如果有 .dll 文件，那么 .lib 一般是一些索引信息，记录了 .dll 中函数的入口和位置，.dll 中是函数的具体的执行内容。如果只有 .lib 文件，那么这个 .lib 文件是静态编译出来的，索引和实现都在文件中。 DLL的编写与使用前面说了这么多，其实就是想带大家先了解一下动态链接库 DLL ，接下来开始编写一个DLL并在另一个工程中使用它，前提是你已经会使用开发工具VS，如果不会先查查教程。 测试环境 VS2013随意版（个人感觉这个版本启动能快一点） Win10畅想版（我也不知道啥版本） 编写DLL编写 DLL 的方法不知一种，这里只简单介绍一种，对于直接写 .def 文件的方法这里不会展开，尽量依靠开发工具一步步向下执行就好，其实当你理解了开发工具的是怎样工作的，一切就没有那么神秘了，有些步骤直接修改配置文件也是可以实现的，只不过开发工具给我们提供了界面，操作起来更加方便了而已，下面我们开始编写： 打开VS新建项目，选择Win32项目，项目名称GenDLL，解决方案名称DLLExample，点击确定： 直接下一步，应用程序类型选择DLL，点击完成： 项目会自动创建一个GenDLL.cpp文件，我们在手动创建一个GenDLL.h文件，两个文件中编写如下代码： 123456789// GenDLL.h#ifdef GENDLL_EXPORTS#define TEST_API __declspec(dllexport)#else#define TEST_API __declspec(dllimport)#endifTEST_API int add(int a, int b); 123456789// GenDLL.cpp : 定义 DLL 应用程序的导出函数。#include "stdafx.h"#include "GenDLL.h"TEST_API int add(int a, int b)&#123; return a + b;&#125; 这段代码中有一个 TEST_API 是我在头文件中自定义的，当存在GENDLL_EXPORTS宏时， TEST_API 代表 __declspec(dllexport) 也就是导出函数，当不存在GENDLL_EXPORTS宏时， TEST_API 代表 __declspec(dllimport) 表示导入函数，而 GENDLL_EXPORTS 这个宏是与项目名相关的，自动生成的宏，在 DLL 项目中存在格式为 “大写项目名_EXPORTS”。 也就是说同一个头文件中计算加法的函数 add 在 GenDLL 这个生成 DLL 的项目中表示导出函数，在其他使用这个 DLL 的项目中表示导入函数。 编译看输出发现有GenDLL.lib和GenDLL.dll两个文件： 123456781&gt;------ 已启动生成: 项目: GenDLL, 配置: Debug Win32 ------1&gt; stdafx.cpp1&gt; dllmain.cpp1&gt; GenDLL.cpp1&gt; 正在创建库 c:\users\administrator\documents\visual studio 2013\Projects\DLLExample\Debug\GenDLL.lib 和对象 c:\users\administrator\documents\visual studio 2013\Projects\DLLExample\Debug\GenDLL.exp1&gt; GenDLL.vcxproj -&gt; c:\users\administrator\documents\visual studio 2013\Projects\DLLExample\Debug\GenDLL.dll========== 生成: 成功 1 个，失败 0 个，最新 0 个，跳过 0 个 ========== 使用DLL 在DLLExample这个解决方案下添加一个新项目，命名为UseDLL，然后点击确定： 直接下一步，应用程序类型选择“控制台应用程序”，点击完成： 在文件UseDLL.cpp文件中引用之前GenDLL项目的头文件，编写使用 add 函数的代码： 123456789101112// UseDLL.cpp : 定义控制台应用程序的入口点。#include "stdafx.h"#include &lt;iostream&gt;#include "../GenDLL/GenDLL.h"int _tmain(int argc, _TCHAR* argv[])&#123; std::cout &lt;&lt; "100+1=" &lt;&lt; add(100, 1) &lt;&lt; std::endl; system("pause"); return 0;&#125; 编译代码发现报错，提示有一个无法解析的外部命令： 1234567891&gt;------ 已启动生成: 项目: UseDLL, 配置: Debug Win32 ------1&gt; UseDLL.cpp1&gt; stdafx.cpp1&gt; 正在生成代码...1&gt;UseDLL.obj : error LNK2019: 无法解析的外部符号 &quot;__declspec(dllimport) int __cdecl add(int,int)&quot; (__imp_?add@@YAHHH@Z)，该符号在函数 _wmain 中被引用1&gt;c:\users\administrator\documents\visual studio 2013\Projects\DLLExample\Debug\UseDLL.exe : fatal error LNK1120: 1 个无法解析的外部命令========== 生成: 成功 0 个，失败 1 个，最新 0 个，跳过 0 个 ========== 提示这个错误本意就是说链接没有找到函数实现，链接需要什么文件，前面提到需要lib文件，那么我们设置一下，让UseDLL工程能够找到GenDLL.lib文件。 打开UseDLL工程的属性，在“配置属性-&gt;链接器-&gt;输入-&gt;附加依赖项”中添加GenDLL.lib: 然后在“配置属性-&gt;链接器-&gt;常规-&gt;附加库目录”中添加GenDLL.lib所在路径“../Debug”即可成功编译： 直接运行就可以看到调用DLL的结果，因为这两个工程在同一解决方案下，所以最终UseDLL.exe和GenDLL.dll在同一目录下，这样不会报找不到DLL的错误 如果是不同的目录就会像下图那样，提示找不到GenDLL.dll，只要把GenDLL.dll复制到和UseDLL.exe相同目录即可： 加载DLL上面提到当运行程序找不到 DLL时可以把 DLL 放到可执行程序程序的目录，有时运行大型软件找不到 DLL 时，我们也会下载一个放到System32目录，其实程序在加载 DLL 的时候是会按照一定顺序的，这些目录包括：包含exe文件的目录、进程的当前工作目录、Windows系统目录、Windows目录、Path环境变量中的一系列目录等等，这些目录的搜索顺序还会受到安全 DLL 搜索模式是否启用的影响。 所以说如果不是对DLL 放置的位置有特殊要求，那么直接放在exe文件所在的目录就好了，一般也是会优先搜索的。 总结 Windows上才有 __declspec(dllexport) 和 __declspec(dllimport) .h 文件是编译时需要的， .lib 是链接时需要的， .dll 是运行时需要的 程序运行时加载 DLL 一般优先从exe文件的所在目录优先加载]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>dllexport</tag>
        <tag>dllimport</tag>
        <tag>DLL</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020年的春节，我们一起抗击新型冠状病毒]]></title>
    <url>%2Fblog%2F2020%2F01%2F29%2F2020%E5%B9%B4%E7%9A%84%E6%98%A5%E8%8A%82%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E6%8A%97%E5%87%BB%E6%96%B0%E5%9E%8B%E5%86%A0%E7%8A%B6%E7%97%85%E6%AF%92%2F</url>
    <content type="text"><![CDATA[终于到了什么都不用做，在家躺着就能为国家做贡献的时候了！ 前言新型冠状病毒，一个看起来陌生的词语，使得原本最热闹的春季变得异常冷清，随着疫情范围的扩大，这个本来陌生的词语一次次冲击着人们的认知。这个病毒到底是什么，为什么扩散起来这么凶猛？ 2019-nCoV新型冠状病毒，在各种媒体上还会以“2019-nCoV”的名字出现，人感染了冠状病毒后常见体征有呼吸道症状、发热、咳嗽、气促和呼吸困难等。在较严重病例中，感染可导致肺炎、严重急性呼吸综合征、肾衰竭，甚至死亡。 引用百度百科中文字，对其描述为： 2019新型冠状病毒，即“2019-nCoV”，因2019年武汉病毒性肺炎病例而被发现，2020年1月12日被世界卫生组织命名。冠状病毒是一个大型病毒家族，已知可引起感冒以及中东呼吸综合征（MERS）和严重急性呼吸综合征（SARS）等较严重疾病。新型冠状病毒是以前从未在人体中发现的冠状病毒新毒株。 目前掌握的情况： 传染源: 野生动物，可能为中华菊头蝠 传播途径: 经呼吸道飞沫传播，亦可通过接触传播 易感人群: 人群普遍易感。老年人及有基础疾病者感染后病情较重，儿童及婴幼儿也有发病 潜伏期: 1 ~ 14 天，平均 10 天，潜伏期内存在传染性 2019-nCoV与SARS这个新型冠状病毒导致的肺炎传播速度如此之快，很多人拿它和03年的非典（SARS）相比，确实这两个病毒有很多相似的地方，同样都是新型冠状病毒，同样都会引起肺炎，甚至连发生的时间都非常相似，非典是02年11月出现病例，而2019-nCoV出现的时间大概是12月。 那它们两个这么像，能不能用相同的方法和药物治疗呢？目前来看是办不到的，两者虽然很相似，但是毕竟都是新型病毒，我们知道一种病毒变异后原来的药物很可能就起不到作用了，更何况这是两种不同的病毒，但是也有好的一面，毕竟在抗击非典时我们积累了宝贵的经验，对于防控类似的疾病能够提供很大的帮助。 比如当年非典时期,北京市在小汤山就建立起了一座封闭式的医院,就是小汤山医院，而武汉参照北京“小汤山模式” 神速建造了“火神山医院”，从开工到投入使用预计会花费10天左右，这个速度也是令人惊叹了，没有之前的经验积累是很难办到的。 为什么传播的这么快其实一开始我们都没有重视这场战斗，导致这个新型冠状病毒钻了空子，传播速度之快达到了让人心惊的地步，感觉主要有下面几方面的原因吧： 华南海鲜市场存在大量新型冠状病毒，源头上就很广 起初没有得到足够重视，认为不存在人传人的可能，导致接触者甚至医务人员被感染 病毒在潜伏期也有可能传播，这是与非典不同的，导致一些携带病毒的人在无意识的情况下成了传染源 正好赶上春节返乡高峰，而武汉作为九省通衢的枢纽，反倒为病毒散播提供了便捷的条件，扩散范围很快就达到了全国 目前的形式新型冠状病毒疫情已经开始进入初期扩散阶段，并呈上升趋势，但是应对措施也已经铺开，延长假期，控制人员流动，积极研制疫苗，组织医疗救援队赶赴武汉等等，相信不久疫情就能够控制住。 本来春节是一年中最忙碌的日子，今年却异常的冷清了，为了大家的健康，今年周围的人都取消了拜年聚会活动，村口也安排了人专门劝返探亲人员，因为这样我们反而多了一些陪伴家人的时间，而我居然有时间来码字了，往年不是在这喝酒就是在那聚会的，现在这样安安静静的待在家里感觉也不错。 引用网上一段顺口溜，写的不错与大家分享： 国家有难，咱不添乱。坐在家里，就是贡献。亲戚不走，来年还有。朋友不聚，回头再叙。利人利己，互不传染。吃好睡好，悠闲过年。坚持几天，你我平安。 新型冠状病毒最新消息目前新型冠状病毒处于蔓延的趋势，各种消息满天飞，真真假假难以辨认，所以我单独建了一个项目用来收集最新的消息，尽可能保证消息的准确，其中包含最新的疫情地图、最新疫情新闻、以及正规的捐助渠道等等，有兴趣的小伙伴可以一起舔砖加瓦。 抗击2019-nCoV最新情报-ChineseVictory 更新于2020年1月29日21:25:30 今天看到一个同类型的记录武汉抗击新型冠状病毒的项目，已经有3000多的star了，我们两个项目创建的时间很接近，再看看我的项目情况有点惨淡啊！贴个图，大家感兴趣可以来逛逛，不过人家那个项目确实很规范，我还有很多东西可以学习。 从star数为0来看确实惨淡，再放一遍项目地址-ChineseVictory，感兴趣可以来看看;)]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>病毒</tag>
        <tag>武汉</tag>
        <tag>春节</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019！一份迟到的年终总结]]></title>
    <url>%2Fblog%2F2020%2F01%2F03%2F2019%EF%BC%81%E4%B8%80%E4%BB%BD%E8%BF%9F%E5%88%B0%E7%9A%84%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[长大了就要为小时候吹过的牛而奋斗~ 前言2019即将过去，瞅一眼桌面右下角的时间，距离2020年还有63分51秒，这是我第一次这么强烈地想在一年结束之际写下点什么。本身是一个喜欢收集和总结知识的性格，但对自己的人生却少有总结，一方面感觉没什么可写，一方面也确实对自己太过宽容。 每年年初总是在朋友圈和各大平台浏览着一个个新年展望，而每年年末总是在相同的位置看着他们把年初展望的日期向后推一年，这是个段子，同样也是事实，很多人的生活过得平平淡淡，我们都是人群中的大多数，平庸而碌碌无为。 迟到之所以在年终总结前冠以“迟到”二字，是因为我突然意识到这份总结早就应该做了，对于参加工作已经6年的我来说，刚刚意识到需要做年终总结确实有些不应该。作为普通人的我来说记忆能力有限，小时候吹过的牛早就忘了，如果能及时的做年终总结，或许我还可以为了去年吹过的牛奋斗一把，可是我没有，我连去年的想法也忘得差不多了，此时此刻才刚刚意识到曾经的失误。 得与失既然是总结就要回想一下在过去的一年中我得到了什么，失去了什么，而在新的一年中我想获得什么，回想即将过去的2019年发现，今年确实发生了很多往年没有发生过的事情，这可能也是我突然非常想写点东西，记录下来的原因。 回顾2019这一年发生的事情太多，相互之间纠葛不断，不过还是从最简单的分类：工作、学习、生活这三个方面来聊聊吧，虽然很多事情不能完全归为某一类，但是贴一个标签总能清楚一点。 工作上依旧是踏踏实实，勤勤恳恳的一年，做了整整一年的游戏开发几乎颗粒无收，这已经不是第一年没有收成了，有时候真的有点后悔为了工作侵占了陪家人的时间，特别是看不到回报的时候。 从年初就开始做收尾工作，几次上线几次调整，不知不觉我们又过了一个年，在我心中这款游戏的开发工作已经接近尾声，这样的状态不应该再持续下去了。 学习上作为一个好学的程序猿，深知“学如逆水行舟，不进则退”的道理，今年在CSDN上写了42篇原创博客，算是高产的一年了，也终于迈进了总排名前一万名的大关，截个图记录一下： 有点小遗憾，访问量差几百才到50万，不过新年第一天应该差不多啦，不仅仅是知识的总结，由于加了CSDN的博客群，今年还认识了许多有意思的小伙伴，比如：铁柱同学（一个冒充小白的大佬）、第三女神（粉丝炸裂式增长）、TRHX（网站做的特漂亮），还有很多小伙伴就不一一列举啦。 关于读书，我只喜欢读纸质的书籍，喜欢那种在书上乱画，随便记笔记的方式，当然有一点不好，就是想查一个知识点，知道是哪一本书，不得不翻一翻才能找到，好希望纸质书能有个搜索按钮，不过这个问题找个电子版就能解决了。 今年一共读了7本关于编程技术的书籍： Redis入门指南(第2版) 图解HTTP 自动化平台测试开发 ——Python测试开发实践 图解密码技术 图解TCP/IP 漫画算法 ——小灰的算法之旅 MySQL必知必会 推荐这本《小灰的算法之旅》，可以把学知识当做一种乐趣，绝对能达到事半功倍的效果。 附上 我的完整书单 生活上今年在生活上发生的事情好像比之前几年加起来都要多，年中得到了一个特别可爱的宝宝，为了解决宝宝上学问题，之前从没考虑买房的我到了售楼处就买了一套，几乎都没挑就定下了，当然这么冲动的行为必须要付出代价，因此背上了近百万的债务，从此变成了一个给银行打工按月还款的房奴。 说实话宝宝刚出生时并不好看，可是越长越可爱，现在已经7个月大了，开始会爬了，真想不去上班一直陪她玩，有时候确实有一种为了她放弃全世界的冲动，宝宝今天有点发烧，凌晨一点了还没有睡，陪我一起跨年总结了，好在这会儿烧退了一些，快点好起来吧！ 2019年生活上发生的另外一件很重要的事情就是投资，入市有风险，投资需谨慎，这不是一句玩笑话，今年年初股市行情一片大好，正当我们陶醉其中的时候，贸易大棒直接挥下，给准备一飞冲天的行情当头一棒，还好跑得快，不然年初那波行情的盈利在贸易战初期就会飞烟灭了。 年初的基金行情也异常火爆，在支付宝买了点指数基金，贸易战开始之后就抛掉了，并没有多少盈利： 说完赚钱的接下来就是赔钱的，P2P暴雷给我炸的遍体鳞伤，从5月份出事到现在毫无音信，真应了那句话，你看上了人家的高息，而人家看上的是你的本金，P2P今年可谓损失惨重。 因为赚钱心切，今年还投资了一点数字货币，结果因爆仓而结束，每次都是到了爆仓点位迅速反弹，好像在提示我压根就不是我应该玩的，不过从这次投资来看，我才明白为什么有钱的人越来越有钱，而穷人一辈子很难翻身。 一句话，穷人没有东山再起的资本和承担风险的能力，举个例子：我和一个富有的人同时买一只数字货币，而我们都只花了500块来买相同的点位，不同的是富有的人保证金更多一些，这样当行情来到我的爆仓点位时，我和富有的人损失相同的钱，但是我爆仓了，而他没有，待到行情迅速反弹，他却赚的盆满钵满。 或者换一种情况，我们两个同时爆仓，他立马在低位投入2倍的钱，迅速赚回损失，而我只能眼睁睁的看着行情反弹却没有投资的砝码，忽然觉得T+1好像真的是帮助我们这些散户的。 展望20202019已经过去，面对着已经到来的2020年，我们需要踏上新的征程，我还没有适应给自己定出量化的目标，不过可以暂时写下大致的方向，也算是给自己一个时刻的提醒。 对工作新的一年不能再碌碌无为，真的需要去闯一闯了，最近和一些互联网公司的员工沟通过，仿佛我们不是生活在一个地球，外面的世界真的很大，外面的机会真的很多，是时候出去看看了，浏览一下世界的另一面，当然，脚踏实地的工作风格不能丢弃。 对学习经过一段时间的与大牛们的沟通，我渐渐的明白了自己的差距，也大致了解了需要重点学习哪些知识，所以简单列举如下： 巩固基础知识，对于一些函数不仅要会用，还应该花时间探究实现的方式，往深处挖掘，比如listen函数backlog参数意义。 阅读 redis 源码，这是很多人都提到的一点，适当可以看一下 STL 源码 看两本有关分布式知识的图书 对生活 尽最大可能陪陪家人 投资达到2019的水平（只看赚的，不看赔的） 总结接触了一些大牛之后备受打击，可是以往的岁月已经无法改变，只要认清了自己从现在开始就不晚，2019悄然离开，2020已经隆重登场！加油~]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>工作</tag>
        <tag>学习</tag>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单继承、多继承、菱形继承的虚函数表]]></title>
    <url>%2Fblog%2F2020%2F01%2F03%2F%E5%8D%95%E7%BB%A7%E6%89%BF%E3%80%81%E5%A4%9A%E7%BB%A7%E6%89%BF%E3%80%81%E8%8F%B1%E5%BD%A2%E7%BB%A7%E6%89%BF%E7%9A%84%E8%99%9A%E5%87%BD%E6%95%B0%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[前言最近被问到一个关于多继承虚函数表的问题，当时回答是可能存在多个虚函数表，应该是顺序排列的，但具体怎么排列还是有些疑惑的，回答的时候到有点儿心虚。之后查了资料，做了简单的实验，可以确定的是对于继承了多个含有虚函数基类的子类来说，指向虚函数表的指针应该不止一个。 问题虚函数表的问题是从C++多态的概念引出的，要想实现多态有3个条件： 存在继承：没有继承就没有多态（运行时），在多态中必须存在有继承关系的父类和子类。 重写函数：父类中需要定义带有 virtual 关键字的函数，而在子类中重写一个名字和参数与父类中定义完全相同的函数。 向上转型：将父类的指针和引用指向子类的对象。 满足以上三个条件，当使用父类的指针调用带有 virtual 关键字的函数时，就会产生多态行为。 实现这种多态表现的核心内容就是虚函数表，对于带有 virtual 关键字的函数地址会被放入一个表格，而在类中会有一个指向虚函数表的指针指向这个表格，表明这个表格属于类的一部分。 对于父类来说，这个表格中都是自己类的虚函数，而对于子类来说，首先这个虚函数表包含父类中所有的虚函数，当子类重写某个虚函数时就会用子类重写后的函数地址替换原来父类中定义的函数地址，同时在子类的虚函数表中还会包含子类独有的虚函数。 由此可见虚函数表的不同和复杂性还是体现在子类上，所以之后会分别测试单继承、多继承、菱形继承三种情况下虚函数表的不同，主要看一下虚函数表的个数和内存布局情况。 测试环境首先来说明一下测试环境，测试工具是VS2013，对于int *p; sizeof(p)的结果是4，说明编译环境是32位的，这个对后面查看内存结构非常关键。 开始测试使用VS2013查看类的内存布局非常方便，因为类的大小在编译期间就已经确定了，不用运行就可以通过添加编译选项知道类的大小和布局，而指向虚函数表的指针也会占用类的大小，如果说编译的时候确定了类的大小，那从侧面也说明了在编译期间虚函数表实际上也确定了。 使用VS2013查看类的布局时，可以在项目的属性页：“配置属性”–&gt;“C/C++”–&gt;“命令行”中输入以下任意一个命令， /d1reportAllClassLayout ：这个选项可以在VS的输出窗口显示所有相关联的类结构，因为一些外部类也会显示，最终的内容会非常多，需要自己辨别有用的信息。 /d1reportSingleClassLayoutXXX ：这个选项只会在输出窗口显示指定的类结构，只需要将XXX替换成想显示的类的名字即可，缺点就是无法同时显示多个想查看的类。 无虚函数简单类结构在查看虚函数表的结构之前，先使用之前的编译参数来查看一下简单的类结构，排除虚函数的干扰，能更清楚的了解类成员在类中的布局情况，有一点需要提一下，成员变量会占用类的大小，但是成员函数不会，如果有虚函数，所有的虚函数会被放入一个表格，而在类中放置一个指向虚函数表的指针，来看一下简单代码： 123456789101112131415class CBase&#123;public: void func() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase&#123;public: void func() &#123;&#125;public: int m_var2;&#125;; 编译输出的类的内存布局为： 1234567891011121&gt; class CBase size(4):1&gt; +---1&gt; 0 | m_var11&gt; +---1&gt;1&gt; class CDerived size(8):1&gt; +---1&gt; | +--- (base class CBase)1&gt; 0 | | m_var11&gt; | +---1&gt; 4 | m_var21&gt; +--- 从上面的输出内容来看，很清楚的可以看到基类 CBase 的大小 size(4) 占用4个字节，只有一个成员变量 m_var1，在类中偏移量为0的位置，而派生类 CDerived 占用8个字节大小，第一个成员继承自基类 CBase 的 m_var1，在类中偏移量为0的位置，还有一个子类独有的成员变量 m_var2，在类中偏移量为4的位置。 掌握着这种简单类的查看类结构的方法，接下来开始看一下包含虚函数的类的内存布局。 包含虚函数的类结构查看包含虚函数的类结构相对来说麻烦一点，先来说两个符号，免得一会看见结构发懵，vfptr 表示类中指向虚函数表的指针，通常放在类的起始位置，比成员变量的位置都要靠前， vftable 表示类中引用的虚函数表，在具体分析是还有有一些修饰符，用来表明是谁的虚函数表。 单继承这种情况的下的子类的虚函数表很简单，在该子类的内存布局上，最开始的位置保存了一个指向虚函数表的指针，虚函数表中包含了从父类继承的虚函数，当子类中重写父类虚函数时会将虚函数表中对应的函数地址替换，最后添加上自己独有的虚函数地址，下面上代码分析一下： 12345678910111213141516171819class CBase&#123;public: void func0() &#123;&#125; virtual void func1() &#123;&#125; virtual void func2() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase&#123;public: virtual void func2() &#123;&#125; virtual void func3() &#123;&#125; void func4() &#123;&#125;public: int m_var2;&#125;; 上面这两个类的内存布局情况如下： 123456789101112131415161718192021222324252627282930313233341&gt; class CBase size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var11&gt; +---1&gt;1&gt; CBase::$vftable@:1&gt; | &amp;CBase_meta1&gt; | 01&gt; 0 | &amp;CBase::func11&gt; 1 | &amp;CBase::func21&gt;1&gt; CBase::func1 this adjustor: 01&gt; CBase::func2 this adjustor: 01&gt;1&gt;1&gt; class CDerived size(12):1&gt; +---1&gt; | +--- (base class CBase)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | m_var11&gt; | +---1&gt; 8 | m_var21&gt; +---1&gt;1&gt; CDerived::$vftable@:1&gt; | &amp;CDerived_meta1&gt; | 01&gt; 0 | &amp;CBase::func11&gt; 1 | &amp;CDerived::func21&gt; 2 | &amp;CDerived::func31&gt;1&gt; CDerived::func2 this adjustor: 01&gt; CDerived::func3 this adjustor: 0 看起来是不是比没有虚函数时复杂多了，不过不要着急，从上到下慢慢分析就好了，这次的基类 CBase 大小是8个字节，首先是{vfptr}这个指向虚函数表的指针，在类中的偏移量是0，接下来是成员变量 m_var1，在类中偏移量是4。 然后是 CBase::$vftable@ 表示基类 CBase 的虚函数表，其中第一行 &amp;CBase_meta 看起来怪怪的，这里我们不展开（因为我也没弄太懂），应该是和虚函数表相关的元数据，第二行是一个0，看起来是一个偏移量，这里没有偏移，当出现偏移时我们再试着分析（相信我，马上就会出现），第三行内容 &amp;CBase::func1 是自己类的虚函数，前面有一个0，应该是指该虚函数在虚函数表中索引，第四行也是相同的情况。 接下来出现了两行非常相似的内容，看一下CBase::func1 this adjustor: 0，这句代码中的关键是 adjustor，其实有是一个偏移量，据说涉及到thunk技术，据说“thunk其实就是一条汇编指令，操作码是0xe9，就是jmp，后面紧跟操作数”，这里我们就不展开了，如果后面弄明白了可以单独写一篇总结，到此为止基类的内存结构就分析完了。 继续看派生类 CDerived，它的大小是12个字节，内部结构首先是 {vfptr} 一个指向虚函数表的指针，偏移量为0，m_var1 是从父类继承的成员变量，偏移量为4，而 m_var2 是自己类独有的成员变量，偏移量是8。 然后看派生类对应的虚函数表 CDerived::$vftable@，跳过前两行直接看一下后面几个函数，发现只有 func1 是基类的，而函数 func2 和 func3 都是派生类的，出现这种情况的原因是子类重写了函数 func2 和 func3 ，所以用重写后的函数地址替换了从基类继承的虚函数，造成了目前看到的状况。 最后又出现了两行 adjustor，很奇怪为什么 func1 函数没有 adjustor，貌似这个 adjustor 只对当前类有效，先留个疑问，接下来看一下多继承。 多继承当多个父类中都包含虚函数的时候，和子类关联的虚函数表就不止一个了，这个情况是可以通过使用sizeof(子类)来简单验证的： 这一部分是在没有VS的情况下预先写下的，本来考虑使用VS展开布局后，这一段就没有什么必要了，但是后来想想还是留着吧，因为这一段使用的g++编译器，64位环境，每个指针占用8个字节，通过不同的环境调试，更加可以证明，多继承下的多个虚函数表的存在性： 1234567class W&#123;public: long n;public: void func()&#123;&#125;&#125;; 对于这样的一个简单类，sizeof(W) = 8，类的大小等于成员变量的大小。 123456789101112131415class W1&#123;public: long n1;public: virtual void func1()&#123;&#125;&#125;;class W2&#123;public: long n2;public: virtual void func2()&#123;&#125;&#125;; 对于上面这两个简单的包含虚函数的类，sizeof(W1) = 16，sizeof(W2) = 16，因为每个类都除了一个 long 类型的成员变量以外，还包含了指向虚函数的一个指针，所以类的大小是16个字节。 1234567class WW : public W1, public W2&#123;public: long nn;public: virtual void func()&#123;&#125;&#125;; 而继承了 W1 和 W2 这两个父类的子类 WW 在继承了两个成员变量 n1 和 n2 之外，还有自己的成员变量 nn，三个变量占用字节24个，而计算类 WW 的的大小 sizeof(W1) = 40，也就是说除了成员变量24个字节，还剩余了16个字节的空间没有着落，我们知道它至少包含一个指向虚函数表的指针，占用8个字节的大小，还剩8个字节没有找到用处，从此处分析应该还有一个指向虚函数表的指针，具体的情况可以看一下内存分布。 接下来和单继承的分析方法一样，写代码编译查看布局： 123456789101112131415161718192021222324252627282930313233class CBase0&#123;public: void func0() &#123;&#125; virtual void func1() &#123;&#125; virtual void func2() &#123;&#125; virtual void func3() &#123;&#125;public: int m_var0;&#125;;class CBase1&#123;public: void func0() &#123;&#125; virtual void func2() &#123;&#125; virtual void func3() &#123;&#125; virtual void func4() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase0, public CBase1&#123;public: virtual void func1() &#123;&#125; virtual void func2() &#123;&#125; virtual void func4() &#123;&#125; virtual void func5() &#123;&#125; void func6() &#123;&#125;public: int m_var2;&#125;; 上面3个类描述了一个简单的多继承的情况，之所以写这么多函数就是构建一种，既有虚函数覆盖，又有单独不被覆盖的情况，下面展示了这段代码的内存布局。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566671&gt; class CBase0 size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var01&gt; +---1&gt;1&gt; CBase0::$vftable@:1&gt; | &amp;CBase0_meta1&gt; | 01&gt; 0 | &amp;CBase0::func11&gt; 1 | &amp;CBase0::func21&gt; 2 | &amp;CBase0::func31&gt;1&gt; CBase0::func1 this adjustor: 01&gt; CBase0::func2 this adjustor: 01&gt; CBase0::func3 this adjustor: 01&gt;1&gt;1&gt; class CBase1 size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var11&gt; +---1&gt;1&gt; CBase1::$vftable@:1&gt; | &amp;CBase1_meta1&gt; | 01&gt; 0 | &amp;CBase1::func21&gt; 1 | &amp;CBase1::func31&gt; 2 | &amp;CBase1::func41&gt;1&gt; CBase1::func2 this adjustor: 01&gt; CBase1::func3 this adjustor: 01&gt; CBase1::func4 this adjustor: 01&gt;1&gt;1&gt; class CDerived size(20):1&gt; +---1&gt; | +--- (base class CBase0)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | m_var01&gt; | +---1&gt; | +--- (base class CBase1)1&gt; 8 | | &#123;vfptr&#125;1&gt; 12 | | m_var11&gt; | +---1&gt; 16 | m_var21&gt; +---1&gt;1&gt; CDerived::$vftable@CBase0@:1&gt; | &amp;CDerived_meta1&gt; | 01&gt; 0 | &amp;CDerived::func11&gt; 1 | &amp;CDerived::func21&gt; 2 | &amp;CBase0::func31&gt; 3 | &amp;CDerived::func51&gt;1&gt; CDerived::$vftable@CBase1@:1&gt; | -81&gt; 0 | &amp;thunk: this-=8; goto CDerived::func21&gt; 1 | &amp;CBase1::func31&gt; 2 | &amp;CDerived::func41&gt;1&gt; CDerived::func1 this adjustor: 01&gt; CDerived::func2 this adjustor: 01&gt; CDerived::func4 this adjustor: 81&gt; CDerived::func5 this adjustor: 0 内容很多，前面两个基类 CBase0 和 CBase1 的布局很简单，参照之前的分析很容易看懂，直接从派生类看起吧。 我们发现派生类 CDerived 中确实有两个指向虚函数表的指针，接下来看一下这两个虚函数表，这个虚函数表和前面遇到的格式一样，除了第一行的元数据，第二行的诡异偏移量0，剩下的虚函数指针有的是从基类继承来的，有的是被当前派生类覆盖的，还有派生类自己独有的。 而第二个虚函数表就有点意思了，首先是少了 &amp;CDerived_meta 这一行，然后偏移量终于不是0了，而是-8，从派生类 CDerived 的内存布局上来看，以下开始大胆假设，至于小心求证的部分放到以后来做（看自己的进步状态了）。 第二个指向虚函数表的指针是不是距离类的起始偏移量是8，我猜这个-8的意思就是指的这个偏移量，这个值有可能被后面使用，第二行出现了 &amp;thunk: this-=8; goto CDerived::func2，其中包含 thunk 字样，表示这个 func2 不归我管，你去-8偏移量的那个虚函数表里找一找。 还有一点你有没有发现 func5 这个函数只在第一个虚函数表中出现，而没有出现在第二个虚函数表中，这也是一个规则，自己独有的虚函数放到第一个虚函数表中，这可能也是为什么只有第一个虚函数表包含元数据行。 最后一点，我们发现对于函数 func4 来说 adjustor 终于不是0了，而值变成了8，仿佛在说这个虚函数只在偏移量的为8的位置。 菱形继承对于这一部分，并没有太多新的内容，只是简单的菱形继承中，最初的基类在最终的子类中会包含两份，而虚函数的样子并没有太大的不同，接下来简单看一下代码和对应的内存布局即可，因为菱形继承并不被提倡，所以也不用花太多时间来分析这个问题。 123456789101112131415161718192021222324252627282930313233343536class CSuper&#123;public: virtual void func0() &#123;&#125; virtual void func1() &#123;&#125;public: int m_var;&#125;;class CBase0 : public CSuper&#123;public: virtual void func1() &#123;&#125; virtual void func2() &#123;&#125;public: int m_var0;&#125;;class CBase1 : public CSuper&#123;public: virtual void func1() &#123;&#125; virtual void func3() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase0, public CBase1&#123;public: virtual void func1() &#123;&#125; virtual void func3() &#123;&#125; virtual void func4() &#123;&#125;public: int m_var2;&#125;; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091921&gt; class CSuper size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var1&gt; +---1&gt;1&gt; CSuper::$vftable@:1&gt; | &amp;CSuper_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CSuper::func11&gt;1&gt; CSuper::func0 this adjustor: 01&gt; CSuper::func1 this adjustor: 01&gt;1&gt;1&gt; class CBase0 size(12):1&gt; +---1&gt; | +--- (base class CSuper)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | m_var1&gt; | +---1&gt; 8 | m_var01&gt; +---1&gt;1&gt; CBase0::$vftable@:1&gt; | &amp;CBase0_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CBase0::func11&gt; 2 | &amp;CBase0::func21&gt;1&gt; CBase0::func1 this adjustor: 01&gt; CBase0::func2 this adjustor: 01&gt;1&gt;1&gt; class CBase1 size(12):1&gt; +---1&gt; | +--- (base class CSuper)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | m_var1&gt; | +---1&gt; 8 | m_var11&gt; +---1&gt;1&gt; CBase1::$vftable@:1&gt; | &amp;CBase1_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CBase1::func11&gt; 2 | &amp;CBase1::func31&gt;1&gt; CBase1::func1 this adjustor: 01&gt; CBase1::func3 this adjustor: 01&gt;1&gt;1&gt; class CDerived size(28):1&gt; +---1&gt; | +--- (base class CBase0)1&gt; | | +--- (base class CSuper)1&gt; 0 | | | &#123;vfptr&#125;1&gt; 4 | | | m_var1&gt; | | +---1&gt; 8 | | m_var01&gt; | +---1&gt; | +--- (base class CBase1)1&gt; | | +--- (base class CSuper)1&gt; 12 | | | &#123;vfptr&#125;1&gt; 16 | | | m_var1&gt; | | +---1&gt; 20 | | m_var11&gt; | +---1&gt; 24 | m_var21&gt; +---1&gt;1&gt; CDerived::$vftable@CBase0@:1&gt; | &amp;CDerived_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CDerived::func11&gt; 2 | &amp;CBase0::func21&gt; 3 | &amp;CDerived::func41&gt;1&gt; CDerived::$vftable@CBase1@:1&gt; | -121&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;thunk: this-=12; goto CDerived::func11&gt; 2 | &amp;CDerived::func31&gt;1&gt; CDerived::func1 this adjustor: 01&gt; CDerived::func3 this adjustor: 121&gt; CDerived::func4 this adjustor: 0 虚继承解决菱形继承的一个常用的办法就是改为虚继承，实际上虚继承中就是将从最基类中继承的公共部分提取出来放在最子类的末尾，然后在提取之前的位置用一个叫做vbptr的指针指向这里。 之前看到过一种说法： 虚继承内部实现也相当复杂，似乎破坏了OO的纯洁性 至于复杂不复杂，看看后面的内存布局就很清楚了，那是相当复杂，其中出现了各种偏移，简单了解下就行了，如果不是维护老代码，谁现在还写这样的结构。 123456789101112131415161718192021222324252627282930313233343536class CSuper&#123;public: virtual void func0() &#123;&#125; virtual void func1() &#123;&#125;public: int m_var;&#125;;class CBase0 : virtual public CSuper&#123;public: virtual void func1() &#123;&#125; virtual void func2() &#123;&#125;public: int m_var0;&#125;;class CBase1 : virtual public CSuper&#123;public: virtual void func1() &#123;&#125; virtual void func3() &#123;&#125;public: int m_var1;&#125;;class CDerived : public CBase0, public CBase1&#123;public: virtual void func1() &#123;&#125; virtual void func3() &#123;&#125; virtual void func4() &#123;&#125;public: int m_var2;&#125;; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281&gt; class CSuper size(8):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | m_var1&gt; +---1&gt;1&gt; CSuper::$vftable@:1&gt; | &amp;CSuper_meta1&gt; | 01&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CSuper::func11&gt;1&gt; CSuper::func0 this adjustor: 01&gt; CSuper::func1 this adjustor: 01&gt;1&gt;1&gt; class CBase0 size(20):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | &#123;vbptr&#125;1&gt; 8 | m_var01&gt; +---1&gt; +--- (virtual base CSuper)1&gt; 12 | &#123;vfptr&#125;1&gt; 16 | m_var1&gt; +---1&gt;1&gt; CBase0::$vftable@CBase0@:1&gt; | &amp;CBase0_meta1&gt; | 01&gt; 0 | &amp;CBase0::func21&gt;1&gt; CBase0::$vbtable@:1&gt; 0 | -41&gt; 1 | 8 (CBase0d(CBase0+4)CSuper)1&gt;1&gt; CBase0::$vftable@CSuper@:1&gt; | -121&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CBase0::func11&gt;1&gt; CBase0::func1 this adjustor: 121&gt; CBase0::func2 this adjustor: 01&gt;1&gt; vbi: class offset o.vbptr o.vbte fVtorDisp1&gt; CSuper 12 4 4 01&gt;1&gt;1&gt; class CBase1 size(20):1&gt; +---1&gt; 0 | &#123;vfptr&#125;1&gt; 4 | &#123;vbptr&#125;1&gt; 8 | m_var11&gt; +---1&gt; +--- (virtual base CSuper)1&gt; 12 | &#123;vfptr&#125;1&gt; 16 | m_var1&gt; +---1&gt;1&gt; CBase1::$vftable@CBase1@:1&gt; | &amp;CBase1_meta1&gt; | 01&gt; 0 | &amp;CBase1::func31&gt;1&gt; CBase1::$vbtable@:1&gt; 0 | -41&gt; 1 | 8 (CBase1d(CBase1+4)CSuper)1&gt;1&gt; CBase1::$vftable@CSuper@:1&gt; | -121&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CBase1::func11&gt;1&gt; CBase1::func1 this adjustor: 121&gt; CBase1::func3 this adjustor: 01&gt;1&gt; vbi: class offset o.vbptr o.vbte fVtorDisp1&gt; CSuper 12 4 4 01&gt;1&gt;1&gt; class CDerived size(36):1&gt; +---1&gt; | +--- (base class CBase0)1&gt; 0 | | &#123;vfptr&#125;1&gt; 4 | | &#123;vbptr&#125;1&gt; 8 | | m_var01&gt; | +---1&gt; | +--- (base class CBase1)1&gt; 12 | | &#123;vfptr&#125;1&gt; 16 | | &#123;vbptr&#125;1&gt; 20 | | m_var11&gt; | +---1&gt; 24 | m_var21&gt; +---1&gt; +--- (virtual base CSuper)1&gt; 28 | &#123;vfptr&#125;1&gt; 32 | m_var1&gt; +---1&gt;1&gt; CDerived::$vftable@CBase0@:1&gt; | &amp;CDerived_meta1&gt; | 01&gt; 0 | &amp;CBase0::func21&gt; 1 | &amp;CDerived::func41&gt;1&gt; CDerived::$vftable@CBase1@:1&gt; | -121&gt; 0 | &amp;CDerived::func31&gt;1&gt; CDerived::$vbtable@CBase0@:1&gt; 0 | -41&gt; 1 | 24 (CDerivedd(CBase0+4)CSuper)1&gt;1&gt; CDerived::$vbtable@CBase1@:1&gt; 0 | -41&gt; 1 | 12 (CDerivedd(CBase1+4)CSuper)1&gt;1&gt; CDerived::$vftable@CSuper@:1&gt; | -281&gt; 0 | &amp;CSuper::func01&gt; 1 | &amp;CDerived::func11&gt;1&gt; CDerived::func1 this adjustor: 281&gt; CDerived::func3 this adjustor: 121&gt; CDerived::func4 this adjustor: 01&gt;1&gt; vbi: class offset o.vbptr o.vbte fVtorDisp1&gt; CSuper 28 4 4 0 总结 虚函数表是用来实现多态的核心内容。 多继承很强大但是不要滥用，当多个基类都含有虚函数时，派生类会有多个指向虚函数表的指针。 忘记菱形继承吧，为了取消二义性引入虚继承，结果造成内存分布复杂而又难以理解，大道至简，回归本质吧！]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>继承</tag>
        <tag>多态</tag>
        <tag>虚函数表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ STL中map的[]操作符使用时的一个坑]]></title>
    <url>%2Fblog%2F2019%2F12%2F14%2FC-STL%E4%B8%ADmap%E7%9A%84-%E6%93%8D%E4%BD%9C%E7%AC%A6%E4%BD%BF%E7%94%A8%E6%97%B6%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9D%91%2F</url>
    <content type="text"><![CDATA[前言学习C++，自从发现了map这个结构以后，就深深的被这种键值对的方式吸引了，写代码时也渐渐离不开这种结构了，一次偶然的机会发现这个map还有个 [] 运算符，仿佛又发现了新大陆一样，写代码更加方便了，殊不知一个深深的大坑正在前面等着我。 问题一开始学到map的时候还是中规中矩的使用函数插入删除，比如定义一个map，先引入头文件和命名空间： 1234#include &lt;map&gt;using namespace std;map&lt;int, int&gt; mapTest; 上面就轻松定义了一个map结构对象，是一个整数到另一个整数的映射，这种映射有什么用呢？举个简单的例子，这个映射可以作为学生的学号和成绩的对应关系，这样只要知道学号，就可以从map中直接获得对应的成绩很方便。 最开始学习插入时通常有以下两种方式： 12mapTest.insert(map&lt;int, int&gt;::value_type(1001, 100));mapTest.insert(make_pair(1002, 98)); 但是学了 map 的 [] 操作符以后，上述代码可以写成： 12mapTest[1001] = 100;mapTest[1002] = 98; 查找一个元素的时候需要用到find()函数，一般写成 123map&lt;int, int&gt;::const_iterator itor = mapTest.find(1001);if (itor != mapTest.end()) return itor-&gt;second; 但是学了 map 的 [] 操作符以后，上述代码就可以简写成： 1return mapTest[1001]; 特别的在插入一个元素的时候，比如用来计数，每次给一个键对应的值加1时，可以直接写成： 1mapTest[1001] += 1; 根本不用检查 1001 这个键是否存在，使用 [] 操作符，在使用前会先默认成0，然后执行+1操作，这个比先使用find()查找，然后+1操作后再插入方便多了。 其实这只是使用map结构的一种语法糖，但是这语法糖简直太好使了，太甜了，让人欲罢不能，所以我就含着这块糖掉进了坑里，因为调用 map 的有时会产生副作用，如果查找一个键不在 map 中，则会在map中对应的这个键的位置插入默认值，接下来看一下例子就明白了。 测试过程测试代码在VS2015中编译运行，C++11标准，如果编译不正确可以看一下环境是否不同，尝试修改代码实现即可，测试的例子也是上面提到的，使用 map 来存储学生学号和成绩的对应关系，下面来简单实现一个类，描述这种关系： 编写测试类1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;map&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;class CReportCard&#123;public: CReportCard() &#123; m_mapStuNo2Score.clear(); &#125; ~CReportCard() &#123; m_mapStuNo2Score.clear(); &#125;public: void LoadScores(); // 模拟录入成绩 int GetScoreByStudentNo(const int nStudentNo); // 根据学号查询成绩 void PrintReportCard(); // 打印成绩单private: map&lt;int, int&gt; m_mapStuNo2Score;&#125;;void CReportCard::LoadScores()&#123; m_mapStuNo2Score[1001] = 99; m_mapStuNo2Score[1002] = 94; m_mapStuNo2Score[1004] = 89; m_mapStuNo2Score[1005] = 92; m_mapStuNo2Score[1007] = 80;&#125;int CReportCard::GetScoreByStudentNo(const int nStudentNo)&#123; return m_mapStuNo2Score[nStudentNo];&#125;void CReportCard::PrintReportCard()&#123; cout &lt;&lt; "show report card start-----&gt;" &lt;&lt; endl; std::for_each(m_mapStuNo2Score.begin(), m_mapStuNo2Score.end(), [](std::map&lt;int, int&gt;::reference socrepair) &#123; std::cout &lt;&lt; socrepair.first &lt;&lt; "'s score = " &lt;&lt; socrepair.second &lt;&lt; "\n"; &#125;); cout &lt;&lt; "show report card end&lt;------" &lt;&lt; endl;&#125; 这个类的内容很简单，使用 map 类型的对象 m_mapStuNo2Score 来存储学号和成绩的对应关系，LoadScores()函数中使用 [] 操作符向 map 中插入元素，模拟成绩录入过程；GetScoreByStudentNo()函数同样使用了 [] 操作符模拟成绩查询过程；PrintReportCard()函数遍历 map 打印成绩单信息。 看似正常的调用接下来编写一个函数来使用这个类，测试如下： 123456789101112int main(int argc, char* argv[])&#123; CReportCard obj; obj.LoadScores(); cout &lt;&lt; "student no = 1001, score = " &lt;&lt; obj.GetScoreByStudentNo(1001) &lt;&lt; endl; cout &lt;&lt; "student no = 1004, score = " &lt;&lt; obj.GetScoreByStudentNo(1004) &lt;&lt; endl; obj.PrintReportCard(); return 0;&#125; 首先调用 LoadScores()函数来加载数据，然后通过 GetScoreByStudentNo() 函数来查找学号为 1001 和 1004 的两个学生的成绩，最后打印一下成绩单，接下来看一下运行结果： student no = 1001, score = 99student no = 1004, score = 89show report card start—–&gt;1001’s score = 991002’s score = 941004’s score = 891005’s score = 921007’s score = 80show report card end&lt;—— 以上结果正常的打印出了查询的分数和成绩单，一切看起来毫无问题，如果查询的学号不存在又会怎么样呢？ 出现问题的调用修改上面的测试函数，将学生学号改成不存在的数值，修改如下： 12345678910111213int main(int argc, char* argv[])&#123; CReportCard obj; obj.LoadScores(); cout &lt;&lt; endl; cout &lt;&lt; "student no = 1011, score = " &lt;&lt; obj.GetScoreByStudentNo(1011) &lt;&lt; endl; cout &lt;&lt; "student no = 1014, score = " &lt;&lt; obj.GetScoreByStudentNo(1014) &lt;&lt; endl; obj.PrintReportCard(); return 0;&#125; 大部分的内容并没有发生变化，只将学号改成了不存在的情况，测试结果如下： student no = 1011, score = 0student no = 1014, score = 0show report card start—–&gt;1001’s score = 991002’s score = 941004’s score = 891005’s score = 921007’s score = 801011’s score = 01014’s score = 0show report card end&lt;—— 不存在的学号对应的分数是0，这应该也说的过去，因为键不存在，所以对 map 使用 [] 操作符查找时，寻找的键不存在则返回了整型的默认值0，但是在打印成绩单的时候居然多了两项，这充分暴露了 [] 操作符可能产生的副作用。 在查找返回时，[] 操作符并不是找不到返回对应类型默认值就完了，还会把查找的键和默认值作为一对，插入到待查的 map，这种操作一般是我们不需要的，所以在你明确不需要这个副作用时，查找 map 元素不要使用 [] 操作符。 亡羊补牢上面说到，[] 操作符查找不到就插入的副作用一般我们不使用，所以在查找时还是使用 find() 函数更规范一些，修改 GetScoreByStudentNo() 函数如下： 123456int CReportCard::GetScoreByStudentNo(const int nStudentNo)&#123; //return m_mapStuNo2Score[nStudentNo]; map&lt;int, int&gt;::const_iterator itor = m_mapStuNo2Score.find(nStudentNo); return itor != m_mapStuNo2Score.end() ? itor-&gt;second : 0;&#125; 此时再运行上面的例子就正常了，成绩单中也不会插入无效值了。 总结 map 的 [] 操作符会有副作用，当查找的键不存在时，会在对应键位置插入默认值 时刻保持清醒的头脑，过分的方便或许会给你自己埋下深深的坑 敬畏自然、敬畏生命、敬畏你写下的每一行代码]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>find</tag>
        <tag>中括号</tag>
        <tag>insert</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL中全局变量、会话变量、用户变量和局部变量的区别]]></title>
    <url>%2Fblog%2F2019%2F12%2F03%2FMySQL%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E3%80%81%E4%BC%9A%E8%AF%9D%E5%8F%98%E9%87%8F%E3%80%81%E7%94%A8%E6%88%B7%E5%8F%98%E9%87%8F%E5%92%8C%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言之前在项目的存储过程中发现有通过 DECLARE 关键字定义的变量如DECLARE cnt INT DEFAULT 0;，还有形如 @count 这样的变量，存储过程中拿过来直接就进行设置，像这样set @count=1;，这两种类型的变量究竟有什么区别却弄不清楚，赶紧上网查询资料，发现还有@@sql_mode这样的变量，这一个圈俩圈的到底是什么啊？会不会出现三个圈的情况？ 变量分类与关系经过一段时间学习和测试，再配合官方的文档，现在大致弄清楚了这些变量的区别，一般可以将MySQL中的变量分为全局变量、会话变量、用户变量和局部变量，这是很常见的分类方法，这些变量的作用是什么呢？可以从前往后依次看一下。 首先我们知道MySQL服务器维护了许多系统变量来控制其运行的行为，这些变量有些是默认编译到软件中的，有些是可以通过外部配置文件来配置覆盖的，如果想查询自编译的内置变量和从文件中可以读取覆盖的变量可以通过以下命令来查询: 1mysqld --verbose --help 如果想只看自编译的内置变量可以使用命令： 1mysqld --no-defaults --verbose --help 接下来简单了解一下这几类变量的应用范围，首先MySQL服务器启动时会使用其软件内置的变量（俗称写死在代码中的）和配置文件中的变量（如果允许，是可以覆盖源代码中的默认值的）来初始化整个MySQL服务器的运行环境，这些变量通常就是我们所说的全局变量，这些在内存中的全局变量有些是可以修改的。 当有客户端连接到MySQL服务器的时候，MySQL服务器会将这些全局变量的大部分复制一份作为这个连接客户端的会话变量，这些会话变量与客户端连接绑定，连接的客户端可以修改其中允许修改的变量，但是当连接断开时这些会话变量全部消失，重新连接时会从全局变量中重新复制一份。 其实与连接相关的变量不只有会话变量一种，用户变量也是这样的，用户变量其实就是用户自定义变量，当客户端连接上MySQL服务器之后就可以自己定义一些变量，这些变量在整个连接过程中有效，当连接断开时，这些用户变量消失。 局部变量实际上最好理解，通常由DECLARE 关键字来定义，经常出现在存储过程中，非常类似于C和C++函数中的局部变量，而存储过程的参数也和这种变量非常相似，基本上可以作为同一种变量来对待。 变量的修改先说全局变量有很多是可以动态调整的，也就是说可以在MySQL服务器运行期间通过 SET 命令修改全局变量，而不需要重新启动 MySQL 服务，但是这种方法在修改大部分变量的时候都需要超级权限，比如root账户。 相比之下会话对变量修改的要求要低的多，因为修改会话变量通常只会影响当前连接，但是有个别一些变量是例外的，修改它们也需要较高的权限，比如 binlog_format 和 sql_log_bin，因为设置这些变量的值将影响当前会话的二进制日志记录，也有可能对服务器复制和备份的完整性产生更广泛的影响。 至于用户变量和局部变量，听名字就知道，这些变量的生杀大权完全掌握在自己手中，想改就改，完全不需要理会什么权限，它的定义和使用全都由用户自己掌握。 测试环境以下给出MySQL的版本，同时使用root用户测试，这样可以避免一些权限问题。 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 变量查询与设置全局变量这些变量来源于软件自编译、配置文件中、以及启动参数中指定的变量，其中大部分是可以由root用户通过 SET 命令直接在运行时来修改的，一旦 MySQL 服务器重新启动，所有修改都被还原。如果修改了配置文件，想恢复最初的设置，只需要将配置文件还原，重新启动 MySQL 服务器，一切都可以恢复原来的样子。 查询查询所有的全局变量： 1show global variables; 一般不会这么用，这样查简直太多了，大概有500多个，通常会加个like控制过滤条件： 12345678910111213141516171819mysql&gt; show global variables like 'sql%';+------------------------+----------------------------------------------------------------+| Variable_name | Value |+------------------------+----------------------------------------------------------------+| sql_auto_is_null | OFF || sql_big_selects | ON || sql_buffer_result | OFF || sql_log_off | OFF || sql_mode | STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION || sql_notes | ON || sql_quote_show_create | ON || sql_safe_updates | OFF || sql_select_limit | 18446744073709551615 || sql_slave_skip_counter | 0 || sql_warnings | OFF |+------------------------+----------------------------------------------------------------+11 rows in set, 1 warning (0.00 sec)mysql&gt; 还有一种查询方法就是通过select语句： 1select @@global.sql_mode; 当一个全局变量不存在会话变量副本时也可以这样 1select @@max_connections; 设置设置全局变量也有两种方式： 1set global sql_mode=''; 或者 1set @@global.sql_mode=''; 会话变量这些变量基本来自于全局变量的复制，与客户端连接有关，无论怎样修改，当连接断开后，一切都会还原，下次连接时又是一次新的开始。 查询类比全局变量，会话变量也有类似的查询方式，查询所有会话变量 1show session variables; 添加查询匹配，只查一部分会话变量： 1show session variables like 'sql%'; 查询特定的会话变量，以下三种都可以： 123select @@session.sql_mode;select @@local.sql_mode;select @@sql_mode; 设置会话变量的设置方法是最多的，以下的方式都可以： 123456set session sql_mode = '';set local sql_mode = '';set @@session.sql_mode = '';set @@local.sql_mode = '';set @@sql_mode = '';set sql_mode = ''; 用户变量用户变量就是用户自己定义的变量，也是在连接断开时失效，定义和使用相比会话变量来说简单许多。 查询直接一个select语句就可以了： 1select @count; 设置设置也相对简单，可以直接使用set命令： 12set @count=1;set @sum:=0; 也可以使用select into语句来设置值，比如： 1select count(id) into @count from items where price &lt; 99; 局部变量局部变量通常出现在存储过程中，用于中间计算结果，交换数据等等，当存储过程执行完，变量的生命周期也就结束了。 查询也是使用select语句： 12declare count int(4);select count; 设置与用户变量非常类似： 1234declare count int(4);declare sum int(4);set count=1;set sum:=0; 也可以使用select into语句来设置值，比如： 12declare count int(4);select count(id) into count from items where price &lt; 99; 其实还有一种存储过程参数，也就是C/C++中常说的形参，使用方法与局部变量基本一致，就当成局部变量来用就可以了 几种变量的对比使用 操作类型 全局变量 会话变量 用户变量 局部变量（参数） 文档常用名 global variables session variables user-defined variables local variables 出现的位置 命令行、函数、存储过程 命令行、函数、存储过程 命令行、函数、存储过程 函数、存储过程 定义的方式 只能查看修改，不能定义 只能查看修改，不能定义 直接使用，@var形式 declare count int(4); 有效生命周期 服务器重启时恢复默认值 断开连接时，变量消失 断开连接时，变量消失 出了函数或存储过程的作用域，变量无效 查看所有变量 show global variables; show session variables; - - 查看部分变量 show global variables like &#39;sql%&#39;; show session variables like &#39;sql%&#39;; - - 查看指定变量 select @@global.sql_mode、select @@max_connections; select @@session.sql_mode;、 select @@local.sql_mode;、 select @@sql_mode; select @var; select count; 设置指定变量 set global sql_mode=&#39;&#39;;、 set @@global.sql_mode=&#39;&#39;; set session sql_mode = &#39;&#39;;、 set local sql_mode = &#39;&#39;;、 set @@session.sql_mode = &#39;&#39;;、 set @@local.sql_mode = &#39;&#39;;、 set @@sql_mode = &#39;&#39;;、 set sql_mode = &#39;&#39;; set @var=1;、 set @var:=101;、 select 100 into @var; set count=1;、 set count:=101;、 select 100 into count; 相信看了这个对比的表格，之前的很多疑惑就应该清楚了，如果发现其中有什么疑惑的地方可以给我留言，或者发现有什么错误也可以一针见血的指出来，我会尽快改正的。 总结 MySQL 中的变量通常分为：全局变量、 会话变量、 用户变量、 局部变量 其实还有一个存储过程和函数的参数，这种类型和局部变量基本一致，当成局部变量来使用就行了 在表格中有一个容易疑惑的点就是无论是全局变量还是会话变量都有select@@变量名的形式。 select@@变量名这种形式默认取的是会话变量，如果查询的会话变量不存在就会获取全局变量，比如@@max_connections 但是SET操作的时候，set @@变量名=xxx 总是操作的会话变量，如果会话变量不存在就会报错]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>全局变量</tag>
        <tag>会话变量</tag>
        <tag>用户变量</tag>
        <tag>局部变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库导入、导出、复制表、重命名表]]></title>
    <url>%2Fblog%2F2019%2F11%2F30%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%BC%E5%85%A5%E3%80%81%E5%AF%BC%E5%87%BA%E3%80%81%E5%A4%8D%E5%88%B6%E8%A1%A8%E3%80%81%E9%87%8D%E5%91%BD%E5%90%8D%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[前言提前说明这是一篇小白总结，高手勿喷请绕行，写这篇总结的原因是发觉自己有时候确实眼高手低了，大道至简，花了很多时间去看索引、缓存、主从等等，等到出现实际问题的时候却发现自己磨磨蹭蹭写出的SQL语句居然有语法错误，看来还得稳扎稳打从基础入手，因为实际工作的用到的SQL并不多，现在把常用的几条总结一下，即使下次不能立马写出来，也能在这篇文章中的快速找到想要的。 正如标题中的提到的这些，数据库的导入和导出在紧急处理线上数据时很常用，而复制表基本上也是为了不影响原数据的情况下进行问题排查，重命名表是为了导入多份备份数据时原数据不被覆盖，比如想对比两天的A表数据，可以先把第一天的数据导入，然后将A表名修改成Aold，接着直接再导入第二天的数据库数据，这样就可以将数据库中表Aold和A进行对比了，可以避免两个数据库中的同一个表进行对比时写很长的SQL。 测试环境 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 11Server version: 5.7.28-log MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 测试过程为了说明实现这些要求的具体SQL，我们先建立一个测试数据库，然后创建测试表格，插入测试数据，最后在这个数据库上依次实现这些要求。 创建测试数据创建测试数据库和表格1234567891011121314151617181920mysql&gt; create database dbtest;Query OK, 1 row affected (0.00 sec)mysql&gt; use dbtestDatabase changedmysql&gt; create table a(id int, num int);Query OK, 0 rows affected (0.02 sec)mysql&gt; create table b(id int, name varchar(32));Query OK, 0 rows affected (0.01 sec)mysql&gt; show tables;+--------------+| Tables_in_zz |+--------------+| a || b |+--------------+2 rows in set (0.00 sec) 插入测试数据1234567891011121314151617181920212223242526272829mysql&gt; insert into a values(1, 100);Query OK, 1 row affected (0.02 sec)mysql&gt; insert into a values(2, 200);Query OK, 1 row affected (0.01 sec)mysql&gt; select * from a;+------+------+| id | num |+------+------+| 1 | 100 || 2 | 200 |+------+------+2 rows in set (0.01 sec)mysql&gt; insert into b values(1, 'albert');Query OK, 1 row affected (0.01 sec)mysql&gt; insert into b values(2, 'tom');Query OK, 1 row affected (0.01 sec)mysql&gt; select * from b;+------+--------+| id | name |+------+--------+| 1 | albert || 2 | tom |+------+--------+2 rows in set (0.00 sec) 数据库导出数据库导出时使用的最基础的工具叫mysqldump，这是单独的工具不是mysql命令，刚学MySQL的时候居然在MySQL的命令行中使用mysqldump，现在只能当笑话看了。 导出指定数据库中所有表结构和数据在系统的命令行工具下输入以下命令，敲入回车输入密码，再回车就可以将数据库dbtest的结构和数据导出到dbtest.sql文件中： 1&gt;mysqldump -uroot -h192.168.1.101 -p dbtest &gt; dbtest.sql 打开dbtest.sql文件，显示如下：文件内容比较长，里面包含了数据库的表结构和其中的数据信息： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273-- MySQL dump 10.13 Distrib 5.7.21, for Win64 (x86_64)---- Host: localhost Database: dbtest-- -------------------------------------------------------- Server version 5.7.21-log/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;/*!40101 SET NAMES utf8 */;/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;/*!40103 SET TIME_ZONE='+00:00' */;/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;---- Table structure for table `a`--DROP TABLE IF EXISTS `a`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `a` ( `id` int(11) DEFAULT NULL, `num` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;---- Dumping data for table `a`--LOCK TABLES `a` WRITE;/*!40000 ALTER TABLE `a` DISABLE KEYS */;INSERT INTO `a` VALUES (1,100),(2,200);/*!40000 ALTER TABLE `a` ENABLE KEYS */;UNLOCK TABLES;---- Table structure for table `b`--DROP TABLE IF EXISTS `b`;/*!40101 SET @saved_cs_client = @@character_set_client */;/*!40101 SET character_set_client = utf8 */;CREATE TABLE `b` ( `id` int(11) DEFAULT NULL, `name` varchar(32) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;/*!40101 SET character_set_client = @saved_cs_client */;---- Dumping data for table `b`--LOCK TABLES `b` WRITE;/*!40000 ALTER TABLE `b` DISABLE KEYS */;INSERT INTO `b` VALUES (1,'albert'),(2,'tom');/*!40000 ALTER TABLE `b` ENABLE KEYS */;UNLOCK TABLES;/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;-- Dump completed on 2019-11-30 11:32:23 只导出指定数据库中所有表的结构只导出表结构的方法和上面是一样的，只是加上 -d 选项就可以了，运行下面命令就可以将dbtest数据库中的所有表结构导出到 dbteststructure.sql 中，因为和上面类似，文件中的内容就不贴了，只比 dbtest.sql 文件少了插入数据的内容： 1&gt;mysqldump -uroot -h192.168.1.101 -p -d dbtest &gt; dbteststructure.sql 只导出指定数据库中的一个表只导出数据库中指定表，可以是一个也可以是多个，在数据库名字后面跟表的名字就可以了，比如导出表a： 1&gt;mysqldump -uroot -h192.168.1.101 -p dbtest a &gt; dbtest_a.sql 导出多个数据库数据出多个数据库数据需要加上 --databases 选项，然后在后面依次跟上数据库名字就行： 1&gt;mysqldump -uroot -h192.168.1.101 -p --databases dbtest dbtest2 &gt; db_more.sql 导出所有数据库数据导出所有的数据库时不需要加数据库的名字，加上 --all-databases 选项就可以了 1&gt;mysqldump -uroot -h192.168.1.101 -p --all-databases &gt; db_all.sql 数据库导入数据库的导入比较简单，实际上就是把sql文件在MySQL中执行一下，可以使用以下两种方式： 系统命令行导入一般需要指定导入的数据库dbtest和sql文件的路径，在Linux上举例： 1&gt;mysql -uroot -h192.168.1.101 -p dbtest &lt; /home/albert/dbtest.sql --default-character-set=utf8 在Windows上举例，主要是路径需要注意，Windows上使用正斜杠/和反斜杠\都可以，默认是反斜杠，如果路径中包含空格可以用双引号将整个路径包起来： 1&gt;mysql -uroot -h192.168.1.101 -p dbtest &lt; D:\albert\dbtest.sql --default-character-set=utf8 注意--default-character-set=utf8是指定默认的字符集，主要是防止导入时出现编码错误，之前总结过，在此复习一下。 MySQL命令行导入首先连接MySQL服务器进行登陆： 1&gt;mysql -uroot -h192.168.1.101 -p --default-character-set=utf8 输入密码登陆后再使用source命令直接导入sql文件就可以： 1mysql&gt; source D:\albert\dbtest.sql 数据表复制数据表的复制可以分为结构复制和完全复制，其中完全复制时可以先复制结构，再将数据复制到新表中： 只复制表结构 使用LIKE语句，只不过5.0版本之后才支持，之前的版本无法使用 1CREATE TABLE new_table LIKE old_table; 1234567891011121314151617181920212223mysql&gt; select * from a;+------+------+| id | num |+------+------+| 1 | 100 || 2 | 200 |+------+------+2 rows in set (0.01 sec)mysql&gt; create table a2 like a;Query OK, 0 rows affected (0.04 sec)mysql&gt; desc a2;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.02 sec)mysql&gt; select * from a2;Empty set (0.00 sec) 使用 SELECT 语句加不成立的条件实现 1CREATE TABLE new_table SELECT * FROM old_table WHERE FALSE; 123456789101112131415mysql&gt; create table a3 select * from a where false;Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc a3;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.01 sec)mysql&gt; select * from a3;Empty set (0.01 sec) 复制表结构和数据 可以先按照上面的语句复制结构，然后再讲数据复制过去： 12CREATE TABLE new_table SELECT * FROM old_table WHERE FALSE;INSERT INTO new_table SELECT * FROM old_table; 123456789101112mysql&gt; insert into a2 select * from a;Query OK, 2 rows affected (0.07 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from a2;+------+------+| id | num |+------+------+| 1 | 100 || 2 | 200 |+------+------+2 rows in set (0.00 sec) 直接将结构和数据全部复制 1CREATE TABLE new_table SELECT * FROM old_table; 123456789101112131415161718192021mysql&gt; create table a4 select * from a;Query OK, 2 rows affected (0.06 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; desc a4;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.01 sec)mysql&gt; select * from a4;+------+------+| id | num |+------+------+| 1 | 100 || 2 | 200 |+------+------+2 rows in set (0.00 sec) 数据表重命名使用 ALTER 命令实现1ALTER TABLE old_table RENAME [TO|AS] new_table; 这个语句中的TO和AS是可选的，加不加都行，也可以选择其中一个，效果是一样的，测试如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || b |+------------------+5 rows in set (0.02 sec)mysql&gt; alter table b rename c;Query OK, 0 rows affected (0.04 sec)mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || c |+------------------+5 rows in set (0.00 sec)mysql&gt; alter table c rename to d;Query OK, 0 rows affected (0.02 sec)mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || d |+------------------+5 rows in set (0.00 sec)mysql&gt; alter table d rename as e;Query OK, 0 rows affected (0.02 sec)mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || e |+------------------+5 rows in set (0.00 sec) 使用RENAME命令1RENAME TABLE old_table TO new_table; 这个语句中TO就不能省略了，否则会报语法错误，测试如下： 123456789101112131415161718192021mysql&gt; show tables -&gt; ;+------------------+| Tables_in_dbtest |+------------------+| a || e |+------------------+5 rows in set (0.00 sec)mysql&gt; rename table e to f;Query OK, 0 rows affected (0.11 sec)mysql&gt; show tables;+------------------+| Tables_in_dbtest |+------------------+| a || f |+------------------+5 rows in set (0.01 sec) 总结 数据库的导出、导入、数据表的复制、重命名都是MySQL操作的基础，需要熟练掌握 数据库导出：mysqldump -uroot -h192.168.1.101 -p dbtest &gt; dbtest.sql 数据库导入：mysql -uroot -h192.168.1.101 -p dbtest &lt; /tmp/dbtest.sql --default-character-set=utf8 数据表复制：CREATE TABLE new_table SELECT * FROM old_table; 表格重命名：RENAME TABLE old_table TO new_table;]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>导入</tag>
        <tag>导出</tag>
        <tag>复制表</tag>
        <tag>重命名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql导入数据库时报错ERROR: Unknown command ' ']]></title>
    <url>%2Fblog%2F2019%2F11%2F20%2FMysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%B6%E6%8A%A5%E9%94%99ERROR-Unknown-command-0%2F</url>
    <content type="text"><![CDATA[前言之前查询数据问题时多次使用过数据库导出导入命令，从来没发生过这种错误，那是一个风和日丽的上午，忽然来了一个紧急的任务，线上数据出问题了，需要马上处理一下，连上数据库备份服务器，找到备份数据直接下载下来，优雅（cong mang）地处理着这一切，本打算在Windows上直接导入查询处理一下算了，结果忙中添乱，导入数据库时居然报了一大堆错误，其中最扎眼的就是一连串的ERROR: Unknown command ‘\0’，没办法了，先找一台Linux服务器，上传导入数据分析处理一气呵成，处理完线上问题终于有时间回头来看看这个问题了。 测试环境数据库版本 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 9Server version: 5.7.28-log MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 系统版本 Win10马马虎虎版本来打算用来快速处理的，结果添了不少乱 问题出现过程直接使用cmd命令行输入mysql -uroot -h192.168.1.101 -p，然后输入密码后成功登录，接着选择数据库use dbtest，导入数据库文件source E:\onlinedb.sql，结果意外发生了，出现了一大堆的如下错误： …………ERROR:Unknown command ‘\0’.ERROR:Unknown command ‘\0’.ERROR:Unknown command ‘\0’.ERROR:Unknown command ‘\0’.ERROR 1064 (42000): You have an error in your SQL syntax; check the manual … 命令行界面中出现了多次错误提醒ERROR: Unknown command &#39;\0&#39;.，最后有一个常见的语法错误提醒，看到这个&#39;\0&#39;，这个字节中的0，程序中nullptr，我猜到可能是编码问题，而最后的语法错误也是由于编码不同而部分解析导致的，于是查了一些资料发现果然是编码问题，只要在客户端连接Mysql服务器时指定UTF8编码就可以了。 问题结果过程导入的流程不变，只要在客户端连接Mysql服务器时指定编码就可以避免前面遇到的错误，连接时的命令修改为mysql -uroot -h192.168.1.101 -p --default-character-set=utf8就没问题了，上面提到的语法错误也不存在了。 其实这种跨平台的坑有很多，因为平台之间的哲学思想不同，导致对一些默认值的处理不太一样，同样的文件在Windows平台上导入报错，但是我换到Linux服务器上就没有任何问题，接触多了自然就释然了。 总结 导入sql文件时报错ERROR: Unknown command &#39;\0&#39;需在Mysql命令行客户端连接服务器时指定编码 连接时指定编码的格式为mysql -uroot -h192.168.1.101 -p --default-character-set=utf8]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>ERROR</tag>
        <tag>source</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中Blob类型字段的插入、查看、截取和拼接]]></title>
    <url>%2Fblog%2F2019%2F11%2F20%2FMysql%E4%B8%ADBlob%E7%B1%BB%E5%9E%8B%E5%AD%97%E6%AE%B5%E7%9A%84%E6%8F%92%E5%85%A5%E3%80%81%E6%9F%A5%E7%9C%8B%E3%80%81%E6%88%AA%E5%8F%96%E5%92%8C%E6%8B%BC%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[前言本来并没有太注意到Blob这个类型，在游戏的开发中存储数据常常使用这个类型，这里的使用其实是“机械”的使用，因为应用程序和Mysql数据库之间的逻辑已经封装好了，我只要把对应的数据扔到接口里就行了，可是最近发生了点问题，所以决定深入研究一下Blob类型的操作方法。 问题是这样的，由于应用程序的一个逻辑错误，导致Mysql数据库中有一个Blob类型的字段的前几个字节被写入了错误的值，当然这个问题，我们可以通过应用程序处理，在逻辑中读出Blob字段的值，修改为正确值以后再写回到数据库中，可是这样有些麻烦，并且这些处理逻辑与业务无关。 为了更方便的解决问题，决定使用SQL语句直接修改数据库，将错误的数据恢复正常，因为之前没有直接用SQL修改过Blob类型的字段，所以多花了一点时间用来测试，现在把整个过程记录一下，方便下次直接操作。 在整个处理的过程中用到了查看、截取和拼接三种操作，为了让例子看起来更加精炼，我们把插入也测一下，然后创造出我们想要的精简后的数据，首先还是来看一下数据库版本。 数据库版本 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.28-log MySQL Community Server (GPL)Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 创建测试表测试的表格结构很简单，只需要带有一个Blob类型的字段就尅可以了，为了操作方便再添加一个id，操作的SQL语句如下： 1234567891011mysql&gt; create table bloboperation(id int, data blob);Query OK, 0 rows affected (0.36 sec)mysql&gt; desc bloboperation;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || data | blob | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.07 sec) 插入数据因为知道Blob是二进制数据，所以首先插入两条用十六进制表示的字节串试一下，提示成功插入，插入两条一样的数据是为了之后修改的时候对比方便： 12345mysql&gt; insert into bloboperation values(1, 0x01020304FFFFFFFF0000000CAACB0000);Query OK, 1 row affected (0.06 sec)mysql&gt; insert into bloboperation values(2, 0x01020304FFFFFFFF0000000CAACB0000);Query OK, 1 row affected (0.04 sec) 插入字节串没有问题，那插入字符串和数字看看会有什么结果，测试语句如下，最后发现均可以正常插入： 12345mysql&gt; insert into bloboperation values(3, 'hellworld');Query OK, 1 row affected (0.04 sec)mysql&gt; insert into bloboperation values(4, 0);Query OK, 1 row affected (0.03 sec) 查看数据上面插入了4条不同类型的数据都成功了，我们简单来查一下看看数据和我们插入的是否一样： 12345678910mysql&gt; select * from bloboperation;+------+------------------+| id | data |+------+------------------+| 1 | ÿÿÿÿ ? || 2 | ÿÿÿÿ ? || 3 | hellworld || 4 | 0 |+------+------------------+4 rows in set (0.00 sec) 这究竟是什么鬼，除了第3、4条和我们插入的数据一样，前两条数据看起来和我们之前插入数据时完全不一样，其实这时候需要用到一个hex()函数来看Blob类型的数据，查询结果如下： 12345678910mysql&gt; select id,hex(data),length(data) from bloboperation;+------+----------------------------------+--------------+| id | hex(data) | length(data) |+------+----------------------------------+--------------+| 1 | 01020304FFFFFFFF0000000CAACB0000 | 16 || 2 | 01020304FFFFFFFF0000000CAACB0000 | 16 || 3 | 68656C6C776F726C64 | 9 || 4 | 30 | 1 |+------+----------------------------------+--------------+4 rows in set (0.02 sec) 这回前两条数据正常了，可是后两条数据为什么又看起来不一样了呢，如果你产生了这样的疑问，就需要好好理解一下内存值和表现值的对应关系了，第4条插入语句的中数据0，实际上是被当做字符串存储的，而字符’0’的ASCII码是十进制的48，表示成十六进制就是0x30，也就是上面查到的这样，同理这个打错了的字符串’hellworld’也是这样存储的。 截取数据本来以为截取数据需要一个特别的函数，没想到用的是字符串截取函数substring(str,startpos,length)，第一个参数是需要截取的字符串或字节串，第二个参数起始位置从1开始，第三个参数就是截取的长度。 以第一条数据为例，截取第4到第8个一共5个字节，测试如下： 1234567mysql&gt; select id,hex(substring(data,4,5)) from bloboperation where id=1;+------+--------------------------+| id | hex(substring(data,4,5)) |+------+--------------------------+| 1 | 04FFFFFFFF |+------+--------------------------+1 row in set (0.00 sec) 拼接数据看到上一个函数之后，你应该有所察觉，这个Blob类型的数据处理起来并不麻烦，那么拼接函数会不会用的是concat()这个处理字符串的函数呢？恭喜你，答对了，就是使用这个函数，我们来把前四个字节和最后四个字节拼接到一起，测试如下： 1234567mysql&gt; select id,hex(concat(substring(data,1,4),substring(data,13,4))) from bloboperation where id=1;+------+-------------------------------------------------------+| id | hex(concat(substring(data,1,4),substring(data,13,4))) |+------+-------------------------------------------------------+| 1 | 01020304AACB0000 |+------+-------------------------------------------------------+1 row in set (0.00 sec) 进制转换我们看到id为1的数据有16个字节，实际上在应用程序的内存中对应了4个int类型，每个int类型占用四个字节，为了修改数据，我们需要知道原数据在程序中代表的数字是多少，这就用到进制转换函数conv，可以先进行一个简单转换，16进制转10进制的例子： 1234567mysql&gt; select conv('FF',16,10);+------------------+| conv('FF',16,10) |+------------------+| 255 |+------------------+1 row in set (0.00 sec) 通过上面的转换十六进制的FF被转换成了十进制的255，应用到Blob字段也是一样，我们看下id为1的数据第一个int保存的数据是多少: 12345678mysql&gt; select id,conv(hex(concat(substring(data,4,1),substring(data,3,1),substring(data,2,1),substring(data,1,1))),16,10) as firstint from bloboperation where id=1;+------+----------+| id | firstint |+------+----------+| 1 | 67305985 |+------+----------+1 row in set (0.01 sec) 现在我们就得到了第一个int类型的值是67305985，可能有的同学会有疑惑，为什么不直接截取前4个字节，而要一个一个的拼接呢？这就涉及到大端数据和小端数据知识了，我们使用的PC机通常是小端的，数据的地位存储在低内存，数据的高位存储在高内存，所以需要把四个字节反过来拼接在一起再进行转换。 实际处理理解了上面的知识，就可以处理之前遇到的问题了，假设这16个字节代表的4个int类型分别是A，B，C，D，需要处理的问题是当变量D的值是52138的时候把变量B清0。 通过分析判断D变量的值之前有类似的，按照刚才第一个变量那样处理，把B变量清零可以通过A变量拼接0，然后再拼接C变量和D变量得到，具体的执行语句如下： 12345678910111213141516171819mysql&gt; update bloboperation set data=concat(substring(data,1,4), 0x00000000, substring(data,9,8))whereconv( hex(concat(substring(data,16,1),substring(data,15,1),substring(data,14,1),substring(data,13,1))), 16,10)=52138and id=1;Query OK, 1 row affected (0.06 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select id,hex(data),length(data) from bloboperation;+------+----------------------------------+--------------+| id | hex(data) | length(data) |+------+----------------------------------+--------------+| 1 | 01020304000000000000000CAACB0000 | 16 || 2 | 01020304FFFFFFFF0000000CAACB0000 | 16 || 3 | 68656C6C776F726C64 | 9 || 4 | 30 | 1 |+------+----------------------------------+--------------+4 rows in set (0.00 sec) 执行更新后查询发现，第5到8个字节对应的变量B确实被清0了，也就是我们的目标达到了。 总结 Blob类型字段的处理常用到的函数hex()、substring()、concat()、conv() 注意conv()函数的第一个参数需要是十六进制表示的字符串，不需要带0x]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Blob substring</tag>
        <tag>hex</tag>
        <tag>concat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（八）：各种形式的变量%0、%i、%%i、var、%var%、!var!的含义和区别]]></title>
    <url>%2Fblog%2F2019%2F11%2F08%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E5%90%84%E7%A7%8D%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%8F%98%E9%87%8F-0%E3%80%81-i%E3%80%81-i%E3%80%81var%E3%80%81-var-%E7%9A%84%E5%90%AB%E4%B9%89%E5%92%8C%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言最近使用批处理程序处理文件的时候，发现这 bat中的变量形式真是“变化多端”，有时候加1个百分号%，有时候加2个百分号%%，还有的时候加感叹号!，真是让初学者一头雾水，于是查询资料做了一些小测试，终于大致弄清楚了这些变量的含义，接下来一一列举出来。 变量对比下面通过一些具体的例子来看下标题中提到的这些变量什么时候使用，使用的时候有哪些注意事项。 %0这个是批处理程序中的固定用法，类似于C++程序main函数中argv变量数组，类比可以知道，argv[0]表示exe程序的文件名，argv[1]表示启动程序的第1个参数，后面依次类推。而在批处理程序中%0表示这个批处理程序的文件名，%1表示调用这个批处理时传入的第1个参数，%2表示调用这个批处理时传入的第2个参数，最大可以到%9，具体用法可以参考之前的总结《.bat批处理（二）：%0 %1——给批处理脚本传递参数》，简单测试如下： 12345@echo offecho param0=%0echo param0=%1echo param0=%2 将上述代码保存在文件testparams.bat中，从cmd命令行运行批处理文件，只传入一个参数，运行结果如下： C:\Users\Administrator\Downloads&gt;testparams.bat “hello world”param0=testparams.batparam1=”hello world”param2= %i在题目所列的这些变量中，这一个比较特殊，因为它不是批处理文件中的变量，只能用于cmd命令行下的for循环中，在命令行中for循环的语法是for %variable in (set) do command [command-parameters]，其中的variable只能是单字母或者非特殊含义的字符，同样的for循环语句如果写在批处理文件中variable之前就要加两个%%了，先来看看%i的用法，直接在命令行中遍历集合打印输出： C:\Users\Administrator\Downloads&gt;for %i in (1,3,5,8) do echo %iC:\Users\Administrator\Downloads&gt;echo 11C:\Users\Administrator\Downloads&gt;echo 33C:\Users\Administrator\Downloads&gt;echo 55C:\Users\Administrator\Downloads&gt;echo 88 如果将其中的%i改成%%i，就会报语法错误，测试结果如下： C:\Users\Administrator\Downloads&gt;for %%i in (1,3,5,8) do echo %%i此时不应有 %%i。 %%i这种类型也是for循环中特有的，与%i相对，属于批处理程序的用法，换句话说就是在for循环中遍历的索引变量，如果在命令行中定义需要一个%，如果相同的语句定义在批处理文件中需要2个%%，语法为for %%variable in (set) do command [command-parameters]，variable同样只能是单个字母或者普通字符，至于为什么同样含义的变量在批处理中要多加一个%，至今也没有找到官方的说法，查找MSDN也没有发现说明，不过就我个人理解可能就像我们在命令行中打印一个%，可以正常打印输出，如果通过printf()想输出%就需要2个%的原理一样吧，测试如下： 1for %%i in (1,3,5,8) do echo %%i 运行结果： C:\Users\Administrator\Downloads&gt;testfor.batC:\Users\Administrator\Downloads&gt;for %i in (1 3 5 8) do echo %iC:\Users\Administrator\Downloads&gt;echo 11C:\Users\Administrator\Downloads&gt;echo 33C:\Users\Administrator\Downloads&gt;echo 55C:\Users\Administrator\Downloads&gt;echo 88 观察运行结果发现，运行批处理文件的时候，实际上去掉了%%i变量的1个%，将文件中代码改为1个%试下： 1for %i in (1,3,5,8) do echo %i 运行结果： C:\Users\Administrator\Downloads&gt;testfor.bat此时不应有 i。 var这个变量看起来挺正常的，也没有那么多奇奇怪怪的字符，和Lua、Python等语言中的变量长得挺像，实际上变量的这种形式很“短暂”，一般只能出现在给变量赋值的时候，也就是set语句之后，作为左值接受赋值时，或者在等号右测可评估的表达式中，举个例子，编写下面代码保存在normalVar.bat中： 1234567@echo offset var1=1set /a var2=var1+1echo var1echo var2 运行之后的结果为: C:\Users\Administrator\Downloads&gt;normalVar.batvar1var2 看完结果之后觉得很神奇是不是，为什么和我学的其他语言不一样呢，我使用set分别为var1和var2赋了值，但是输出的时候居然不是数字而是变量名，其实这就引出了之后%var%这种用法，接着往下看。 %var%在批处理中除了上面所说的在set语句后面的两种情况，再要想引用变量就需要在变量两端各加一个百分号%，明确的告诉引用者这是一个变量，使用时需要评估一下值，而不要当成字符串，上一个例子中echo后面想要输出的变量没有加%，那就被当成一个字符串处理，原样输出了，修改上个例子如下： 12345678910@echo offset var1=1set /a var2=var1+1set var3=%var2%echo %var1%echo %var2%echo %var3% 运行之后运行结果入下： C:\Users\Administrator\Downloads&gt;normalVar.bat122 看了这次的结果感觉正常多了，有一点需要注意，set var3=%var2%这一句中var2变量中的%不能省略，因为它既不属于左值也不属于被评估值的表达式，如果不加%，赋值后var3的值会变成“var2”这个字符串。 !var!这是最后一种常见的变量形式，同时也是一种不太好理解的形式，需要记住一点，这种变量与延迟环境变量扩展有关，如果没开启延迟环境变量扩展，那么!var!就是一个普通的包含5个字母的字符串，如果开启了延迟环境变量扩展，那么它就是变量var的实际值，可能说到这有的人会产生疑惑，引用变量var的值不是使用%var%吗？那么在开启延迟环境变量扩展的情况下，%var%和!var!有什么区别呢？下面举个例子测试下，编写如下代码保存在extVar.bat文件中： 1234@echo offset var1=110set var1=120&amp;echo %var1% 运行之后的结果为： C:\Users\Administrator\Downloads&gt;extVar.bat110 看到结果的时候是不是再次怀疑了世界，在打印变量var1之前明明重新赋值了120，为什么打印出来还是110呢？其实这是批处理脚本执行机制导致的，它会按行执行，在执行之前会先预处理，当执行set var1=110之后，变量var1变成了110，在执行set var1=120&amp;echo %var1%之前先预处理，将变量%var1%替换成了110，然后语句变成了set var1=120&amp;echo 110，所以就得到了我们上面测试的结果。 想要解决这个问题就需要开启延迟环境变量扩展，语句为setlocal enabledelayedexpansion，然后将引用变量的形式由%var1%改为!var1!即可，所以可以修改代码如下： 12345@echo offsetlocal enabledelayedexpansionset var1=110set var1=120&amp;echo !var1! 运行之后的结果为： C:\Users\Administrator\Downloads&gt;extVar.bat120 这回输出的结果符合预期了，开启了延迟环境变量扩展之后，!var!形式的变量在用之前才会评估确切的值，这是一个知识点，也是一个易错点，特别是在for循环中要格外注意，因为for循环语句的循环体括号中，所有的操作被看成是同一行，所以经常会用到延迟环境变量扩展。 总结 for循环在cmd命令行中的固定用法for %i in (set) do (...)，循环变量格式为%i for循环在bat处理程序中的固定用法for %%i in (set) do (...)，循环变量格式为%%i 至于为什么for语法在批处理中需要多写一个%，希望知道的小伙伴能给出答案和参考资料，不胜感激 想要变量被使用的时候再评估值需要开启延迟环境变量扩展，语法为setlocal enabledelayedexpansion，同时使用!var!形式的变量]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下常用的打包、压缩、解压命令（tar、gzip、bzip2、zip）]]></title>
    <url>%2Fblog%2F2019%2F11%2F06%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%B8%B8%E7%94%A8%E7%9A%84%E6%89%93%E5%8C%85%E3%80%81%E5%8E%8B%E7%BC%A9%E3%80%81%E8%A7%A3%E5%8E%8B%E5%91%BD%E4%BB%A4%EF%BC%88tar%E3%80%81gzip%E3%80%81bzip2%E3%80%81zip%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言经常使用电脑的人常常会接触到压缩文件，不管是软件、数据还是资料，下载之后通常就是一个压缩包，在Windows平台上如果安装了WinRAR或者360压缩，不管是什么格式的压缩文件，一般点击压缩文件右键选择解压选项即可，非常地方便。正因为长时间在Windows平台上方便的解压文件，导致我对打包、压缩的概念理解错误，结果在linux操作压缩文件时有很多疑问，今天终于明白了一点，专门总结一下，同时列举常用的压缩、解压命令，方便日后查找使用。 linux上操作压缩文件也是通过命令实现的，但是压缩文件的后缀有很多，比如.tar.gz、.tar.bz2、.gz、.zip、.Z等等，而生成和解压这些文件的命令同样很多，比如tar、gzip、bzip2、zip、unzip等，看得人眼花缭乱，记忆的过程中也常常出现偏差，不是命令不对应就是参数错误，特别是一些不常用的压缩格式，经常需要查询尝试，浪费了不少时间，其实造成这些问题的原因还是由于对打包压缩的概念不太清楚，接下来先了解一下这些概念。 基础概念在Windows上经常直接在图形化界面上操作压缩和解压文件，导致我将这种操作行为带到了linux上，而实际上在linux上压缩和解压文件之外还有一个操作就是“打包”，原因就是linux的压缩和解压通常作用在一个文件上，如果想将一大堆文件压缩最终成为一个文件，需要先打成一个包，然后对这个包文件进行压缩。 打包/归档打包或者叫归档，就是将多个文件和目录（也可以是一个文件）就变成了一个总的文件，但不是将所有文件进行融合，使用tar命令。 压缩压缩是将一个大的文件通过特定的压缩算法尽可能变成一个小文件，可以减少存储空间，加快网络传输效率，使用gzip、bzip2、zip等命令。 解压解压是将压缩生成的最终的小文件还原为压缩之前的大文件，可以使用gzip、gunzip、bunzip2、unzip等命令。 打包压缩通过上面的概念解析我们可以知道，我们之前所说的压缩操作通常是指打包和压缩两个步骤，由于linux大部分的压缩命令都是只能压缩一个文件，所以在压缩之前需要将待压缩的所有文件先进行打包，生成一个文件后再进行压缩操作。 明白了打包和压缩操作的含义，我们可以通过一些约定俗成的命名规则，选择合适的压缩和解压方法，比如下面这些文件： xxx.tar：这是一个归档文件，也就是只通过tar进行了打包操作 xxx.tar.gz：这是一个压缩文件，打包之后，以gzip方式进行了压缩 xxx.tar.bz2：这是一个压缩文件，打包之后，以bzip2方式进行了压缩 xxx.gz：这是一个压缩文件，没有经过打包操作，只是gzip方式进行了压缩 如果能按照这些命名规则生成压缩文件，那么解压文件的时候会方便很多，但有时压缩文件的扩展名是不标准的，可以通过file命令查看文件实际的格式，使用方法如下： 12345[albert@localhost#15:03:05#/home/albert/compress]$file test.tar.bz2test.tar.bz2: bzip2 compressed data, block size = 900k[albert@localhost#15:03:24#/home/albert/compress]$file test.tar.gztest.tar.gz: gzip compressed data, from Unix, last modified: Wed Nov 6 12:02:05 2019 压缩解压命令压缩文件的格式和命令真的是太多，所以在此总结一份常用命令表格，方便日后需要的时候直接拿来就用，加快解决问题的速度。假设原始文件是a.log和b.txt，当前目录下还有一个output目录，可以作为解压后存放文件的目录，那么常用压缩和解压命令如下： 文件格式 压缩命令 命令备注 解压命令 命令备注 xxx.tar tar -cvf test.tar a.log b.txt - tar -xvf test.tar -C ./output 不使用-C则解包在当前目录 xxx.tar.gz tar -zcvf test.tar.gz a.log b.txt - tar -zxvf test.tar.gz -C ./output 不使用-C则解压在当前目录 xxx.tar.bz2 tar -jcvf test.tar.bz2 a.log b.txt - tar -jxvf test.tar.bz2 -C ./output 不使用-C则解压在当前目录 xxx.tar.Z tar -Zcvf test.tar.Z a.log b.txt - tar -Zxvf test.tar.Z -C ./output 不使用-C则解压在当前目录 xxx.gz gzip -c a.log &gt; test.gz、gzip a.log 前者保留a.log，后者直接删除a.log gzip -d test.gz、gunzip test.gz 不能指定解压文件存储目录 xxx.bz2 bzip2 -c a.log &gt; test.bz2、bzip2 a.log 前者保留a.log，后者直接删除a.log bzip2 -d test.bz2、bunzip2 test.bz2 不能指定解压文件存储目录 xxx.Z compress -c a.log &gt; test.Z、compress a.log 前者保留a.log，后者直接删除a.log compress -d test.Z、uncompress test.Z 不能指定解压文件存储目录 xxx.rar rar a test.rar a.log - unrar e test.rar 将e选项换成x可以指定目录 xxx.zip zip test.zip a.log b.txt - unzip test.zip -d ./output 不使用-d则解压在当前目录 分析对比上面的压缩也解压命令可以发现，tar这个命令可以将打包和压缩合并到一起，也可以将解压和解包合并到一起，只需要修改选项中的参数就可以调用不同的程序压缩或者解压，比如-cvf表示只打包不压缩，而-zcvf表示打包后使用gzip压缩，改为-jcvf表示打包后使用bzip2压缩，其实还有很多的压缩方式，可以参考一下tar命令的帮助文档，具体压缩选项如下。 压缩选项: -a, –auto-compress 使用归档后缀名来决定压缩程序 -I, –use-compress-program=PROG 通过 PROG 过滤(必须是能接受 -d 选项的程序) -j, –bzip2 通过 bzip2 过滤归档 -J, –xz 通过 xz 过滤归档 –lzip 通过 lzip 过滤归档 –lzma 通过 lzma 过滤归档 –lzop –no-auto-compress 不使用归档后缀名来决定压缩程序 -z, –gzip, –gunzip, –ungzip 通过 gzip 过滤归档 -Z, –compress, –uncompress 通过 compress 过滤归档 总结 上述这些命令只是基础用法，还有很多参数选型没有提到，比如tar -tf test.tar可以不解压直接查看归档文件中的内容。 gzip命令只能压缩一个文件，如果在命令后面添加多个文件，则会分别压缩生成多个文件。 据说compress命令是一个相当古老的 unix 档案压缩指令，现在基本被gzip命令取代了。 由于文中涉及的命令较多，难免有些笔误，为了不传播错误用法，我也进行了多次检查，如果大家还发现其他错误，欢迎批评指正。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>tar</tag>
        <tag>gzip</tag>
        <tag>bzip2</tag>
        <tag>zip</tag>
        <tag>打包</tag>
        <tag>压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb调试程序时跳进函数和跳出函数]]></title>
    <url>%2Fblog%2F2019%2F11%2F01%2Fgdb%E8%B0%83%E8%AF%95%E7%A8%8B%E5%BA%8F%E6%97%B6%E8%B7%B3%E8%BF%9B%E5%87%BD%E6%95%B0%E5%92%8C%E8%B7%B3%E5%87%BA%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言说实话平时在Windows平台上开发，gdb调试用的并不是很多，但是一些在linux平台才会出现的BUG，或者在linux运行时宕机产生了core文件，这些还是需要使用gdb调试的，之前的文章《linux环境下服务器程序的查看与gdb调试》列举了常用的gdb命令，基本上调试一些core文件和简单bug使用这些命令足以了，但是新的需求总是会出现。 新的需求也很常见，就是跳进一个函数，调试一部分代码后还要跳出这个函数，一般情况就是这个函数特别长，调试前几行已经明白函数的逻辑和用意，如果使用 next 命令逐行运行需要花费较多时间，所以需要跳出函数回到调用的位置，这两个操作在Visual Studio中的快捷键分别是F11和Shift+F11，使用起来非常的方便，其实在gdb调试的过程中也有对应的命令，分别是step(s)和finish(fin)，括号中的内容为命令的简写，此外还有一个return命令也可以使函数返回，接下来可以看一下它们的区别。 测试代码测试的代码很简单，只需要写一个简单的函数，并且在主函数中调用这个函数即可，代码如下： 12345678910111213141516171819#include &lt;iostream&gt;using namespace std;int son_func()&#123; int a = 100; int b = 1; return a + b;&#125;int main()&#123; int i = 10; cout &lt;&lt; i &lt;&lt; endl; int result = son_func(); cout &lt;&lt; result &lt;&lt; endl;&#125; 代码编译编译代码时只需要注意一点，那就是加上-g选项，否则可能会影响调试： 1albert@localhost#11:56:18#/home/albert/gdbtest]$g++ -g stepfinish.cpp -o stepfinishtest step/finish组合这个组合不会影响函数的运行结果，简单的调试过程如下： 1234567891011121314151617181920212223242526272829303132333435363738394041[albert@localhost#11:32:17#/home/albert/gdbtest]$gdb stepfinishtestGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/gdbtest/stepfinishtest...done.(gdb) startTemporary breakpoint 1 at 0x400809: file stepfinish.cpp, line 13.Starting program: /home/albert/gdbtest/stepfinishtestTemporary breakpoint 1, main () at stepfinish.cpp:1313 int i = 10;Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-23.el6.x86_64(gdb) n14 cout &lt;&lt; i &lt;&lt; endl;(gdb) n1016 int result = son_func();(gdb) stepson_func () at stepfinish.cpp:66 int a = 100;(gdb) finishRun till exit from #0 son_func () at stepfinish.cpp:60x0000000000400831 in main () at stepfinish.cpp:1616 int result = son_func();Value returned is $1 = 101(gdb) n18 cout &lt;&lt; result &lt;&lt; endl;(gdb) n10119 &#125;(gdb) cContinuing.Program exited normally.(gdb) 首先使用start命令启动程序，然后使用next(n)命令让程序运行到调用函数son_func()所在的行，使用step命令进入函数，我们看到a的值为100，假设这时我们想回到这个函数被调用的位置，直接敲finish命令就可以，函数完整的执行并返回结果101，最后连续执行next(n)命令，程序正常退出，整个过程只是调试查看数据，并没有改变程序运行结果。 step/return组合这个组合有可能会影响函数的运行结果，具体要看return命令使用的位置和返回的参数： 12345678910111213141516171819202122232425262728293031323334353637383940414243[albert@localhost#11:53:42#/home/albert/gdbtest]$gdb stepfinishtestGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/gdbtest/stepfinishtest...done.(gdb) startTemporary breakpoint 1 at 0x400809: file stepfinish.cpp, line 13.Starting program: /home/albert/gdbtest/stepfinishtestTemporary breakpoint 1, main () at stepfinish.cpp:1313 int i = 10;Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-23.el6.x86_64(gdb) n14 cout &lt;&lt; i &lt;&lt; endl;(gdb) n1016 int result = son_func();(gdb) stepson_func () at stepfinish.cpp:66 int a = 100;(gdb) n7 int b = 1;(gdb) return 119Make son_func() return now? (y or n) y#0 0x0000000000400831 in main () at stepfinish.cpp:1616 int result = son_func();(gdb) n18 cout &lt;&lt; result &lt;&lt; endl;(gdb) print result$1 = 119(gdb) cContinuing.119Program exited normally.(gdb) 首先同样使用start命令启动程序，然后使用next(n)命令让程序运行到调用函数son_func()所在的行，使用step命令进入函数，我们看到a的值为100，这时再敲入next(n)命令让程序运行一步，可以看到b的值为1，假设这时我们想返回一个自定义值而不返回a+b的结果，可以直接敲命令return 119，表示直接返回119这个值，再打印返回值变量result发现是值119，跳出函数的同时，程序运行结果也已经被我们改变了。 总结 gdb中跳入函数的命令是step，相当于Visual Studio中的快捷键F11 gdb中跳出函数的命令是finish，相当于Visual Studio中的快捷键Shift+F11，函数完整执行后返回 gdb中还有一个直接返回的命令是return，它会跳过当前函数后面的语句直接返回，返回值可以自定义，紧跟在return命令后面即可]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>step</tag>
        <tag>finish</tag>
        <tag>return</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python使用scatter函数绘制点在线的上层]]></title>
    <url>%2Fblog%2F2019%2F10%2F30%2FPython%E4%BD%BF%E7%94%A8scatter%E5%87%BD%E6%95%B0%E7%BB%98%E5%88%B6%E7%82%B9%E5%9C%A8%E7%BA%BF%E7%9A%84%E4%B8%8A%E5%B1%82%2F</url>
    <content type="text"><![CDATA[前言前几天在QQ群里发现有人问这样一个问题，使用Python的matplotlib库绘制图形时，函数 scatter() 绘制的点总是在 plot() 函数绘制的线下边，看起来样子很丑，大概就是下图这个样子，问有没有方法让点显示到线的上面。 看到这个问题一开始以为是绘制顺序的原因，调整 scatter() 函数和 plot() 函数的调用顺序并没有达到预想的效果，点还是在线的下面，原来一直没注意这个问题，是因为我一直把线和标注用的点使用了同一种颜色，所有看不出来是谁覆盖了谁，现在抛出这个问题居然让人有点不知所措，直觉上认定肯定有个属性可以设置这个显示顺序，但究竟是图的属性？坐标轴的属性？还是函数的参数呢？这个还需要查一查。 解决办法最后经过一顿查找发现函数 scatter() 和 plot() 都有个参数 zorder，这时候才恍然大悟，做游戏界面开发时常常使用这个参数来控制UI的层级，现在怎么突然忘了呢，赶紧设置一下发现与原来在游戏开发中的参数含义相同，zorder这个整数越大，显示的时候越靠上，所以写了下面的测试代码： 1234567891011121314import numpy as npimport matplotlib.pyplot as pltdef draw_point_on_line(): plt.title("draw point on line") plt.xlabel("x-axis") plt.ylabel("y-axis") X = np.linspace(1, 100, 10, endpoint=True) Y = np.random.randint(60, 100, len(X)) plt.plot(X, Y, color ='blue', linewidth=3.5, zorder=1) # 在第一层画线 plt.scatter(X, Y, 50, color ='red', zorder=2) # 在第二层画点 plt.show() 效果展示这次终于正常了，因为scatter()函数中的zorder值较大，所以放到上面绘制，效果如下： 总结 帮助他人解决问题是一个自我提升的好机会 有很多我们认为很简单的事情，其实并不是我们想的那样 代码文件前后图例代码对比-传送门]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>matplotlib</tag>
        <tag>plt</tag>
        <tag>scatter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python切割超大日志文件、保留文件最后几行]]></title>
    <url>%2Fblog%2F2019%2F10%2F24%2FPython%E5%88%87%E5%89%B2%E8%B6%85%E5%A4%A7%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E3%80%81%E4%BF%9D%E7%95%99%E6%96%87%E4%BB%B6%E6%9C%80%E5%90%8E%E5%87%A0%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[前言关于日志这个东西的存在，主要是为了记录发生的事情，编程的过程中也常常用到，记得我们在刚刚学习编程的时候，常常会出现程序错误，这时候就需要输出一下，其实这个输出也是日志的一种体现，随着编程水平的提升，各种调试工具和方法渐渐进入我们的视线，但是输出一下这种方法却一直被使用，特别是一些偶发性问题，调试工具很难捕捉到他们，这时候往往需要将中间过程输出到日志文件中，这些日志文件就是我们分析问题的基础。 随着程序规模的渐渐扩大，出现问题时需要打印的日志也越来越多，最近就出现这样一个情况，游戏程序总是莫名的崩溃，看代码找不到问题的原因，所以采用了打印日志文件的方法，有时候大约跑半天就能出现崩溃，日志文件大概600M，Windows系统自带的记事本很难打开，但是使用Notepad++等几秒钟是可以看到内容的。 比较变态的是最近一次跑了2天才崩溃，导出日志文件发现大概有3G，这次使用Notepad++打开时也卡死了，使用sublime打开时进度卡在了80%左右，据说非常强大的GVim打开文件时也毫无反应了，这就尴尬了，崩溃之前的日志内容就在文件中，可是我们却看不见。 问题出现问题很明显摆在这了，文件由于太大无法看到其中的内容，得想个办法。很直接的一个想法就进入了脑海，把文件拆开成几份，这样每个文件缩小了就可以看到了啊，所以我们找到了一个解决问题的办法，接下来使用Python来简单写一下切分文件。 分割日志文件按照文件大小分割分割文件的规则需要先确定一下，可以很简单的按照文件大小分割，一个源文件大小为10M的日志文件，可以切分成10个大小为1M的日志文件，分割的大小不用太绝对，每一份近似相等就可以，整体思路就是先获得源文件的大小，然后计算出分割结束每个文件的大小，接着不断从源文件中读内容，往目标文件中写内容，达到之前计算的字节大小时，再生成新的目标文件，简单代码如下： 12345678910111213141516171819202122import osDATA_LEN_PER_READ = 1024 * 1024def split_file_by_size(file_name, parts=3): file_size = os.path.getsize(file_name) per_file_size = file_size//parts file_size_list = [] for x in range(parts-1): file_size_list.append(per_file_size) file_size_list.append(file_size-per_file_size*(parts-1)) output_file, ext = os.path.splitext(file_name) with open(file_name, 'rb') as rfile: for n in range(parts): with open('&#123;0&#125;_part&#123;1&#125;&#123;2&#125;'.format(output_file, n+1, ext), 'wb') as wfile: read_size = 0 while read_size &lt; file_size_list[n]: want_read = min(DATA_LEN_PER_READ, file_size_list[n] - read_size) wfile.write(rfile.read(want_read)) read_size += want_read 按照文件行数分割以上按照文件大小分割的日志文件有一点小问题，不能保证行是完整的，当遇到汉字这种占用多字节的字符，甚至都不能保证汉字是完整的，所以我们可以换一个思路，尝试使用按照行数分割，整体思路就是先获得文件的行数，然后计算出分割结束每个文件的行数，接着不断从源文件中一行行读内容，往目标文件中一行行写内容，达到之前计算的文件行数时，再生成新的目标文件，简单代码如下： 12345678910111213141516171819202122232425import osdef get_file_line_count(file_name): line_count = 0 for index, line in enumerate(open(file_name, 'r')): line_count += 1 return line_countdef split_file_by_line(file_name, parts=3): total_line = get_file_line_count(file_name) per_file_line = total_line//parts file_line_list = [] for x in range(parts-1): file_line_list.append(per_file_line) file_line_list.append(total_line-per_file_line*(parts-1)) output_file, ext = os.path.splitext(file_name) with open(file_name, 'rb') as rfile: for n in range(parts): with open('&#123;0&#125;_part&#123;1&#125;&#123;2&#125;'.format(output_file, n+1, ext), 'wb') as wfile: read_line = 0 while read_line &lt; file_line_list[n]: wfile.write(rfile.readline()) read_line += 1 获取日志文件尾部内容对于分析奔溃这类问题，出现问题的日志往往就在最后几行，所以没有必要非得打开整个日志文件，也不一定需要将整个文件分割成几部分，只需将日志文件的最后一部分读出来写到新的文件中供我们分析就可以了，这时候开头几个字符的不完整也是可以接受的，所以没必要按行读取，只需按照经验，从尾部截取指定字节大小的内容就可以了，比如我们的日志，最后有用的部分也就10M左右，一般的文本文件就都能打开了。 代码思路就是先打开文件，然后将读指针定位到尾部往前10M左右，然后读取所有内容保存到新的文件中，简单的代码示例如下： 12345678import osdef tailslice(file_name, data_size= 1024): output_file, ext = os.path.splitext(file_name) with open(file_name, 'rb') as rfile: rfile.seek(-data_size, 2) with open('&#123;0&#125;_tail&#123;1&#125;'.format(output_file, ext), 'wb') as wfile: wfile.write(rfile.read()) 总结 有时候要问题需要变通一下，如果一个文件大到打不开的地步，我们就把它分割成可以接受的范围 当文件中的内容只有一部分可用时，我们完全可以不分割，直接取出可用部分即可。 代码传送门 分割超大日志文件 获取日志文件尾部内容]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>日志文件</tag>
        <tag>split</tag>
        <tag>切割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中查询当前用户、当前数据库等基础信息]]></title>
    <url>%2Fblog%2F2019%2F10%2F14%2FMysql%E4%B8%AD%E6%9F%A5%E8%AF%A2%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7%E3%80%81%E5%BD%93%E5%89%8D%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AD%89%E5%9F%BA%E7%A1%80%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言生活中有时会出现灵魂三问：我是谁？我在哪？我在做什么？特别的喝醉酒的第二天，完全不记得昨天发生了什么。而在数据库操作中也会出现这种灵魂拷问，我用的是哪个用户，为什么会没有权限？我操作的是哪个数据库，刚刚不会把线上正式服务器数据删了吧？ 上面描述的问题常常出现在切换数据库处理问题的时候，通过一个客户端连接到Mysql数据库服务器，操作数据库1，然后切换再操作数据库2，这时如果中间有人打扰，很容易忘记刚刚操作的是哪个数据库，或者中途处理个其他紧急的事情，回来连操作的用户都忘了，这时就需要一些基础信息的查询命令，帮助你来恢复记忆。 数据库基础信息查询数据库的基础信息涉及到方方面面，这里只列举几个常用的查询命令，用来回答上面的灵魂拷问，其他命令还有很多，用到了再总结吧。 查询当前操作的用户1234567mysql&gt; select user();+----------------+| user() |+----------------+| root@localhost |+----------------+1 row in set (0.07 sec) 查询当前操作的数据库1234567mysql&gt; select database();+------------+| database() |+------------+| sqltest2 |+------------+1 row in set (0.09 sec) 查询当前数据库端口1234567mysql&gt; show variables like 'port';+---------------+-------+| Variable_name | Value |+---------------+-------+| port | 3306 |+---------------+-------+1 row in set (0.07 sec) 查询当前数据库版本1234567mysql&gt; select version();+------------+| version() |+------------+| 5.7.21-log |+------------+1 row in set (0.06 sec) 数据库结构信息查询这里的结构我指的是DDL中定义的那些元素，比如表、存储过程等，有一些常用的查询命令，要是一段时间不使用还是会忘记，比如查询一个数据库中的存储过程，每次查询时都要上网搜一下，所以今天总结在一起方便查找。 查询当前数据库中的所有表1234567891011121314mysql&gt; show tables;+--------------------+| Tables_in_sqltest2 |+--------------------+| a || b || c || d || m || p || tb_test || tb_with_index |+--------------------+14 rows in set (0.11 sec) 查询创建表的sql语句12345678910mysql&gt; show create table a;+-------+----------------------------------------+| Table | Create Table |+-------+----------------------------------------+| a | CREATE TABLE `a` ( `id` int(11) DEFAULT NULL, `num` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+----------------------------------------+1 row in set (0.11 sec) 查询指定表中的所有字段12345678mysql&gt; desc a;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.12 sec) 查询当前数据库中的所有存储过程这个命令我得吐槽一下，为什么不能像查询当前数据库的中所有表一样，搞个show procedures;命令，非得通过where子句指定数据库呢，具体的原因还不知道，等我弄明白了再回来补充。123456789mysql&gt; show procedure status where db='sqltest2';+----------+-----------------------+-----------+----------------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+| Db | Name | Type | Definer | Modified | Created | Security_type | Comment | character_set_client | collation_connection | Database Collation |+----------+-----------------------+-----------+----------------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+| sqltest2 | fill_slow_query_test | PROCEDURE | root@localhost | 2019-03-25 11:14:01 | 2019-03-25 11:14:01 | DEFINER | | utf8mb4 | utf8mb4_general_ci | utf8_general_ci || sqltest2 | fill_tb_without_index | PROCEDURE | root@localhost | 2019-03-18 09:53:32 | 2019-03-18 09:53:32 | DEFINER | | utf8mb4 | utf8mb4_general_ci | utf8_general_ci || sqltest2 | fill_tb_with_index | PROCEDURE | root@localhost | 2019-03-18 09:53:33 | 2019-03-18 09:53:33 | DEFINER | | utf8mb4 | utf8mb4_general_ci | utf8_general_ci |+----------+-----------------------+-----------+----------------+---------------------+---------------------+---------------+---------+----------------------+----------------------+--------------------+3 rows in set (0.17 sec) 查询创建存储过程的sql语句1234567891011121314151617mysql&gt; show create procedure fill_tb_with_index;+--------------------+----------------------------------------------------------------+-------------------+----------------------+----------------------+--------------------+| Procedure | sql_mode | Create Procedure | character_set_client | collation_connection | Database Collation |+--------------------+----------------------------------------------------------------+-------------------+----------------------+----------------------+--------------------+| fill_tb_with_index | STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |CREATE DEFINER=`root`@`localhost` PROCEDURE `fill_tb_with_index`()BEGIN DECLARE i int default 1; WHILE i &lt;= 100000 do insert into tb_with_index values(i, i, i); set i = i + 1; END WHILE;END | utf8mb4 | utf8mb4_general_ci | utf8_general_ci |+--------------------+----------------------------------------------------------------+-------------------+----------------------+----------------------+--------------------+1 row in set (0.09 sec) 查询指定表上的索引123456789mysql&gt; show index from tb_with_index;+---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| tb_with_index | 1 | id_index | 1 | id | A | 100035 | NULL | NULL | YES | BTREE | | || tb_with_index | 1 | num_index | 1 | num | A | 98715 | NULL | NULL | YES | BTREE | | || tb_with_index | 1 | money_index | 1 | money | A | 100035 | NULL | NULL | YES | BTREE | | |+---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+3 rows in set (0.03 sec) 查询当前用户连接的权限1234567mysql&gt; show grants;+---------------------------------------------------------------------+| Grants for root@localhost |+---------------------------------------------------------------------+| GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' WITH GRANT OPTION || GRANT PROXY ON ''@'' TO 'root'@'localhost' WITH GRANT OPTION |+---------------------------------------------------------------------+ 查询指定用户连接的权限1234567mysql&gt; show grants for 'guest';+---------------------------------------------------------------------------------------------------------------+| Grants for guest@% |+---------------------------------------------------------------------------------------------------------------+| GRANT ALL PRIVILEGES ON *.* TO 'guest'@'%' IDENTIFIED BY PASSWORD '*6C8DE74065898C44C21EF74D67A834C5256BFA1C' |+---------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec) 总结 以上总结的查询语句都是我经常用到，相比Mysql所有查询语句来说简直是冰山一角，总结到一起主要是方便日后查找，同时也希望给他人带来帮助 查看这些语句会发现，有些是select开头，有些是show开头，实际上很多show开头的都是对information_schema数据库数据的封装 information_schema 数据库是Mysql系统自带的数据库，记录了整个数据库实例上所有数据结构信息，更像是记录数据库的数据库，包含表结构、字符集，权限等太多的信息，有机会后续找时间聊聊这个数据库，在此就不展开了 正因为很多show开头的都是对information_schema数据库数据的封装，所以这些查询语句基本都可以通过在information_schema数据库查询得到，比如show procedure status where db=&#39;sqltest2&#39;;就可以改写成select routine_name from information_schema.routines where routine_schema=&#39;sqltest2&#39;; 此时此刻，我正在距离天安门500多米的现场等待阅兵仪式的开始，一边学习一边为祖国庆生的感觉真好！（注：500多米多了5公里，现场是卧室床前的电视机旁，这么近的距离不知道一会能不能看见接受检阅的飞机o(\￣︶￣*)o）*]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>用户</tag>
        <tag>当前信息</tag>
        <tag>表结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下C++代码打印函数堆栈调用情况]]></title>
    <url>%2Fblog%2F2019%2F09%2F06%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8BC-%E4%BB%A3%E7%A0%81%E6%89%93%E5%8D%B0%E5%87%BD%E6%95%B0%E5%A0%86%E6%A0%88%E8%B0%83%E7%94%A8%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[前言关于什么是函数调用堆栈在上篇文章《windows环境下C++代码打印函数堆栈调用情况》中已经介绍过了，简单的来说就是可以展现出函数之间的调用关系，上篇文章展示了如何在windows上打印出函数调用堆栈，其中用到了windows系统上的API，这些接口在linux上是无法使用的，因为工作的关系，也常常需要在linux的调试程序，所以本文介绍一下如何在linux上打印出C++程序的调用堆栈。 实现打印堆栈信息的函数在linux系统上想打印函数调用堆栈信息，需要引用头文件&lt;execinfo.h&gt;，然后利用函数backtrace、backtrace_symbols来获取当时的函数调用堆栈信息，以下的代码实现了一个简单的打印堆栈新的函数，堆栈深度最大同样设置为12层。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;execinfo.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#define STACK_INFO_LEN 1024void ShowTraceStack(const char* szBriefInfo)&#123; static const int MAX_STACK_FRAMES = 12; void *pStack[MAX_STACK_FRAMES]; static char szStackInfo[STACK_INFO_LEN * MAX_STACK_FRAMES]; char ** pStackList = NULL; int frames = backtrace(pStack, MAX_STACK_FRAMES); pStackList = backtrace_symbols(pStack, frames); if (NULL == pStackList) return; strcpy(szStackInfo, szBriefInfo == NULL ? "stack traceback:\n" : szBriefInfo); for (int i = 0; i &lt; frames; ++i) &#123; if (NULL == pStackList[i]) break; strncat(szStackInfo, pStackList[i], STACK_INFO_LEN); strcat(szStackInfo, "\n"); &#125; printf("%s", szStackInfo); // 输出到控制台，也可以打印到日志文件中&#125;void func2()&#123; bool isError = true; if (isError) &#123; ShowTraceStack("error in func2\n"); &#125; else &#123; printf("this is func2\n"); &#125;&#125;void func1()&#123; int sum = 0; for (int i = 0; i &lt; 100; ++i) sum += i; func2();&#125;int main(int argc, char* argv[])&#123; printf("hello world\n"); func1(); return 0;&#125; 显示堆栈调用信息上面的测试代码中函数的调用逻辑为：main()函数调用func1()函数，然后func1()函数调用func2()函数，当func2()中发生问题的时候打印当时的堆栈信息，然后我们编译一下查看运行结果 12345678910[albert@localhost#10:59:03#/home/albert/test/backtrace]$g++ -rdynamic linuxtraceback.cpp -o linuxtraceback[albert@localhost#10:59:05#/home/albert/test/backtrace]$./linuxtracebackhello worlderror in func2./linuxtraceback(_Z14ShowTraceStackPKc+0x25) [0x400a19]./linuxtraceback(_Z5func2v+0x1c) [0x400b06]./linuxtraceback(_Z5func1v+0x32) [0x400b46]./linuxtraceback(main+0x1e) [0x400b66]/lib64/libc.so.6(__libc_start_main+0xfd) [0x7fbbcae43d1d]./linuxtraceback() [0x400939] 上面的运行结果已经展示了程序函数的调用关系，其中编译选项中的-rdynamic是很重要的，它实际上是一个链接选项，作用是把所有符号（而不仅仅只是程序已使用到的外部符号）都添加到动态符号表里，以便那些通过 dlopen() 或 backtrace()这样的函数使用，换句话说就是如果不加这个选项在调用堆栈中就可能看不到函数名。 上面的调用堆栈中函数名大致能看出来，但是有些奇怪的字母，可以通过工具c++fileter来处理，处理之后就可以看到正常的函数名了，具体使用方式如下：123456789[albert@localhost#10:59:12#/home/albert/test/backtrace]$./linuxtraceback | c++filthello worlderror in func2./linuxtraceback(ShowTraceStack(char const*)+0x25) [0x400a19]./linuxtraceback(func2()+0x1c) [0x400b06]./linuxtraceback(func1()+0x32) [0x400b46]./linuxtraceback(main+0x1e) [0x400b66]/lib64/libc.so.6(__libc_start_main+0xfd) [0x7f73aea34d1d]./linuxtraceback() [0x400939] 编译时无法添加-rdynamic选项如果是自己写的小项目或者小程序，编译选项是可以随便改的，没有什么关系，需要查看堆栈信息加上-rdynamic就可以了，但是如果是公司的大型项目，编译选项是不会随便改的，可能是直接使用automake生成的，先来看一下不添加-rdynamic选项编译之后的运行结果 12345678910[albert@localhost#11:22:15#/home/albert/test/backtrace]$g++ linuxtraceback.cpp -o linuxtraceback[albert@localhost#11:22:18#/home/albert/test/backtrace]$./linuxtracebackhello worlderror in func2./linuxtraceback() [0x4007d9]./linuxtraceback() [0x4008c6]./linuxtraceback() [0x400906]./linuxtraceback() [0x400926]/lib64/libc.so.6(__libc_start_main+0xfd) [0x7f4e12ba2d1d]./linuxtraceback() [0x4006f9] 可以看到不添加-rdynamic选项编译之后运行虽然能显示出调用堆栈，但都是一些函数地址，无法看到函数名，这时可以通过工具addr2line帮助我们定位问题，这个工具的作用就是将函数地址转换成函数所在的行，使用方法就是在命令行运行addr2line 0x4008c6 -e ./linuxtraceback，具体使用时替换函数地址和可运行程序的名字即可 说实话这个小程序中使用运行addr2line 0x4008c6 -e ./linuxtraceback没有看到我想要的，只显示了??:0，仿佛被优化掉了，但是我在正式的项目中使用这个方法是可以得到函数所在行的，这也帮助我查到了一个隐藏很深的BUG。 总结 linux平台下可以利用函数backtrace、backtrace_symbols、backtrace_symbols_fd来获取当时的函数调用堆栈信息 使用上述函数时，需要引用头文件&lt;execinfo.h&gt;，编译时最好加上-rdynamic选项 如果实在无法添加-rdynamic，可以通过addr2line辅助查找问题 程序源码打印堆栈信息–源码传送门]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>函数</tag>
        <tag>linux</tag>
        <tag>堆栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows环境下C++代码打印函数堆栈调用情况]]></title>
    <url>%2Fblog%2F2019%2F09%2F03%2Fwindows%E7%8E%AF%E5%A2%83%E4%B8%8BC-%E4%BB%A3%E7%A0%81%E6%89%93%E5%8D%B0%E5%87%BD%E6%95%B0%E5%A0%86%E6%A0%88%E8%B0%83%E7%94%A8%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[前言程序运行的过程中，函数之间的是会相互调用的，在某一时刻函数之间的调用关系，可以通过函数调用堆栈表现出来，这个调用堆栈所展现的就是函数A调用了函数B，而函数B又调用了函数C，这些调用关系在代码中都是静态的，不需要程序运行就可以知道。 既然函数之间的调用关系可以通过分析代码就可以知道，那么查看函数调用的堆栈是不是作用不大了呢？事实上恰恰相反，查看函数调用堆栈的作用非常大。因为在较大型的项目中，函数之间的调用不是简单的一条线，常常会出现复杂的网状结构，这时如果函数C被调用了，可能不是仅仅是B函数调用过来的，也有可能是D、E、F等函数调用了C函数，所以知道在程序运行时究竟是哪个函数调用了C函数显得很重要，特别是有众多函数会调用C函数的时候。 查看函数堆栈的作用举个例子就明白了，假如C函数中逻辑的执行需要一些特殊条件状态，理论上执行C函数时这些条件都应该满足的，但是程序在运行的过程中有时运行C函数时条件就是不满足的，那就说明有些调用C函数的逻辑分支有问题，无法满足C函数中逻辑所需条件，这时候知道是谁调用C函数导致条件不满足就是确定问题的关键。 如果是在VS调试状态下，在C函数不满足条件的逻辑中打一个断点，然后运行程序等待断点触发时，就可以通过VS工具自带的调用堆栈窗口，就可以看到程序从主函数main()开始怎样一步步调用的出错的函数C的。 可实际项目中，出错的时候不总是在VS的调试状态下，也有可能发生在程序实际的工作环境中，这时没有办法通过加断点来查看调用堆栈，如果此时有一个函数，可以打印当前的函数调用堆栈那就太好了，这样我就可以在需要调试的逻辑中，调用这个函数，将当时的函数调用堆栈信息打印到文件中，方便查找程序逻辑问题，这篇文章要做的就是在Windows环境下，利用现有的API实现这样一个函数。 实现打印堆栈信息的函数在Windows系统上想打印函数调用堆栈信息，需要引用头文件&lt;dbghelp.h&gt;，添加库引用DbgHelp.Lib，然后利用函数CaptureStackBackTrace、SymFromAddr、SymGetLineFromAddr64来获取当时的函数调用堆栈信息，以下的代码实现了一个简单的打印堆栈新的函数，堆栈深度最大设置为12层，实际情况肯定是越深越好，设置为12一般就可以查到问题了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include &lt;windows.h&gt;#include &lt;dbghelp.h&gt;#include &lt;stdio.h&gt;#if _MSC_VER#define snprintf _snprintf#endif#define STACK_INFO_LEN 1024void ShowTraceStack(char* szBriefInfo)&#123; static const int MAX_STACK_FRAMES = 12; void *pStack[MAX_STACK_FRAMES]; static char szStackInfo[STACK_INFO_LEN * MAX_STACK_FRAMES]; static char szFrameInfo[STACK_INFO_LEN]; HANDLE process = GetCurrentProcess(); SymInitialize(process, NULL, TRUE); WORD frames = CaptureStackBackTrace(0, MAX_STACK_FRAMES, pStack, NULL); strcpy(szStackInfo, szBriefInfo == NULL ? "stack traceback:\n" : szBriefInfo); for (WORD i = 0; i &lt; frames; ++i) &#123; DWORD64 address = (DWORD64)(pStack[i]); DWORD64 displacementSym = 0; char buffer[sizeof(SYMBOL_INFO)+MAX_SYM_NAME * sizeof(TCHAR)]; PSYMBOL_INFO pSymbol = (PSYMBOL_INFO)buffer; pSymbol-&gt;SizeOfStruct = sizeof(SYMBOL_INFO); pSymbol-&gt;MaxNameLen = MAX_SYM_NAME; DWORD displacementLine = 0; IMAGEHLP_LINE64 line; line.SizeOfStruct = sizeof(IMAGEHLP_LINE64); if (SymFromAddr(process, address, &amp;displacementSym, pSymbol) &amp;&amp; SymGetLineFromAddr64(process, address, &amp;displacementLine, &amp;line)) &#123; snprintf(szFrameInfo, sizeof(szFrameInfo), "\t%s() at %s:%d(0x%x)\n", pSymbol-&gt;Name, line.FileName, line.LineNumber, pSymbol-&gt;Address); &#125; else &#123; snprintf(szFrameInfo, sizeof(szFrameInfo), "\terror: %d\n", GetLastError()); &#125; strcat(szStackInfo, szFrameInfo); &#125; printf("%s", szStackInfo); // 输出到控制台，也可以打印到日志文件中&#125;void func2()&#123; bool isError = true; if (isError) &#123; ShowTraceStack("error in func2\n"); &#125; else &#123; printf("this is func2\n"); &#125;&#125;void func1()&#123; int sum = 0; for (int i = 0; i &lt; 100; ++i) sum += i; func2();&#125;int main(int argc, char* argv[])&#123; printf("hello world\n"); func1(); return 0;&#125; 显示堆栈调用信息上面的测试代码中函数的调用逻辑为：main()函数调用func1()函数，然后func1()函数调用func2()函数，当func2()中发生问题的时候打印当时的堆栈信息，然后我们查看一下打印结果 hello worlderror in func2 ShowTraceStack() at e:\vs2013projects\trackback\windowstrackback\windowstrackback.cpp:24(0xe01440) func2() at e:\vs2013projects\trackback\windowstrackback\windowstrackback.cpp:59(0xe01840) func1() at e:\vs2013projects\trackback\windowstrackback\windowstrackback.cpp:74(0xe017c0) main() at e:\vs2013projects\trackback\windowstrackback\windowstrackback.cpp:82(0xe018c0) __tmainCRTStartup() at f:\dd\vctools\crt\crtw32\dllstuff\crtexe.c:626(0xe01d40) mainCRTStartup() at f:\dd\vctools\crt\crtw32\dllstuff\crtexe.c:466(0xe020c0) error: 487 error: 487 error: 487 总结 Windows平台下可以利用函数CaptureStackBackTrace、SymFromAddr、SymGetLineFromAddr64来获取当时的函数调用堆栈信息 使用上述函数时，需要引用头文件&lt;dbghelp.h&gt;，添加库引用DbgHelp.Lib 程序源码打印堆栈信息–源码传送门]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>函数</tag>
        <tag>windows</tag>
        <tag>堆栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[.bat批处理（七）：PC端从手机内复制文件到本地]]></title>
    <url>%2Fblog%2F2019%2F08%2F21%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9APC%E7%AB%AF%E4%BB%8E%E6%89%8B%E6%9C%BA%E5%86%85%E5%A4%8D%E5%88%B6%E6%96%87%E4%BB%B6%E5%88%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[前言解决方案都是在实际工作中遇到问题时想出来解决方法，很多方法乍一看根本用不上，但实际操作中发现真的很有用，今天提到的这个方法就是这种类型的。 游戏开发中常常会将一些关键信息或者调试信息写入到日志文件中，这样可以在出现BUG的情况时，通过分析日志文件来进一步定位问题的原因，在真机上跑游戏时就需要将手机中的日志文件导出到电脑上，方便查看，这就是这篇文章所讲的内容。 可能有人会说，现在手机连接电脑很方便，直接插一根数据线，在“我的电脑”里找到手机，然后就可以像从其他文件夹复制一下，从手机中把文件复制下来，可事实上并不是这样的，手机连接电脑有个缓存的毛病。 这种问题就是第一次连接的时候查看文件是正常，但是复制删除几次文件以后就会出现缓存的现象，我明明新建了一个文件就是找不到，比如产生了新的日志文件，通过数据线连接电脑以后，在文件夹中看不到，这时可以通过adb命令复制出来，虽然看不到，但是文件是确实存在的。 准备条件 需要电脑安装adb，常用来调试手机的电脑一定会安装过这个东西，有些版本直接可以使用，具体怎么安装，网上的教程有很多。 手机需要打开USB调试模式，打开模式前可能需要开启开发者选项，同样开启USB调试的教程也有很多。 实现代码1234567891011121314151617@SET LOG_FILE_NAME=project_%date:~0,4%%date:~5,2%%date:~8,2%%time:~0,2%%time:~3,2%%time:~6,2%.logadb pull /storage/emulated/0/project2.log ./%LOG_FILE_NAME%@echo offecho running result:if %errorlevel%==0 goto endSuccess:endFailecho Copy data from phone to pc falied!!!pauseexit /b 1:endSuccessecho Copy data from phone to pc success!!!pauseexit /b 0 代码分析其实这一大段中核心的代码只有一句adb pull /storage/emulated/0/project2.log ./%LOG_FILE_NAME%，之前的变量LOG_FILE_NAME是为了解决多次导出文件时同名会覆盖的问题，加上时间字符串可以防止重名出现，adb pull 手机中路径+文件名 本地PC路径+文件名就是实际复制的过程 如果复制过程中不报错就会走到:endSuccess代码段，如果报错就会走到:endFail代码段，两段代码会返回不同的值供调用者判断，整个代码文件加了一些提示消息，如果嫌麻烦的话直接使用adb pull /storage/emulated/0/project2.log ./%LOG_FILE_NAME%也是可以的。 代码测试直接在cmd命令行中运行就可以，假设以上的bat文件名为CopydataPhone2PC.bat，手机根目录下有文件project.log，我们可以尝试拷贝project.log和project2.log两个文件到手机看看效果，当然project2.log文件是不存在的肯定会失败 拷贝成功1234567E:\batTool&gt;CopydataPhone2PC.batE:\batTool&gt;adb pull /storage/emulated/0/project.log ./project_20190822102324.log124 KB/s (1284 bytes in 0.010s)running result:Copy data from phone to pc success!!!请按任意键继续. . . 拷贝失败1234567E:\batTool&gt;CopydataPhone2PC.batE:\batTool&gt;adb pull /storage/emulated/0/project2.log ./project_20190822102422.logremote object '/storage/emulated/0/project2.log' does not existrunning result:Copy data from phone to pc falied!!!请按任意键继续. . . 总结有些领域真的很奇妙，如果你之前没有接触过，直接告诉你，手机里有个很普通的文件，但是你就是看不到，你会不会觉得很奇怪，针对于这些奇怪的问题其实别人可能早就有了解决方案，百思不得其解时不妨浏览一下。]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[std::bind（二）：包装成员函数]]></title>
    <url>%2Fblog%2F2019%2F08%2F15%2Fstd-bind%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8C%85%E8%A3%85%E6%88%90%E5%91%98%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言关于std::bind()对普通函数的包装作用，在之前的总结文章《std::bind（一）：包装普通函数》已经举例说明过了，后来发现丢下了普通函数嵌套包装的情况，所以在这篇文章中继续说明一下，然后重点总结std::bind()函数对成员函数的包装，在面向对象的大潮还未褪去的今天，还是成员函数见到的更多一些，所以讲讲对它的包装。 普通函数嵌套包装实际上就是普通函数包装的变形和组合，直接写个例子吧，如果test1_1()、test1_2()、test1_3()三个函数的输出结果都答对了就说明已经掌握了。12345678910111213141516171819202122232425262728293031323334353637void func(int n1, int n2, int n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl;&#125;int calc_value(int c1)&#123; return c1 * c1;&#125;void calc_value2(int c1)&#123; int result = c1 * c1;&#125;void test1_1()&#123; auto f1 = std::bind(func, placeholders::_1, 101, std::bind(calc_value, placeholders::_2)); f1(11, 2); // same as call func(11, 101, calc_value(2))&#125;void test1_2()&#123; int n = 2; auto f1 = std::bind(func, placeholders::_1, 101, std::bind(calc_value, std::ref(n))); n = 4; f1(11, 2); // same as call func(11, 101, calc_value(44)) 多出的参数2无人使用&#125;void test1_3()&#123; auto f1 = std::bind(func, placeholders::_1, 101, std::bind(calc_value2, placeholders::_2)); //f1(11, 2); // 编译出错，无法将参数 3 从“void”转换为“int”&#125;// 11 101 4// 11 101 16 第一个test1_1函数的逻辑应该很容易理解，就是把函数calc_value(2)的返回值作为函数func的第三个参数，而函数test1_2中利用了std::ref()传递引用的功能，将变量n作为引用变量进行传递，在包装调用之前可以感知到参数n的变化。 其实难点在第三个函数test1_3，可能大家知道这里会报错，因为我们需要返回值但是却包装了一个没有返回值的函数，但其实把第二行注释掉之后，程序就可以成功编译，也就是说包装错误的函数如果不被调用，是不会报错的，这一点和模板函类不使用就不会创建很相似，最终是相同的。 包装类成员在深入学习std::bind()这个函数之前一直以为它只能用来包装函数，后来通过进一步了解发现它还能用来包装成员变量，我们一起来看一下简单的实现方法。 成员函数的包装这里我们不考虑静态成员函数，因为静态函数没有this指针，和普通的函数基本一样，在用法上也没有很大的差异，所以此处的包装只考虑成员非静态函数，可以尝试分析以下几个例子。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class CTest&#123;public: CTest() &#123;&#125; ~CTest() &#123;&#125;public: void func1(int n1, int n2) &#123; cout &lt;&lt; "func1 " &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; endl; &#125; int n_public;private: void func2(int n1, int n2) &#123; cout &lt;&lt; "func2 " &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; endl; &#125; int n_private;&#125;;void test2_1()&#123; CTest testObj; auto f2 = std::bind(&amp;CTest::func1, testObj, 101, placeholders::_1); f2(1); // same as call testObj.func1(101, 1)&#125;void test2_2()&#123; CTest testObj; auto f2 = std::bind(&amp;CTest::func1, &amp;testObj, 101, placeholders::_1); f2(2); // same as call testObj.func1(101, 2)&#125;void test2_3()&#123; CTest testObj; CTest&amp; obj = testObj; auto f2 = std::bind(&amp;CTest::func1, obj, 101, placeholders::_1); f2(3); // same as call testObj.func1(101, 3)&#125;void test2_4()&#123; CTest testObj; auto f2 = std::bind(&amp;CTest::func1, placeholders::_1, placeholders::_2, 101); f2(testObj, 4); // same as call testObj.func1(4, 101)&#125;void test2_5()&#123; CTest testObj; // auto f2 = std::bind(&amp;CTest::func2, &amp;testObj, 101, placeholders::_1); // 编译错误，func2不可访问&#125;//func1 101 1//func1 101 2//func1 101 3//func1 4 101 前三个函数tes2_1()、tes2_2()、tes2_3()的作用基本一致，就是将一个类的非静态成员函数和对象绑定，并且可以动态绑定一些参数，三种调用方式都可以，暂时没有发现什么问题，大家知道区别的可以指导我一下，我补充上来，需要注意的是函数std::bind()参数个数需要在原函数参数个数的基础上加两个，第一个很明显就是函数名，而第二个必须是调用这个函数的对象，至于传递的是指针还是引用都没有什么问题，这两个参数过后才是真正的原函数的参数。 函数test2_4()相对于前三个来说更加灵活，将对象也最为参数在调用时传入，这就相当于把一个成员函数看成，一个普通函数然后在第一个参数前加this指针的形式，后面这种调用方式在查看C++调用堆栈时应该很容易看到，本质上是一样，其实这里还有一个对象传递的问题，我们在成员变量时再测试一下。 函数test2_5()出现了编译错误，原因是在使用函数std::bind()的时候也要考虑到原函数的访问权限，在测试函数中访问对象的私有函数显然是不可以的。 成员变量的包装1234567891011121314151617181920212223242526272829303132333435void test3_1()&#123; CTest testObj; auto f3 = std::bind(&amp;CTest::n_public, testObj); f3(1) = 10; cout &lt;&lt; f3(1) &lt;&lt; endl; cout &lt;&lt; testObj.n_public &lt;&lt; endl;&#125;void test3_2()&#123; CTest testObj; auto f4 = std::bind(&amp;CTest::n_public, placeholders::_1); f4(testObj) = 4; cout &lt;&lt; f4(testObj) &lt;&lt; endl; cout &lt;&lt; testObj.n_public &lt;&lt; endl;&#125;void test3_3()&#123; CTest testObj; auto f3 = std::bind(&amp;CTest::n_public, std::ref(testObj)); f3(1) = 11; cout &lt;&lt; f3(1) &lt;&lt; endl; cout &lt;&lt; testObj.n_public &lt;&lt; endl;&#125;//10//-858993460//4//4//11//11 这个成员变量的绑定测试结果，有没有让人意想不到呢？或者说这种f3(1) = 10;写法已经让人很惊讶了，其实我在写例子的时候就是简单试试，没想到这样写居然可以，看起来好像把一个值赋值给了一个函数一样。 函数test3_1()的第二个输出可能有点想不到，但是看到结果是有些人可能就明白了，因为在上一篇里提到“std::bind()函数中的参数在被复制或者移动时绝不会以引用的方式传递，除非你使用了std::ref()或者std::cref()包装的参数”。 因为没有使用std::ref()函数包装，所以std::bind()函数绑定的testObj对象实际上是原对象的副本，那么针对于副本的操作和修改自然就不会反应到原对象上，这也就是打印testObj.n_public会输出随机值的原因。 函数test3_2()在绑定时并没有具体到特定的对象，而是使用了placeholders::_1占位符，这样生成的函数，在调用的时候再传入操作对象，那么此时修改对象属性就可以起作用了。 函数test3_3()是针对于函数test3_1()的，添加了std::cref()包装的原对象，可以通过绑定后的函数修改。 总结 std::bind()函数可以嵌套绑定 std::bind()函数绑定成员函数时，函数名参数后面需要紧跟着类的对象作为参数 std::bind()不仅可以绑定普通函数、成员函数、还可以绑定成员变量 完整代码代码传送门]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>bind</tag>
        <tag>placeholders</tag>
        <tag>class</tag>
        <tag>object</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[雷电模拟器一键宏实现循环点击]]></title>
    <url>%2Fblog%2F2019%2F08%2F09%2F%E9%9B%B7%E7%94%B5%E6%A8%A1%E6%8B%9F%E5%99%A8%E4%B8%80%E9%94%AE%E5%AE%8F%E5%AE%9E%E7%8E%B0%E5%BE%AA%E7%8E%AF%E7%82%B9%E5%87%BB%2F</url>
    <content type="text"><![CDATA[前言今天在使用雷电模拟器测试游戏的时候，有一个领奖界面需要点击领奖100次，程序猿作为解放劳动力的先锋，必须想个办法解决这个事情，按键精灵是个好东西，但是重装系统之后还没有安装，然后发现这个雷电模拟器里除了简单的按键映射，还有一键宏的功能，那就用它解决了。 解决过程关于雷电模拟器的按键操控功能，官方论坛-帮助教程已经写得很清楚了，一键宏怎么设置教程里也写的很清楚，唯一的缺点就是各个截图中的代码太模糊了，根本看不清，所以我在尝试的过程中还花了点时间，其中遇到了几个坑和大家分享下。 我用的模拟器版本是3.54，这个版本在编写一键宏的时候可以直接在界面上获得对应点的坐标，非常的方便，在开始写一键宏的时候有一个误区，就是怎么控制我写的这段宏代码的开始与结束，起初我还控制按键的按下和抬起，发现没有什么用，最后测试发现，就是设置的按键按下时执行，如果存在循环就一直执行，按键抬起时执行结束，简单粗暴。 代码编写其实这个一键宏也算不上代码，最多也就算个伪代码，然后通过模拟器解析一下，要实现循环点击的功能需要的指令不多，只有4个： size：指定模拟器的分辨率 loop：说明以下的指令开始循环 touch：点击屏幕上的像素点 wait：休息一下，防止点击过快，单位是毫秒 其中坑人最深的就是size这个命令，我一开始以为是设置分辨率的，而我玩游戏默认的分辨率是1600X900，所以设置的点击位置也是在这个分辨率下取的点，然后就没加这个指令，结果一键宏一直不生效，后来加上了size 1600 900这一句才好使，这时我才明白这个指令不是设置分辨率的，而是告诉以下指令，当前的分辨率是多少，在操作像素点时不至于选错，宏的内容很简单，只有四句： 1234size 1600 900looptouch 1400 350wait 1000 这个宏组合的意思也很清楚，在分辨率1600X900的情况下，循环点击(1400, 350)这个像素点，每次点击间隔1s，防止点击过快。 总结 一键宏设置完成后，按键按下执行，如果是循环指令，则按键抬起结束 size指令并不是设置分辨率，而是说明现在的分辨率]]></content>
      <categories>
        <category>game</category>
      </categories>
      <tags>
        <tag>雷电</tag>
        <tag>模拟器</tag>
        <tag>按键操控</tag>
        <tag>一键宏</tag>
        <tag>loop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[std::bind（一）：包装普通函数]]></title>
    <url>%2Fblog%2F2019%2F08%2F01%2Fstd-bind%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8C%85%E8%A3%85%E6%99%AE%E9%80%9A%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言不知道大家在做项目写程序的过程中有没有遇到这样的情况，别的模块类提供了一个拥有很多参数接口函数，但是我这个功能只用到其中几个，其他的参数都是固定的，可是为了调用这个接口函数，不得不将所有的参数写一遍，每次写一堆固定参数都感觉在浪费生命。 有的人可能想到默认参数，的确，默认参数可以解决部分问题，因为默认参数只能出现参数列表的尾部，如果4个参数中，我需要传递的参数是第4个，而前3个参数想默认的话，默认参数是做不到这种效果的，并且别人的接口函数也不一定会有默认参数。 函数封装，这是一个办法，我们在自己的模块中添加一个对接口函数进行包装后的函数，将不变的参数进行固定，然后只留可变的参数供我们自己调用，如果我们有3种常用的调用方式可能就需要定义3个函数，这种方法可行，不过比较麻烦，而std::bind()函数就是为了包装函数而生的，使用起来更加方便。 std::bind()的作用std::bind()的作用就是对原函数进行包装，可以将参数值固定，调换顺序然后生成新的函数供我们调用。举个例子，一块铁片你可以拿它来做很多事情，打造一下可以做成一把刀，敲敲打打可以做成一个桶，甚至直接拿来就可以挡窗户上的洞。std::bind()的作用就是把这块铁的作用固定，比如给她安上一个刀把，这样我们每次使用就可以把这块铁片当成菜刀来使用了。 std::bind()可以包装各种函数，但是这篇文章只总结一下包装普通函数的方法，因为在学习的过程中我发现单单是包装普通函数也会遇到很多问题，所以为了列举出诸多可能，说明各种注意事项，本文还是只关注于普通函数的包装，至于成员函数的包装还是放到以后的文章，给自己埋下一个坑。 在包装普通函数时，std::bind()的第1个参数就是原函数的名字，当然也可以是指向函数的指针，或者函数引用，从第2个参数开始，填写的内容依次对应原函数中的各个参数，所以说如果原函数是3个参数，如果想包装它，那么std::bind()需要传入4个参数，如果原函数是8个参数，那么包装它的std::bind()就需要传入9个参数，这里为了将原函数和包装后的函数参数建立联系，需要引入命名空间std::placeholders。 placeholders的作用std::placeholders的命名空间下有多个参数占位符，比如placeholders::_1、placeholders::_2等等，最大为placeholders::_20，在包装普通函数时，固定的参数很好说，就是填写固定值就可以，但是要想原函数的参数和包装后函数的参数建立联系就需要用到刚刚提到的占位符， placeholders::_1就表示包装后函数的调用时的第1个参数，同理placeholders::_2就表示包装后函数的调用时的第2个参数。 有了占位符的概念，我们就可以推断出，包装后的函数与原函数相比，不但可以减少函数参数，也可以增加函数参数，虽然暂时没有想到什么实际的使用场景，但是理论上是可行的。 std::bind()使用测试首先需要先引入头文件，免得找不到命名空间和函数定义123#include &lt;iostream&gt;#include &lt;functional&gt;using namespace std; 固定参数、调换顺序1234567891011121314151617181920void func1(int n1, int n2, int n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl;&#125;void test1_1()&#123; auto f1 = std::bind(func1, placeholders::_1, 101, placeholders::_2); f1(11, 22); // same as call func1(11, 101, 22)&#125;void test1_2()&#123; auto f1 = std::bind(func1, placeholders::_2, 101, placeholders::_1); f1(11, 22); // same as call func1(22, 101, 11)&#125;// 输出//11 101 22//22 101 11 函数test1_1()展示了std::bind()函数最常见的用法，其中参数n2被固定为101，参数n1使用占位符placeholders::_1表示，表示包装后函数的第1个参数会传给形参n1使用，同理包装后函数的第2个参数会传给形参n3使用，所以调用函数f1(11, 22) 就等同于调用函数 func1(11, 101, 22)，test1_2()函数简单展示了调换参数顺序的方法，只要明白了placeholders的作用，这两个例子也就明白了。 包装后函数的参数个数可增可减123456789101112131415161718192021222324252627void func2(int n1, int n2, int n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl;&#125;void test2_1()&#123; auto f2 = std::bind(func2, placeholders::_3, 101, placeholders::_1); f2(11, 22, 33); // same as call func2(33, 101, 11)&#125;void test2_2()&#123; auto f2 = std::bind(func2, placeholders::_1, 101, placeholders::_1); f2(11); // same as call func2(11, 101, 11)&#125;void test2_3()&#123; auto f2 = std::bind(func2, placeholders::_1, 101, placeholders::_2); f2(11); // 报错，因为没有参数传给placeholders::_2&#125;// 输出//33 101 11//11 101 11//编译错误 其实在理解了placeholders的作用之后，这个测试结果也能想到的，函数test2_1()中使用了placeholders::_3，所以包装后函数的参数至少要传3个才不会报错，而test2_2()函数中使用了placeholders::_1，所以被包装函数调用时只需要传入一个参数，最后是函数test2_3()，绑定时引用了placeholders::_2，而在调用时只传了一个参数，所以出现编译错误。 bind()绑定时参数个数固定，类型需匹配12345678910111213141516171819202122void func3(int n1, int n2, int n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl;&#125;void test3_1()&#123; auto f3 = std::bind(func3, placeholders::_1, 101); //f3(11); // 编译错误，因为bind函数中少了一个参数&#125;void test3_2()&#123; auto f3 = std::bind(func3, placeholders::_1, 101, 102, 103); //f3(11); // 编译错误，因为bind函数中多了一个参数&#125;void test3_3()&#123; auto f3 = std::bind(func3, placeholders::_1, "test", placeholders::_1); //f3(11); // 编译错误，第二个参数类型不匹配，无法将参数 2 从“const char *”转换为“int”&#125; 看了之前的测试之后，是不是觉得参数的个数很随意，可以随便增加和减少，所以在绑定的时候也不好好写了，结果发现上述3个函数全部编译错误，test3_1()函数中因为绑定时少了一个参数而报错，test3_2()函数中因为绑定时多了一个参数而报错，而test3_3()函数中因为绑定时第二个参数的类型不匹配而报错，所以参数个数的增减只能是包装后的函数，而绑定时必须严格与原函数的参数个数以及类型相匹配。 普通函数的参数中有引用类型弄明白上面的例子之后，可能会产生一种我会了的错觉，想象一下如果原函数参数中包含引用类型应该怎样写，可以自己先想一下，然后看看下面的例子1234567891011121314151617181920212223242526272829303132333435363738394041424344void func4(int n1, int n2, int&amp; n3)&#123; cout &lt;&lt; n1 &lt;&lt; ' ' &lt;&lt; n2 &lt;&lt; ' ' &lt;&lt; n3 &lt;&lt; endl; n3 = 101;&#125;void test4_1()&#123; int n = 10; auto f4 = std::bind(func4, 11, 22, n); n = 33; f4(); // same as call func4(11, 22, 10) cout &lt;&lt; "n = " &lt;&lt; n &lt;&lt; endl;&#125;void test4_2()&#123; const int n = 30; auto f4 = std::bind(func4, 11, 22, n); f4(); // same as call func4(11, 22, 30)&#125;void test4_3()&#123; int n = 30; auto f4 = std::bind(func4, 11, 22, ref(n)); n = 33; f4(); // same as call func4(11, 22, n) cout &lt;&lt; "n = " &lt;&lt; n &lt;&lt; endl;&#125;void test4_4()&#123; const int n = 30; auto f4 = std::bind(func4, 11, 22, ref(n)); //f4(); // 编译错误，无法将参数 3 从“const int”转换为“int &amp;”&#125;// 输出//11 22 10//n = 33//11 22 30//11 22 33//n = 101 如果能准确说出test4_1()函数的输出结果，那么后面的内容应该是不需要看了，如果只回答对了部分内容，或者干脆全错了，那么我们还有很长的路要走。 在std::bind()的官方文档中有这样一句话，std::bind()函数中的参数在被复制或者移动时绝不会以引用的方式传递，除非你使用了std::ref()或者std::cref()包装的参数，如果知道了这个限定，就很容易明白函数test4_1()函数的输出结果了。 在函数test4_1()中std::bind(func4, 11, 22, n)就相当于std::bind(func4, 11, 22, 10)，所以输出结果为11 22 10，可是函数func4()中还有一句 n3 = 101;，这就很让人奇怪了，我们知道常数是没办法作为参数传递给可变引用变量的，如果说把10作为参数传递给参数int&amp; n3肯定会报错，而函数test4_1()却正常执行，没有任何问题。 我们猜测常数10到参数int&amp; n3并不是直接传递，而是发生了拷贝，而函数func4()中修改的n3变量也是修改的拷贝内容，所以我们做了test4_2()这个实验，发现将变量n改为常量也是可以正常执行的，甚至直接写成std::bind(func4, 11, 22, 10)也是没问题的，这也验证了我们上面的想法。 既然文档了提到了std::ref()和std::cref()函数，那么我们想传递引用给原函数只能使用它们了，看下函数test4_3()的实现，这才是正确传递引用变量的方式，变量n被函数 std::ref() 包装之后，既能够感受到本函数中变量n的变化，也能够传入到原函数中被原函数的逻辑改变，并将结果反映回来。 函数test4_4()只是一个常量传递的简单测试，将一个常量作为可变引用变量来传递肯定是无法通过编译的，这在函数调用时很明确，但是在std::bind()加入之后显得有些混乱，只要记住一点，常量不应该被改变，如果传递之后内容可能会变化，那么很可能这种写法就是错误的。 总结 其实std::bind()函数测试到现在远远没有结束，配合std::ref()和std::cref()函数会产生多种组合情况，不过主要的问题上面都提到了一些，出现问题的时候对照着定义和概念看看应该就能理解了。 需要理解std::placeholders的占位作用，它们是std::bind()函数最基本的用法。 完整代码代码传送门]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>auto</tag>
        <tag>bind</tag>
        <tag>placeholders</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中char和varchar的区别]]></title>
    <url>%2Fblog%2F2019%2F07%2F27%2FMysql%E4%B8%ADchar%E5%92%8Cvarchar%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言这个标题起的过于俗套，前一段时间我还写了一篇总结《Mysql5.7版本中数据表字段可用的类型》来批判这种对比，原因是对比时没有指明数据库，内容写的是char、varchar和nvarchar的对比，结果我测试了半天发现Mysql当前版本根本没有nvarchar，浪费来了不少时间。 问题起因真香定律来的总是这么快，这才过了几天，我也来写写Mysql中char和varchar究竟有什么区别，起因是看到CSDN好友“铁柱同学”一篇关于innodb主键长度最大为767字节的讲解，里面涉及到一个char类型最大存储255个字符，按照utf8编码来看最大的字节数应该是255*3=765个字节的知识点。现在来看767的来源好像并不是256*3-1，而是255*3+2，这个2就是存储char类型字段中实际有多少个字节的。 有点跑题了，实际上是在研究索引长度的过程中，我发现我对char和varchar这两个类型一直存在着误解，因为一直做游戏开发的缘故，游戏数据的存储一般使用varbinary来存，导致我把字符和字节有点弄混了，所以我一直认为在utf8编码下char(9)可以存储9个英文字符，或者3个中文汉字，实际我做完实验后发现char(9)也可以正常存储9个汉字。 提到字符和字节，初学者可能会有点蒙，实际上它们两者之间是需要通过编码来转换的，之前做过游戏的多语言版本，所以对这一块还是比较熟的，字节就是计算机中的8个二进制位，而字符是每个语言中的不可分割的单元，字符转换成字节需要依赖编码，实际上编码就是一本大字典，里面对应了每个字符在当前编码下转换成字节是什么样的，ANSI是一本字典，UTF8也是一本字典，编码的类型还有很多，每种编码都记录了各自的转换结果。 举个例子，“中”这个字是汉字中的一个字符，在ANSI这本字典中对应的是2个字节，而在UTF8这本字典中对应的是3个字节，而C这个字母是英文中的一个字符，在ANSI这本字典中对应的是1个字节，而在UTF8这本字典中对应的同样是1个字节，从这个例子中可以简单理解下字节与字符的关系。 多说一句，不要认为UTF8编码中汉字转换成字节都是3个字节，实际情况是常用字一般都占用了3个字节，但是中国语言博大精深，光语言平面就单独占了好几个，有些汉字转换成UTF8编码可能需要4个字节，5个字节甚至是6个字节，这一点不要形成思维定式，认为汉字在UTF8编码下都是3个字节。 length 和 char_length今天之前我是不知道Mysql还有一个char_length函数的，发现这个函数后越发感觉Mysql的强大，这两个函数的区别就是length用来统计字段中的字节数，char_length用来统计字段中的字符数，接下来我们用一个例子来看看这两个函数以及char、varchar的区别。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 10Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 测试过程 首先创建一个带有char和varchar类型的测试表，查看表结构发现编码为utf8 1234567891011121314mysql&gt; create table diff(id int, s1 char(10), s2 varchar(10));Query OK, 0 rows affected (0.07 sec)mysql&gt; show create table diff;+-------+------------------------------------------------------------------------+| Table | Create Table |+-------+------------------------------------------------------------------------+| diff | CREATE TABLE `diff` ( `id` int(11) DEFAULT NULL, `s1` char(10) DEFAULT NULL, `s2` varchar(10) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 |+-------+------------------------------------------------------------------------+1 row in set (0.07 sec) 插入少于10个字符的测试数据，然后查看结果，发现字节数为10，字符数为6 12345678910mysql&gt; insert into diff values(1, "测试test", "测试test");Query OK, 1 row affected (0.01 sec)mysql&gt; select id, s1, length(s1), char_length(s1), s2, length(s2), char_length(s2) from diff;+----+----------+------------+-----------------+----------+------------+-----------------+| id | s1 | length(s1) | char_length(s1) | s2 | length(s2) | char_length(s2) |+----+----------+------------+-----------------+----------+------------+-----------------+| 1 | 测试test | 10 | 6 | 测试test | 10 | 6 |+----+----------+------------+-----------------+----------+------------+-----------------+1 row in set (0.06 sec) 增加插入长度后发现，可以超过10个字节，可以完整存储10个汉字 1234567891011mysql&gt; insert into diff values(2, "测试策划和开发做游戏", "测试策划和开发做游戏");Query OK, 1 row affected (0.01 sec)mysql&gt; select id, s1, length(s1), char_length(s1), s2, length(s2), char_length(s2) from diff;+----+----------------------+------------+-----------------+----------------------+------------+-----------------+| id | s1 | length(s1) | char_length(s1) | s2 | length(s2) | char_length(s2) |+----+----------------------+------------+-----------------+----------------------+------------+-----------------+| 1 | 测试test | 10 | 6 | 测试test | 10 | 6 || 2 | 测试策划和开发做游戏 | 30 | 10 | 测试策划和开发做游戏 | 30 | 10 |+----+----------------------+------------+-----------------+----------------------+------------+-----------------+2 rows in set (0.10 sec) 分别增加s1和s2字段长度后发现，均无法正常插入，Mysql给出报错信息 12345mysql&gt; insert into diff values(2, "测试策划和开发做游戏OK", "测试策划和开发做游戏");1406 - Data too long for column 's1' at row 1mysql&gt; insert into diff values(2, "测试策划和开发做游戏", "测试策划和开发做游戏OK");1406 - Data too long for column 's2' at row 1mysql&gt; 至此没有看出区别，在插入内容前后都加上空格测试一下 123456789mysql&gt; select id, s1, concat('#', s1, '$'), length(s1) as len_s1, char_length(s1) as clen_s1, s2, concat('#', s2, '$'), length(s2) as len_s2, char_length(s2) as clen_s2 from diff;+----+----------------------+------------------------+--------+---------+----------------------+------------------------+--------+---------+| id | s1 | concat('#', s1, '$') | len_s1 | clen_s1 | s2 | concat('#', s2, '$') | len_s2 | clen_s2 |+----+----------------------+------------------------+--------+---------+----------------------+------------------------+--------+---------+| 1 | 测试test | #测试test$ | 10 | 6 | 测试test | #测试test$ | 10 | 6 || 2 | 测试策划和开发做游戏 | #测试策划和开发做游戏$ | 30 | 10 | 测试策划和开发做游戏 | #测试策划和开发做游戏$ | 30 | 10 || 3 | OK | # OK$ | 3 | 3 | OK | # OK $ | 4 | 4 |+----+----------------------+------------------------+--------+---------+----------------------+------------------------+--------+---------+ 这一次出现了区别，char类型的字段去掉了尾部的空格，而varcahr了类型的字段原样存储，没有去掉尾部空格，两者对于头部的空格都是存储的，这导致两者显示的字节数和字符数都不相同了。 分别使用不带空格、带头部空格，头尾都带空格进行测试12345678910111213141516171819202122232425262728293031323334353637mysql&gt; select * from diff where s1 = 'OK ';Empty setmysql&gt; select * from diff where s2 = 'OK ';Empty setmysql&gt; select * from diff where s1 = ' OK ';+----+-----+------+| id | s1 | s2 |+----+-----+------+| 3 | OK | OK |+----+-----+------+1 row in set (0.05 sec)mysql&gt; select * from diff where s2 = ' OK ';+----+-----+------+| id | s1 | s2 |+----+-----+------+| 3 | OK | OK |+----+-----+------+1 row in set (0.05 sec)mysql&gt; select * from diff where s1 = ' OK';+----+-----+------+| id | s1 | s2 |+----+-----+------+| 3 | OK | OK |+----+-----+------+1 row in set (0.05 sec)mysql&gt; select * from diff where s2 = ' OK';+----+-----+------+| id | s1 | s2 |+----+-----+------+| 3 | OK | OK |+----+-----+------+1 row in set (0.04 sec) 测试结果可能让人出乎意料，虽然s1和s2中存储的内容不同（差一个空格），但是查找时的行为却完全一样，这说明查找时尾部的空格并不会被考虑。 char和varchar区别做了半天试验发现char和varchar还是没有多大区别，实际上有些区别通过表面数据是测试不出来的，具体区别整理如下： 行为 char字段 varchar字段 最大长度 255字符 65535个字节，所以括号中最大的字符数还得通过编码来算 是否定长 定长，不足的部分用隐藏空格填充 不定长 空间使用 会有浪费 更加节省 查找效率 高 低 尾部空格 插入时省略 插入时不会省略，查找时省略 like查找 语句中like后的’ ‘不会省 语句中like后的’ ‘不会省，字段结尾的空格也不会省 总结 char(n)中的n是字符数，范围是0~255（额外需要1到2个字节来存长度） varchar(n)中的n也是字符数，但是最大值需要通过编码来算，不能超过65535字节（从中还需要拿出1到2个字节来存长度） 一般定长的数据选用char类型，比如身份证号，手机号，电话等，长度变化很大的可以使用varchar类型 注意尾部空格的匹配，特别是插入时和使用like查找时]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>char</tag>
        <tag>Mysql</tag>
        <tag>varchar</tag>
        <tag>length</tag>
        <tag>char_length</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时分秒针在一天之内重合多少次]]></title>
    <url>%2Fblog%2F2019%2F07%2F23%2F%E6%97%B6%E5%88%86%E7%A7%92%E9%92%88%E5%9C%A8%E4%B8%80%E5%A4%A9%E4%B9%8B%E5%86%85%E9%87%8D%E5%90%88%E5%A4%9A%E5%B0%91%E6%AC%A1%2F</url>
    <content type="text"><![CDATA[前言分析问题之前先给出问题的答案：2次，送给急需要知道答案又不求甚解的朋友。 这个问题之前听过类似的，一直没有当回事，今天在解题的时候发现了这道题，于是动脑筋想了一下，从12点位置时分秒3个表针重合开始，第一次应该在1点5分之后，那是分针转了一圈快追上时针了，再稍微走一点就能追上，然后秒针再转过来就完成了第一次重合，同理在2点10分之后也有一次，在3点15之后还有一次，这样算下来12小时之内有11次，那么一天24小时就有22次。 正在为自己的想法得意时，查了一下参考答案发现我被幼稚的想法打败了，实际上一天24小时内，时分秒针只重合了2次，原因就是我设想从12点开始到1点5分，分针转了一圈快追上时针了，此刻时针与分针确实会有一次相遇，但是此时的秒针却没办法跟他们相逢，因为三个表针是联动的，针对于每个精确到秒的时间，三个针都有固定的位置，不是想重合就能重合的。 在1点5分附近的情况就是，时针和分针快要重合了，然后秒针匆匆赶来，然后时针和分针重合了，秒针还差一点才能到，然后秒针继续走，但是秒针走会继续带动分针和时针运动，然后秒针赶到了分针时针相遇的附近，却发现它俩已经“分手”了，秒针只能大步流星的一个个越过它们俩，期待着下次它们仨能相遇在一处。 时针和分针的相遇在考虑时分秒三针重合情况之前，我们可以先想一下一天24小时内，分针和时针相遇了多少次，其实这才是我刚才想的那个答案22次，知道了次数之后我们还想知道具体的时间，可不可以算出来呢？当然可以！ 接下来我们以一种通俗的方式来解这个问题，那就是列方程式求解，首先将时间作为连续值来看待，我们设时针的角速度是ω，因为时针走1格，分针会走1圈，也就是12格，所以分针的角速度是12ω，分针转了一圈追上时针用的是t，时针和分针转过角度差为1圈，也就是2π，那么此时可以列出方程： $$12ωt-ωt=2π$$ 关于角速度ω的值，因为1圈的角度是2π，转一圈需要花的时间是12小时，所以ω=2π/12小时，带入方程得到t=12小时/11，同理如果分针转两圈追上时针，那么方程式为： $$12ωt-ωt=4π$$ 可以求得t=24小时/11，由此我们就得到了，时针分针相遇时刻与分针转的圈数i的关系： $$t=i*12小时/11$$ 代码实现有了上面的分析，我们可以写代码计算一下一天之中时针和分针相遇具体时刻，因为开着lua编辑器，顺手就用lua写了，代码如下： 123456789101112function print_meet(id, meet) local h = math.floor(meet) local t = meet - h; local ts = t * 3600; local m = ts // 60; local s = ts - m * 60; print(string.format("%02dth meet, time = %02d:%02d:%05.2f", id, h, m, s));endfor i=1,24 do print_meet(i, i * 12 / 11)end 运行结果 01th meet, time = 01:05:27.2702th meet, time = 02:10:54.5503th meet, time = 03:16:21.8204th meet, time = 04:21:49.0905th meet, time = 05:27:16.3606th meet, time = 06:32:43.6407th meet, time = 07:38:10.9108th meet, time = 08:43:38.1809th meet, time = 09:49:05.4510th meet, time = 10:54:32.7311th meet, time = 12:00:00.0012th meet, time = 13:05:27.2713th meet, time = 14:10:54.5514th meet, time = 15:16:21.8215th meet, time = 16:21:49.0916th meet, time = 17:27:16.3617th meet, time = 18:32:43.6418th meet, time = 19:38:10.9119th meet, time = 20:43:38.1820th meet, time = 21:49:05.4521th meet, time = 22:54:32.7322th meet, time = 24:00:00.0023th meet, time = 25:05:27.2724th meet, time = 26:10:54.55 分析从上面的结果来看，处于一天内的时间相遇时刻只有前22次，12点之后第一次相遇是在01:05:27.27，此时虽然时针和分针相遇，但是秒针大概在27秒的位置，离他们还很远，同理分针时针第二次相遇时刻02:10:54.55，秒针也没有跟他们在一起，但是有两次例外，那就是12:00:00.00和24:00:00.00，这两次时针、分针、秒针完全重合了，所以我们也得到了本文标题中的答案。 总结 时分秒针在一天之内重合2次 从连续的时间来看，时针和分针在一天之内重合22次 有一种现实情况就是表盘上的时间是离散的，不连续的，最小的时间间隔是1秒，此时我们计算的第一次相遇时间01:05:27.27是不存在的，01:05:27的时候分针在时针之前，而01:05:28的时候分针在时针之后，它们也错过了，所以时针和分针考虑离散的情况，一天之后也只是重合2次。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Lua</tag>
        <tag>time</tag>
        <tag>interview</tag>
        <tag>clock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OSI七层模型中各层协议及作用]]></title>
    <url>%2Fblog%2F2019%2F07%2F18%2FOSI%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B%E4%B8%AD%E5%90%84%E5%B1%82%E5%8D%8F%E8%AE%AE%E5%8F%8A%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言OSI七层模型在网络这门学科中占有很大的比重，最近在看《图解TCP/IP》这本书，其中对模型中的各个层的作用和对应的协议讲的很详细，而自己有时候总是记错，所以想总结一下，巩固记忆，毕竟好记性不如烂笔头嘛，现在烂笔头不好找了，应该说烂键盘吗？ 各层简析对比 名称 作用 常用协议或标准 相关设备 传输单位 应用层 特定应用对接收数据的处理 HTTP、FTP、SMTP 终端、服务器 - 表示层 设备数据格式与网络标准数据格式转换 LPP、GIF、JPEG 终端、服务器 - 会话层 通信管理，建立和断开通信连接 RPC、SSL、TLS 终端、服务器 - 传输层 管理两个网络终端之间的数据传输 TCP、UDP 终端、服务器 段 网络层 网络地址管理和路由选择 IP/IPv6、ICMP 路由器、三层交换机 分组、包 数据链路层 互联设备之间传送和识别数据帧 ARP、PARP 网桥、二层交换机 帧 物理层 比特流与电子信号之间的转换 IEEE 802.3/802.2 网卡、网线、集线器、中继器、调制解调器 比特位 总结 这里只是我看书之后的基础理解与总结，后续关系紧密的内容也会更新到这里。 本文很多内容中掺杂着个人的理解，如果有不正确的地方欢迎批评指正，我会尽快修改，这也是一种有效的学习方式。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>OSI</tag>
        <tag>七层模型</tag>
        <tag>网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++自定义全部替换函数replace]]></title>
    <url>%2Fblog%2F2019%2F07%2F16%2FC-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%85%A8%E9%83%A8%E6%9B%BF%E6%8D%A2%E5%87%BD%E6%95%B0replace%2F</url>
    <content type="text"><![CDATA[前言今天遇到一个问题，需要把源字符串中的所有A串替换成B串，可能是最近写脚本写的太多了，第一反应就是使用replace()函数就完成了，在 Lua 和 Python 中确实如此，但是我现在正在写C++啊，查询std::string发现确实有一个repalce()函数，但是查看定义后发现事情却不像想象的那样简单。 C++中的这个replace()函数显得过于“原始”，相比于其他脚本语言来说，用起来显得不太方便，不过很符合基础工具语言的特点，这个自带的repalce(pos, len, dst)函数的作用是从源字符串的第pos个字符开始，往后数len个字符，然后将这一部分替换成dst串。 有了这个替换函数，我们完全可以使用循环和查找函数完成全部替换，查找函数可以选择string::find()，从返回的找到的位置开始替换即可，若没有找到则会返回 string::npos，这时也就完成了所有的替换。 函数实现代码很简单，就是利用循环、string::find()函数、string::replace()函数来进行适当的组合，逻辑很清晰，代码如下：1234567891011121314#include &lt;string&gt;#include &lt;iostream&gt;using namespace std;string replace(string&amp; base, string src, string dst)&#123; int pos = 0, srclen = src.size(), dstlen = dst.size(); while ((pos = base.find(src, pos)) != string::npos) &#123; base.replace(pos, srclen, dst); pos += dstlen; &#125; return base;&#125; 关于是否需要返回值完全看你自己定义，我这里加了返回值只要是为了测试输出方便。 测试函数12345678910111213141516int main()&#123; string base1 = "1.0.0.1"; cout &lt;&lt; replace(base1, ".", "[.]") &lt;&lt; endl; string base2 = "1【.】0【.】0【.】1"; cout &lt;&lt; replace(base2, "【.】", "[.]") &lt;&lt; endl; string base3 = "1【.】0【.】0【.】1"; cout &lt;&lt; replace(base3, "【.】", ".") &lt;&lt; endl; string base4 = "this is a book"; cout &lt;&lt; replace(base4, "is", "are") &lt;&lt; endl; return 0;&#125; 运行结果 1[.]0[.]0[.]11[.]0[.]0[.]11.0.0.1thare are a book 总结 注意string::replace()函数与脚本中常用替换函数的不同 使用string::find()函数查找不到待查串时会返回string::npos]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>字符串</tag>
        <tag>替换</tag>
        <tag>replace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb使用watch命令设置数据断点]]></title>
    <url>%2Fblog%2F2019%2F07%2F16%2Fgdb%E4%BD%BF%E7%94%A8watch%E5%91%BD%E4%BB%A4%E8%AE%BE%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%96%AD%E7%82%B9%2F</url>
    <content type="text"><![CDATA[前言“数据断点”这个说法是沿用在Visual Studio中学到的设置断点的方法，在gdb中一般被叫做“硬件断点”，算是断点调试中一种较为高级的调试方法了，这个方法起初是在VS中学会的，属于有需求必有响应的产物。刚开始调试程序的时候只会设置普通断点，就是在要调试的程序代码所在行设置断点，然后等程序运行到断点处可以单步执行，查看内存变量，遇到多个位置修改一个变量并且要查看是谁改变了变量的时候，就要设置多个断点，当时就想如果可以设置一个断点，当变量值被改变就触发这个断点那该多好啊。 当年果然是太年轻，后来发现这个功能就是VS中的数据断点，同样作用的还有gdb工具的中硬件断点，硬件断点不仅可以处理上面提到的需求，更是查找内存写超过的强大工具，要想知道一个正常的变量如何被“不正常”地修改了，硬件断点可以说是最佳工具了。 数据变化断点在gdb工具中设置普通断点的语法是b 变量名/函数名/文件位置，设置数据变化断点（硬件断点）语法也很简单，只需要一个watch命令即可，写法为watch 变量名，但是与普通断点不同的是，数据断点必须在程序运行时设置，在敲入r命令之前对变量设置数据断点会提示找不到符号。 编写测试程序代码 首先新建测试文件watchtest.cpp然后添加下面的代码： 123456789101112131415161718#include &lt;iostream&gt;using namespace std;int main()&#123; int k = 1; int n; n = 1; k = 2; cout &lt;&lt; n &lt;&lt; "," &lt;&lt; k &lt;&lt; endl; n = 3; k = 4; cout &lt;&lt; n &lt;&lt; "," &lt;&lt; k &lt;&lt; endl; return 0;&#125; 将C++源代码编译成可执行文件，为了调试记得加-O0 -g选项 1[albert@localhost#17:08:00#/home/albert/test]$g++ watchtest.cpp -O0 -g -o watchtest 加数据断点并调试 以下为gdb添加数据变化断点（硬件断点）并调试的整个过程，(gdb)后面的内容为敲入的命令： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[albert@localhost#17:52:47#/home/albert/test]$gdb watchtestGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/test/watchtest...done.(gdb) b watchtest.cpp : 6Breakpoint 1 at 0x40085c: file watchtest.cpp, line 6.(gdb) watch nNo symbol "n" in current context.(gdb) rStarting program: /home/albert/test/watchtestBreakpoint 1, main () at watchtest.cpp:66 int k = 1;Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-23.el6.x86_64(gdb) watch nHardware watchpoint 2: n(gdb) cContinuing.Hardware watchpoint 2: nOld value = 0New value = 1main () at watchtest.cpp:1010 k = 2;(gdb) cContinuing.1,2Hardware watchpoint 2: nOld value = 1New value = 3main () at watchtest.cpp:1414 k = 4;(gdb) cContinuing.3,4Watchpoint 2 deleted because the program has left the block inwhich its expression is valid.0x00007ffff72c6d1d in __libc_start_main () from /lib64/libc.so.6(gdb) qA debugging session is active. Inferior 1 [process 18567] will be killed.Quit anyway? (y or n) y[albert@localhost#17:55:04#/home/albert/test]$ 总结 设置数据断点需要在程序启动之后，在运行r命令之前设置断点给出信息：No symbol &quot;n&quot; in current context. 当程序运行到监控变量的作用域之外以后，断点自动被删除，这一点观察执行q命令之前的文字可以看出 添加数据变化断点（硬件断点）格式：watch 变量名]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>watch</tag>
        <tag>断点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql5.7版本中数据表字段可用的类型]]></title>
    <url>%2Fblog%2F2019%2F07%2F02%2FMysql5-7%E7%89%88%E6%9C%AC%E4%B8%AD%E6%95%B0%E6%8D%AE%E8%A1%A8%E5%AD%97%E6%AE%B5%E5%8F%AF%E7%94%A8%E7%9A%84%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[前言为什么会有这个总结，因为在测试Mysql的字符串函数时发现，char 和 varchar 有些不同，网上搜索一番发现了各种char、varchar、nvarchar 类型的对比，还有一些奇奇怪怪的这里就不说了，然后我就开始了对这几种类型字符串的测试，接着就悲剧了，测试多次之后发现创建为nvarchar类型的字段居然是varchar类型的，再查询官方文档后发现，当前版本（5.7.21）的Mysql根本就没有nvarchar类型的字段，白白浪费了时间，所以要把Mysql支持的字段列举在这里，方便后面查找使用。 从13年开始工作到现在，数据库主要使用Mysql，关于常使用的字段类型无非 int、char、varchar、blob、datetime 这几种，工作之前用的最多的是SqlServer，其次就是Oracle和db2了，当时数据库的规模也不大，也没有注意到字段都有哪些类型，基本也是使用上述几种，因为今天在Mysql中的数据类型这栽了跟头，所以查了下官方文档，看看到底都有哪些类型。 支持类型真是不查不知道，查询后发现当前版本（5.7.21-log MySQL Community Server）支持的数据类型居然有40种，这还是超出我的想象的，以字典排序列举在此方便查找： bigint，binary，bit，blob，char，date，datetime，decimal，double，enum，float，geometry，geometrycollection，int，integer，json，linestring，longblob，longtext，mediumblob，mediumint，mediumtext，multilinestring，multipoint，multipolygon，numeric，point，polygon，real，set，smallint，text，time，timestamp，tinyblob，tinyint，tibytext，varbinary，varchar，year。 类型简述数字类型 BIT[(M)]比特值类型，M默认为1，范围是[1,64]。 TINYINT[(M)] [UNSIGNED] [ZEROFILL]单字节整数，有符号时范围是[-128,127]，无符号时范围是[0,255]。 BOOL, BOOLEAN布尔值类型，需要注意的是创建表时如果指定这两种类型会被自动转为TINYINT类型，0代表false，非0代表true。 SMALLINT[(M)] [UNSIGNED] [ZEROFILL]两字节整数，有符号时范围是[-32768,32767]，无符号时范围是[0,65535]。 MEDIUMINT[(M)] [UNSIGNED] [ZEROFILL]三字节整数，有符号时范围是[-8388608,8388607]，无符号时范围是[0,16777215]，这个类型在编程语言中很少见。 INT[(M)] [UNSIGNED] [ZEROFILL]四字节整数，有符号时范围是[-2147483648,2147483647]，无符号时范围是[0,4294967295]，与INTEGER等价。 BIGINT[(M)] [UNSIGNED] [ZEROFILL]八字节整数，有符号时范围是[-9223372036854775808,9223372036854775807]，无符号时范围是[0, 18446744073709551615]。 SERIAL是BIGINT UNSIGNED NOT NULL AUTO_INCREMENT UNIQUE类型的别名，感觉可以直接拿来做主键。 DECIMAL[(M[,D])] [UNSIGNED] [ZEROFILL]用于存储精确小数，M表示有效数字位数，范围是[1,65]，默认是10，D表示小数点后位数，范围是[0,30]，默认是0。 NUMERIC[(M[,D])] [UNSIGNED] [ZEROFILL]是DECIMAL的别名，同样含义的还有DEC[(M[,D])] [UNSIGNED] [ZEROFILL]、FIXED[(M[,D])] [UNSIGNED] [ZEROFILL]。 FLOAT[(M,D)] [UNSIGNED] [ZEROFILL]单精度浮点数，M表示有效数字位数，D表示小数点后位数，范围有三部分[-3.402823466E+38,-1.175494351E-38]，0，[1.175494351E-38,3.402823466E+38]，该类型属于Mysql自己的扩展，依赖硬件和操作系统，指定UNSIGNED表示禁用负数。 FLOAT(p) [UNSIGNED] [ZEROFILL]单精度浮点数，p用来表示精度，取值为0-24等价于没有M和D的FLOAT，取值为25-53等价于没有M和D的DOUBLE。 DOUBLE[(M,D)] [UNSIGNED] [ZEROFILL]双精度浮点数，表示有效数字位数，D表示小数点后位数，范围有三部分[-1.7976931348623157E+308,-2.2250738585072014E-308]，0，[2.2250738585072014E-308, 1.7976931348623157E+308]，该类型属于Mysql自己的扩展，依赖硬件和操作系统，指定UNSIGNED表示禁用负数。等价于DOUBLE PRECISION[(M,D)] [UNSIGNED] [ZEROFILL]。 REAL[(M,D)] [UNSIGNED] [ZEROFILL]一般情况等价于DOUBLE[(M,D)] [UNSIGNED] [ZEROFILL]，但如果SQL mode指定了REAL_AS_FLOAT，那么它等价于FLOAT[(M,D)] [UNSIGNED] [ZEROFILL]。 日期和时间类型 DATE日期类型，展示格式为’YYYY-MM-DD’，支持的范围是[‘1000-01-01’ , ‘9999-12-31’]。 DATETIME[(fsp)]日期时间格式，展示格式为’YYYY-MM-DD hh:mm:ss[.fraction]，支持范围是[‘1000-01-01 00:00:00.000000’, ‘9999-12-31 23:59:59.999999’]，fsp表示小数位数，默认是0，取值范围是[0,6]。 TIMESTAMP[(fsp)]时间戳，范围是[‘1970-01-01 00:00:01.000000’ UTC, ‘2038-01-19 03:14:07.999999’ UTC]，注意到起始秒数从1开始，是因为0被保留用来代表’0000-00-00 00:00:00’了，fsp表示小数位数，默认是0，取值范围是[0,6]。 TIME[(fsp)]时间类型，展示格式为 ‘hh:mm:ss[.fraction]’，支持的范围是[‘-838:59:59.000000’, ‘838:59:59.000000’]，fsp表示小数位数，默认是0，取值范围是[0,6]。 YEAR[(4)]代表年份类型，展示格式为’YYYY’，支持的范围是[1901, 2155]和0000。 字符串类型 [NATIONAL] CHAR[(M)] [CHARACTER SET charset_name] [COLLATE collation_name]固定长度的字符串，M表示字符串最大长度，范围是(0,255]，若实际长度不足M，实际串右侧会填充空格，M默认为1。 [NATIONAL] VARCHAR(M) [CHARACTER SET charset_name] [COLLATE collation_name]可变长度的字符串，M表示字符串最大长度，范围是(0, 65535],当存储UTF8编码中文时，一般需要3个字节存储一个汉字。 BINARY[(M)]与CHAR类似，只是存储的是二进制字节串而非普通的字符串。 VARBINARY(M)]与VARCHAR类似，只是存储的是二进制字节串而非普通的字符串。 TINYBLOB字节串，最大长度是255。 TINYTEXT [CHARACTER SET charset_name] [COLLATE collation_name]字符串，最大长度是255。 BLOB[(M)]字节串，最大长度64K-1，若指定M，则会创建一个能存储M字节最小的BLOB类型，比如TINYBLOB。 TEXT[(M)] [CHARACTER SET charset_name] [COLLATE collation_name]字符串，最大长度64K-1，若指定M，则会创建一个能存储M字节最小的BLOB类型，比如TINYTEXT。 MEDIUMBLOB字节串，最大长度16M-1。 MEDIUMTEXT [CHARACTER SET charset_name] [COLLATE collation_name]字符串，最大长度16M-1。 LONGBLOB字节串，最大长度4G-1。 LONGTEXT [CHARACTER SET charset_name] [COLLATE collation_name]字符串，最大长度4G-1。 ENUM(‘value1’,’value2’,…) [CHARACTER SET charset_name] [COLLATE collation_name]枚举值，一个字符串代表一个值，内部通过整数实现，理论上最多可以有65535个不同的值，但实际上这个值小于3000。 SET(‘value1’,’value2’,…) [CHARACTER SET charset_name] [COLLATE collation_name]集合，包含一组字符串，其内部还是呈现为一个整数，最大可以有64个不同的字符串对象。 特殊数据类型 Mysql提供了GEOMETRY、POINT、LINESTRING、POLYGON等特殊类型来与OpenGIS类一一对应，用来存储一些图形数据，同时还有MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、GEOMETRYCOLLECTION可以表示这些集合，我感觉我是没机会用这些了，用到了再展开说吧。 Json数据类型 自从Mysql5.7.8之后添加的一种类型，可以存储{“k1”: “val”, “k2”: 110}形式的数据。 常用数据类型大小 类型 存储数据范围（只考虑无符号） 单位 TINYINT 0-255 整数 SMALLINT 0-65535 整数 MEDIUMINT 0-16777215 整数 INT 0-4294967295 整数 BIGINT 0-18446744073709551615 整数 DATETIME 1000-01-01 00:00:00.000000 -&gt; 9999-12-31 23:59:59.999999 时间点 TIMESTAMP 1970-01-01 00:00:01.000000 UTC -&gt; 2038-01-19 03:14:07.999999 UTC. 时间点 TIME -838:59:59.000000 -&gt; 838:59:59.000000 时间点 CHAR 0-255 字符数 VARCHAR 0-65535 字符数 BINARY 0-255 字节数 VARBINARY 0-65535 字节数 TINYBLOB 255 字节数 BLOB 65535(64K-1) 字节数 MEDIUMBLOB 16777215(16M-1) 字节数 LONGBLOB 4294967295(4G-1) 字节数]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>char</tag>
        <tag>Mysql</tag>
        <tag>varchar</tag>
        <tag>blob</tag>
        <tag>varbinary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逐步砍掉树杈的堆排序]]></title>
    <url>%2Fblog%2F2019%2F06%2F29%2F%E9%80%90%E6%AD%A5%E7%A0%8D%E6%8E%89%E6%A0%91%E6%9D%88%E7%9A%84%E5%A0%86%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言在实现堆排序之前，我们先来看看常见的数据结构，在网上我看到了一个特别全的版本：数组，栈，链表，队列，树，堆，图，散列表，本着鸡蛋里挑骨头的态度，我们来看看数组和链表，这两个到底算不算数据结构，貌似它们应该算是线性表这个结构，它们更应该被称作是一个实现结构的元素，比如通过数组和链表可以实现线性表、队列、栈，二叉树等等，可是看看数据结构的定义是计算机存储、组织数据的方式，貌似它们又算是数据结构，反正这个概念模模糊糊，不太清楚，要按我的理解常见结构应该只有线性表、栈、队列、树、图，其他的像堆其实是一种树，散列表很多的内部实现也是树。 好了，数据结构的事情也放一边，今天的目的是排序，主角是堆，那么究竟什么是堆呢？它的形状和山一样，只不过比山要“便宜”，比如土堆、煤堆、垃圾堆，这和金山、银山、绿水青山是没法比的，但是形状相似，只是小一点而已，上边小下边大，尖尖的，从侧面看就是第一个三角形。堆排序中的堆也是这种形状，上边窄下边宽呈现出一个三角形，其本质是一颗完全二叉树，一个n层的二叉树只有最后一层叶子节点可能不完整，但是都靠左排列，最多只有一个单叶子节点，如果说到这里你根本不知道什么是完全二叉树，甚至对树结构都是一头雾水，那么请去补补课，查询一下相关的定义就明白了。 一棵树要想被称为堆结构，光满足完全二叉树还不够，其中的元素也有要求，如果每个父节点都大于等于左右子节点被称为大根堆，如果每个父节点都小于等于左右子节点被称为小根堆，这样我们就知道在堆这个结构中，堆的顶端不是最大的值就是最小的值。 堆顶元素恰恰是堆排序的关键，试想一个有大根堆，我们把堆顶的数据拿下来放到一旁，把剩下的元素再调整成一个大根堆，然后再把堆顶数据拿下来放在刚才拿出那个元素的前面，再调整剩下的元素，反复这样操作，最后拿出来的这些元素就构成了一个有序序列，也就达到了排序的目的。 在进行升序排列时常使用大根堆，降序排列时常使用小根堆，这个知识点不是绝对的，也不需要记忆，只是这样操作更加方便，当你理解了堆排序的流程之后，很自然就能明白这样的用意，并且到那时候你完全可以反着操作来提升自己，不过效果和可读性可能会差一点。 堆排序树结构可以用数组表示出来，特别是完全二叉树用数组表示起来更加方便，从上往下，从左往右依次把数据放入数组，我们就把一颗完全二叉树塞进了一维数组里，本来打算一个图都不画的，但是突然良心发现了，不能对你们太残忍，还是画一个吧，下面这个图展示了完全二叉树与数组的对应关系，这可是本文中唯一的一个图了，可要珍惜一下，把这个图弄明白，之后我们就只操作数组，不再画树了，因为我太懒了。 1234567891011graph TB 2--&gt;3; 2--&gt;9; 3--&gt;4; 3--&gt;7; 9--&gt;12; 9--&gt;6; 4--&gt;1; 4--&gt;11; 7--&gt;5; 7--&gt;8; idx_0 idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 2 3 9 4 7 12 6 1 11 5 8 这个用数组表示的完全二叉树有一个性质，这个性质是我编的，你之前可能没有听过，那就是从后面删除n个节点之后，剩下的元素还是一颗完全二叉树，这一点隐含在堆排序的整个过程中，并且二叉树的父节点都排列在数组前面，叶子节点都排在数组后边，父节点和子节点对应的索引满足一定的关系： 假设父节点索引是i，左节点索引=2*i+1 假设父节点索引是i，右节点索引=2*i+2 假设子节点索引是i，父节点的索引=(int)((i-1)/2) 明白了上面的关系，先简单描述一下堆排序的整个过程，操作的数据源就是这个数组，长度n=11，先将整个数组表示的完全二叉树调整成一个大根堆，这时树的根节点也就是数组的第一个元素就是最大值，把它和数组的最后一个元素交换，之后不再管最后这个数据，相当于把这个树杈砍掉了，根据上段提到的性质，砍掉最后一个叶子节点的二叉树仍然是一颗完全二叉树，调整数据使得前n-1个节点再次成为一个大根堆，继续把根节点索引为0的元素，也就是这个次最大值，与倒数第二个元素交换，之后也放弃倒数第二个元素了，相当于再次砍掉了这棵二叉树的一个树杈，如此反复操作，当“砍”到根节点时，整个数组也就从小到大排好序了。 排序过程通过上面描述可能还是不太明白堆排序的过程，接下来可以通过一个例子来实际操作一次，看看数组是怎样在不断调整大根堆的过程中慢慢变成有序的，数组的初始状态是： idx_0 idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_8 idx_9 idx_10 2 3 9 4 7 12 6 1 11 5 8 先将数组代表的完全二叉树调整成一个大根堆 首先需要确认的是这个调整应该从下往上调整，先看最下边的子树，调整父节点和子节点的数据，使得父节点数据最大，然后再看前一个子树，继续把父子节点的数据调整好，这样一直调整到根节点时，整个完全二叉树的最大值就被调整到根节点了。这个过程有点像冒泡，从下往上，把最大的数据慢慢的冒到最上面。 反过来想想，如果是从上往下挨个子树来看，当从根节点调整到最后一个（最下最右）子树，并不能保证根节点数据最大，只是把较大数据向上整体移动了一次，所以还是要从下往上调整。 知道了这一点以后就要找到最后一个子树的位置，其实就是找到最后一个父节点，这个节点之前的数据都在非叶子节点上，这个节点之后的数据都在叶子节点上，只要调整这个最后父节点以及前面的所有节点就可以影响所有数据，关键是找到这个节点的位置。 要想找到最后一个父节点需要用到之前我们提到的公式，整个数组的元素个数n=11，最后一个元素的索引为n-1，那么其父节点就是最后一个子树的父节点，其索引应该为(int)((n-1-1)/2)，也就是n/2-1，这就是最后一个子树父节点的在数组中的索引，其数值为4，接着从这个节点开始从后往前调整子树： 子树父节点，索引i=4时，调整父节点和右孩子的值（从左右孩子中找一个较大的值，并且要大于父节点） |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9|4|7(parent)|12|6|1|11|5(left)|8(right)| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9|4|8(parent)|12|6|1|11|5(left)|7(right)| 子树父节点，索引i=3时，调整父节点和右孩子的值 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9|4(parent)|8|12|6|1(left)|11(right)|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9|11(parent)|8|12|6|1(left)|4(right)|5|7| 子树父节点，索引i=2时，调整父节点和左孩子的值 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|9(parent)|11|8|12(left)|6(right)|1|4|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3|12(parent)|11|8|9(left)|6(right)|1|4|5|7| 子树父节点，索引i=1时，调整父节点和左孩子的值，这时左孩子同时也是下面子树的父节点，所以还要调整一下该节点作为父节点的子树 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|3(parent)|12|11(left)|8(right)|9|6|1|4|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|11(parent)|12|3(left)|8(right)|9|6|1|4|5|7| 左子树节点作为父节点进行调整 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|11|12|3(tmp parent)|8|9|6|1(tmp left)|4(tmp right)|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2|11|12|4(tmp parent)|8|9|6|1(tmp left)|3(tmp right)|5|7| 子树父节点，索引i=0时，调整父节点和右孩子的值，这时右孩子同时也是下面子树的父节点，同样还要调整一下该节点作为父节点的子树 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||2(parent)|11(left)|12(right)|4|8|9|6|1|3|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12(parent)|11(left)|2(right)|4|8|9|6|1|3|5|7| 右子树节点作为父节点进行调整 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12|11|2(tmp parent)|4|8|9(tmp left)|6(tmp right)|1|3|5|7| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12|11|9(tmp parent)|4|8|2(tmp left)|6(tmp right)|1|3|5|7| 到此为止整个大根堆就调整完成了，为了看的更加清楚。我不得不打脸再画一个图了，有时候还是看图更加方便，一图胜千言啊： |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12|11|9|4|8|2|6|1|3|5|7| 1234567891011graph TB 12--&gt;11; 12--&gt;9; 11--&gt;4; 11--&gt;8; 9--&gt;2; 9--&gt;6; 4--&gt;1; 4--&gt;3; 8--&gt;5; 8--&gt;7; 将大根堆根节点保存的最大值与当前大根堆最后一个节点进行交换，然后将这个节点“砍掉” 交换数据后，将剩余的这个完全二叉树继续调整成大根堆，既然已经打脸了，那就再画个图，这个砍树杈的动作已经和标题呼应了，每次生成大根堆后，将根节点和堆的最后一个结点交换，然后砍掉这个树杈，等整棵树被砍的只剩下根节点，排序也就完成了。 |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||12(head)|11|9|4|8|2|6|1|3|5|7(tail)| |idx_0|idx_1|idx_2|idx_3|idx_4|idx_5|idx_6|idx_7|idx_8|idx_9|idx_10||:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||7(head)|11|9|4|8|2|6|1|3|5|12(tail)| 1234567891011graph TB 7--&gt;11; 7--&gt;9; 11--&gt;4; 11--&gt;8; 9--&gt;2; 9--&gt;6; 4--&gt;1; 4--&gt;3; 8--&gt;5; 8--&gt;|X|12; 重复上面的步骤，循环执行调整剩余元素为大根堆，首位交换，砍掉末尾元素这三步 这里需要注意的是除了第一次初始化成大根堆的过程比较麻烦，后面重复调整成大根堆的过程都很容易，因为只有这一个根节点不满足大根堆的定义，所以只从这个节点调整就可以，同时递归调整其不符合条件的子树即可。 每次交换之后都会“砍掉”树杈，所以大根堆每次都会减少元素，交换的索引也发生这变化，第一个是array[0]和array[n-1]，然后是array[0]和array[n-2]，最后一直交换到array[0]和array[1]，也就完成了整体的排序。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/*功能： 交换两个变量参数： element1--被交换的第一个元素的地址 element2--被交换的第二个元素的地址返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void swap_data(int* element1, int* element2)&#123; int middle_value = *element1; *element1 = *element2; *element2 = middle_value;&#125;/*功能： 从start到end构成最大堆，前提是start之后的部分已满足最大堆， 也就是说start存在左右子树的情况下，子树已经是最大堆参数： array--表示待排序的数组，此处会退化成指针 start--需要调整的父节点索引 end --最后一个可以被调整的节点索引，当形成最大堆后，第一个节点与当前最后节点交换后，那么这个当前最后节点下一轮就不能被调整了返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void max_heapify(int array[], int start, int end)&#123; int parent_index = start; int child_index = start * 2 + 1; while (child_index &lt;= end) &#123; if (child_index + 1 &lt;= end &amp;&amp; array[child_index] &lt; array[child_index + 1]) ++child_index; // 如果右边的孩子更大，选择右边的 if (array[parent_index] &gt; array[child_index]) break; swap_data(&amp;array[parent_index], &amp;array[child_index]); parent_index = child_index; child_index = parent_index * 2 + 1; &#125;&#125;/*功能： 堆排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 count--数组元素的个数返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void heap_sort(int array[], int count)&#123; for (int pos = count / 2 - 1; pos &gt;= 0; --pos) max_heapify(array, pos, count - 1); for (int target_pos = count - 1; target_pos &gt; 0; --target_pos) &#123; swap_data(&amp;array[0], &amp;array[target_pos]); max_heapify(array, 0, target_pos - 1); &#125;&#125; 代码分析堆排序其实是一种选择排序，但是堆排序的代码比选择排序要复杂一下，其实理解了算法思路这些代码还是很容易看懂的，函数heap_sort是堆排序的主体逻辑，第一个for循环是从最后一个父节点开始调整，将父节点与较大的子节点交换，一直调整到根节点，初始化成一个大根堆。 第二个for循环就是重复做交换首尾元素，然后调整剩余元素使其成为大根堆这两件事，重复n-1轮，排序过程也就完成了。 运行测试在线编辑器是一个很方便的测试代码的环境，如果想本地调试一下，也可以直接下载堆排序–源码，在本地编译后进行调试，其实单步调试是理解算法思路很有效的方式。]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
        <tag>堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[float的精度和取值范围]]></title>
    <url>%2Fblog%2F2019%2F06%2F27%2Ffloat%E7%9A%84%E7%B2%BE%E5%BA%A6%E5%92%8C%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4%2F</url>
    <content type="text"><![CDATA[前言关于float的精度和取值范围这个问题，我查询了很多次，每次都是用完就忘了，等到再使用的时候还需要再次查询，关键是这个问题大家给出的结果并不都是一致的，我得从众多的资料当中选择出正确的观点，这还要额外花一些时间，所以我决定也总结一次，方便我以后拿来直接用了，如果能给大家带来帮助那就更好了。下面提到一些说法很多都是我个人的理解，如果大家有疑义，欢迎讨论。 精度限制首先考虑下为什么会产生精度问题，是因为存储数据的空间有限，以一个四字节整数int n;为例，一共有32位，取值范围是 [-2147483648‬, 21474836487] ，一共是4,294,967,296种可能，它的精度可以说是小数点后一位都不保留，也就是只有整数，换句话说变量n可以表示实数范围内的4,294,967,296个数值。 如果换成float类型呢？一个变量float f所能表示多少个数呢？实际上由于存储空间未发生变化，同样是4字节32位，那么float类型也只能表示，或者说精确表示4,294,967,296个数值（实际上由于一些特殊的规则，最终所表示的数字个数还要少），说到这里很多人可能会疑惑，因为他知道float可以表示比4,294,967,296大的数，同时也能表示小数，如果只有4,294,967,296种可能，那究竟是怎么做到的呢？ 这里也就开始提到精度了，整数很好理解，每个数字的间隔都是1，int类型所表示的4,294,967,296个数字都是等间距的，步长为1。而float也只能表示4,294,967,296个数字，同时要表示比int还大的范围，一个很直观的想法就是把间距拉大，这样范围就大了，但是float还要表示小数，像0.2、0.4这样的数字间距明显要小于1啊，想要存储小数貌似要把间距缩小，这就和前面矛盾了啊。 实际上float类型存储数据的间隔不是等间距的，而是在0的附近间距小，在远离0的位置间距大，为什么会这样，一会我们看一下float类型数据的存储规则就明白了，这里先来看一下int类型和float类型所表示数字的范围对比，这只是一个示意图。 1234//int [ * * * 0 * * * ]//float[ * * * * * * * * * * * 0 * * * * * * * * * * * ] 上面的示意图就是两者表示数字范围的差异，每个星号*就表示一个数字，float通过这种不等间距的分布，既扩大了范围也表示了小数，那么有没有问题呢？ 当然有问题，饭就这么多，人多了自然不够吃了，因为远离0的位置间距越来越大，当要表示间距中间的一个数字时，只能找它附近离它最近的一个可以表示的数字来代替，这就导致了精度问题，比如我给一个float类型变量分别赋值为 4294967244 和 4294967295 ，再次输出时都变成了 4294967296，因为超过了精度，所以只能找最接近的数字代替。 float存储方式这部分内容基本上各篇文章说的都一致，我也简单描述下，后面根据这部分的定义来推算一下float的精度和取值范围。 首先我们知道常用科学计数法是将所有的数字转换成(±)a.b x $10^c$ 的形式，其中a的范围是1到9共9个整数，b是小数点后的所有数字，c是10的指数。而计算机中存储的都是二进制数据，所以float存储的数字都要先转化成(±)a.b x $2^c$，由于二进制中最大的数字就是1，所以表示法可以写成(±)1.b x $2^c$的形式，float要想存储小数就只需要存储(±)，b和c就可以了。 float的存储正是将4字节32位划分为了3部分来分别存储正负号，小数部分和指数部分的： Sign（1位）：用来表示浮点数是正数还是负数，0表示正数，1表示负数。 Exponent（8位）：指数部分。即上文提到数字c，但是这里不是直接存储c，为了同时表示正负指数以及他们的大小顺序，这里实际存储的是c+127。 Mantissa（23位）：尾数部分。也就是上文中提到的数字b。 三部分在内存中的分布如下，用首字母代替类型 S E E E E E E E E M M M M M M M M M M M M M M M M M M M M M M 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 float存储示例以数字6.5为例，看一下这个数字是怎么存储在float变量中的： 先来看整数部分，模2求余可以得到二进制表示为110。 再来看小数部分，乘2取整可以得到二进制表示为.1（如果你不知道怎样求小数的二进制，请主动搜索一下）。 拼接在一起得到110.1然后写成类似于科学计数法的样子，得到1.101 x $2^2$。 从上面的公式中可以知道符号为正，尾数是101，指数是2。 符号为正，那么第一位填0，指数是2，加上偏移量127等于129，二进制表示为10000001，填到2-9位，剩下的尾数101填到尾数位上即可 S E E E E E E E E M M M M M M M M M M M M M M M M M M M M M M 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 内存中二进制数01000000 11010000 00000000 00000000表示的就是浮点数6.5 float范围明白了上面的原理就可求float类型的范围了，找到所能表示的最大值，然后将符号为置为1变成负数就是最小值，要想表示的值最大肯定是尾数最大并且指数最大，那么可以得到尾数为 0.1111111 11111111 11111111，指数为 11111111，但是指数全为1时有其特殊用途，所以指数最大为 11111110，指数减去127得到127，所以最大的数字就是1.1111111 1111111 11111111 x $2^{127}$，这个值为 340282346638528859811704183484516925440，通常表示成 3.4028235E38，那么float的范围就出来了： [-3.4028235E38, 3.4028235E38] float精度float 类型的数据精度取决于尾数，相信大家都知道这一点，但是精度怎么算我也是迷糊了好久，最近在不断尝试的过程中渐渐的明白了，首先是在不考虑指数的情况下23位尾数能表示的范围是[0, $2^{23}-1$]，实际上尾数位前面还隐含了一个”1”，所以应该是一共24位数字，所能表示的范围是[0, $2^{24}-1$]（因为隐含位默认是”1”，所以表示的数最小是1不是0，但是先不考虑0，后面会特殊介绍，这里只按一般值计算），看到这里我们知道这24位能表示的最大数字为$2^{24}$-1，换算成10进制就是16777215，那么[0, 16777215]都是能精确表示的，因为他们都能写成1.b x $2^c$的形式，只要配合调整指数c就可以了。 16777215 这个数字可以写成1.1111111 11111111 1111111 $2^{23}$，所以这个数可以精确表示，然后考虑更大的数16777216，因为正好是2的整数次幂，可以表示1.0000000 00000000 00000000 $2^{24}$，所以这个数也可以精确表示，在考虑更大的数字16777217，这个数字如果写成上面的表示方法应该是 1.0000000 00000000 00000000 1 * $2^{24}$，但是这时你会发现，小数点后尾数位已经是24位了，23位的存储空间已经无法精确存储，这时浮点数的精度问题也就是出现了。 看到这里发现 16777216 貌似是一个边界，超过这个数的数字开始不能精确表示了，那是不是所有大于16777216的数字都不能精确表示了呢？其实不是的，比如数字 33554432 就可以就可以精确表示成1.0000000 00000000 00000000 * $2^{25}$，说道这里结合上面提到的float的内存表示方式，我们可以得出大于 16777216 的数字（不超上限），只要可以表示成小于24个2的n次幂相加，并且每个n之间的差值小于24就能够精确表示。换句话来说所有大于 16777216 的合理数字，都是[0, 16777215]范围内的精确数字通过乘以$2^n$得到的，同理所有小于1的正数，也都是 [0, 16777215] 范围内的精确数字通过乘以$2^n$得到的，只不过n取负数就可以了。 16777216 已经被证实是一个边界，小于这个数的整数都可以精确表示，表示成科学技术法就是1.6777216 * $10^{7}$，从这里可以看出一共8位有效数字，由于最高位最大为1不能保证所有情况，所以最少能保证7位有效数字是准确的，这也就是常说float类型数据的精度。 float小数从上面的分析我们已经知道，float可表示超过16777216范围的数字是跳跃的，同时float所能表示的小数也都是跳跃的，这些小数也必须能写成2的n次幂相加才可以，比如0.5、0.25、0.125…以及这些数字的和，像5.2这样的数字使用float类型是没办法精确存储的，5.2的二进制表示为101.0011001100110011001100110011……最后的0011无限循环下去，但是float最多能存储23位尾数，那么计算机存储的5.2应该是101.001100110011001100110，也就是数字 5.19999980926513671875，计算机使用这个最接近5.2的数来表示5.2。关于小数的精度与刚才的分析是一致的，当第8位有效数字发生变化时，float可能已经无法察觉到这种变化了。 float特殊值我们知道float存储浮点数的形式是(±)1.b x $2^c$，因为尾数位前面一直是个1，所以无论b和c取什么样的值，都无法得到0，所以在float的表示方法中有一些特殊的约定，用来表示0已经其他的情况。 float的内存表示指数位数有8位，范围是[0, 255]，考虑偏移量实际的指数范围是[-127,128]，但实际情况下指数位表示一般数字时不允许同时取0或者同时取1，也就是指数位的实际范围是[-126,127]，而指数取-127和128时有其特殊含义，具体看下面表格： 符号位 指数位 尾数位 数值 含义 0 全为0 全为0 +0 正数0 1 全为0 全为0 +0 负数0 0 全为0 任意取值f $0.f * 2^{-126}$ 非标准值，尾数前改为0，提高了精度 1 全为0 任意取值f $-0.f * 2^{-126}$ 非标准值，尾数前改为0，提高了精度 0 全为1 全为0 +Infinity 正无穷大 1 全为1 全为0 -Infinity 负无穷大 0/1 全为1 不全为0 NaN 非数字，用来表示一些特殊情况 总结 float的精度是保证至少7位有效数字是准确的 float的取值范围[-3.4028235E38, 3.4028235E38]，精确范围是[-340282346638528859811704183484516925440, 340282346638528859811704183484516925440] 一个简单的测试float精度方法，C++代码中将数字赋值给float变量，如果给出警告warning C4305: “=”: 从“int”到“float”截断，则超出了float的精度范围，在我的测试中赋值为16777216及以下整数没有警告，赋值为16777217时给出了警告。]]></content>
      <categories>
        <category>concepts</category>
      </categories>
      <tags>
        <tag>float</tag>
        <tag>精度</tag>
        <tag>取值范围</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中使用count加条件统计]]></title>
    <url>%2Fblog%2F2019%2F06%2F03%2FMysql%E4%B8%AD%E4%BD%BF%E7%94%A8count%E5%8A%A0%E6%9D%A1%E4%BB%B6%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[前言最近发现在处理Mysql问题时，count()函数频繁上镜，常常出现在分组统计的情景下，但是有时候并不是使用group by分好组就可以直接统计了，比如说一个常见的需求，统计每个班级男生所占的比例，这种情况一般会按照班级分组，但是分组内不但要统计班级的人数，还要统计男生的人数，也就是说统计是有条件的，之前确实没有考虑过怎样实心，后来查询了资料，总结在这里，方便日后查找使用。 Mysql中count()函数的一般用法是统计字段非空的记录数，所以可以利用这个特点来进行条件统计，注意这里如果字段是NULL就不会统计，但是false是会被统计到的，记住这一点，我们接下来看看几种常见的条件统计写法。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 准备工作 新建一个Mysql数据表a，包含id和num两个字段 12mysql&gt; create table a(id int, num int);Query OK, 0 rows affected (0.04 sec) 插入测试数据，为了看count()函数的效果，我们插入两个空数据 123mysql&gt; insert into a values (1,100),(2,200),(3,300),(4,300),(8,null),(9,null);Query OK, 6 rows affected (0.01 sec)Records: 6 Duplicates: 0 Warnings: 0 查询表a中的数据，与后面的统计做比较 123456789101112mysql&gt; select * from a;+----+------+| id | num |+----+------+| 1 | 100 || 2 | 200 || 3 | 300 || 4 | 300 || 8 | NULL || 9 | NULL |+----+------+6 rows in set (0.09 sec) 调用count()函数看效果，如果使用count(*)会查询出所有的记录数，但如果使用count(num)发现只有4条数据，num为NULL的记录并没有统计上 123456789101112131415mysql&gt; select count(*) from a;+----------+| count(*) |+----------+| 6 |+----------+1 row in set (0.03 sec)mysql&gt; select count(num) from a;+------------+| count(num) |+------------+| 4 |+------------+1 row in set (0.04 sec) 条件统计 count()函数中使用条件表达式加or null来实现，作用就是当条件不满足时，函数变成了count(null)不会统计数量 1234567mysql&gt; select count(num &gt; 200 or null) from a;+--------------------------+| count(num &gt; 200 or null) |+--------------------------+| 2 |+--------------------------+1 row in set (0.22 sec) count()函数中使用if表达式来实现，当条件满足是表达式的值为非空，条件不满足时表达式值为NULL; 1234567mysql&gt; select count(if(num &gt; 200, 1, null)) from a;+-------------------------------+| count(if(num &gt; 200, 1, null)) |+-------------------------------+| 2 |+-------------------------------+1 row in set (0.05 sec) count()函数中使用case when表达式来实现，当条件满足是表达式的结果为非空，条件不满足时无结果默认为NULL; 1234567mysql&gt; select count(case when num &gt; 200 then 1 end) from a;+---------------------------------------+| count(case when num &gt; 200 then 1 end) |+---------------------------------------+| 2 |+---------------------------------------+1 row in set (0.07 sec) 总结使用count()函数实现条件统计的基础是对于值为NULL的记录不计数，常用的有以下三种方式，假设统计num大于200的记录 select count(num &gt; 200 or null) from a; select count(if(num &gt; 200, 1, null)) from a select count(case when num &gt; 200 then 1 end) from a]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>if</tag>
        <tag>Mysql</tag>
        <tag>count</tag>
        <tag>条件统计</tag>
        <tag>casewhen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb启动多进程程序并切换调试进程]]></title>
    <url>%2Fblog%2F2019%2F05%2F24%2Fgdb%E5%90%AF%E5%8A%A8%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%A8%8B%E5%BA%8F%E5%B9%B6%E5%88%87%E6%8D%A2%E8%B0%83%E8%AF%95%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言gdb是linux环境下调试C/C++程序的强大工具，但是最近在使用gdb启动一个多进程程序的时候总是意外退出，显示信息中包含Detaching after fork from child process 25377.这一句，而用attach命令附加到正在运行的进程却没有问题，因为需要调试启动逻辑的部分代码，所以必须使用gdb启动多进程程序，后来发现可以通过gdb的follow-fork-mode选项来切换进程，达到调试指定进程的目的。 使用方法1set follow-fork-mode [parent|child] 这个命令只要gdb启动程序之后，在运行r命令之前敲入即可，如果不设置默认是parent模式，如果调试的child模式，就需要手动切换，我遇到的问题就是，程序启动使用fork()函数创建出子进程之后就把父进程退出了，gdb默认调试parent进程，也跟着结束了，所以出现了之前所说的Detaching after fork from child process 25377.信息，接下来可以写个简单的例子测试一下。 测试环境123456789101112131415[albert@localhost#20:15:45#/home/albert/gdbtest]$cat /etc/issueCentOS release 6.3 (Final)Kernel \r on an \m[albert@localhost#20:16:25#/home/albert/gdbtest]$gdb --versionGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.[albert@localhost#20:16:36#/home/albert/gdbtest]$^C 具体例子 先写一个简单的多进程程序，模拟我遇到的问题，父进程退出，子进程继续工作 1234567891011121314151617181920212223242526#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;int main ()&#123; pid_t pid; //pid表示fork函数返回的值，会根据不同进程返回不同值 pid = fork(); if (pid &lt; 0) &#123; exit(-1); &#125; else if (pid == 0) // 子进程返回pid为0 &#123; unsigned int u = 0; while(true) &#123; ++u; sleep(1); &#125; &#125; else // 父进程返回pid为子进程的id，大于0 &#123; exit(1); &#125; return 0;&#125; 将代码编译成可执行程序 1[albert@localhost#20:04:31#/home/albert/gdbtest]$g++ multiprocess.cpp -o multiprocess gdb启动程序并运行，其中我只输入了r和q两个gdb命令，发现程序运行r之后输出几行信息就退出了 1234567891011121314151617181920212223242526[albert@localhost#20:04:45#/home/albert/gdbtest]$gdb ./multiprocessGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/gdbtest/multiprocess...(no debugging symbols found)...done.(gdb) rStarting program: /home/albert/gdbtest/multiprocesswarning: the debug information found in "ld-2.12.so.debug" does not match "ld-linux-x86-64.so.2" (CRC mismatch).warning: the debug information found in "ld-2.12.so.debug" does not match "ld-linux-x86-64.so.2" (CRC mismatch).warning: the debug information found in "libm-2.12.so.debug" does not match "libm.so.6" (CRC mismatch).warning: the debug information found in "libm-2.12.so.debug" does not match "libm.so.6" (CRC mismatch).warning: the debug information found in "libc-2.12.so.debug" does not match "libc.so.6" (CRC mismatch).warning: the debug information found in "libc-2.12.so.debug" does not match "libc.so.6" (CRC mismatch).Detaching after fork from child process 25377.Program exited with code 01.Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-18.el6_9.2.x86_64(gdb) q[albert@localhost#20:05:03#/home/albert/gdbtest] 使用set follow-fork-mode child命令调试子进程，在r之前输入即可，这次发现程序停在了[New process 27522]，此时就可以打断点调试了 12345678910111213141516171819202122232425262728[albert@localhost#20:23:12#/home/albert/gdbtest]$gdb ./multiprocessGNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/gdbtest/multiprocess...(no debugging symbols found)...done.(gdb) set follow-fork-mode child(gdb) rStarting program: /home/albert/gdbtest/multiprocesswarning: the debug information found in "ld-2.12.so.debug" does not match "ld-linux-x86-64.so.2" (CRC mismatch).warning: the debug information found in "ld-2.12.so.debug" does not match "ld-linux-x86-64.so.2" (CRC mismatch).warning: the debug information found in "libm-2.12.so.debug" does not match "libm.so.6" (CRC mismatch).warning: the debug information found in "libm-2.12.so.debug" does not match "libm.so.6" (CRC mismatch).warning: the debug information found in "libc-2.12.so.debug" does not match "libc.so.6" (CRC mismatch).warning: the debug information found in "libc-2.12.so.debug" does not match "libc.so.6" (CRC mismatch).[New process 27522]^CProgram received signal SIGINT, Interrupt.[Switching to process 27522]0x00007ffff7354c30 in __nanosleep_nocancel () from /lib64/libc.so.6Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-18.el6_9.2.x86_64(gdb) b multiprocess.cpp:17 总结 gdb调试多进程程序时使用set follow-fork-mode [parent|child]命令 默认调试parent进程，想调试child进程，使用set follow-fork-mode child命令切换]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>fork</tag>
        <tag>child</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql调优之Using filesort]]></title>
    <url>%2Fblog%2F2019%2F05%2F16%2FMysql%E8%B0%83%E4%BC%98%E2%80%94%E4%B9%8BUsing-filesort%2F</url>
    <content type="text"><![CDATA[前言在使用 explain 命令优化SQL语句的时候常常会在Extra列的描述中发现 Using filesort 选项，其实这个名字很容易造成误解，一开始我以为是“文件排序”的意思，进一步说可能就是使用了磁盘空间来进行排序，但是这个理解是错误的，Using filesort 真正含义其实只有 sort 这一个单词，和 file 没有什么关系，Mysql一般是通过内存进行排序的，不过，要是超过了配置中的限制，应该会生成临时表。 分析Using filesort 的含义很简单，就是使用了排序操作，出现这个选项的常见情况就是 Where 条件和 order by 子句作用在了不同的列上，这种情况还有优化的余地，有些场景由于数据量太小或者语句的简单性可能都不需要优化，既然说Using filesort是使用了排序的意思，那么是不是包含了 order by 子句的查询语句都会有这个选项呢？其实这个排序操作有时是可以避免的。 如果你想把一个表中的所有数据按照指定顺序输出，那么整个排序几乎是不可避免的，比如这个语句select * from a order by id，即使在id列上建立了索引，为了生成指定顺序的数据，那么整个数据的排序也是需要，不过个别时候这个排序还是可以省略的，比如id是该表的主键，并且是自增长的，数据本身就是有序的，那么直接返回数据就行了，相当于 order by id 这一部分被忽略了。 上面提到的常见情况，SQL语句通常写成这样select * from a where type = 5 order by id，这类语句一般会产生 Using filesort 这个选项，即使你在 type 和 id 上分别添加了索引。我们想一下它的工作过程，先根据type的索引从所有数据信息中挑选出满足 type = 5 条件的，然后根据id列的索引信息对挑选的数据进行排序，所以产生了Using filesort选项，想想怎样可以把后面排序的这个步骤省略掉？联合索引可以解决这个问题。 可以在 type, id 两列上建立一个联合索引，索引类型一般是 BTREE，根据Mysql索引的最左原则，可以知道一共建立了type_index和type_id_index两条索引，由于有了 type_id_index 这个联合索引，后面的排序步骤就可以省略了，在按照type = 5 条件挑选数据时，挂在type = 5 节点下的数据，其实按照id列的值也是有顺序的，我们只需要在挑选数据的同时，按照id从小到大的顺序挑选即可，最后得到的数据就是有序的，直接返回就行了，从这一点可以看出，“排序”操作并不是不存在了，只是隐含在了前面必要的步骤中，不需要单独操作了而已，下面举个简单例子，看看具体的效果。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 具体操作 先建立一个测试表格tb，一般为了加快查询速度，会在常用的字段上建立索引 12mysql&gt; create table tb(id int, type int, weight int, index t_index(type), index w_index(weight));Query OK, 0 rows affected (0.02 sec) 创建一个存储fill_test_data用来插入测试数据，创建完成调用一下 12345678910111213141516CREATE PROCEDURE `fill_test_data`()BEGIN DECLARE i int default 1; DECLARE w int default 100; DECLARE t int default 1; WHILE i &lt;= 100000 do insert into tb values(i, t, w); set i = i + 1; set w = (w + 10) % 1000; set t = (t + 1) % 10; END WHILE;ENDmysql&gt; call fill_test_data();Query OK, 1 row affected (25.36 sec) 查询数据，让 Where 条件和 order by 子句作用在不同的列上 1234567891011121314mysql&gt; select * from tb where type = 3 order by weight;+-------+------+--------+| id | type | weight |+-------+------+--------+| 193 | 3 | 20 || 293 | 3 | 20 || 393 | 3 | 20 |...| 99683 | 3 | 920 || 99783 | 3 | 920 || 99883 | 3 | 920 || 99983 | 3 | 920 |+-------+------+--------+10000 rows in set (2.22 sec) 使用 explain命令分析查询语句，就会发现Using filesort出现在了Extra条目中 123456789101112131415mysql&gt; explain select * from tb where type = 3 order by weight\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb partitions: NULL type: refpossible_keys: t_index key: t_index key_len: 5 ref: const rows: 17672 filtered: 100.00 Extra: Using index condition; Using filesort1 row in set, 1 warning (0.00 sec) 使用SQL命令给表tb的type列和id列添加联合索引 123mysql&gt; alter table tb add index tw_index(type, weight);Query OK, 0 rows affected (0.17 sec)Records: 0 Duplicates: 0 Warnings: 0 再次查询数据，看看与上一次的查询时间相比有没有变化 1234567891011121314mysql&gt; select * from tb where type = 3 order by weight;+-------+------+--------+| id | type | weight |+-------+------+--------+| 193 | 3 | 20 || 293 | 3 | 20 || 393 | 3 | 20 |...| 99683 | 3 | 920 || 99783 | 3 | 920 || 99883 | 3 | 920 || 99983 | 3 | 920 |+-------+------+--------+10000 rows in set (2.13 sec) 再次使用 explain命令分析查询语句，就会发现Using filesort选项已经消失了 123456789101112131415mysql&gt; explain select * from tb where type = 3 order by weight\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb partitions: NULL type: refpossible_keys: t_index,tw_index key: tw_index key_len: 5 ref: const rows: 17672 filtered: 100.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec)]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>查询</tag>
        <tag>Mysql</tag>
        <tag>Usingfilesort</tag>
        <tag>排序</tag>
        <tag>调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下查看C/C++程序的堆栈信息]]></title>
    <url>%2Fblog%2F2019%2F05%2F08%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9F%A5%E7%9C%8BC-C-%E7%A8%8B%E5%BA%8F%E7%9A%84%E5%A0%86%E6%A0%88%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[前言经常在Windows上开发的工程师们可能已经习惯了图形化的调试界面，在源代码的编辑框上点击就可以添加断点，在调用堆栈的窗口就可以看到程序运行的堆栈信息，但是在 linux 环境下，面对命令行的天下，我们需要掌握一些命令，才能够查看C/C++程序的堆栈信息。 测试环境1234567[albert@localhost#13:58:34#/home/albert]$cat /etc/issueCentOS release 6.3 (Final)Kernel \r on an \m[albert@localhost#13:58:43#/home/albert]$g++ --versiong++ (GCC) 4.4.7 20120313 (Red Hat 4.4.7-18)Copyright ?? 2010 Free Software Foundation, Inc. 查看方法 使用gdb程序调试core文件，格式为 gdb test_proc core.proc_id 使用gdb程序附加到调试程序的进程上，格式为 gdb attach proc_id 使用pstack程序输出调试程序的堆栈信息，格式为 pstack proc_id 使用strace程序打印调试程序的运行信息，格式为 strace -p proc_id 具体实践 一般查看堆栈信息时常常面对的都是多线程的程序，所以我们也来写一个简单的多线程小程序，代码如下： 12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;static void* thread_proc(void* arg)&#123; unsigned int sum = 3; while(true) &#123; for (int idx = 0; idx &lt; 1000000000; ++idx) sum += idx; printf("thread sum = %u\n", sum); sleep(2); &#125; return 0;&#125;int main()&#123; pthread_t thread_id; pthread_create(&amp;thread_id, NULL, thread_proc, NULL); unsigned int sum = 0; while(true) &#123; for (int idx = 0; idx &lt; 1000000000; ++idx) sum += idx; printf("main sum = %u\n", sum); sleep(1); &#125; return 0;&#125; 编译程序并运行，程序开始不断的打印计算结果 1234567891011[albert@localhost#15:06:54#/home/albert/test/threadtest]$g++ threadtest.cpp -O0 -pthread -o threadtest[albert@localhost#15:08:27#/home/albert/test/threadtest]$./threadtestthread sum = 3051657987main sum = 3051657984thread sum = 1808348675main sum = 1808348672main sum = 565039360thread sum = 565039363main sum = 3616697344thread sum = 3616697347... 现在可以通过上面描述的方法来查看threadtest程序堆栈信息了，几乎所有的命令都需要进程id，所以我们可以再开一个终端先通过pidof命令来获得： 12[albert@localhost#15:39:35#/home/albert/test/threadtest]$pidof threadtest21473 gdb调试core文件 通过kill命令产生core文件 使用命令 kill -11 21473可以将正在运行的程序杀死，并且产生core文件core.21473，-11表示段错误信号，通常是访问了无效的内存导致 通过gcore命令产生core文件 使用命令 gcore 21473可以产生core文件core.21473，但是不会杀死程序，适用于调试线上程序，又不影响用户使用的情况，可以测试一下： 12345678910111213[albert@localhost#15:39:43#/home/albert/test/threadtest]$gcore 21473warning: the debug information found in "/usr/lib/debug//lib64/libm-2.12.so.debug" does not match "/lib64/libm.so.6" (CRC mismatch)warning: the debug information found in "/usr/lib/debug/lib64/libm-2.12.so.debug" does not match "/lib64/libm.so.6" (CRC mismatch)warning: the debug information found in "/usr/lib/debug//lib64/libpthread-2.12.so.debug" does not match "/lib64/libpthread.so.0" (CRC mismatch)warning: the debug information found in "/usr/lib/debug/lib64/libpthread-2.12.so.debug" does not match "/lib64/libpthread.so.0" (CRC mismatch)[New LWP 21474][Thread debugging using libthread_db enabled]warning: the debug information found in "/usr/lib/debug//lib64/libc-2.12.so.debug" does not match "/lib64/libc.so.6" (CRC mismatch)warning: the debug information found in "/usr/lib/debug/lib64/libc-2.12.so.debug" does not match "/lib64/libc.so.6" (CRC mismatch)warning: the debug information found in "/usr/lib/debug//lib64/ld-2.12.so.debug" does not match "/lib64/ld-linux-x86-64.so.2" (CRC mismatch)warning: the debug information found in "/usr/lib/debug/lib64/ld-2.12.so.debug" does not match "/lib64/ld-linux-x86-64.so.2" (CRC mismatch)0x00000000004006eb in main ()Saved corefile core.21473 然后使用gdb调试core文件： 123456789101112131415161718192021222324252627[albert@localhost#15:47:13#/home/albert/test/threadtest]$gdb threadtest core.21473GNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/albert/test/threadtest/threadtest...(no debugging symbols found)...done.[New Thread 21474][New Thread 21473]Missing separate debuginfo forTry: yum --enablerepo='*-debug*' install /usr/lib/debug/.build-id/80/1b9608daa2cd5f7035ad415e9c7dd06ebdb0a2Reading symbols from /usr/lib64/libstdc++.so.6...(no debugging symbols found)...done.Loaded symbols for /usr/lib64/libstdc++.so.6Reading symbols from /lib64/libm.so.6......省略无关信息(no debugging symbols found)...done.Loaded symbols for /lib64/ld-linux-x86-64.so.2Core was generated by `./threadtest'.#0 0x0000000000400691 in thread_proc(void*) ()Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-18.el6_9.2.x86_64(gdb) 显示所有线程信息，可以使用gdb命令thread apply all bt： 123456789(gdb) thread apply all btThread 2 (Thread 0x7f1b4e1d2720 (LWP 21473)):#0 0x00000000004006eb in main ()Thread 1 (Thread 0x7f1b4d270700 (LWP 21474)):#0 0x0000000000400691 in thread_proc(void*) ()#1 0x00007f1b4d60caa1 in start_thread () from /lib64/libpthread.so.0#2 0x00007f1b4d359bcd in clone () from /lib64/libc.so.6 gdb附加到进程可以通过 gdb attach pid 直接附加到正在运行的程序上，然后查看线程信息thread apply all bt：123456789101112131415161718192021222324252627282930313233343536[albert@localhost#15:54:59#/home/albert/test/threadtest]$gdb attach 21473GNU gdb (GDB) Red Hat Enterprise Linux (7.2-83.el6)Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...attach: 没有那个文件或目录.Attaching to process 21473Reading symbols from /home/albert/test/threadtest/threadtest...(no debugging symbols found)...done.Reading symbols from /usr/lib64/libstdc++.so.6...(no debugging symbols found)...done.Loaded symbols for /usr/lib64/libstdc++.so.6Reading symbols from /lib64/libm.so.6......省略无关信息(no debugging symbols found)...done.Loaded symbols for /lib64/ld-linux-x86-64.so.20x00007f1b4d31dc4d in nanosleep () from /lib64/libc.so.6Missing separate debuginfos, use: debuginfo-install glibc-2.12-1.209.el6_9.2.x86_64 libstdc++-4.4.7-18.el6_9.2.x86_64(gdb) thread apply all btThread 2 (Thread 0x7f1b4d270700 (LWP 21474)):#0 0x00007f1b4d31dc4d in nanosleep () from /lib64/libc.so.6#1 0x00007f1b4d31dac0 in sleep () from /lib64/libc.so.6#2 0x00000000004006b6 in thread_proc(void*) ()#3 0x00007f1b4d60caa1 in start_thread () from /lib64/libpthread.so.0#4 0x00007f1b4d359bcd in clone () from /lib64/libc.so.6Thread 1 (Thread 0x7f1b4e1d2720 (LWP 21473)):#0 0x00007f1b4d31dc4d in nanosleep () from /lib64/libc.so.6#1 0x00007f1b4d31dac0 in sleep () from /lib64/libc.so.6#2 0x0000000000400721 in main () pstack输出堆栈信息如果不需要调试，只想查看运行程序当前的堆栈信息，可以使用pstack命令，输出信息很简洁：123456789[albert@localhost#15:57:53#/home/albert/test/threadtest]$pstack 21473Thread 2 (Thread 0x7f1b4d270700 (LWP 21474)):#0 0x0000000000400683 in thread_proc(void*) ()#1 0x00007f1b4d60caa1 in start_thread () from /lib64/libpthread.so.0#2 0x00007f1b4d359bcd in clone () from /lib64/libc.so.6Thread 1 (Thread 0x7f1b4e1d2720 (LWP 21473)):#0 0x00007f1b4d31dc4d in nanosleep () from /lib64/libc.so.6#1 0x00007f1b4d31dac0 in sleep () from /lib64/libc.so.6#2 0x0000000000400721 in main () strace打印程序运行情况strace输出的不是堆栈信息，而是类似于程序的运行步骤，具体信息如下：123456789101112131415161718[albert@localhost#15:57:56#/home/albert/test/threadtest]$strace -p 21473Process 21473 attachedwrite(1, "main sum = 2580918016\n", 22) = 22rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0rt_sigaction(SIGCHLD, NULL, &#123;SIG_DFL, [], 0&#125;, 8) = 0rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0nanosleep(&#123;1, 0&#125;, 0x7fff56a49c50) = 0write(1, "main sum = 1337608704\n", 22) = 22rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0rt_sigaction(SIGCHLD, NULL, &#123;SIG_DFL, [], 0&#125;, 8) = 0rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0nanosleep(&#123;1, 0&#125;, 0x7fff56a49c50) = 0write(1, "main sum = 94299392\n", 20) = 20rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0rt_sigaction(SIGCHLD, NULL, &#123;SIG_DFL, [], 0&#125;, 8) = 0rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0nanosleep(&#123;1, 0&#125;, 0x7fff56a49c50) = 0^CProcess 21473 detached 总结 在解决实际问题的过程中，上述几种方法可以结合使用，选取合适的使用方法，比如面对程序突然崩溃，那么gdb proc core就是调试的首选方法。 如果只是想简单的查看堆栈信息，可以使用pstack pid这种方式，免去了生成巨大core文件的麻烦。 如果还想查看运行逻辑中的变量信息，那么gdb使我们可以帮助我们动态调试程序，查看一些程序运行时的状态。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>linux</tag>
        <tag>gdb</tag>
        <tag>strace</tag>
        <tag>pstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python利用requests模块实现代理访问网络]]></title>
    <url>%2Fblog%2F2019%2F05%2F05%2FPython%E5%88%A9%E7%94%A8requests%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AE%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[前言代理相信很多人都听过，即使没有自己感受到，在无形之中可能也使用过，网络代理作为一项技术，在访问互联网时被广泛使用，那是因为使用代理有着诸多好处。 使用代理IP能够突破自身的访问限制，不要把突破限制看成是坏事情，有时后恰恰是为了网络安全才使用了代理，比如内网的一台服务器只针对特定的IP提供访问权限，这时如果给内部人员分配指定的代理就可以进行访问，不比对所有的IP地址都开放，代理IP还可以进行自主管理。 使用代理IP还提高访问速度，通常代理IP服务器都配置了一个较大的硬盘缓冲区，当缓冲区中保存有用户的请求信息时，则直接由缓冲区中取出信息，返回给用户，以提高访问速度。 测试环境12PS E:\&gt; python --versionPython 3.6.7 代码实现其实在Python 3中利用requests可以很方便的使用代理访问网络，比如下面这个简单的get方法：1requests.get(target_url, proxies=proxy_data) 其中需要注意的就是 proxies 参数的值，这里换成可以代理的ip就可以了，网上流传着众多的代理IP，只要可用就可以拿来代理IP访问，不过这些免费的IP失效性非常差，常常过几分钟就失效了，下面就给出一个完整的例子，检测代理IP是否可用： 1234567891011121314151617181920212223242526272829303132import requeststest_ip = '116.209.56.118'test_port = '9999'def test_proxy_request(ip, port): # 代理IP地址 proxy_data = &#123; 'http': 'http://' + ip + ':' + port, 'https': 'http://' + ip + ':' + port, &#125; # 客户端说明 head_data = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0', 'Connection': 'keep-alive' &#125; try: # 该返回当前的IP地址，http://icanhazip.com提供返回当前外网IP的服务 response = requests.get('http://icanhazip.com', headers=head_data, proxies=proxy_data) outer_ip = response.text.strip().replace('\n', '') return outer_ip == ip except: return Falseif __name__ == '__main__': test_result = test_proxy_request(test_ip, test_port) if test_result: print("IP代理成功 ==&gt; &#123;0&#125;:&#123;1&#125;".format(test_ip, test_port)) else: print("IP代理失败 ==&gt; &#123;0&#125;:&#123;1&#125;".format(test_ip, test_port)) 需要注意，其中只有这一句requests.get(&#39;http://icanhazip.com&#39;, headers=head_data, proxies=proxy_data)是代理的重点。 测试结果 IP代理成功 ==&gt; 116.209.56.118:9999]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>IP</tag>
        <tag>Python</tag>
        <tag>requests</tag>
        <tag>网络代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中explain命令简析]]></title>
    <url>%2Fblog%2F2019%2F04%2F27%2FMysql%E4%B8%ADexplain%E5%91%BD%E4%BB%A4%E7%AE%80%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言之前总结了Mysql慢查询日志的开启与配置方法，通过分析慢查询日志可以锁定执行效率差的SQL，但是这仅仅是发现了需要优化的部分，还要分析执行缓慢的原因，这时候就可以使用EXPLAIN命令去分析，所执行的操作究竟慢在哪里，是不是可以通过加索引或者改变查询方法来解决。 通过查询资料发现除了EXPLAIN命令，还有一个DESCRIBE命令，看起来很陌生是不是，但是如果写出简写desc应该很多人的就熟悉了，这不是查询表结构的时候常用的命令吗？实际上以上三个命令在mysql中是等价的，不过在使用时有些习惯性的偏向，通常使用 EXPLAIN 来分析SQL语句的执行缓慢的问题，而使用 DESCRIBE 或者 desc 来查看表的结构，就类似于惯用法，知道就好。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. EXPLAIN 的使用关于 EXPLAIN 使用其实很简单，就是在正常的执行语句之前加一个explain 就可以了，不过这里也存在一个疑问，就是发现很多篇文章，提到下面这种说法： explain 只能解释select查询，并不会对存储过程、insert、update、delete或其他语句做解释 但是我查阅了官方文档发现，EXPLAIN 后面可以跟 SELECT、 DELETE、 INSERT、 REPLACE、和 UPDATE语句，另外之前使用的 EXPLAIN EXTENDED 选项现在也默认开启，EXTENDED 关键字后续会在 Mysql 8.0 版本删除，应该是版本问题导致了explain语句使用的差异，所以请记住在使用新版本的Mysql时，需要分析语句执行情况的，只需要在语句前面添加一个 explain关键字即可。 不过在分析 select 语句时，explain命令会给出额外的提示信息，帮助我们优化查询语句，这也是我们需要学习的重点，先来简单看一下使用方法： 普通的查询语句 123456789mysql&gt; select * from a;+----+-----+| id | num |+----+-----+| 1 | 100 || 2 | 200 || 3 | 300 |+----+-----+3 rows in set (0.05 sec) 使用 EXPLAIN 来分析普通的查询语句 1234567mysql&gt; explain select * from a;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+1 row in set (0.04 sec) 通过上面的例子可以很清楚的知道 EXPLAIN 命令的使用方法，使用了 EXPLAIN 关键之后会生成一个分析结果的表格，该表格有id、select_type、table、partitions、type、possible_keys、key、key_len、ref、rows、filtered、Extra一共12列，而这12列中的内容代表的含义是我们学习的重点，也是我们进行优化的依据，这个结果集可能包含多行，其中每一行都是关于一个表的查询信息，可以针对于具体的表查询优化。 本来想每种情况后面紧跟一个例子的，但是发现这样会造成重点内容分散，不利于整体把握，所以还是先把各列可能的取值说清楚，然后在文末针对于上文的取值给出例子，如果看描述就能明白就可以省略例子的内容，否则可以对照着例子的写法理解一下, [eg：&lt;id-1&gt;]表示参考后面的例子&lt;id-1&gt;，想对照的话搜索即可。 EXPLAIN 各列的可能取值idid 列的取值通常是一组数字，表示select查询的序号，也有可能是一个NULL： 取值 含义 例子编号 id数字相同 优先级相同，从上往下执行 [eg：&lt;id-1&gt;] id数字不同 数字越大优先级越高，越先执行，比如包含子查询的语句，内部查询优先执行 [eg：&lt;id-2&gt;] id值为NULL 通常是使用了union，表示该行是一个结果集，不需要使用它来进行查询 [eg：&lt;id-2&gt;] select_typeselect_type 列表示查询的类型，主要是用于区分普通查询、联合查询、子查询等复杂的情况： 取值 含义 例子编号 SIMPLE 简单的查询语句，查询中不包括UNION操作和子查询 [eg：&lt;id-1&gt;] PRIMARY 在复杂查询中处于最外层的查询 [eg：&lt;id-2&gt;] UNION 查询语句中处于 UNION 关键字之后的查询 [eg：&lt;id-2&gt;] DEPENDENT UNION 查询语句中处于 UNION 关键字之后的查询，需要依赖于外部查询 [eg：`&lt;id-&gt;`]* UNION RESULT 表示 UNION 操作之后的结果，本身并不需要参与查询，通常该记录id字段为 NULL [eg：&lt;id-2&gt;] SUBQUERY 在子查询中的第一个查询 [eg：`&lt;id-&gt;`]* DEPENDENT SUBQUERY 在子查询中的第一个查询，需要依赖于外部查询 [eg：`&lt;id-&gt;`]* DERIVED 在 FROM 列表中包含的子查询 [eg：&lt;id-4&gt;] MATERIALIZED 物化子查询 [eg：&lt;id-5&gt;] UNCACHEABLE SUBQUERY 一个结果不能被缓存并且对于外部查询的每一行都需要重新评估自身的子查询 [eg：`&lt;id-&gt;`]* UNCACHEABLE UNION 查询语句中处于 UNION 关键字之后的子查询，并且其结果属于UNCACHEABLE SUBQUERY类型 [eg：`&lt;id-&gt;`]* tabletable 列表示查询所引用到的表名，如果查询中使用了别名，那么会显示别名，此外还有一些其他类型的引用： 取值 含义 例子编号 表名、别名 查询时引用了这个表 [eg：&lt;id-1&gt;] &lt;unionM,N&gt; 查询时引用了由id为M和N的两个查询的结果集构成的临时结果集 [eg：&lt;id-2&gt;] &lt;derivedN&gt; 查询时引用了id为N的查询形成的结果集 [eg：&lt;id-4&gt;] &lt;subqueryN&gt; 查询时引用了id为N的物化子查询形成的结果集 [eg：&lt;id-5&gt;] partitionspartitions 列表示查询结果集所涉及到的分区，值为 NULL 时表示该表并未分区： 取值 含义 例子编号 分区名 查询结果集引用到了这个分区 [eg：&lt;id-6&gt;] NULL 该表格并未分区，或者结果集中数据不再分区中 [eg：&lt;id-1&gt;] typetype 列表示访问类型，也就是找到所需数据所能使用的最好方式，取值类型很多，在下表中从上到下效果越来越差： 取值 含义 例子编号 NULL 查询经过优化执行时不用访问表数据，可能通过索引就搞定了，或者根本没有数据 [eg：&lt;id-8&gt;] system 在MyISAM类型的表中只有一行数据时出现，如果在 Innodb 类型表中只有一行数据通常显示 ALL [eg：&lt;id-9&gt;] const 表格使用了唯一索引或者主键，并且将其作为判定相等的筛选条件，得到一条记录 [eg：&lt;id-10&gt;] eq_ref 连接查询时，驱动表的每一条记录的条件列，与后面连接表的主键或唯一索引判定相等，后表采用的连接方式 [eg：&lt;id-11&gt;] ref 连接查询时，驱动表的每一条记录的条件列，与后面连接表的索引列判定相等，后表采用的连接方式，索引列数据不要求唯一，不连接表时就是查索引列等于一个具体的值 [eg：&lt;id-7&gt;] ref_or_null 与 ref 基本一致，另外包含查询索引列为 NULL 的记录 [eg：&lt;id-12&gt;] fulltext 包含全文索引的表的查询方式，全文索引的优先级要高于普通索引 [eg：`&lt;id-&gt;`]* index_merge 至少用到了两个索引，并且用到了索引合并优化 [eg：`&lt;id-&gt;`]* unique_subquery where条件in形式的子查询子查询返回唯一结果时，等价于将类型为 eq_ref 的查询作为子查询 [eg：`&lt;id-&gt;`]* index_subquery where条件in形式的子查询引用了非唯一索引，等价于将类型为 ref 的查询作为子查询 [eg：`&lt;id-&gt;`]* range 对于表的索引列使用范围判定的查询 [eg：&lt;id-13&gt;] index 除了查找索引树之外，与 ALL 选项基本一致，通常由于索引较小查询会快一点 [eg：&lt;id-14&gt;] ALL 完整浏览整个表格，查找符合条件的结果，属于最差的访问方式 [eg：&lt;id-1&gt;] possible_keyspossible_keys 列表示查询所需数据过程可能用到的索引名，具体是否使用还要依赖于查询过程中表的连接顺序，该值为 NULL 时表示无索引可用，此时需要考虑对表进行优化来改善查询结果情况了。 取值 含义 例子编号 索引名 查询时可能用到的索引名，是否使用取决于查询连接顺序 [eg：&lt;id-7&gt;] NULL 该查询没有可用索引，需要考虑优化 [eg：&lt;id-1&gt;] keykey 列表示查询所需数据过程确实用到的索引名，该值为 NULL 时表示无索引可用，此时也需要考虑对表进行优化。 取值 含义 例子编号 索引名 查询时确实用到的索引名 [eg：&lt;id-7&gt;] NULL 查询时没有可用索引，需考虑优化 [eg：&lt;id-1&gt;] key_lenkey_len 列表示查询所需数据过程用到的索引长度，该值为 NULL 时表示没有使用索引，由于存储格式的不同，对于可以为 NULL 的列储存索引所需空间要比不能为 NULL 列的大一个字节。 取值 含义 例子编号 索引长度 查询时确实用到的索引长度 [eg：&lt;id-7&gt;] NULL 没有使用到索引 [eg：&lt;id-1&gt;] refkey_len 列表示查询时使用常数或者某一列来和索引列比较，有时会显示 func，表示使用了一些函数的结果与索引比较，值为 NULL 时表示没用到索引比较 取值 含义 例子编号 引用列名 查询时与索引比较的列名，形式可能为&lt;subquery2&gt;.id，表示引用了子查询结果中的id列 [eg：&lt;id-5&gt;] const 查询时与索引比较的为常数 [eg：&lt;id-10&gt;] func 查询时与索引比较的一些函数结果 [eg：`&lt;id-&gt;`]* NULL 不是上述几种情况，可能没有使用索引比较 [eg：&lt;id-1&gt;] rowsrows 列表示查询符合条件的结果时所要检查的数据行数，在 InnoDB 类型的表中，这个值是一个估计值，可用来参考并不精确，值为 NULL 时表示表中无数据，或者无法找到匹配行，比如查找一条主键中不包含的数据。 取值 含义 例子编号 数字 查询时所要检查的数据行数 [eg：&lt;id-3&gt;] NULL 没有数据或者不需要检测 [eg：&lt;id-1&gt;] filteredfiltered 列表示通过筛选条件的记录数占可能参与检查的记录数，是一个估计值，该值与 rows 的乘积大概就是结果集中的记录数： 取值 含义 例子编号 数字 通过筛选条件的记录数占可能参与检查的记录数，最大为 100.00 [eg：&lt;id-3&gt;] ExtraExtra 列表示Mysql处理查询所使用的额外信息，类型很多，其中一些情况是需要进行优化的信号，对于SQL分析很有帮助： 取值 含义 例子编号 Child of &#39;tbl_name&#39; pushed join@1 当表被当做另一个表’tbl_name’的子表能被存放在 NDB 内核的时候，该值只出现在存储选项被开启的 NDB 集群上 - const row not found 当一个系统表没有数据可查的时候 - Deleting all rows 对于 DELETE 操作， MyISAM 引擎支持一个可以简单快速删除表数据的方法，如果使用了整个优化则显示此选项 - Distinct 查询时使用了 distinct 关键字，当查找到一个第一个匹配值后，相同匹配就不再搜索了 - const 当一个系统表没有数据可查的时候 - FirstMatch(tbl_name) 当 semi-join FirstMatch 访问简化策略被使用时候， 通常出现在 Where的 in 子句中，找到一个值后，后面相同值不再匹配出现 - Full scan on NULL key 当优化器不能使用索引查找访问方法时，将会显示该值，表示将子查询作为一种后备策略 - Impossible HAVING 当 HAVING 子句的条件总是不成立，无法匹配出任何数据 [eg：&lt;id-23&gt;] Impossible WHERE 当 WHERE 子句的条件总是不成立，无法匹配出任何数据 [eg：&lt;id-22&gt;] Impossible WHERE noticed after reading const tables 在读取const表和system表时，WHERE 子句的条件总是不成立 - LooseScan(m..n) 当 semi-join LooseScan 策略被使用的时候. m 和 n 是索引的编号 - No matching min/max row 在查询中包括系统函数，但是通过条件查询无法匹配出数据的时候， 比如SELECT MIN(...) FROM ... WHERE CONDITION - no matching row in const table 当连表查询时有一个空表或者没有匹配唯一索引的数据时，会给出此提示 [eg：&lt;id-18&gt;] No tables used 当查询中没有 FROM 子句或者只有 FROM DUAL子句时 [eg：&lt;id-20&gt;] Not exists 发生在左外连接的优化，当要求右侧表字段为空时，如果查找到一条不为空匹配，则停止查找匹配这项记录， 比如 SELECT * FROM t1 LEFT JOIN t2 ON t1.id=t2.id WHERE t2.id IS NULL - Plan isn&#39;t ready yet 执行命令 EXPLAIN FOR CONNECTION 时，优化器在命名连接中还没有完成为语句执行创建执行计划 - Range checked for each record(index map: N) 当没有好的默认索引可使用时，但当我们可以将以前表中的所有列都视为常量时，可能会使用某些索引就是这种情况 - Scanned N databases 表示在执行对INFORMATION_SCHEMA表的查询时，服务器执行了多少次目录扫描，数字可以是0,1或者任何整数 - Select tables optimized away 优化器发现只有一行，并且通过索引直接皆可以获得想要的数据，而不需要真正访问表数据，比如在索引列使用聚合函数 [eg：&lt;id-16&gt;] Skip_open_table 对于 INFORMATION_SCHEMA 表的查询，不需要打开表，只需要浏览目录就可以完成查询 - Open_frm_only 对于 INFORMATION_SCHEMA 表的查询，需要打开 .frm 文件完成查询 - Open_full_table 对于 INFORMATION_SCHEMA 表的查询，需要打开 .frm, .MYD, .MYI 文件完成查询 - Start temporary,End temporary 表示使用临时表用于 semi-join Duplicate Weedout 策略 [eg：&lt;id-15&gt;] unique row not found 当一个拥有 UNIQUE 索引或者 PRIMARY 索引的表没有查到满足条件数据时 - Using filesort 无法仅通过引用索引就完成排序，需要一个额外的阶段来进行外部排序，并且按排序结果取回记录 [eg：&lt;id-17&gt;] Using index 只通过索引排序就可以取得排序后的数据，无需做额外的搜索真实记录数据的工作 [eg：&lt;id-7&gt;] Using index condition 首先通过访问索引元组的方式来读取表格，除非必要时会通过索引索引信息延迟读取整个表格数据 - Using index for group-by 索引用于处理包含 GROUP BY 和 DISTINCT 的查询，由于重复项会被快速跳过。所以非常高效 - Using join buffer (Block Nested Loop) 连接访问之前表格数据被部分读入连接缓存区，然后使用缓存中的行与当前表进行连接，括号内容为使用算法 [eg：&lt;id-1&gt;] Using join buffer (Batched Key Access) 连接访问之前表格数据被部分读入连接缓存区，然后使用缓存中的行与当前表进行连接，括号内容为使用算法 - Using MRR 表格数据会通过 Multi-Range Read 优化策略来读取 - Using sort_union(...), Using union(...), Using intersect(...) 针对于 index_merge 选项，表明索引浏览被合并的特定算法 - Using temporary 需要创建一个临时表来存储结果，通常出现在包含了作用在不同列的上的 GROUP BY 子句和 ORDER BY子句 [eg：&lt;id-2&gt;] Using where 当查询使用了 WHERE 子句来过滤结果发送给客户端的时候 [eg：&lt;id-3&gt;] Using where with pushed condition 仅适用于 NDB 类型表，它意味着NDB集群正在使用 “条件存储”优化选项来提高接近于非索引列和常量之间直接比较的效率。 - Zero limit 当查询语句包含 LIMIT 0子句并不能查到任何记录的时候 [eg：&lt;id-19&gt;] EXPLAIN 各列的可能取值对应的例子建表操作(1为了展示id取值的不同先创建 a、b两个表，然后插入测试数据，由于是测试的开始，我们分别查看表结构和数据，之后为了减少篇幅，只给出命令，不再查询表结构和数据，接着我们开始测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051mysql&gt; create table a(id int, num int);Query OK, 0 rows affected (0.04 sec)mysql&gt; insert into a values(1, 100),(2,200),(3,300);Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; desc a;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.02 sec)mysql&gt; select * from a;+----+-----+| id | num |+----+-----+| 1 | 100 || 2 | 200 || 3 | 300 |+----+-----+3 rows in set (0.02 sec)mysql&gt; create table b(id int, num int);Query OK, 0 rows affected (0.03 sec)mysql&gt; insert into b values(1, 100),(2,200),(4,400);Query OK, 3 rows affected (0.00 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; desc b;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | | NULL | || num | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+2 rows in set (0.03 sec)mysql&gt; select * from b;+----+-----+| id | num |+----+-----+| 1 | 100 || 2 | 200 || 4 | 400 |+----+-----+3 rows in set (0.03 sec) &lt;id-1&gt;12345678mysql&gt; explain select * from a, b;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL || 1 | SIMPLE | b | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+---------------------------------------+2 rows in set (0.05 sec) &lt;id-2&gt;123456789mysql&gt; explain select * from a union select * from b;+------+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+------+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+| 1 | PRIMARY | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL || 2 | UNION | b | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL || NULL | UNION RESULT | &lt;union1,2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | NULL | Using temporary |+------+--------------+------------+------------+------+---------------+------+---------+------+------+----------+-----------------+3 rows in set (0.05 sec) &lt;id-3&gt;12345678mysql&gt; explain select id from a where id = (select id from b where num = 100);+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | PRIMARY | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 33.33 | Using where || 2 | SUBQUERY | b | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 33.33 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+2 rows in set (0.06 sec) &lt;id-4&gt;123456789101112131415161718192021222324252627282930313233mysql&gt; select @@version;+-----------+| @@version |+-----------+| 5.6.33 |+-----------+1 row in set (0.05 sec)mysql&gt; explain select num from (select * from a) t where t.id &gt; 1;+----+-------------+------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+------+---------------+------+---------+------+------+-------------+| 1 | PRIMARY | &lt;derived2&gt; | ALL | NULL | NULL | NULL | NULL | 3 | Using where || 2 | DERIVED | a | ALL | NULL | NULL | NULL | NULL | 3 | NULL |+----+-------------+------------+------+---------------+------+---------+------+------+-------------+2 rows in set (0.06 sec)-- 很神奇的是我在5.7版本操作了半天也没出现，内部进行了优化，相同的语句操作如下：mysql&gt; select @@version;+------------+| @@version |+------------+| 5.7.21-log |+------------+1 row in set (0.05 sec)mysql&gt; explain select num from (select * from a) t where t.id &gt; 1;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 33.33 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set (0.05 sec) 建表操作(2新建两个带索引的表格 c 和 d，其中表格c带有普通索引，表格d带有主键，这两个表格会参与后面用作展示的例子： 12345678910111213mysql&gt; create table c(id int, num int, key idindex(id));Query OK, 0 rows affected (0.21 sec)mysql&gt; insert into c values(1,103),(2,203),(6,603);Query OK, 3 rows affected (0.05 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; create table d(id int, num int, primary key(id));Query OK, 0 rows affected (0.14 sec)mysql&gt; insert into d values(1,104),(2,204),(6,504);Query OK, 3 rows affected (0.11 sec)Records: 3 Duplicates: 0 Warnings: 0 &lt;id-5&gt;123456789mysql&gt; explain select id from d where id in (select id from a);+----+--------------+-------------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------+-------------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+| 1 | SIMPLE | &lt;subquery2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | NULL | 100.00 | Using where || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | &lt;subquery2&gt;.id | 1 | 100.00 | Using index || 2 | MATERIALIZED | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | NULL |+----+--------------+-------------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+3 rows in set (0.05 sec) 建表操作(3为了测试 partitions 字段的取值，创建表格 p，其实就是创建了一个带有分区的表，这个表格会参与后面用作展示的例子： 123456mysql&gt; create table p(id int, num int) partition by range(id)(partition p0 values less than(3), partition p1 values less than(6));Query OK, 0 rows affected (0.02 sec)mysql&gt; insert into p values(1, 111),(2,222),(4,444),(5,555);Query OK, 4 rows affected (0.01 sec)Records: 4 Duplicates: 0 Warnings: 0 &lt;id-6&gt;1234567mysql&gt; explain select * from p where id &lt; 5;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | p | p0,p1 | ALL | NULL | NULL | NULL | NULL | 4 | 33.33 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set (0.05 sec) &lt;id-7&gt;1234567mysql&gt; explain select id from c where id = 1;+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------------+| 1 | SIMPLE | c | NULL | ref | idindex | idindex | 5 | const | 1 | 100.00 | Using index |+----+-------------+-------+------------+------+---------------+---------+---------+-------+------+----------+-------------+1 row in set (0.06 sec) 建表操作(4为了测试 type 字段的取值，有时需要特定的数据引擎才可以，所以创建了以 MyISAM引擎类型的表格 m，然后进行一些测试： 12mysql&gt; create table m(id int, num int)engine=myisam;Query OK, 0 rows affected (0.02 sec) &lt;id-8&gt;1234567mysql&gt; explain select * from m;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+1 row in set (0.03 sec) &lt;id-9&gt;12345678910mysql&gt; insert into m values(1,1001);Query OK, 1 row affected (0.00 sec)mysql&gt; explain select * from m;+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | m | NULL | system | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+---------------+------+---------+------+------+----------+-------+1 row in set (0.04 sec) &lt;id-10&gt;1234567mysql&gt; explain select id from d where id = 1;+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------------+| 1 | SIMPLE | d | NULL | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | Using index |+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------------+1 row in set (0.04 sec) &lt;id-11&gt;12345678mysql&gt; explain select a.id from a, d where a.id = d.id;+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+-------------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using where || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | sqltest2.a.id | 1 | 100.00 | Using index |+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+-------------+2 rows in set (0.05 sec) &lt;id-12&gt;1234567mysql&gt; explain select id from c where id = 1 or id is null;+----+-------------+-------+------------+-------------+---------------+---------+---------+-------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------------+---------------+---------+---------+-------+------+----------+--------------------------+| 1 | SIMPLE | c | NULL | ref_or_null | idindex | idindex | 5 | const | 2 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------------+---------------+---------+---------+-------+------+----------+--------------------------+1 row in set (0.04 sec) &lt;id-13&gt;1234567mysql&gt; explain select id from c where id &gt; 1;+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+--------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+--------------------------+| 1 | SIMPLE | c | NULL | range | idindex | idindex | 5 | NULL | 2 | 100.00 | Using where; Using index |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+--------------------------+1 row in set (0.04 sec) &lt;id-14&gt;1234567mysql&gt; explain select id from c;+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+| 1 | SIMPLE | c | NULL | index | NULL | idindex | 5 | NULL | 3 | 100.00 | Using index |+----+-------------+-------+------------+-------+---------------+---------+---------+------+------+----------+-------------+1 row in set (0.03 sec) &lt;id-15&gt;123456789mysql&gt; explain select id from c where id in (select a.id from a, d where a.id = d.id);+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+------------------------------+| 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using where; Start temporary || 1 | SIMPLE | c | NULL | ref | idindex | idindex | 5 | sqltest2.a.id | 1 | 100.00 | Using index || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | sqltest2.a.id | 1 | 100.00 | Using index; End temporary |+----+-------------+-------+------------+--------+---------------+---------+---------+---------------+------+----------+------------------------------+3 rows in set (0.04 sec) &lt;id-16&gt;1234567mysql&gt; explain select min(id) from c;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Select tables optimized away |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------------------+1 row in set (0.05 sec) &lt;id-17&gt;1234567mysql&gt; explain select id from c order by num;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| 1 | SIMPLE | c | NULL | ALL | NULL | NULL | NULL | NULL | 3 | 100.00 | Using filesort |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+1 row in set (0.04 sec) &lt;id-18&gt;1234567mysql&gt; explain select id from d where id = 100;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | no matching row in const table |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------+1 row in set (0.04 sec) &lt;id-19&gt;1234567mysql&gt; explain select id from d limit 0;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Zero limit |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------+1 row in set (0.04 sec) &lt;id-20&gt;1234567mysql&gt; explain select 1 from dual;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------+1 row in set (0.04 sec) &lt;id-22&gt;1234567mysql&gt; explain select * from a where 1 = 2;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible WHERE |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+------------------+1 row in set (0.03 sec) &lt;id-23&gt;1234567mysql&gt; explain select sum(num) from a group by id having 1 =2;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Impossible HAVING |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------------------+1 row in set (0.04 sec) 总结 关于 EXPLAIN 命令的所有可能取值后面，还有部分例子是空的，完全是由于个人水平有限，等找到所说的取值情况再补充，也欢迎大家提供例子 另外 EXPLAIN 提供的信息中没有关于触发器、存储过程的信息或者评估用户自定义函数对查询的影响情况 所有的可能取值中 possible_keys、rows、filtered中的统计信息基本是估算的，并非精确值，只能用来做优化参考 Extra 列的信息对于尝试优化起到了至关重要的作用，当出现 Using filesort、Using temporary、Using join buffer的时候一般就要考虑采取优化方案了 首先了解这些可能出现的情况，之后我们留这里利用这些说明来进行查询优化了]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>查询</tag>
        <tag>Mysql</tag>
        <tag>EXPLAIN</tag>
        <tag>Extra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gcc编译生成可执行文件的过程中发生了什么]]></title>
    <url>%2Fblog%2F2019%2F04%2F16%2Fgcc%E7%BC%96%E8%AF%91%E7%94%9F%E6%88%90%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[前言一直好奇程序的编译过程到底做了哪些工作，后来学会在Ubuntu上使用gcc编译程序，知道了生成可执行文件需要分为预编译、编译、汇编和链接4个步骤，逐渐了解了其中的细节，但是过一段时间之后总是记不太清楚了，所以总结一下增强记忆，同时方便日后查找使用。 编译方式一步到位使用gcc命令可以一步将main.c源文件编译生成最终的可执行文件main_direct 1gcc main.c –o main_direct 分步执行gcc的编译流程通常认为包含以下四个步骤，实际上就是将上面的命令分成4步执行，这也是gcc命令实际的操作流程，生成的可执行文件main与上面单条命令生成的可执行文件main_direct是一模一样的 预处理，生成预编译文件（.i文件）：gcc –E main.c –o main.i 编译，生成汇编代码（.s文件）：gcc –S main.i –o main.s 汇编，生成目标文件（.o文件）：gcc –c main.s –o main.o 链接，生成可执行文件（executable文件）：gcc main.o –o main 编译流程这里的编译是指将源文件（.c）生成可执行文件（executable）的这个完整的过程，而不是上面提到的四个步骤中的第二步，为了弄清楚编译过程究竟都做了哪些工作，接下来我们可以分步骤来看一下gcc编译.c文件的过程，了解了每一步的内容，也就明白了整个编译流程，先给出源文件 mian.c 的源代码。1234567891011121314151617#include &lt;stdio.h&gt;#define A 100// calc sumint sum(int a, int b)&#123; return a + b;&#125;int main()&#123; int b = 1; int c = sum(A, b); printf("sum = %d\n", c); return 0;&#125; 预处理预处理又叫预编译，是完整编译过程的第一个阶段，在正式的编译阶段之前进行。预处理阶段将根据已放置在文件中的预处理指令来修改源文件的内容，对于C语言来说预处理的可执行程序叫做 cpp，全称为C Pre-Processor（C预处理器），是一个与 C 编译器独立的小程序，预编译器并不理解 C 语言语法，它仅是在程序源文件被编译之前，实现文本替换的功能。简单来说，预处理就是将源代码中的预处理指令根据语义预先处理，并且进行一下清理、标记工作，然后将这些代码输出到一个 .i 文件中等待进一步操作。 一般地，C/C++ 程序的源代码中包含以 # 开头的各种编译指令，被称为预处理指令，其不属于 C/C++ 语言的语法，但在一定意义上可以说预处理扩展了 C/C++。根据ANSI C 定义，预处理指令主要包括：文件包含、宏定义、条件编译和特殊控制等4大类。 预处理阶段主要做以下几个方面的工作： 文件包含：#include 是 C 程序设计中最常用的预处理指令，格式有尖括号 #include &lt;xxx.h&gt; 和双引号 #include &quot;xxx.h&quot; 之分，分别表示从系统目录下查找和优先在当前目录查找，例如常用的 #include &lt;stdio.h&gt; 指令，就表示使用 stdio.h 文件中的全部内容，替换该行指令。 添加行号和文件名标识： 比如在文件main.i中就有类似 # 2 &quot;main.c&quot; 2 的内容，以便于编译时编译器产生调试用的行号信息及用于编译时产生编译错误或警告时能够显示行号。 宏定义展开及处理： 预处理阶段会将使用 #define A 100 定义的常量符号进行等价替换，文中所有的宏定义符号A都会被替换成100，还会将一些内置的宏展开，比如用于显示文件全路径的__FILE__，另外还可以使用 #undef 删除已经存在的宏，比如 #undef A 就是删除之前定义的宏符号A。 条件编译处理: 如 #ifdef，#ifndef，#else，#elif，#endif等，这些条件编译指令的引入使得程序员可以通过定义不同的宏来决定编译程序对哪些代码进行处理，将那些不必要的代码过滤掉，防止文件重复包含等。 清理注释内容： // xxx 和 /*xxx*/ 所产生的的注释内容在预处理阶段都会被删除，因为这些注释对于编写程序的人来说是用来记录和梳理逻辑代码的，但是对编译程序来说几乎没有任何用处，所以会被删除，观察 main.i 文件也会发现之前的注释都被删掉了。 特殊控制处理: 保留编译器需要使用 #pragma 编译器指令，另外还有用于输出指定的错误信息，通常来调试程序的 #error 指令。 查看main.i文件 编译编译过程是整个程序构建的核心部分，也是最复杂的部分之一，其工作就是把预处理完生成的 .i 文件进行一系列的词法分析、语法分析、语义分析以及优化后产生相应的汇编代码文件，也就是 .s 文件，这个过程调用的处理程序一般是 cc 或者 ccl。汇编语言是非常有用的，因为它给不同高级语言的不同编译器提供了可选择的通用的输出语言，比如 C 和 Fortran 编译产生的输出文件都是汇编语言。 词法分析： 主要是使用基于有线状态机的Scanner分析出token，可以通过一个叫做 lex 的可执行程序来完成词法扫描，按照描述好的词法规则将预处理后的源代码分割成一个个记号，同时完成将标识符存放到符号表中，将数字、字符串常量存放到文字表等工作，以备后面的步骤使用。 语法分析： 对有词法分析产生的token采用上下文无关文法进行分析，从而产生语法树，此过程可以通过一个叫做 yacc 的可执行程序完成，它可以根据用户给定的语法规则对输入的记号序列进行解析，从而构建一棵语法树，如果在解析过程中出现了表达式不合法，比如括号不匹配，表达式中缺少操作符、操作数等情况，编译器就会报出语法分析阶段的错误。 语义分析： 此过程由语义分析器完成，编译器 cc 所能分析的语义都是静态语义，是指在编译期间可以确定的语义，通常包括声明和类型的匹配，类型的转换等。比如将一个浮点型的表达式赋值给一个整型的表达式时，语义分析程序会发现这个类型不匹配，编译器将会报错。而动态语义一般指在运行期出现的语义相关问题，比如将0作为除数是一个运行期语义错误。语义分析过程会将所有表达式标注类型，对于需要隐式转换的语法添加转换节点，同时对符号表里的符号类型做相应的更新。 代码优化： 此过程会通过源代码优化器会在源代码级别进行优化，针对于编译期间就可以确定的表达式（例如：100+1）给出确定的值，以达到优化的目的，此外还包括根据机器硬件执行指令的特点对指令进行一些调整使目标代码比较短，执行效率更高等操作。 查看main.s文件 汇编汇编过程是整个程序构建中的第三步，是将编译产生的汇编代码文件转变成可执行的机器指令。相对来说比较简单，每个汇编语句都有相对应的机器指令，只需根据汇编代码语法和机器指令的对照表翻译过来就可以了，最终生成目标文件，也就是 .o 文件，完成此工作的可执行程序通常是 as。目标文件中所存放的也就是与源程序等效的目标的机器语言代码，通常至少包含代码段和数据段两个段，并且还要包含未解决符号表，导出符号表和地址重定向表等3个表。汇编过程会将extern声明的变量置入未解决符号表，将static声明的全局变量不置入未解决符号表，也不置入导出符号表，无法被其他目标文件使用，然后将普通变量及函数置入导出符号表，供其他目标文件使用。 代码段： 包含主要是程序的指令。该段一般是可读和可执行的，但一般却不可写。 数据段： 主要存放程序中要用到的各种全局变量或静态的数据，一般数据段都是可读，可写，可执行的。 未解决符号表： 列出了在本目标文件里有引用但不存在定义的符号及其出现的地址。 导出符号表： 列出了本目标文件里具有定义，并且可以提供给其他目标文件使用的符号及其在出现的地址。 地址重定向表： 列出了本目标文件里所有对自身地址的引用记录。 查看main.o文件 链接链接过程是程序构建过程的最后一步，通常调用可执行程序 ld 来完成，可以简单的理解为将目标文件和库文件打包组装成可执行文件的过程，其主要内容就是把各个模块之间相互引用的部分都处理好，将一个文件中引用的符号同该符号在另外一个文件中的定义连接起来，使得各个模块之间能够正确的衔接，成为一个能够被操作系统装入执行的统一整体。 虽然汇编之后得到的文件已经是机器指令文件，但是依然无法立即执行，其中可能还有尚未解决的问题，比如源代码 main.c 中的 printf 这个函数名就无法解析，需要链接过程与对应的库文件对接，完成的重定位，将函数符号对应的地址替换成正确的地址。前面提到的库文件其实就是一组目标文件的包，它们是一些最常用的代码编译成目标文件后打成的包。比如 printf的头文件是 stdio.h，而它的实现代码是放在动态库 libc.so.6 中的，链接的时候就要引用到这个库文件。 从原理上讲，连接的的工作就是把一些指令对其他符号地址的引用加以修正，主要包括了地址和空间分配、符号决议和重定位等步骤，根据开发人员指定的链接库函数的方式不同，链接过程可分为静态链接和动态链接两种，链接静态的库，需要拷贝到一起，链接动态的库需要登记一下库的信息。 静态链接： 函数的代码将从其所在地静态链接库中被拷贝到最终的可执行程序中。这样该程序在被执行时，代码将被装入到该进程的虚拟地址空间中，静态链接库实际上是一个目标文件的集合，其中的每个文件含有库中的一个或者一组相关函数的代码，最终生成的可执行文件较大。 动态链接： 函数的代码被放到动态链接库或共享对象的某个目标文件中。链接处理时只是在最终的可执行程序中记录下共享对象的名字以及其它少量的登记信息。在这样该程序在被执行时，动态链接库的全部内容将被映射到运行时相应进程的虚地址空间，根据可执行程序中记录的信息找到相应的函数代码。这种连接方法能节约一定的内存，也可以减小生成的可执行文件体积。 ​查看main可执行文件 总结 gcc编译器的工作过程：源文件 --&gt; 预处理 --&gt; 编译 --&gt; 汇编 --&gt; 链接 --&gt; 可执行文件 gcc编译过程文件变化：main.c –&gt; main.i –&gt; mian.s –&gt; main.o –&gt; main 通过上面分阶段的解释编译过程，我们也明白了gcc其实只是一个后台程序的包装，它会根据阶段要求来调用 cpp、cc、as、ld 等命令 源代码整个编译过程产生的中间文件及最终结果可以通过传送门—— gcc编译项目 来获得，其中还有gcc和g++分别调用的对比，查看生成的文件可以发现，同样的源代码使用gcc和g++生成的文件是不一样的，总的来说使用g++编译生成的可执行文件要大一些。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>gcc</tag>
        <tag>预处理</tag>
        <tag>编译</tag>
        <tag>汇编</tag>
        <tag>链接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C/C++版本更迭历程]]></title>
    <url>%2Fblog%2F2019%2F04%2F09%2FC-C-%E7%89%88%E6%9C%AC%E6%9B%B4%E8%BF%AD%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言使用 C/C++ 实现功能的时候经常需要上网搜索一些解决方案，但是当你把代码粘贴到自己项目中时偶尔会出现编译失败的问题，其中一个原因就是新加的代码所使用的特性在当前的编译环境中并不支持，就好像不久前我们还在使用VS2003写着C++98标准的代码（2015年），虽然对C++11的特性垂涎已久，但是无奈在项目中就是无法使用，只能是遥望着它发飞快地发展出了C++14和C++17。 涉及到C/C++版本和标准的最常见的地方就是编译选项了，比如常见的 -std=c++11 就是使用C++11的标准编译，关于 C/C++ 各个版本标准的差异我们可能无法全部记住，但是一些主要的版本更替，还是很有必要了解一下的。 C语言版本更迭 年份 C标准 通用名 别名 标准编译选项 GNU扩展选项 1972 Birth C - - - - 1978 K&amp;R C - - - - 1989-1990 X3.159-1989, ISO/IEC 9899:1990 C89 C90, ANSI C, ISO C -ansi, -std=c90, -std=iso9899:1990 -std=gnu90 1995 ISO/IEC 9899/AMD1:1995 AMD1 C94, C95 -std=iso9899:199409 - 1999 ISO/IEC 9899:1999 C99 - -std=c99, -std=iso9899:1999 -std=gnu99 2011 ISO/IEC 9899:2011 C11 - -std=c11, -std=iso9899:2011 -std=gnu11 2018 ISO/IEC 9899:2018 C18 - -std=c18, -std=iso9899:2018 -std=gnu18 C++版本更迭 年份 C++标准 通用名 别名 标准编译选项 GNU扩展选项 1978 C with Classes - - - - 1998 ISO/IEC 14882:1998 C++98 - -std=c++98 -std=gnu++98 2003 ISO/IEC 14882:2003 C++03 - -std=c++03 -std=gnu++03 2011 ISO/IEC 14882:2011 C++11 C++0x std=c++11, std=c++0x std=gnu++11, std=gnu++0x 2014 ISO/IEC 14882:2014 C++14 C++1y std=c++14, std=c++1y std=gnu++14, std=gnu++1y 2017 ISO/IEC 14882:2017 C++17 C++1z std=c++17, std=c++1z std=gnu++17, std=gnu++1z 2020 to be determined C++20 C++2a -std=c++2a std=gnu++2a 号外C/C++标准 看了C++的发展史才知道，原来从1978年Bjarne Stroustrup就开始了C++雏形的使用，直到20年后的1998年才确定了第一个C++标准 C++11之前被称为C++0x，据说C++0x是C++11的草案，所以有些编译器使用C++11的编译参数是：-std=c++0x，后来使用：-std=c++11，但是据说不完全相同 关于C++20，协程的加入应该是一大惊喜了，值得期待！官方还表示，C++20 应该会是一个像 C++11 那样的大版本 gcc/g++ gcc发展到今天已经不单单可以编译C语言了，还可以编译C++、Java、Object-C等多种其他语言 有一种说法是GCC的全名是GNU Compiler Collection(GUN 编译器集合)，而gcc是GCC中用于编译c语言的编译器 事实上，gcc看起来并不像是一个编译器，而像一个调度器，针对于不同的文件调用不同编程语言的编译器 对于后缀为*.c的文件，gcc把它当作是C语言程序源代码，而g++当作是C++程序源代码 对于后缀为*.cpp的文件，gcc和g++都会当作是C++程序源代码 使用g++编译文件时，g++会自动链接标准库STL，而gcc不会自动链接STL，所以再使用gcc编译C++程序是有时会报错 在用gcc编译C++文件时，为了能够使用STL，需要加参数 –lstdc++ ，但这并不代表 gcc –lstdc++ 和 g++等价 据说g++会调用gcc，对于C++代码，因为gcc命令不能自动和C++程序使用的库联接，所以通常用g++来完成链接 需要注意的是，虽说g++会调用gcc，对于*.c文件来说，编译出来的可执行文件也不一样，因为gcc会当成C语言程序编译，而g++调用的gcc会把它当做C++语言程序来编译，这或许就能解释为什么用g++就可以编译所有C/C++的程序，还要有gcc的存在（就我测试来看，同样的C语言代码，g++编译出来的程序体积要大一些）]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>历史</tag>
        <tag>标准</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql表连接：内连接、外连接、交叉连接、自然连接真的都不一样吗]]></title>
    <url>%2Fblog%2F2019%2F04%2F03%2FMysql%E8%A1%A8%E8%BF%9E%E6%8E%A5%EF%BC%9A%E5%86%85%E8%BF%9E%E6%8E%A5%E3%80%81%E5%A4%96%E8%BF%9E%E6%8E%A5%E3%80%81%E4%BA%A4%E5%8F%89%E8%BF%9E%E6%8E%A5%E3%80%81%E8%87%AA%E7%84%B6%E8%BF%9E%E6%8E%A5%E7%9C%9F%E7%9A%84%E9%83%BD%E4%B8%8D%E4%B8%80%E6%A0%B7%E5%90%97%2F</url>
    <content type="text"><![CDATA[前言提起这几种表连接方式就让人头大，想当初还因为这个面试被刷了，长得挺像，用法挺像，可就是有点不一样，其实的它们的差异不是固定的，要在一个具体的环境下才能进行对比，比如在Mysql环境下, JOIN, INNER JOIN, CROSS JOIN 三者在语法上是等价的，也就是作用相同，但是在标准的SQL下却又存在差异。 选一个自己熟悉的环境对比一下，那就是Mysql数据库的表连接了，测试的多了渐渐的发现了一些规律和神坑，貌似一切表连接都是以内连接为基础，然后再此基础上进行变换可以得到一种新的连接，接下来就采用这种对比的逻辑，看看这些连接类型都有什么区别和联系。 测试环境 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. 创建测试数据 新建第一个测试表格a，包含id和name两列 1create table a(id int, name varchar(64), primary key(id)); 插入测试数据 1234insert into a values(1, 'albert');insert into a values(2, 'bella');insert into a values(3, 'amy');insert into a values(4, 'forier'); 新建第二个测试表格b，包含id和age两列 1create table b(id int, age int, primary key(id)); 插入测试数据 1234insert into b values(1, 18);insert into b values(2, 19);insert into b values(3, 25);insert into b values(5, 70); 分别查看两表中的数据如下 123456789101112131415161718192021mysql&gt; select * from a;+----+--------+| id | name |+----+--------+| 1 | albert || 2 | bella || 3 | amy || 4 | forier |+----+--------+4 rows in set (0.04 sec)mysql&gt; select * from b;+----+-----+| id | age |+----+-----+| 1 | 18 || 2 | 19 || 3 | 25 || 5 | 70 |+----+-----+4 rows in set (0.05 sec) 对比测试这篇对比文章可能和以往你看到的不太一样，对比的基础是内连接，其他的连接基本可以看做是在内连接的基础上加了一些条件和扩展得到的，所以首先我们需要先来看一下内连接。 内连接内连接基础语法是a inner join b，不过其中的inner可以省略，也就是可以写成a join b，如果不添加条件就是a表中的每条记录分别与b表中的每条记录做匹配，形成笛卡尔积，查询结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445mysql&gt; select * from a inner join b;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 1 | 18 || 3 | amy | 1 | 18 || 4 | forier | 1 | 18 || 1 | albert | 2 | 19 || 2 | bella | 2 | 19 || 3 | amy | 2 | 19 || 4 | forier | 2 | 19 || 1 | albert | 3 | 25 || 2 | bella | 3 | 25 || 3 | amy | 3 | 25 || 4 | forier | 3 | 25 || 1 | albert | 5 | 70 || 2 | bella | 5 | 70 || 3 | amy | 5 | 70 || 4 | forier | 5 | 70 |+----+--------+----+-----+16 rows in set (0.03 sec)mysql&gt; select * from a join b;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 1 | 18 || 3 | amy | 1 | 18 || 4 | forier | 1 | 18 || 1 | albert | 2 | 19 || 2 | bella | 2 | 19 || 3 | amy | 2 | 19 || 4 | forier | 2 | 19 || 1 | albert | 3 | 25 || 2 | bella | 3 | 25 || 3 | amy | 3 | 25 || 4 | forier | 3 | 25 || 1 | albert | 5 | 70 || 2 | bella | 5 | 70 || 3 | amy | 5 | 70 || 4 | forier | 5 | 70 |+----+--------+----+-----+16 rows in set (0.04 sec) 需要注意的是内连接的连接条件是可选择，如果不加就是笛卡尔积，如果想加的话可以选择on子句或者using子句，比如需要得到a表与b表中id一致的数据记录就可以使用如下on子句的写法： 123456789mysql&gt; select * from a join b on a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec) 同时对于上述例子中这个on子句中是被连接的两表的同时存在的字段时，可以使用using子句简化，写成如下查询，需要注意下结果集的变化，记录的条数与on子句相同，但是共有的id列被优化掉了一个，这也是on和using子句的区别，使用时根据需要选择： 123456789mysql&gt; select * from a join b using(id);+----+--------+-----+| id | name | age |+----+--------+-----+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 |+----+--------+-----+3 rows in set (0.04 sec) 交叉连接交叉连接基础语法是a cross join b，在Mysql的语法环境中，内连接与交叉连接完全一致，这一点可以通过下面几条查询与内连接的查询做对比得知： 123456789101112131415161718192021222324252627282930313233343536373839404142mysql&gt; select * from a cross join b;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 1 | 18 || 3 | amy | 1 | 18 || 4 | forier | 1 | 18 || 1 | albert | 2 | 19 || 2 | bella | 2 | 19 || 3 | amy | 2 | 19 || 4 | forier | 2 | 19 || 1 | albert | 3 | 25 || 2 | bella | 3 | 25 || 3 | amy | 3 | 25 || 4 | forier | 3 | 25 || 1 | albert | 5 | 70 || 2 | bella | 5 | 70 || 3 | amy | 5 | 70 || 4 | forier | 5 | 70 |+----+--------+----+-----+16 rows in set (0.04 sec)mysql&gt; select * from a cross join b on a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec)mysql&gt; select * from a cross join b using(id);+----+--------+-----+| id | name | age |+----+--------+-----+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 |+----+--------+-----+3 rows in set (0.04 sec) 外连接在Mysql中外连接分为左外连接和右外连接，但不存在全外连接，这一点与Oracle有些不同，不过可以通过左外连接和右外连接合并出全外连接的结果集，需要注意的是外连接必须添加on子句或者using子句，否则会报语法错误，对于左、有外连接可以分别看一下： 左外连接左外连接基础语法是a left outer join b，其中的outer可以省略，与内连接相比就是在与内连接相同条件下，在内连接的结果集中添加a表在b表中找不到匹配的记录，换句话说就是，结果集中会包含a表中的所有记录，如果b表中有匹配的记录就出现在结果集，否则使用NULL代替，也就是把a表看成根本，不可缺失记录，查询结果如下: 123456789101112131415161718192021mysql&gt; select * from a left outer join b on a.id = b.id;+----+--------+------+------+| id | name | id | age |+----+--------+------+------+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 || 4 | forier | NULL | NULL |+----+--------+------+------+4 rows in set (0.04 sec)mysql&gt; select * from a left join b on a.id = b.id;+----+--------+------+------+| id | name | id | age |+----+--------+------+------+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 || 4 | forier | NULL | NULL |+----+--------+------+------+4 rows in set (0.03 sec) 这个左外连接查询同样可以使用using子句来化简，并且也会将共有的字段省略一个： 12345678910mysql&gt; select * from a left join b using(id);+----+--------+------+| id | name | age |+----+--------+------+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 || 4 | forier | NULL |+----+--------+------+4 rows in set (0.04 sec) 右外连接右外连接基础语法是a right outer join b，其中的outer可以省略，与内连接相比就是在与内连接相同条件下，在内连接的结果集中添加b表在a表中找不到匹配的记录，换句话说就是，结果集中会包含b表中的所有记录，如果a表中有匹配的记录就出现在结果集，否则使用NULL代替，也就是把b表看成根本，不可缺失记录，作用与左外连接恰好相反，查询结果如下: 1234567891011121314151617181920212223242526272829303132mysql&gt; select * from a right outer join b on a.id = b.id;+------+--------+----+-----+| id | name | id | age |+------+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 || NULL | NULL | 5 | 70 |+------+--------+----+-----+4 rows in set (0.03 sec)mysql&gt; select * from a right join b on a.id = b.id;+------+--------+----+-----+| id | name | id | age |+------+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 || NULL | NULL | 5 | 70 |+------+--------+----+-----+4 rows in set (0.04 sec)mysql&gt; select * from a right join b using(id);+----+-----+--------+| id | age | name |+----+-----+--------+| 1 | 18 | albert || 2 | 19 | bella || 3 | 25 | amy || 5 | 70 | NULL |+----+-----+--------+4 rows in set (0.04 sec) 自然连接自然连接从名字来看就是两个表很自然的就连接上了，这要求两个表需要有可以参照的数据，具体到表设计上就是要求两个表必须要有相同的列，需要注意的是自然连接不允许添加连接子句，否则会报语法错误。自然连接分为一般自然连接、左外连接和自然右外连接连接，还是以内连接为标准，看看自然连接有什么不同： 一般自然连接一般自然连接基础语法是a natural join b，它不能加连接条件，使用两个表共有的字段id来“自然”地链接，同时会省略共有的字段，其作用相同于内连接使用using子句来查询，通过下面的对比，你会发现他们的作用是一样的。 12345678910111213141516171819mysql&gt; select * from a natural join b;+----+--------+-----+| id | name | age |+----+--------+-----+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 |+----+--------+-----+3 rows in set (0.03 sec)mysql&gt; select * from a join b using(id);+----+--------+-----+| id | name | age |+----+--------+-----+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 |+----+--------+-----+3 rows in set (0.04 sec) 自然左外连接自然左外连接基础语法是a natural left outer join b，其中的outer可以省略，它也不能加连接条件，使用两个表共有的字段id来“自然”地链接，同时会省略共有的字段，其作用相同于内连接使用using子句同时包含a表中的所有记录，以a表作为根本，包含所有记录，并且显示b表中匹配记录，如没有与a表匹配的记录则以NULL代替，其实就是左外连接省略掉using子句： 1234567891011121314151617181920212223242526272829303132mysql&gt; select * from a natural left outer join b;+----+--------+------+| id | name | age |+----+--------+------+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 || 4 | forier | NULL |+----+--------+------+4 rows in set (0.04 sec)mysql&gt; select * from a natural left join b;+----+--------+------+| id | name | age |+----+--------+------+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 || 4 | forier | NULL |+----+--------+------+4 rows in set (0.03 sec)mysql&gt; select * from a left join b using(id);+----+--------+------+| id | name | age |+----+--------+------+| 1 | albert | 18 || 2 | bella | 19 || 3 | amy | 25 || 4 | forier | NULL |+----+--------+------+4 rows in set (0.03 sec) 自然右外连接自然左外连接基础语法是a natural right outer join b，其中的outer可以省略，它也不能加连接条件，其作用与自然左外连接相反，其实就是右外连接省略掉using子句： 1234567891011121314151617181920212223242526272829303132mysql&gt; select * from a natural right outer join b;+----+-----+--------+| id | age | name |+----+-----+--------+| 1 | 18 | albert || 2 | 19 | bella || 3 | 25 | amy || 5 | 70 | NULL |+----+-----+--------+4 rows in set (0.04 sec)mysql&gt; select * from a natural right join b;+----+-----+--------+| id | age | name |+----+-----+--------+| 1 | 18 | albert || 2 | 19 | bella || 3 | 25 | amy || 5 | 70 | NULL |+----+-----+--------+4 rows in set (0.04 sec)mysql&gt; select * from a right join b using(id);+----+-----+--------+| id | age | name |+----+-----+--------+| 1 | 18 | albert || 2 | 19 | bella || 3 | 25 | amy || 5 | 70 | NULL |+----+-----+--------+4 rows in set (0.04 sec) STRAIGHT_JOINSTRAIGHT_JOIN的基础语法是a STRAIGHT_JOIN b，确实没有找到这种连接的中文说法，不过它与内连接几乎一样，只是它总是把左侧a表作为驱动表优先读入，它只能加on子句，无法使用using子句，在Sql优化的过程中常常使用，也就是拒绝了Mysql的语句优化，而使用自己指定的顺序来连接表格，不过使用时需慎重，你得比Mysql聪明才可以！ 123456789mysql&gt; select * from a straight_join b on a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec) 逗号分隔连接表在from之后用逗号分隔的两个表格像极了内连接，只不过用逗号分隔的表不能使用子句连接，只可以用where来做条件筛选，不过作用之后的结果是一致的，可以对比看一下： 12345678910111213141516171819mysql&gt; select * from a join b on a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec)mysql&gt; select * from a, b where a.id = b.id;+----+--------+----+-----+| id | name | id | age |+----+--------+----+-----+| 1 | albert | 1 | 18 || 2 | bella | 2 | 19 || 3 | amy | 3 | 25 |+----+--------+----+-----+3 rows in set (0.04 sec) 各种连接对比通过描述可能有些关系还是没理解太清楚，所以整理了下面的表格，对比的更清楚一点，其中[]中的内容在编写sql时可以省略： 连接类型 语法 不加条件 加ON子句 加USING子句 与内连接关系 内连接 a [INNER] JOIN b 两表中任意两条记录分别匹配，形成笛卡尔积 只按照条件匹配，所有列均显示 按照共有的列匹配，去除重复列 X 交叉连接 a [CROSS] JOIN b 两表中任意两条记录分别匹配，形成笛卡尔积 只按照条件匹配，所有列均显示 按照共有的列匹配，去除重复列 语法等价，完全相同 左外连接 a LEFT [OUTER] JOIN b 必须加条件，否则报语法错误 只按照条件匹配，并且包含a表中没有匹配上的记录 按照共有的列匹配，并且包含a表中没有匹配上的记录 额外包含a表中没有匹配上的记录 右外连接 a RIGHT [OUTER] JOIN b 必须加条件，否则报语法错误 只按照条件匹配，并且包含b表中没有匹配上的记录 按照共有的列匹配，并且包含b表中没有匹配上的记录 额外包含b表中没有匹配上的记录 一般自然连接 a NATURAL JOIN b 使用两表中共有的字段匹配 不能使用ON子句 不能使用USING子句 相当于内连接使用USING子句 自然左外连接 a NATURAL LEFT [OUTER] JOIN b 使用两表中共有的字段匹配，并且包含a表中没有匹配上的记录 不能使用ON子句 不能使用USING子句 相当于内连接使用USING子句，并且包含a表中没有匹配上的记录 自然右外连接 a NATURAL RIGHT [OUTER] JOIN b 使用两表中共有的字段匹配，并且包含b表中没有匹配上的记录 不能使用ON子句 不能使用USING子句 相当于内连接使用USING子句，并且包含b表中没有匹配上的记录 STRAIGHT_JOIN a STRAIGHT_JOIN b 两表中任意两条记录分别匹配，形成笛卡尔积 只按照条件匹配，所有列均显示 不能使用USING子句 在内连接基础上确定读表顺序 逗号分隔表 a, b 两表中任意两条记录分别匹配，形成笛卡尔积 不能使用ON子句 不能使用USING子句 不能使用连接子句，只能使用Where筛选 总结 总的来看外连接中的outer是最没有存在感的，凡是它出现的地方都可以省略 黑魔法：a inner join b与a cross join b是等价的，后来我偶然间拼写错误发现a across join b也是可以的，另外a love join b也行，开始还以为发现了bug，后来再理解应该是拼错的单词作了表a的别名，虚惊一场！ 通过上面的表格发现每种连接貌似都和内连接扯上了关系，那就以内连接为基础，通过扩展来记忆也是不错的 如果表格不够清晰，换成思维导图或许会更好一些]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>查询</tag>
        <tag>Mysql</tag>
        <tag>表连接</tag>
        <tag>内连接</tag>
        <tag>外连接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IP地址常见分类：A类、B类、C类、D类、E类]]></title>
    <url>%2Fblog%2F2019%2F04%2F03%2FIP%E5%9C%B0%E5%9D%80%E5%B8%B8%E8%A7%81%E5%88%86%E7%B1%BB%EF%BC%9AA%E7%B1%BB%E3%80%81B%E7%B1%BB%E3%80%81C%E7%B1%BB%E3%80%81D%E7%B1%BB%E3%80%81E%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[前言虽然IPv6渐渐出现在了人们的视线之中，但是目前来看IPv4仍然占据着主导地位，在日常的编码过程中两者都会接触到，但实际上两者在使用范围、消息头结构等细节上有诸多不同，具体的那些细节对于应用层来说可能体会不到，所以我们先从两者的表示方式来看看，学会认出哪些是IPv4类型的地址，而哪些是IPv6类型的地址。 IPv4地址表示方法每个IPv4地址占用4个字节，长度为32位，由网络号和主机号部分组成，最常采用点分十进制表示法，格式为 ddd.ddd.ddd.ddd，其中 0 &lt;= ddd &lt;= 255，而每个 d 都是十进制数，可省略前导零，比如常见的192.168.1.1，理论上最多能表示的地址个数为$2^32$。 IPv6地址表示方法每个IPv6地址占用16个字节，长度为128位，占用空间是IPv4地址的4倍，但是这里要注意，IPv4所能表示的地址个数相比于IPv4来说可不是4倍的关系，IPv6理论上最多能表示的地址个数为$2^128$，是IPv4能力的$2^96$倍，换算成10进制也就是大约79228162514264337593543950336倍，在很长一段时间内不用再担心IP地址不够用的问题了。 而IPv6地址由于占用位数较多，所以采用更容易书写和理解的冒分十六进制表示法，格式为xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx，其中每个 x 表示一个十六进制的符号，也就是0-9A-F，比如一个普通IPv6地址23CD:0F01:0005:0789:ABED:EF01:2345:67D9，前导零也是可以省略的，例如前一个地址可以记作23CD:F01:5:789:ABED:EF01:2345:67D9。 在某些情况下，一个IPv6地址内可能包含很长的一段0，这时可以把连续的一段0压缩为::, 但要注意是，地址中::只能出现一次，这样才能保证地址解析的唯一性，比如地址FF20:A:0:0:0:0:0:1AC2，可以写成FF20:A::1AC2，而地址FF20:A:0:0:1:0:0:1AC2中间有两段0，为保证解析的唯一性，只能选择一段0来压缩，比如写成FF20:A::1:0:0:1AC2或者是FF20:A:0:0:1::1AC2。 为了实现IPv4地址和IPv6地址互相通信，在IPv6的环境下，IPv4地址会被扩展成IPv6地址，此时地址的格式常表示为：xxxx:xxxx:xxxx:xxxx:xxxx:FFFF:ddd.ddd.ddd.ddd，前面的96位采用IPv6冒分十六进制表示，而后面32位地址则使用IPv4的点分十进制表示，例如常用的IPv4地址192.168.1.1与表示成IPv6地址就是FFFF:192.168.1.1。 IPv4地址常见分类192.168.1.1这个IP地址随着路由器的普及逐渐为人们所熟知，有些人可能知道这是一个C类地址，究竟什么是C类地址？难道还有A类、B类地址？不是说IP地址是惟一表示一台主机的吗，为什么我家的路由器和邻居家的路由器地址都是192.168.1.1，关于这些问题就涉及到了IPv4地址的常见分类，其实IPv4地址一般被分为A、B、C、D、E五类，其中还包含一些保留地址和局域网地址，关于这些信息为了对比方便，我整理了下面这幅图，有关分类的疑惑可以看图了解一下： 书愤陆游早岁那知世事艰，中原北望气如山。楼船夜雪瓜洲渡，铁马秋风大散关。塞上长城空自许，镜中衰鬓已先斑。出师一表真名世，千载谁堪伯仲间。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>网络</tag>
        <tag>IP</tag>
        <tag>IPv4</tag>
        <tag>Ipv6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql开启、查看慢查询日志]]></title>
    <url>%2Fblog%2F2019%2F03%2F25%2FMysql%E5%BC%80%E5%90%AF%E3%80%81%E6%9F%A5%E7%9C%8B%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[前言想要优化现有的数据库结构或者查询语句，首先要找到需要的优化的地方，不然就会出现费了很大精力优化却不达目的的情况，这就和上学考试一样，想要取得好的成绩，先要分析自己差在哪里，重点学习才会有快速的提升。 关于查询Mysql的瓶颈，或者说查询Mysql出现操作缓慢的问题，我们可以使用Mysql自带的慢查询日志来分析，优化不是改正错误，那种错误在开发过程中叫做BUG，伴随着软件开发工程师的一生，而优化是指在逻辑正确的前提下，让程序运行的更快更稳定，一般来说就是优化比较耗时的操作，提升用户体验。比如一个信息系统，如果输入账号密码后需要10分钟才能登录成功，我想基本上也不会有人使用了。 慢查询日志就是一种特殊的记录，用于统计Mysql操作过程中一些耗时的语句，生成文件或者表格，为优化Mysql操作提供分析数据，从名字也很好理解，慢查询日志就是记录一些查询的比较慢的记录，帮助使用者定位具体的问题，接下来就简单地描述一下慢查询日志是如何开启和查看的。 测试环境 Windows 10Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 7Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.Type ‘help;’ or ‘\h’ for help. Type ‘\c’ to clear the current input statement. 开启慢查询日志修改过Mysql配置的小伙伴应该知道，Mysql的一些环境配置可以通过命令行直接修改，也可以修改配置文件后重新启动Mysql服务来生效，我们这次选择修改配置文件的方法，首先找到配置文件my.ini（Mysql5.7Windows版本，Linux版本应该叫my.cnf吧），一般在安装目录所在的ProgramData目录下，比如我的是在 C:/ProgramData/MySQL/MySQL Server 5.7 ，如果找不到可以下载一个叫Everything的软件搜一下（顺便安利一下，真的挺好用）。 用记事本或者其他编辑软件打开，搜索 long_query_time找到配置区域，其中有一些其他的配置，与慢查询无关已经剔除：12345# General and Slow logging.log-output=FILEslow-query-log=1slow_query_log_file="0491NPORIURNUYO-slow.log"long_query_time=0.01 slow-query-log：慢查询日志的开关，1表示开启，0表示关闭。Mysql5.1之前貌似是默认关闭的，而我这个版本在安装完成后自动开启了，如果没有可以将其设置为1，重启服务后会自动开启 long_query_time：这个参数很关键，表示慢查询日志的阈值，花费时间超过这个值的sql语句才会被记录，单位是秒，这一点需要注意，网上有些文章说这个参数是毫秒，不知道是不是版本问题，使用时可以测试一下，经测试可以配置成小数 log-output：表示日志存储的方式。FILE 表示将日志存入文件，默认值是FILE，另外可以取值 TABLE ，表示将日志存入数据库表，当然也可以两者都选择，配成 FILE,TABLE 。需要注意的是将日志记录到数据库表中，需耗费更多的系统资源，建议优先记录到文件 log-slow_query_log_file：当配置将日志记录到文件中时，这个参数可以指定文件名，路径一般在配置文件的同级的Data目录下，比如我的是在C:/ProgramData/MySQL/MySQL Server 5.7/Data，当发现sql运行较慢时可以打开这个文件看一下 具体例子 首先我们将long_query_time设置为0.05，也就是记录查询时间超过50毫秒的sql操作，这是个很随意的值，具体的生产环境根据具体情况配置，重启Mysql服务使其生效 创建一张测试表格 slow_query_test 1create table slow_query_test(id int, num int, money int); 然后创建一个存储过程，用来给数据表填充数据，命名为fill_slow_query_test 123456789CREATE PROCEDURE `fill_slow_query_test`()BEGIN DECLARE i int default 1; WHILE i &lt;= 100000 do insert into slow_query_test values(i, i, i); set i = i + 1; END WHILE;END 调用存储过程插入测试数并查询，得到以下结果 12345678910mysql&gt; call fill_slow_query_test();Query OK, 1 row affected (24.45 sec)mysql&gt; select * from slow_query_test where id = 9990;+------+------+-------+| id | num | money |+------+------+-------+| 9990 | 9990 | 9990 |+------+------+-------+1 row in set (0.09 sec) 通过观察结果可以发现，两次操作的时间都超过了0.05s，所以都应该被记录到慢查询日志日志中，打开日志查看内容 12345678910111213C:\Program Files\MySQL\MySQL Server 5.7\bin\mysqld.exe, Version: 5.7.21-log (MySQL Community Server (GPL)). started with:TCP Port: 3306, Named Pipe: (null)Time Id Command Argument# Time: 2019-03-25T03:14:50.241968Z# User@Host: root[root] @ localhost [::1] Id: 2# Query_time: 24.447215 Lock_time: 0.000364 Rows_sent: 0 Rows_examined: 0SET timestamp=1553483690;call fill_slow_query_test();# Time: 2019-03-25T03:15:27.862107Z# User@Host: root[root] @ localhost [::1] Id: 2# Query_time: 0.050133 Lock_time: 0.000133 Rows_sent: 1 Rows_examined: 100000SET timestamp=1553483727;select * from slow_query_test where id = 9990; 日志中的内容与我们猜想的基本一致，但是还要多一些，首先前3行是Mysql服务启动的记录，接下来的是Mysql慢查询日志的正文，每组都通过3行注释分隔： Time：记录执行操作时的日期和时间，默认没有包含时区信息，是标准的UTC时间 User@Host 记录执行操作的主机和用户，以及Mysql连接id等信息 Query_time 记录了查询消耗的时间，以及其他的一些操作信息接下来未被注释的内容就是真正执行的操作，包含当时的时间戳和具体执行的语句 简单优化针对于上述的例子，我们可以运用上一篇文章《Mysql查询可通过给条件字段添加索引提高查询速度》 提到的方法，简单优化一下： 首先给id字段添加简单索引 123mysql&gt; ALTER TABLE slow_query_test ADD INDEX id_index(id);Query OK, 0 rows affected (0.14 sec)Records: 0 Duplicates: 0 Warnings: 0 然后使用相同的sql语句再次查询 1234567mysql&gt; select * from slow_query_test where id = 9990;+------+------+-------+| id | num | money |+------+------+-------+| 9990 | 9990 | 9990 |+------+------+-------+1 row in set (0.04 sec) 对比前后的查询我们发现，加了索引的表格查询耗时已经小于0.05s，所以该查询不会被记录到慢查询日志中了 慢查询日志格式化处理有时面对慢查询日志文件内众多的数据确实无从下手，这是可以考虑使用mysql自带的mysqldumpslow工具来分析慢查询日志，该工具一般与mysql可执行文件在同一目录，比如我的是在C:/Program Files/MySQL/ySQL Server 5.7/bin&gt;，直接运行发现会报错 12C:\Program Files\MySQL\MySQL Server 5.7\bin&gt;mysqldumpslow'mysqldumpslow' 不是内部或外部命令，也不是可运行的程序或批处理文件。 仔细观察会发现，这个工具不知何时已经从可执行文件变成了一个Perl脚本mysqldumpslow.pl（之前使用linux版本是可执行文件）了，不能直接运行，需要安装Perl运行环境，这里有官方的下载地址，如果下载太慢的话可以下载我的备份文件，版本是一样的。 安装完成之后查看帮助（注意文件路径）12345678910111213141516171819202122232425262728PS C:\Program Files\MySQL\MySQL Server 5.7\bin&gt; perl mysqldumpslow.pl --helpUsage: mysqldumpslow [ OPTS... ] [ LOGS... ]Parse and summarize the MySQL slow query log. Options are --verbose verbose --debug debug --help write this text to standard output -v verbose -d debug -s ORDER what to sort by (al, at, ar, c, l, r, t), 'at' is default al: average lock time ar: average rows sent at: average query time c: count l: lock time r: rows sent t: query time -r reverse the sort order (largest last instead of first) -t NUM just show the top n queries -a don't abstract all numbers to N and strings to 'S' -n NUM abstract numbers with at least n digits within names -g PATTERN grep: only consider stmts that include this string -h HOSTNAME hostname of db server for *-slow.log filename (can be wildcard), default is '*', i.e. match all -i NAME name of server instance (if using mysql.server startup script) -l don't subtract lock time from total time 看完使用方法，我们可以按照查询消耗的时间排序输出前两条日志（注意日志的路径，为方便可以拷贝到mysqldumpslow工具目录）: 12345678PS C:\Program Files\MySQL\MySQL Server 5.7\bin&gt; perl mysqldumpslow.pl -s t -v -t 2 0491NPORIURNUYO-slow.logReading mysql slow query log from 0491NPORIURNUYO-slow.logCount: 1 Time=24.45s (24s) Lock=0.00s (0s) Rows=0.0 (0), root[root]@localhost call fill_slow_query_test()Count: 1 Time=0.05s (0s) Lock=0.00s (0s) Rows=1.0 (1), root[root]@localhost select * from slow_query_test where id = N 可以发现还是很方便的，最耗时的操作已经排到了第一位，在实际的优化过程中，这或许就是我们需要拿来开刀的目标了。 总结 通过修改配置文件my.ini中的slow-query-log、long_query_time来调整慢查询日志的开关和具体阈值 通过mysqldumpshow工具可以格式化慢查询日志，方便定位问题和分析问题]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>慢查询</tag>
        <tag>日志</tag>
        <tag>分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql查询可通过给条件字段添加索引提高查询速度]]></title>
    <url>%2Fblog%2F2019%2F03%2F15%2FMysql%E6%9F%A5%E8%AF%A2%E5%8F%AF%E9%80%9A%E8%BF%87%E7%BB%99%E6%9D%A1%E4%BB%B6%E5%AD%97%E6%AE%B5%E6%B7%BB%E5%8A%A0%E7%B4%A2%E5%BC%95%E6%8F%90%E9%AB%98%E6%9F%A5%E8%AF%A2%E9%80%9F%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[前言当使用sql语句查询表数据时，会发现随着表中记录的增多，查询的速度也会也来越慢，特别是那种日志记录，少则几十万，多则上百万，甚至上千万数据，如果查询一次耗时太长，会严重影响业务逻辑，这时候可以考虑给经常作为条件的字段添加索引了，这样做会大大加快查询速度，这里所说的条件字段，就是指sql语句中放到where条件中用于筛选记录的字段，关于加索引提高查询速度的做法，我们可以做一下试验，对比一下看看是否真的有效。 测试环境 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 8Server version: 5.7.21-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. 测试过程 首先创建一个不带有索引的数据表 tb_without_index 1create table tb_without_index(id int, num int, money int); 然后创建一个存储过程，用来给无索引数据表填充数据，命名为fill_tb_without_index 123456789CREATE PROCEDURE `fill_tb_without_index`()BEGIN DECLARE i int default 1; WHILE i &lt;= 100000 do insert into tb_without_index values(i, i, i); set i = i + 1; END WHILE;END 接着创建一个带有索引用来做对比的数据表 tb_with_index 1create table tb_with_index(id int, num int, money int, key `id_index`(id)); 同样创建一个给带索引数据表填充数据的存储过程 fill_tb_with_index 123456789CREATE PROCEDURE `fill_tb_with_index`()BEGIN DECLARE i int default 1; WHILE i &lt;= 100000 do insert into tb_with_index values(i, i, i); set i = i + 1; END WHILE;END 分别调用存储过程来填充数据，每个表填充需要20多秒，还是挺费时间的 12345mysql&gt; call fill_tb_without_index();Query OK, 1 row affected (25.48 sec)mysql&gt; call fill_tb_with_index();Query OK, 1 row affected (25.64 sec) 查询对比 对于单条数据的查询对比 123456789101112131415mysql&gt; select * from tb_with_index where id = 67853;+-------+-------+-------+| id | num | money |+-------+-------+-------+| 67853 | 67853 | 67853 |+-------+-------+-------+1 row in set (0.05 sec)mysql&gt; select * from tb_without_index where id = 67853;+-------+-------+-------+| id | num | money |+-------+-------+-------+| 67853 | 67853 | 67853 |+-------+-------+-------+1 row in set (0.08 sec) 对于范围数据的查询对比 123456789101112131415mysql&gt; select count(id) from tb_without_index where id &gt; 87862;+-----------+| count(id) |+-----------+| 12138 |+-----------+1 row in set (0.09 sec)mysql&gt; select count(id) from tb_with_index where id &gt; 87862;+-----------+| count(id) |+-----------+| 12138 |+-----------+1 row in set (0.05 sec) 结果分析 通过上面两种情况的对比，我们可以发现虽然每组对比只差零点零几秒的时间，但是从耗时来看有索引的表格查询比没有索引的表格查询节省了大约40%的时间，由此可见，给待查字段添加上索引，确实可以加快查询速度。 既然加上索引的效率可以提升这么多，那么可不可以把所有字段都加上索引呢？答案是不可以，这一点可以从测试过程的第5步结果来分析，这一步中给表格 tb_without_index 添加10万条数据耗时25.48秒，给表格 tb_with_index 添加10万条数据耗时25.64秒，也就是给有索引的表添加数据时要多花0.16秒的时间，这不是偶然的，可以反复测试，每次的测试结果都是有索引表的数据插入过程更耗时一点。 通过上面的对比和分析，可以知道，虽然添加索引可以加快查找速度，但是会拖慢插入和更新的速度，因为在有索引的数据表上更新和插入需要多花费时间来维护索引，至于两者之间的平衡，就需要使用者自己把握了。 添加索引 像上面提到的那样，可以在建表的时候就定义好索引，查询表结构发现字段id所在行的Key列值为MUL，表示它的值是可以重复的索引，其他两个字段都没有 12345678910create table tb_with_index(id int, num int, money int, key `id_index`(id));mysql&gt; desc tb_with_index;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | MUL | NULL | || num | int(11) | YES | | NULL | || money | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+3 rows in set (0.05 sec) 在已有的表格上创建索引，比如可以在列num上创建一个索引，语法：CREATE INDEX index_name ON table_name(column_list) 12345678910111213mysql&gt; CREATE INDEX num_index ON tb_with_index(num);Query OK, 0 rows affected (0.23 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc tb_with_index;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | MUL | NULL | || num | int(11) | YES | MUL | NULL | || money | int(11) | YES | | NULL | |+-------+---------+------+-----+---------+-------+3 rows in set (0.05 sec) 修改表结构添加索引，比如可以给列num添加一个索引，语法：ALTER TABLE table_name ADD INDEX index_name(column_list) 12345678910111213mysql&gt; ALTER TABLE tb_with_index ADD INDEX money_index(money);Query OK, 0 rows affected (0.21 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc tb_with_index;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | YES | MUL | NULL | || num | int(11) | YES | MUL | NULL | || money | int(11) | YES | MUL | NULL | |+-------+---------+------+-----+---------+-------+3 rows in set (0.06 sec) 查看索引可以查看一个表上的所有索引信息，语法为：show index from table_name，查询结果如下 123456789 mysql&gt; show index from tb_with_index; +---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+ ---------------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | +---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+ ---------------+ | tb_with_index | 1 | id_index | 1 | id | A | 98715 | NULL | NULL | YES | BTREE | | | | tb_with_index | 1 | num_index | 1 | num | A | 100035 | NULL | NULL | YES | BTREE | | | | tb_with_index | 1 | money_index | 1 | money | A | 100035 | NULL | NULL | YES | BTREE | | | +---------------+------------+-------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+ ---------------+3 rows in set (0.06 sec) 总结 给条件字段添加索引可以大大加快数据的查询速度，提高系统的性能。 不要考虑在所有的字段上添加索引，创建索引和维护索引都要耗费时间，这种时间随着数据量的增加而增加。 适合添加索引的字段：总是作为条件查询的字段、常用来做连接的字段、作为主键或者强调唯一的列上。 不适合加索引的字段：块数据类型的字段、取值很少的字段。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>索引</tag>
        <tag>index</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下查找包含指定内容的文件及其所在行数]]></title>
    <url>%2Fblog%2F2019%2F03%2F13%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9F%A5%E6%89%BE%E5%8C%85%E5%90%AB%E6%8C%87%E5%AE%9A%E5%86%85%E5%AE%B9%E7%9A%84%E6%96%87%E4%BB%B6%E5%8F%8A%E5%85%B6%E6%89%80%E5%9C%A8%E8%A1%8C%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前言在linux系统下搜索文件一般情况下一个命令就搞定了，之前搜索文件的时候一直使用find，今天排查问题时想查一个函数的调用者在哪个文件中，发现不会写了，搜了一下发现使用grep命令就可以实现，改变了我对grep命令的理解，原来使用grep命令的情况通常是作为结果的过滤函数，比如ps aux | grep gameserver，这次发现他居然还可以直接用来搜索，其实也是过滤的一种。 使用方法这里直接给出命令的写法，简单替换搜索内容就可以使用，也方便自己后续查找使用(例如查找包含stream的文件)：1grep -rn 'stream' . --include='*.cpp' 命令解析上述命令是一种比较常用的写法，就是在当前目录下（一定要注意那个.）查找包含stream的文件，并显示其所在的行，搜索的文件类型是.cpp，其实--include=后面的内容是遵循glob语法的，详细的就不展开了，简单来说就是支持通配符，而查找选项-rn中的r表示递归查找，其中的n表示显示行号，此外还可以使用选项-i表示忽略大小写，下面简单展示一下3个选项的功能： -r：只递归查找不显示行号 1234567891011[albert@localhost#18:17:41#/home/albert/test]$grep -r 'stream' . --include='*.cpp'./testPtr.cpp:#include &lt;iostream&gt;./crab-server/code/main/crab.cpp:#include &lt;iostream&gt;./testConstructor.cpp:#include &lt;iostream&gt;./smartptr/auto_ptr.cpp:#include &lt;iostream&gt;./io.cpp:#include &lt;fstream&gt;./io.cpp: ofstream logfile("./logs/players_distribution.txt", ios::out);./ptr.cpp:#include &lt;iostream&gt;./gdbtest/main.cpp:#include &lt;iostream&gt;./test_t.cpp:#include &lt;iostream&gt;./testshareptr.cpp:#include &lt;iostream&gt; -rn：递归查找并显示行号 1234567891011[albert@localhost#18:17:48#/home/albert/test]$grep -rn 'stream' . --include='*.cpp'./testPtr.cpp:4:#include &lt;iostream&gt;./crab-server/code/main/crab.cpp:3:#include &lt;iostream&gt;./testConstructor.cpp:1:#include &lt;iostream&gt;./smartptr/auto_ptr.cpp:4:#include &lt;iostream&gt;./io.cpp:1:#include &lt;fstream&gt;./io.cpp:9: ofstream logfile("./logs/players_distribution.txt", ios::out);./ptr.cpp:4:#include &lt;iostream&gt;./gdbtest/main.cpp:1:#include &lt;iostream&gt;./test_t.cpp:1:#include &lt;iostream&gt;./testshareptr.cpp:4:#include &lt;iostream&gt; -rni：递归查找显示行号并且忽略大小写 12345678910111213141516171819202122[albert@localhost#18:17:53#/home/albert/test]$grep -rni 'stream' . --include='*.cpp'./testPtr.cpp:4:#include &lt;iostream&gt;./crab-server/code/main/crab.cpp:3:#include &lt;iostream&gt;./testConstructor.cpp:1:#include &lt;iostream&gt;./epoll_cs_demo/testfd.cpp:5: int listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:8: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:11: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:14: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:21: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/testfd.cpp:25: listen_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/client.cpp:18: int client_fd = socket(AF_INET, SOCK_STREAM, 0);./epoll_cs_demo/server.cpp:24: int listen_fd = socket(AF_INET, SOCK_STREAM, 0);./smartptr/auto_ptr.cpp:4:#include &lt;iostream&gt;./io.cpp:1:#include &lt;fstream&gt;./io.cpp:9: ofstream logfile("./logs/players_distribution.txt", ios::out);./network/zgetaddrinfo.cpp:37: hints.ai_socktype = SOCK_STREAM;/* Stream socket */./linux_version/client.cpp:15: int client_fd = socket(AF_INET, SOCK_STREAM, 0);./linux_version/server.cpp:15: int listen_fd = socket(AF_INET, SOCK_STREAM, 0);./ptr.cpp:4:#include &lt;iostream&gt;./gdbtest/main.cpp:1:#include &lt;iostream&gt;./test_t.cpp:1:#include &lt;iostream&gt;./testshareptr.cpp:4:#include &lt;iostream&gt; 总结 查找指定内容的简单命令：grep -rn &#39;stream&#39; . --include=&#39;*.cpp&#39; 这个grep有很多附加的参数，看了文档之后发现了一个点，原来用法：egrep即grep -E，fgrep即rep -F，但是 egrep 和 fgrep现在都不建议使用了，无论是man手册还是--help选项中都提到了这一点]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>find</tag>
        <tag>查找</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为目标打好基础的希尔排序]]></title>
    <url>%2Fblog%2F2019%2F03%2F09%2F%E4%B8%BA%E7%9B%AE%E6%A0%87%E6%89%93%E5%A5%BD%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言刚刚分析过的插入排序通常被叫做简单插入排序或者直接插入排序，而这篇文章刚好以插入排序为基础来说说希尔排序，还是先从名字开始，结果发现完全没有头绪，说实话第一次听说这个排序时还以为是个特别神奇的高端算法，结果了解一番之后发现其实是一个被改造的插入排序，“希尔”居然是发明者的名字，所以从名字来判断算法思想在这里行不通，甚至说快速排序起码说明了这种方法排序快，而希尔排序等于什么都没说。 希尔排序的基础是插入排序，整个排序也是在新元素不断插入到有序序列适当位置的过程中完成的，唯一的不同的就是通过不同的步长将整个序列划分为不同的小序列不断插入，直到步长为1时就退化成了最基本的直接插入排序，但是此时整个序列已经“基本”有序了，需要交换的元素对比一开始直接插入的方法明显减少，从而可以加快排序的速度，因为最后步长为1的一次插入排序与简单插入排序完全相同，所以前面的几趟排序完全可以看做是最后的排序目标“打基础”，让最后一次的排序序列尽可能有序，下面描述一下希尔排序的过程，前提是你已经了解简单插入排序的过程，可以参考文章抓扑克牌风格的插入排序熟悉一下。 希尔排序希尔排序的有一个关键的元素是步长，关于步长的选择有很多种方法，比如只选奇数，选择互为质数等等，其目的就是为了减少重复比较的次数，我们现在只为了解希尔排序的过程，所以先选择一种简单的步长选定方法，以元素个数的一半为基础，每次减少一半直到步长降为1，比如10个元素的步长选择分别为5,2,1，本质思想就是分别以步长5,2,1对整个待排序列进行简单的插入排序，最后就完成了整个序列的排序。 我们用物品重量排序作为例子吧，原来插入排序的例子是将新得到的扑克牌不断插入到有序序列中得到最终排序，这次可以直接先给出物品质量序列的初始排列，假设为99, 34, 54, 65, 11, 1, 5, 12, 89, 42，一共10件物品摆在面前，目标为将物品重量从小到大排序，首先选取步长5开始排序过程： 最开始的排序序列如下:99, 34, 54, 65, 11, 1, 5, 12, 89, 42 以步长为5将整个序列分为5组，分组情况如下：99, _, _, _, _, 1, _, _, _, __, 34, _, _, _, _, 5, _, _, __, _, 54, _, _, _, _, 12, _, __, _, _, 65, _, _, _, _, 89, __, _, _, _, 11, _, _, _, _, 42 将这五组子序列分别使用简单插入排序，得到以下序列：1, _, _, _, _, 99, _, _, _, __, 5, _, _, _, _, 34, _, _, __, _, 12, _, _, _, _, 54, _, __, _, _, 65, _, _, _, _, 89, __, _, _, _, 11, _, _, _, _, 42 这五个子序列组成完整的中间临时序列为：1, 5, 12, 65, 11, 99, 34, 54, 89, 42 然后以步长为2将整个序列划分，得到以下分组情况：1, _, 12, _, 11, _, 34, _, 89, __, 5, _, 65, _, 99, _, 54, _, 42 将这两组子序列使用简单插入排序，得到以下序列：1, _, 11, _, 12, _, 34, _, 89, __, 5, _, 42, _, 54, _, 65, _, 99 将子序列整体来看得到中间临时序列：1, 5, 11, 42, 12, 54, 34, 65, 89, 99 最后再将整个待排序列进行一次简单插入排序，便可得到最终排好的序列，实际上最后一次插入排序只有中间几个元素需要移动了：1, 5, 11, 12, 34, 42, 54, 65, 89, 99 代码实现12345678910111213141516171819202122232425262728293031323334353637/*功能： 交换两个变量参数： element1--被交换的第一个元素的地址 element2--被交换的第二个元素的地址返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void swap_data(int* element1, int* element2)&#123; int middle_value = *element1; *element1 = *element2; *element2 = middle_value;&#125;/*功能： 希尔排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 count--数组元素的个数返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void shell_sort(int array[], int count)&#123; int step = count / 2; while (step &gt; 0) &#123; for (int pos = step; pos &lt; count; ++pos) &#123; for (int insert_index = pos; insert_index &gt;= step; insert_index -= step) &#123; if (array[insert_index] &lt; array[insert_index - step]) swap_data(&amp;array[insert_index], &amp;array[insert_index - step]); &#125; &#125; step /= 2; &#125;&#125; 对比插入排序源代码，找找不同 1234567891011void insert_sort(int array[], int count)&#123; for (int pos = 1; pos &lt; count; ++pos) &#123; for (int insert_index = pos; insert_index &gt; 0; --insert_index) &#123; if (array[insert_index] &lt; array[insert_index - 1]) swap_data(&amp;array[insert_index], &amp;array[insert_index - 1]); &#125; &#125;&#125; 代码分析以上代码就是希尔排序的实现方式了，对比直接插入的源代码发现，如果将希尔排序的初始步长设置成1，那么整个希尔排序的代码就和简单插入排序完全一样了，这也符合我们之前分析的过程，其实希尔排序就是分多次，每次用不同的步长执行简单插入排序。 还有一点就是代码执行的过程与上面示例中的分组插入看起来有些不同，只是因为这个写起来更方便一些，分组只是为了人脑能更快的理解算法的思想，但是代码编写时还要考虑复杂性，将数据拆分成几组然后分别进行插入排序完全可以做到，但是实际上完全没有必要。 比如分成两组排序的那一步，直观上先排索引为0,2,4,6,8上的元素，依次做插入操作，然后排索引为1,3,5,7,9上的元素，在依次做插入操作，但是在实现的代码中就做了变通，反正都要做插入操作，并且步长都是2，所以可以直接对索引是0,1,2,3,4,5,6,7,8,9上的元素做插入排序，只要注意步长是2，就不会影响到其他组（实际上并不存在）的元素了，整个过程顺着代码，一步步执行就明白了。 运行测试希尔排序–源码 如果你想运行测试一下，可以复制或者下载源代码，在本机运行测试一下，当然也可以选择在线C++编译器，把源代码复制到网页中运行查看结果，建议不明白的可以在本地环境单步调试一下，这样有助于理解算法思路。]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
        <tag>希尔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下运行程序常用的nohup和&的区别]]></title>
    <url>%2Fblog%2F2019%2F02%2F25%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F%E5%B8%B8%E7%94%A8%E7%9A%84nohup%E5%92%8C-%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言复杂问题简单记，先了解一下概念，对于一般的小程序而言这两种启动方法应该用不上，如果程序瞬间的就结束了，是否挂起与是否后台也就没有了意义，所以标题中提到的方式常用来启动需要一直运行的程序，比如游戏服务器。 假如我们直接通过命令行./game_server运行一个简单的游戏服务器，那么会发现这个运行程序霸占了整个命令窗口，此时，我们无法再运行其他的程序，所有的输入都变成了game_server的输入，而命令终端此时也只能输出game_server程序的输出信息了。 接着再来了解两个信号，针对于霸占了命令终端的game_server我们可以采用以下方式将其终止掉，使用Ctrl+C组合键，实际上是给程序发送了SIGINT信号，可以以直接关掉命令终端，这个进程也会死掉，实际上是给程序发送了SIGHUP信号，而标题中的所说的两种方式就是针对于这两种信号的。 两种方式的区别 nohupnohup是no hang up的缩写，就是不挂断的意思，忽略SIGHUP信号，在关闭命令终端后程序依旧运行 &amp;&amp;是只后台运行，即忽略SIGINT信号，也就是按Ctrl+C不会终止程序，但是关闭命令行终端程序终止 总结所以要想程序忽略SIGINT和SIGHUP两种信号需要两种表示方法一同使用，总结如下 命令 忽略信号 按Ctrl+C结果 关闭终端 标准输入 输出 ./game 无 程序终止 程序终止 只能给game输入 终端输出 nohup ./game SIGHUP 程序终止 依旧运行 输入被忽略 输出到nohup.out文件 ./game &amp; SIGINT 依旧运行 程序终止 输入正常，终端可用 无输出 nohup ./game &amp; SIGINT、SIGHUP 依旧运行 依旧运行 输入正常，终端可用 输出到nohup.out文件]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nohup</tag>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux环境下服务器程序的查看与gdb调试]]></title>
    <url>%2Fblog%2F2019%2F01%2F11%2Flinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9F%A5%E7%9C%8B%E4%B8%8Egdb%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[前言这一篇主要是记录下调试服务器程序常用的命令，内容很简单，但是长时间不用很容易记混，因为游戏服务器也不是天天宕机，所以当有一天突然挂掉需要调试的时候，如果记不清调试命令很容易耽误时间，有好几次我就把gdb gameserver core记成了gdb core gameserver，所以干脆把这些内容统计到一起，查询的时候也方便。 查询程序的运行情况 ps aux命令是常用来查询程序进程运行情况的，基本上不会漏掉，但是显示的无关程序太多，看着不方便所以常配合grep过滤 ps aux | grep gameserver可以显示指定过滤内容的程序，但是这种显示方式没有标题，对于不熟悉的人来说看不太明白，就像下面这样 123$ps aux | grep initroot 1 0.0 0.0 19232 976 ? Ss 2018 0:01 /sbin/init510 2042 0.0 0.0 105492 932 pts/2 S+ 10:04 0:00 grep init 所以这里给出ps aux默认的输出格式：USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND其中STAT: 该行程的状态，linux的进程常见状态：D 不可中断 uninterruptible sleep (usually IO)R 运行 runnable (on run queue)S 中断 sleepingT 停止 traced or stoppedZ 僵死 a defunct or zombie process注: 其它状态还包括W(无驻留页), &lt;(高优先级进程), N(低优先级进程), L(内存锁页). ps -eo pid,lstart,etime,command | grep gameserver有时需要查询特定进程的指定信息，比如运行时间，那么可以通过-o选项来指定，显示信息很明确 1215499 Thu Jan 10 23:22:53 2019 11:24:12 ./gameserver -d18097 Fri Jan 11 10:47:04 2019 00:01 grep gameserver 杀死指定进程 killall -10 gameserver: 按照进程名杀死进程，-10为自定义杀死信号 kill -10 gameserver_pid: 按照进程ID杀死进程，-10为自定义杀死信号 kill -9 gameserver_pid: -9为强制杀死进程的信号，无法被捕捉 kill -6 gameserver_pid: -6可以杀死进程并产生core文件 gdb调试通常要想使用gdb调试需要在编译程序时加上-g选项，之后才能用gdb进行调试：gcc -g main.c -o gameserver，如果想在程序崩溃时产生core文件，还需要设置系统命令ulimit -c unlimited才可以，调试程序又分为直接启动调试、调试core文件和附加到正在运行的进程调试，每种方式的参数略有不同： 直接启动调试，gdb gameserver这种方式相当于直接通过gdb启动了程序，并开启了调试模式，所以是拉取了新的进程 调试core文件，gdb gameserver core.xxx这种方式相当于展示程序崩溃前的堆栈情况，并进行调试，所以也算是拉取了新的进程 附加进程调试，gdb attach gameserver_pid/gdb gameserver gameserver_pid这种方式是将gdb调试工具附加到程序运行的当前进程上，并没有拉取新的进程，操作上也可以先敲gdb回车，然后再attach gameserver_pid，不过这种情况对于其他用户启动的程序，通常会提示“ptrace: 不允许的操作.”，所以需要使用sudo运行gdb gdb常用命令以下命令为调试linux程序常用的gdb命令，都是在调试服务器程序core文件过程中不断积累的，还有一些高级命令一般很少用到，掌握下面这些基本上就可以应付很多场景了，其中打印信息的命令print在打印map、vector等显示不友好，可以参考gdb调试脚本中的内容。 run/r：重新开始运行文件 break/b: 设置断点（I. b filename:linenum, II. b functioname, 条件断点: b position if condition） info/i: 查看信息（I. i b:查看断点信息，i locals: 查看当前帧局部变量值，i threads: 查看线程） delete/d: 删除断点（delete 3：删除通过info breakpoints查到的第3个断点，其实还可以删除别的） list/l: 查看原代码（list -n：显示第n行前后的源代码。list 函数名：查看具体函数） next/n: 逐过程调试（类似于VS的F10） step/s: 逐语句调试（类似于VS的F11） frame/f：切换函数的栈帧（可以调试输出指定函数内的情况） print/p：打印值及地址（用来显示变量值） thread/t: 切换线程（调试指定线程，用来处理多线程程序） continue/c：继续运行（常用调试完断点之后） backtrace/bt：显示堆栈（可以查看函数的调用的栈帧和层级关系） source: 加载脚本（可以在调试过程中使用脚本完成复杂逻辑调试）]]></content>
      <categories>
        <category>gdb</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>实用工具</tag>
        <tag>gdb</tag>
        <tag>ps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抓扑克牌风格的插入排序]]></title>
    <url>%2Fblog%2F2018%2F12%2F04%2F%E6%8A%93%E6%89%91%E5%85%8B%E7%89%8C%E9%A3%8E%E6%A0%BC%E7%9A%84%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言上次聊到了快速排序，我们说到快排这个名字是非常抽象的，究竟什么是快排，从名字上我们无从得知，或许叫二分排序都比快速排序要形象的多，可是这又和归并排序重复了，所以我们还是不要在意快排的名字了，接下来看一下今天的插入排序，这里指的是简单的插入排序。 插入排序相比于快速排序要形象很多，整个排序过程就是在不断的插入操作中完成的，如果你打过扑克基本上很容易理解这种排序方法，排序的过程几乎与抓扑克牌的过程一模一样，假设三个人斗地主，每人一张牌依次抓取，其实一旦开始抓一张牌，那么牌堆里哪些牌是你的就已经确定了，只不过是隔两张之后的那张就是你的，所有这些归属于你的牌在牌堆里的顺序就是这些牌的初始顺序，而你抓牌摆牌的过程就是给这些牌从小到大（当然可以从大到小）排序的过程。 插入排序整个排序过程可以使用抓牌来模拟，抓第一张牌的时候无所谓顺序，放在手里就好，抓第二张牌的时候和第一张比较，按从小到大排好顺序，抓第三张牌的时候，和前面两张比较，“插入”适当的位置，后面的牌依次类推插入正确位置，最后手里的牌也就排好了顺序，还有一点需要注意，抓牌时可以真的将一张牌插入到另外两张牌之间的（实际上也是占用了原来牌的位置），但是在内存中，比如连续下标的一个数组中，要想在元素2和元素3中间插入一个数字是做不到的，如果确实要放到这两个数中间，那就需要将元素3往后移动，给需要插入的这个数字腾出一个地方，元素3后面如果也有其他元素呢？那就也需要向后移动，一直到后面没有需要移动的元素为止。 想象一下完整的抓牌过程，其中的关键点就在于拿到一张新牌（元素）后，和之前有序的手牌进行比较，找到合适的插入位置，依次移动手牌位置（为了仿照内存中移动，我们把牌向后移动，也就是从后向前比较找插入位置），为新来的牌腾出一个位置，把新抓到的牌放入空位，一直到完成最后一张牌的插入，我们也就同时完成了手牌的排序。其中的关键词有之前有序、移动、插入。 接下来可以举个例子操作一下，假设我开了天眼，可以看到牌堆里所有的牌，那么确定了抓牌顺序之后，我也就知道我会抓到哪些牌了，他们分别是6, 2, 7, 3, 8, 9，下面来模拟一下这个牌堆中的牌到了我的手里时候怎么就有序了，还有一个情况就是我用右手摸牌，左手拿牌，但是左手比较小，只能放的下6张牌，这时候可以看看实际的抓牌流程了。 起初情况是左手没有牌，右手抓了一张6:L=_, _, _, _, _, _，R=6 这时候没有什么犹豫的，直接放到左手第一个位置就好了：L=6, _, _, _, _, _ ，R=_ 然后又抓到一张2，移动左手的牌，拿2和左手有序的牌进行比较，这是左手就1张6，将其向后移动得到：L=_, 6, _, _, _, _ ，R=2 接下来需要把右手的2放到左手腾出的位置即可：L=2, 6, _, _, _, _ ，R=_ 紧接着又抓到一张7，发现放到后面就可以，不用移动元素了：L=2, 6, 7, _, _, _ ，R=_ 然后又抓到一张3，其实找插入位置还有另一种形式，就是放到最后，然后不断的换到合适的位置，用这张3来试一下：L=2, 6, 7, _, _, _ ，R=3 先放到左手最后：L=2, 6, 7, 3, _, _ ，R=_ 然后和前面比3大的换一下位置：L=2, 6, 3, 7, _, _ ，R=_ 再换一次找到了真正插入的位置：L=2, 3, 6, 7, _, _ ，R=_ 后面的8，9两张都不需要交换位置，直接放到最后就得到了最终的结果：L=2, 3, 6, 7, 8, 9 ，R=_ 代码实现1234567891011121314151617181920212223242526272829303132/*功能： 交换两个变量参数： element1--被交换的第一个元素的地址 element2--被交换的第二个元素的地址返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void swap_data(int* element1, int* element2)&#123; int middle_value = *element1; *element1 = *element2; *element2 = middle_value;&#125;/*功能： 插入排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 count--数组元素的个数返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void insert_sort(int array[], int count)&#123; for (int pos = 1; pos &lt; count; ++pos) &#123; for (int insert_index = pos; insert_index &gt; 0; --insert_index) &#123; if (array[insert_index] &lt; array[insert_index - 1]) swap_data(&amp;array[insert_index], &amp;array[insert_index - 1]); &#125; &#125;&#125; 代码分析以上代码就是模拟的抓牌过程，新加入的牌放到手牌最后，然后不断的和前面的手牌交换位置，“插入”到有序的手牌序列中，最后得到整体有序，配合前面具体的例子，可以把具体的那些数字带入到这段代码中，头脑中或者在纸上“运行”一下，你就会了解插入排序的原理了。 运行测试快速排序–源码 如果你想运行测试一下，可以复制或者下载源代码，在本机运行测试一下，当然也可以选择在线C++编译器，把源代码复制到网页中运行查看结果，建议不明白的可以在本地环境单步调试一下，这样有助于理解算法思路。]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
        <tag>插入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql中使用select into语句给变量赋值没有匹配记录时的结果]]></title>
    <url>%2Fblog%2F2018%2F11%2F17%2FMysql%E4%B8%AD%E4%BD%BF%E7%94%A8select-into%E8%AF%AD%E5%8F%A5%E7%BB%99%E5%8F%98%E9%87%8F%E8%B5%8B%E5%80%BC%E6%B2%A1%E6%9C%89%E5%8C%B9%E9%85%8D%E8%AE%B0%E5%BD%95%E6%97%B6%E7%9A%84%E7%BB%93%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[前言对select into语句感兴趣是因为看了项目中的一个存储过程引起的，在程序运行之前看了存储过程的逻辑，本以为没有数据时会报错，结果程序却正常运行，这说明我对select into语句理解的问题，同时也暴露了一个知识盲点，所以写了个小例子测试一下，并把测试的过程记录方便日后查找。 创建测试表格为了更清楚的表明问题，我们创建的表格尽可能的简单，同时为了测试空值的情况，数据列我们不设置默认值，表格命名为’intotest’，创建语句如下： 12345CREATE TABLE `intotest` ( `id` int(4) NOT NULL AUTO_INCREMENT, `number` int(4), PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=binary ROW_FORMAT=DYNAMIC; 插入测试数据表格建立完成可以使用可视化工具或者insert语句插入测试数据，插入测试数据后查询结果如下： 123456789mysql&gt; select * from intotest;+----+--------+| id | number |+----+--------+| 1 | 1 || 2 | 2 || 3 | NULL |+----+--------+3 rows in set (0.00 sec) 建立一个存储过程我们建立一个用于测试的存储过程，主要的逻辑就是看看当select into语句找不到匹配记录时，被赋值的变量会怎么样，建立存储过程的代码如下： 1234567CREATE PROCEDURE `select_into_value2`()BEGIN DECLARE _value INT DEFAULT 0; SELECT number FROM intotest WHERE id=1 INTO _value; SELECT _value;END 这个存储过程运行正常，配合刚才我们插入表格的记录可以知道，运行后的结果为1: 123456789mysql&gt; call select_into_value();+--------+| _value |+--------+| 1 |+--------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) 测试过程 当查询结果中不存在符合条件的记录时会怎样，修改存储过程定义，然后查看运行结果： 1234567CREATE PROCEDURE `select_into_value2`()BEGIN DECLARE _value INT DEFAULT 0; SELECT number FROM intotest WHERE id=5 INTO _value; SELECT _value;END 123456789mysql&gt; call select_into_value();+--------+| _value |+--------+| 0 |+--------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) 结果为0，也就是说当查不到匹配结果时，不会执行select into的赋值效果。 当匹配到查询结果但是查询出来的数值为null会怎样，修改存储过程定义，然后查看运行结果： 1234567CREATE PROCEDURE `select_into_value2`()BEGIN DECLARE _value INT DEFAULT 0; SELECT number FROM intotest WHERE id=3 INTO _value; SELECT _value;END 123456789mysql&gt; call select_into_value();+--------+| _value |+--------+| NULL |+--------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) 结果为NULL，也就是说当查到匹配结果时，不管结果时什么都会赋值到指定的变量中（类型不匹配的sql错误除外）。 当连续查询赋值中间出现不匹配会怎样，修改存储过程定义，然后查看运行结果： 12345678CREATE PROCEDURE `select_into_value2`()BEGIN DECLARE _value INT DEFAULT 0; SELECT number FROM intotest WHERE id=2 INTO _value; SELECT number FROM intotest WHERE id=5 INTO _value; SELECT _value;END 123456789mysql&gt; call select_into_value();+--------+| _value |+--------+| 2 |+--------+1 row in set (0.00 sec)Query OK, 0 rows affected (0.00 sec) 想必明白了前两种情况，这第三种也应该明白了，两条语句顺序执行，找到匹配的就赋值，找不到就放弃操作，结果就保留了上一次成功赋值的结果。 总结 关于select into语句赋值的规则就一句话，找到了符合条件的记录就赋值，找不到就算了。 在找到记录的前提下，如果类型不匹配会导致赋值失败并报错，比如查询到字符串赋值给整型变量。]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>查询赋值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lua中关于table对象引用传递的注意事项]]></title>
    <url>%2Fblog%2F2018%2F09%2F18%2FLua%E4%B8%AD%E5%85%B3%E4%BA%8Etable%E5%AF%B9%E8%B1%A1%E5%BC%95%E7%94%A8%E4%BC%A0%E9%80%92%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[前言最近写了挺长一段时间的Lua，发现Lua这个语言真的是很随意，产生这种感觉的根本原因应该是它把“函数” 作为了“第一类值”，也就是说函数也可以作为变量的“值”，这使得Lua可以随处定义函数，进而改变逻辑的走向，整个流程任你摆布。 虽说把一个函数来回设置方便了许多，但是同样带来了一些不容易发现的问题，如果搞不清定义域和引用关系，常常会导致程序错误，比如最近用Lua写按钮的触发事件时，使用公有函数创建了对应的闭包，一开始感觉table的引用有问题，写了很多中转的代码，最后发现直接就可以使用，浪费了不少时间，最后仔细分析就是闭包最根本的形式，只不过被业务逻辑给干扰了视线，接下来我们一起看看，table和闭包究竟会发生什么关系！ 代码测试 table作为函数参数时的操作 123456789101112131415print("\nexample 1:");data_table = &#123;a = 1, b = 2, 3, 4, 5, 6&#125;;function filter(data_tb) for k,v in pairs(data_tb) do if v % 2 == 0 then data_tb[k] = nil; end endend-- 过滤掉偶数filter(data_table);for k,v in pairs(data_table) do print(k,v)end 1234example 1:1 33 5a 1 以上为去掉table中的偶数的代码，直接操作参数data_tb就可以对传入的data_table进行改变，这样的逻辑一般不会出错，接着我们看下接下来需求，直接将表中数据清空。 1234567891011print("\nexample 2:");data_table = &#123;a = 1, b = 2, 3, 4, 5, 6&#125;;function destroy(data_tb) data_tb = &#123;&#125;;end-- 销毁整个表destroy(data_table);for k,v in pairs(data_table) do print(k,v)end 1234567example 2:1 32 43 54 6b 2a 1 看到这次的输出可能有些人就感到奇怪了，怎么上个例子改变元素可以，而这里直接给变量data_tb赋值，变成空表怎么不行了？这是因为data_tb是对变量data_table的整体引用，所以可以通过data_tb来改变变量data_table内部的值，但是当执行data_tb = {};代码时表示data_tb不再引用data_table，而去引用{}了，也就是data_tb和data_table脱离了关系，这一点可以类比C++代码： 1234567891011121314151617181920#include &lt;iostream&gt;using namespace std;void change_string(char* pStr)&#123; pStr[0] = '5'; pStr[1] = '0'; pStr = "only test\n";&#125;int main()&#123; char szContent[32] = "help"; change_string(szContent); cout &lt;&lt; szContent &lt;&lt; endl; return 0;&#125; 分析一下这段代码的输出结果，如果你能知道结果为50lp，那说明你的C++水平已经超过了入门级别，理解了这段代码有助于清楚的理解前两段Lua代码。 看一个标准闭包实现的计数器 12345678910111213print("\nexample 3:");function counter() local count = 0; return function() count = count + 1; return count; endendfunc = counter();print(func());print(func());print(func()); 1234example 3:123 这段代码的不同之处就在于变量count，这是一个标准的计数器，也是一个标准的闭包，也就是说Lua支持这样的语法，闭包中可以在定义之后一直引用外部的变量，并且在返回函数的整个使用生命周期内都可以引用这个变量，加入外部修改了这个变量，闭包中引用的值也会改变，换句话来说就是闭包这种引用是引用的变量，而不是仅仅保存了一个值。 lua中常见的table引用 12345print("\nexample 4:");local t1 = &#123;i = 1&#125;;local t2 = t1;t1.i = 666;print(t2.i) 12example 4:666 这个例子类似于前面“过滤掉偶数”的代码，首先定义了表t1，然后定义了变量t2引用了变量t1，实际上这里t2不是定义了变量t1本身，而是引用了t1的值，也就是引用的是{i=1}，这里通过t1.i = 666也可以影响到变量t2，其实这个例子看不出引用的究竟是变量t1还是t1的值，可以接着看下面的例子。 12345print("\nexample 5:");local t1 = &#123;i = 1&#125;;local t2 = t1;t1 = &#123;i = 11&#125;;print(t2.i) 12example 5:1 通过这个例子就很清楚了，前面的部分和上个例子一致，但是后面直接给变量t1赋值时并没有改变t2的值，由此可以看出t1和t2已经“分离”了。 table引用和闭包结合的例子 12345678910111213print("\nexample 6:");local tb = &#123;i= 1&#125;;function outer() return function() local t = tb; print(t.i); endendlocal show = outer();tb = &#123;i = 6&#125;;show(); 12example 6:6 这个例子应该会有猜错结果的人，我自己就是在类似的代码中搞糊涂的，一种想法是函数outer定义的时候变量t的值已经定义了，还有一种就是认为在返回函数show的时候变量t的值会定义，但是这两种想法都是错误的，实际上是调用函数show的时候才给t赋值，这时变量t引用的是拥有最新值的tb，所以t.i的值是6，如果你猜对了这个例子的结果，接下来看看下面的代码。 12345678910111213print("\nexample 7:");local tb = &#123;i= 1&#125;;function outer() local t = tb; return function() print(t.i); endendlocal show = outer();tb = &#123;i = 7&#125;;show(); 12example 7:1 如果清楚了上个例子的运行过程，就应该很容易知道这个例子的结果，其中变量t的值是在调用函数outer时确定的，所以后面的赋值tb = {i = 7};对变量t的值没有影响。 总结 lua中操作变量注意值和引用，其实很多语言都有这种区分。 注意闭包可以访问外部变量的特性，程序中使用起来非常方便。 实际使用过程中往往还夹杂着业务逻辑，要学会挖掘本质问题，这样往往可以看到真正的运行逻辑。 测试源码示例传送门：lua中table引用]]></content>
      <categories>
        <category>Lua</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Lua</tag>
        <tag>table</tag>
        <tag>引用传递</tag>
        <tag>闭包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unique_ptr浅析]]></title>
    <url>%2Fblog%2F2018%2F09%2F12%2Funique-ptr%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言unique_ptr这个指针是C++11标准时被引入标准库的，有一种说法称它是boost::scoped_ptr的一个分身，并且它在C++11的时候“转正”了，但是scoped_ptr还被留在boost库中，看来没有转正的机会了，不过unique_ptr与scoped_ptr确实很像，unique_ptr只比scoped_ptr多了一个移动语义，可以通过std::move()函数来转移内部对象的所有权。 其实在我看来，unique_ptr与auto_ptr是最像的，他设计之初就是为了替代auto_ptr，其实两者基本上没有区别，如果把auto_ptr限制一下，使其不能通过拷贝构造和赋值获得所有权，但是可以通过std::move()函数获得所有权，那基本上就变成了unique_pr，这一点通过下面的函数分析也可以看出，两者的函数基本一致。 unique_pr作为一个模板类，可以直接用它来定义一个智能指针的对象，例如std::unique_pr&lt;Test&gt; pa(new Test);，查看unique_pr的代码时发现，它主要有get、release、reset、operator*、operator-&gt;、operator=、swap、operator bool、get_deleter几个函数，相比于auto_ptr常用函数来说，只多了swap、operator bool、get_deleter这三个函数，基本上没什么变化，不过get_deleter这个函数值的详细解释一下，下面通过一些例子来了解一下unique_pr的具体用法。 使用环境 VS2015 + Windows7（应该是C++11标准） 头文件#include &lt;memory&gt; 命名空间using namespace std; 测试过程首先我们先编写一个测试类，用来测试智能指针各个函数的作用，以及可能出现的问题，测试类的代码如下：123456789101112131415161718class Example&#123;public: Example(int param = 0) &#123; number = param; cout &lt;&lt; "Example: " &lt;&lt; number &lt;&lt; endl; &#125; ~Example() &#123; cout &lt;&lt; "~Example: " &lt;&lt; number &lt;&lt; endl; &#125; void test_print() &#123; cout &lt;&lt; "in test print: number = " &lt;&lt; number &lt;&lt; endl; &#125; void set_number(int num) &#123; number = num; &#125;private: int number;&#125;; 测试函数get、release、reset、operator*、operator-&gt;、swap、operator bool这些函数在解释auto_ptr的时候基本都提到过，swap、operator bool作为两个新的函数在解释shared_ptr的时候也演示过，所以此处就不花过多的篇幅举例了，这里写到一个测试函数中，体会一下用法就好： 123456789101112131415161718192021222324252627282930void test1()&#123; unique_ptr&lt;Example&gt; ptr1(new Example(1)); // Example: 1(输出内容) if (ptr1.get()) // 调用get函数，判断内部指针的有效性 &#123; ptr1.get()-&gt;test_print(); // in test print: number = 1(输出内容) ptr1-&gt;set_number(2); // 调用了operator-&gt; (*ptr1).test_print(); // in test print: number = 2(输出内容) &#125; if (ptr1) // 调用operator bool 检测内部对象的有效性 cout &lt;&lt; "ptr1 is valid\n"; // ptr1 is valid(输出内容) Example *p = ptr1.release(); // 调用release函数，取出内部对象 if (!ptr1) // 调用operator bool 检测内部对象的有效性 cout &lt;&lt; "ptr1 is invalid\n"; // ptr1 is invalid(输出内容) ptr1.reset(p); // 调用reset函数，重新设置内部对象 if (ptr1) // 调用operator bool 检测内部对象的有效性 cout &lt;&lt; "ptr1 is valid\n"; // ptr1 is valid(输出内容) ptr1-&gt;test_print(); // in test print: number = 2(输出内容) unique_ptr&lt;Example&gt; ptr2(new Example(20)); // Example: 20(输出内容) ptr1.swap(ptr2); // 调用swap函数，重新设置内部对象 ptr1-&gt;test_print(); // in test print: number = 20(输出内容) ptr2-&gt;test_print(); // in test print: number = 2(输出内容) ptr1.reset(); // ~Example: 20(输出内容)// 重置内部对象被销毁&#125; // ~Example: 2(输出内容) // 出作用域被析构 测试函数operator=operator=这个函数是unique_ptr与auto_ptr最大的区别，因为在auto_ptr中，这个操作函数往往是导致问题出现的罪魁祸首，赋值之后所有权转移，原智能指针对象无效，这样往往会导致程序崩溃，所以在unique_ptr中operator=被禁止使用了，取而代之的是具有移动语义的std::move()函数，如果unique_ptr的对象直接赋值的话，会在编译期间就提示错误： 12345678910void test2()&#123; //unique_ptr&lt;Example&gt; ptr2 = new Example(2);// 编译错误，不支持原始指针到智能指针的隐式转换 unique_ptr&lt;Example&gt; ptr2(new Example(2)); // Example: 2(输出内容) //unique_ptr&lt;Example&gt; ptr3 = ptr2; // 编译错误，...: 尝试引用已删除的函数 //unique_ptr&lt;Example&gt; ptr4(ptr2); // 编译错误，...: 尝试引用已删除的函数 unique_ptr&lt;Example&gt; ptr5(std::move(ptr2)); // 正常编译，使用move移动语义，符合预期效果 ptr5-&gt;test_print(); // in test print: number = 2(输出内容)&#125; // ~Example: 2(输出内容) // 出作用域被析构 测试unique_ptr作为参数和返回值unique_ptr是可以作为参数和返回值的，不过因为operator=不允许使用，所以在作为参数的时候需要使用函数std::move()，但是作为返回值却不需要，这里留个疑问，最后分析一下： 1234567891011121314151617181920212223242526void test3_inner1(unique_ptr&lt;Example&gt; ptr3_1)&#123; ptr3_1-&gt;test_print(); // in test print: number = 3（输出内容）&#125; // ~Example: 3(输出内容) // 出作用域被析构unique_ptr&lt;Example&gt; test3_inner2()&#123; unique_ptr&lt;Example&gt; ptr3_2(new Example(32));// Example:32（输出内容） ptr3_2-&gt;test_print(); // in test print: number = 32（输出内容） return ptr3_2;&#125;void test3()&#123; unique_ptr&lt;Example&gt; ptr3(new Example(3)); // Example:3（输出内容） ptr3-&gt;test_print(); // in test print: number = 3（输出内容） //test3_inner1(ptr3); // 直接作为参数传递会报编译错误,不存在拷贝构造 test3_inner1(std::move(ptr3)); // 但是可以使用std::move的移动语义来实现 if (!ptr3) cout &lt;&lt; "ptr3 is invalid\n"; // ptr1 is valid(输出内容),移动之后ptr3无效 ptr3 = test3_inner2(); // 由于不允许调用构造或者赋值，此处使用了移动语义move ptr3-&gt;test_print(); // in test print: number = 32（输出内容）&#125; // ~Example: 32（输出内容）,出定义域ptr3释放内部对象 测试unique_ptr类型的指针或者引用作为参数这一点没有什么问题，因为不会发生所有权的转移和引用计数的增加，所有的智能指针，包括auto_ptr在内在这种用法的情况下都不会发生问题： 123456789101112131415161718void test4_inner1(unique_ptr&lt;Example&gt;* ptr4_1)&#123; (*ptr4_1)-&gt;test_print(); // in test print: number = 4（输出内容） &#125; // 指针传递没有析构void test4_inner2(unique_ptr&lt;Example&gt;&amp; ptr4_2)&#123; ptr4_2-&gt;test_print(); // in test print: number = 4（输出内容）&#125; // 引用传递没有析构void test4()&#123; unique_ptr&lt;Example&gt; ptr4(new Example(4)); // Example:4（输出内容） ptr4-&gt;test_print(); // in test print: number = 4（输出内容） test4_inner1(&amp;ptr4); // 取地址作为参数 test4_inner2(ptr4); // 引用作为参数&#125; // ~Example: 4（输出内容）,出定义域ptr4释放内部对象 测试unique_ptr作为容器元素前面分析auto_ptr的时候已经说过，auto_ptr在作为容器元素时，是不具有跨平台性质的，因为在有的平台表现很正常，有的环境下直接编译报错，原因就是使用auto_ptr很容易出错，不是说一定会出错，而是可能出问题，所以个别平台直接在编译期报错，防止后续的错误。而unique_ptr作为容器元素时，表现很统一，没有任何问题，但是我感觉这里就有点牵强，后续再说，注意v[6] = unique_ptr&lt;Example&gt;(new Example(56));这一句，是不是感觉很神奇，居然不报编译错误，我感觉和作为返回值时是相同的处理。 12345678910111213141516171819202122232425262728void test5()&#123; vector&lt;unique_ptr&lt;Example&gt;&gt; v(7); for (int i = 0; i &lt; 6; i++) &#123; v[i] = unique_ptr&lt;Example&gt;(new Example(50 + i)); // 依次输出Example:70,...Example:75 &#125; // 直接赋值，迷之成功，不是不能operator=吗,这里实际上调用的还是std::move类似的移动语义？ v[6] = unique_ptr&lt;Example&gt;(new Example(56));// Example:56（输出内容） // 直接将unique_ptr对象push_back v.push_back(unique_ptr&lt;Example&gt;(new Example(57))); // Example:57（输出内容） // 利用移动语义push_back v.push_back(std::move(unique_ptr&lt;Example&gt;(new Example(58)))); // Example:58（输出内容） // 利用make_unique创建unique_ptr,C++14才支持 v.push_back(make_unique&lt;Example&gt;(59)); // Example:59（输出内容） // 循环调用 for (int i = 0; i &lt; 10; i++) &#123; v[i]-&gt;test_print(); &#125;// 依次输出in test print: number = 50....in test print: number = 59&#125;// 依次输出~Example: 50,~Example: 51...~Example: 59 测试函数get_deleter这个函数还是第一次提到，作用就是获得unique_ptr对象的“删除器”，如果不手动指定就会获得默认的删除器，否则就返回你指定的，举个例子一看就明白了，代码如下： 123456789101112131415161718192021222324252627// a custom deleterclass custom_deleter &#123; int flag;public: custom_deleter(int val) : flag(val) &#123;&#125; template &lt;class T&gt; void operator()(T* p) &#123; std::cout &lt;&lt; "use custom deleter, flag=" &lt;&lt; flag ; delete p; &#125;&#125;;void test6()&#123; custom_deleter dlter(666); unique_ptr&lt;Example, custom_deleter&gt; ptr6(new Example(6), dlter); // Example:6（输出内容） ptr6-&gt;test_print(); // in test print: number = 6（输出内容） unique_ptr&lt;Example, custom_deleter&gt; ptr7(new Example(7), ptr6.get_deleter()); // 调用get_deleter // 重置智能指针，内部对象使用自定义删除器删除 ptr6.reset(); // 输出：use custom deleter, flag = 666~Example: 6 ptr7-&gt;test_print(); // in test print: number = 7（输出内容）&#125; // 输出：use custom deleter, flag = 666~Example: 7 现象分析上面的几个例子都很简单，基本上看一遍就知道怎么用了，但是有一点让人很迷惑，就是operator=的使用，最开始已经说过了，unique_ptr中的operator=已经被禁止使用了，但是例子中有两处很有争议，就是unique_ptr作为函数返回值和直接把unique_ptr赋值给vector元素，一开始我也不是太清楚，后来找资料时发现了一些线索，和大家分享一下: 当函数返回一个对象时，理论上会产生临时变量，那必然是会导致新对象的构造和旧对象的析构，这对效率是有影响的。C++编译针对这种情况允许进行优化，哪怕是构造函数有副作用，这叫做返回值优化（RVO)，返回有名字的对象叫做具名返回值优化(NRVO)，就那RVO来说吧，本来是在返回时要生成临时对象的，现在构造返回对象时直接在接受返回对象的空间中构造了。假设不进行返回值优化，那么上面返回unique_ptr会不会有问题呢？也不会。因为标准允许编译器这么做：1.如果支持move构造，那么调用move构造。2.如果不支持move，那就调用copy构造。3.如果不支持copy，那就报错吧。 很显然，unique_ptr本身是支持move构造的，所以unique_ptr对象可以被函数返回，另外我推测将unique_ptr直接赋值给vector元素也利用了相似的操作，这里不太确定，希望了解的小伙伴能告知一下其中的原因。 说到这里，我们对unique_ptr也有了整体的认识，说unique_ptr是auto_ptr的替代品，可是unique_ptr真的优秀了吗？我看未必，它并非不会再犯错，只是犯错的成本大了一些，如果使用std::move()转移了所有权之后，再直接使用原来的智能指针对象，同样会使得程序崩溃。 其实auto_ptr和unique_ptr给我的感觉就是就好比租房子，租房时有些人喜欢看一下房东的房产证，有的人则无所谓，来个人说是房东他就敢跟人签合同，房屋所有权是通过房产证来转移的，使用auto_ptr就好像两个人可以私下交易，把钱和房产证直接交换，房产证的转移很随便，使用unique_ptr就好比在转移房产的时候需要放鞭炮、然后在全世界广播一下，比较麻烦，并且有可能被租房的人看到，但是本质是一样的，都是拿钱来转移房的所有权，关键还是看租房的人，如果租房先看房产证，即使是房产证的转移很随便（也就是使用auto_ptr），也不会出问题，如果租房根本不看房产证，即使房产证交易通知了世界上所有人（即使用unique_ptr），也会租到没证的房子（程序崩溃）。 所以说unique_ptr并没有消除错误，仅仅是提高了犯错的成本。 总结 对比auto_ptr和unique_ptr后发现，unique_ptr几乎只是将auto_ptr的operator=改为std::move()函数。 现在标准库中只剩下了shared_ptr、weak_ptr和unique_ptr三个智能指针，weak_ptr是为了解决shared_ptr的循环引用问题而存在的，有其特定的使用情况，所以只剩下了shared_ptr和unique_ptr的选择，选择的标准就是看是否需要对原对象共享所有权，如果需要使用shared_ptr，如果不需要是独占所有权的使用unique_ptr。 unique_ptr并没有从根本上消除可能错误，仅仅是提高了犯错的成本，并且给出移动所有权的提示，但是在容器vector元素赋值时依然很隐晦，可能造成auto_ptr相同的错误。 测试源码示例传送门：unique_ptr用法]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>unique_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[weak_ptr浅析]]></title>
    <url>%2Fblog%2F2018%2F09%2F01%2Fweak-ptr%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言weak_ptr这个指针天生一副“小弟”的模样，也是在C++11的时候引入的标准库，它的出现完全是为了弥补它老大shared_ptr天生有缺陷的问题，其实相比于上一代的智能指针auto_ptr来说，新进老大shared_ptr可以说近乎完美，但是通过引用计数实现的它，虽然解决了指针独占的问题，但也引来了引用成环的问题，这种问题靠它自己是没办法解决的，所以在C++11的时候将shared_ptr和weak_ptr一起引入了标准库，用来解决循环引用的问题。 weak_ptr本身也是一个模板类，但是不能直接用它来定义一个智能指针的对象，只能配合shared_ptr来使用，可以将shared_ptr的对象赋值给weak_ptr，并且这样并不会改变引用计数的值。查看weak_ptr的代码时发现，它主要有lock、swap、reset、expired、operator=、use_count几个函数，与shared_ptr相比多了lock、expired函数，但是却少了get函数，甚至连operator* 和 operator-&gt;都没有，可用的函数数量少的可怜，下面通过一些例子来了解一下weak_ptr的具体用法。 使用环境 VS2015 + Windows7（应该是C++11标准） 头文件#include &lt;memory&gt; 命名空间using namespace std; 测试过程 weak_ptr解决shared_ptr循环引用的问题定义两个类，每个类中又包含一个指向对方类型的智能指针作为成员变量，然后创建对象，设置完成后查看引用计数后退出，看一下测试结果： 123456789101112131415161718192021222324252627282930313233343536373839class CB;class CA&#123;public: CA() &#123; cout &lt;&lt; "CA() called! " &lt;&lt; endl; &#125; ~CA() &#123; cout &lt;&lt; "~CA() called! " &lt;&lt; endl; &#125; void set_ptr(shared_ptr&lt;CB&gt;&amp; ptr) &#123; m_ptr_b = ptr; &#125; void b_use_count() &#123; cout &lt;&lt; "b use count : " &lt;&lt; m_ptr_b.use_count() &lt;&lt; endl; &#125; void show() &#123; cout &lt;&lt; "this is class CA!" &lt;&lt; endl; &#125;private: shared_ptr&lt;CB&gt; m_ptr_b;&#125;;class CB&#123;public: CB() &#123; cout &lt;&lt; "CB() called! " &lt;&lt; endl; &#125; ~CB() &#123; cout &lt;&lt; "~CB() called! " &lt;&lt; endl; &#125; void set_ptr(shared_ptr&lt;CA&gt;&amp; ptr) &#123; m_ptr_a = ptr; &#125; void a_use_count() &#123; cout &lt;&lt; "a use count : " &lt;&lt; m_ptr_a.use_count() &lt;&lt; endl; &#125; void show() &#123; cout &lt;&lt; "this is class CB!" &lt;&lt; endl; &#125;private: shared_ptr&lt;CA&gt; m_ptr_a;&#125;;void test_refer_to_each_other()&#123; shared_ptr&lt;CA&gt; ptr_a(new CA()); shared_ptr&lt;CB&gt; ptr_b(new CB()); cout &lt;&lt; "a use count : " &lt;&lt; ptr_a.use_count() &lt;&lt; endl; cout &lt;&lt; "b use count : " &lt;&lt; ptr_b.use_count() &lt;&lt; endl; ptr_a-&gt;set_ptr(ptr_b); ptr_b-&gt;set_ptr(ptr_a); cout &lt;&lt; "a use count : " &lt;&lt; ptr_a.use_count() &lt;&lt; endl; cout &lt;&lt; "b use count : " &lt;&lt; ptr_b.use_count() &lt;&lt; endl;&#125; 测试结果如下： 123456CA() called!CB() called!a use count : 1b use count : 1a use count : 2b use count : 2 通过结果可以看到，最后CA和CB的对象并没有被析构，其中的引用效果如下图所示，起初定义完ptr_a和ptr_b时，只有①③两条引用，然后调用函数set_ptr后又增加了②④两条引用，当test_refer_to_each_other这个函数返回时，对象ptr_a和ptr_b被销毁，也就是①③两条引用会被断开，但是②④两条引用依然存在，每一个的引用计数都不为0，结果就导致其指向的内部对象无法析构，造成内存泄漏。 解决这种状况的办法就是将两个类中的一个成员变量改为weak_ptr对象，因为weak_ptr不会增加引用计数，使得引用形不成环，最后就可以正常的释放内部的对象，不会造成内存泄漏，比如将CB中的成员变量改为weak_ptr对象，代码如下：1234567891011class CB&#123;public: CB() &#123; cout &lt;&lt; "CB() called! " &lt;&lt; endl; &#125; ~CB() &#123; cout &lt;&lt; "~CB() called! " &lt;&lt; endl; &#125; void set_ptr(shared_ptr&lt;CA&gt;&amp; ptr) &#123; m_ptr_a = ptr; &#125; void a_use_count() &#123; cout &lt;&lt; "a use count : " &lt;&lt; m_ptr_a.use_count() &lt;&lt; endl; &#125; void show() &#123; cout &lt;&lt; "this is class CB!" &lt;&lt; endl; &#125;private: weak_ptr&lt;CA&gt; m_ptr_a;&#125;;测试结果如下：12345678CA() called!CB() called!a use count : 1b use count : 1a use count : 1b use count : 2~CA() called!~CB() called!通过这次结果可以看到，CA和CB的对象都被正常的析构了，引用关系如下图所示，流程与上一例子相似，但是不同的是④这条引用是通过weak_ptr建立的，并不会增加引用计数，也就是说CA的对象只有一个引用计数，而CB的对象只有2个引用计数，当test_refer_to_each_other这个函数返回时，对象ptr_a和ptr_b被销毁，也就是①③两条引用会被断开，此时CA对象的引用计数会减为0，对象被销毁，其内部的m_ptr_b成员变量也会被析构，导致CB对象的引用计数会减为0，对象被销毁，进而解决了引用成环的问题。 测试weak_ptr对引用计数的影响其实weak_ptr本身设计的很简单，就是为了辅助shared_ptr的，它本身不能直接定义指向原始指针的对象，只能指向shared_ptr对象，同时也不能将weak_ptr对象直接赋值给shared_ptr类型的变量，最重要的一点是赋值给它不会增加引用计数： 1234567891011121314151617181920212223void test1()&#123; // 编译错误 // error C2665: “std::weak_ptr&lt;CA&gt;::weak_ptr”: 3 个重载中没有一个可以转换所有参数类型 // weak_ptr&lt;CA&gt; ptr_1(new CA()); shared_ptr&lt;CA&gt; ptr_1(new CA()); cout &lt;&lt; "ptr_1 use count : " &lt;&lt; ptr_1.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 1 shared_ptr&lt;CA&gt; ptr_2 = ptr_1; cout &lt;&lt; "ptr_1 use count : " &lt;&lt; ptr_1.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 2 cout &lt;&lt; "ptr_2 use count : " &lt;&lt; ptr_2.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 2 weak_ptr&lt;CA&gt; wk_ptr = ptr_1; cout &lt;&lt; "ptr_1 use count : " &lt;&lt; ptr_1.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 2 cout &lt;&lt; "ptr_2 use count : " &lt;&lt; ptr_2.use_count() &lt;&lt; endl; // 输出：ptr_1 use count : 2 // 编译错误 // error C2440 : “初始化”: 无法从“std::weak_ptr&lt;CA&gt;”转换为“std::shared_ptr&lt;CA&gt;” // shared_ptr&lt;CA&gt; ptr_3 = wk_ptr;&#125; 测试weak_ptr常用函数的用法weak_ptr中只有函数lock和expired两个函数比较重要，因为它本身不会增加引用计数，所以它指向的对象可能在它用的时候已经被释放了，所以在用之前需要使用expired函数来检测是否过期，然后使用lock函数来获取其对应的shared_ptr对象，然后进行后续操作： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void test2()&#123; shared_ptr&lt;CA&gt; ptr_a(new CA()); // 输出：CA() called! shared_ptr&lt;CB&gt; ptr_b(new CB()); // 输出：CB() called! cout &lt;&lt; "ptr_a use count : " &lt;&lt; ptr_a.use_count() &lt;&lt; endl; // 输出：ptr_a use count : 1 cout &lt;&lt; "ptr_b use count : " &lt;&lt; ptr_b.use_count() &lt;&lt; endl; // 输出：ptr_b use count : 1 weak_ptr&lt;CA&gt; wk_ptr_a = ptr_a; weak_ptr&lt;CB&gt; wk_ptr_b = ptr_b; if (!wk_ptr_a.expired()) &#123; wk_ptr_a.lock()-&gt;show(); // 输出：this is class CA! &#125; if (!wk_ptr_b.expired()) &#123; wk_ptr_b.lock()-&gt;show(); // 输出：this is class CB! &#125; // 编译错误 // 编译必须作用于相同的指针类型之间 // wk_ptr_a.swap(wk_ptr_b); // 调用交换函数 wk_ptr_b.reset(); // 将wk_ptr_b的指向清空 if (wk_ptr_b.expired()) &#123; cout &lt;&lt; "wk_ptr_b is invalid" &lt;&lt; endl; // 输出：wk_ptr_b is invalid 说明改指针已经无效 &#125; wk_ptr_b = ptr_b; if (!wk_ptr_b.expired()) &#123; wk_ptr_b.lock()-&gt;show(); // 输出：this is class CB! 调用赋值操作后，wk_ptr_b恢复有效 &#125; // 编译错误 // 编译必须作用于相同的指针类型之间 // wk_ptr_b = wk_ptr_a; // 最后输出的引用计数还是1，说明之前使用weak_ptr类型赋值，不会影响引用计数 cout &lt;&lt; "ptr_a use count : " &lt;&lt; ptr_a.use_count() &lt;&lt; endl; // 输出：ptr_a use count : 1 cout &lt;&lt; "ptr_b use count : " &lt;&lt; ptr_b.use_count() &lt;&lt; endl; // 输出：ptr_b use count : 1&#125; 现象分析引用计数的出现，解决了对象独占的问题，但是也带来了循环引用的困扰，使用weak_ptr可以打破这种循环，当你理不清引用关系的时候，不妨采用文中画图的方式来理一理头绪，或许就会有眼前一亮的感觉。 总结 weak_ptr虽然是一个模板类，但是不能用来直接定义指向原始指针的对象。 weak_ptr接受shared_ptr类型的变量赋值，但是反过来是行不通的，需要使用lock函数。 weak_ptr设计之初就是为了服务于shared_ptr的，所以不增加引用计数就是它的核心功能。 由于不知道什么之后weak_ptr所指向的对象就会被析构掉，所以使用之前请先使用expired函数检测一下。 测试源码示例传送门：weak_ptr用法]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>weak_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shared_ptr浅析]]></title>
    <url>%2Fblog%2F2018%2F08%2F15%2Fshared-ptr%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言这个指针近乎完美，原来出现在boost库中，C++11时引入了标准库，解决了auto_ptr对内部对象独占的机制，转而采用引用计数的方式，每增加一次赋值，则引用计数加1，每析构一个智能指针对象，则引用计数减1，当引用计数为1时销毁智能指针对象的同时，也析构内部对象。这种采用引用计数方式避免了对象所有权转移，所以作为函数返回值，函数参数，容器的元素都不会有问题，但是因为引用计数的加入，相应的会带来对引用计数维护的开销。 与auto_ptr一样，shared_ptr本身也是一个模板类，那么一般情况下直接用它来定义一个智能指针的对象，例如std::shared_ptr&lt;Test&gt; pa(new Test);需要注意的是pa虽然叫智能指针，但是它是一个对象，在它的内部保存着一个原始的对象的指针。查看shared_ptr的代码时发现，它主要有get、swap、reset、unique、use_count、operator bool、operator*、operator-&gt;、operator=几个函数，与auto_ptr相比少了release函数，但是多了swap、unique、use_count、operator bool四个函数,下面通过一些例子来了解一下shared_ptr的具体用法。 使用环境 VS2015 + Windows7（应该是C++11标准） 头文件#include &lt;memory&gt; 命名空间using namespace std; 测试过程首先我们先编写一些测试类，用来测试智能指针各个函数的作用，以及可能出现的问题，测试类的代码如下：1234567891011121314151617181920class Example&#123;public: Example(int param = 0) &#123; number = param; cout &lt;&lt; "Example: " &lt;&lt; number &lt;&lt; endl; &#125; ~Example() &#123; cout &lt;&lt; "~Example: " &lt;&lt; number &lt;&lt; endl; &#125; void test_print() &#123; cout &lt;&lt; "in test print: number = " &lt;&lt; number &lt;&lt; endl; &#125; void set_number(int num) &#123; number = num; &#125; int get_number() &#123; return number; &#125;private: int number;&#125;; 测试函数get reset operator* operator-&gt;这几个函数与auto_ptr智能指针的用法一样，可以参考auto_ptr用法，get函数可以获得智能指针包装的原始指针，可以用来判断被包装对象的有效性，也可以用来访问被包装对象，operator*可以直接对智能指针包装的原始指针解引用，获得被包装的对象，operator-&gt;用来取得原始对象的指针，引用成员时与get函数作用相同，reset函数用于重新设置内部对象，若参数为空，则表示取消对内部对象的引用，此时若引用计数大于1则进行减1操作，否则直接析构内部对象。需要注意的是普通的对象指针是无法隐式转换成shared_ptr的，需要利用构造函数实现，示例代码如下： 123456789101112131415161718void test1()&#123; //error C2440: “初始化”: 无法从“Example *”转换为“std::shared_ptr&lt;Example&gt;” //shared_ptr&lt;Example&gt; ptr1 = new Example(1); shared_ptr&lt;Example&gt; ptr1(new Example(1)); // Example: 1（输出内容） if (ptr1.get()) // 调用函数get，获取原始指针，判断有效性 &#123; cout &lt;&lt; "ptr1 is valid" &lt;&lt; endl; // 原始指针有效 &#125; ptr1-&gt;test_print(); // in test print: number = 1（输出内容），调用operator-&gt; ptr1.reset(); // ~Example: 1（输出内容）,调用函数reset，设置为空，释放原内部对象 ptr1.reset(new Example(2)); // Example: 2（输出内容）,重新申请对象并设置 (*ptr1).test_print(); // in test print: number = 1（输出内容），调用operator*&#125; // ~Example: 1（输出内容）,出定义域，释放内部对象 测试函数operator bool用法operator bool函数其实就是用来判断内部对象是否有效的，若内部对象不为空则返回true，否则返回false，大概的实现就是return this-&gt;get() != nullptr;，测试代码如下： 1234567891011void test2()&#123; shared_ptr&lt;Example&gt; ptr2(new Example(2)); // Example: 2（输出内容） if (ptr2) // 调用operator bool cout &lt;&lt; "ptr2 is valid" &lt;&lt; endl; // ptr2 is valid（输出内容），说明ptr2是有效的 ptr2.reset(); // ~Example: 2（输出内容），设置内部对象为空 if (ptr2) // 调用operator bool cout &lt;&lt; "ptr2 is valid" &lt;&lt; endl; // 没有输出，说明ptr2已经无效&#125; 测试函数swap用法从这个名字就可以看出，这个函数用于交换，那么是用来交换什么的呢？实际上是用来交换内部对象的，看下面的例子一试便知，代码运行过后，通过打印可以发现智能指针对象ptr3和ptr4的内部对象进行了交换： 12345678910111213void test3()&#123; shared_ptr&lt;Example&gt; ptr3(new Example(3)); // Example: 3（输出内容） shared_ptr&lt;Example&gt; ptr4(new Example(4)); // Example: 4（输出内容） ptr3-&gt;test_print(); // in test print: number = 3（输出内容） ptr4-&gt;test_print(); // in test print: number = 4（输出内容） ptr3.swap(ptr4); // 调用函数swap ptr3-&gt;test_print(); // in test print: number = 4（输出内容） ptr4-&gt;test_print(); // in test print: number = 3（输出内容）&#125; // ~Example: 3（输出内容）,出定义域，释放内部对象 // ~Example: 4（输出内容）,出定义域，释放内部对象 测试函数unique use_count operator=用法为什么把这几个函数放到一起来说，因为他们是息息相关的，首先函数operator=是用来处理赋值操作的，而赋值操作就会影响引用计数的变化，也就是赋值操作后，use_count函数查询到的引用计数会发生变化，而当use_count返回引用计数是1时，用来表明是否独自引用内部对象的函数unique也会返回true，换句话说unique函数的实现基本就是return this-&gt;use_count() == 1，测试代码如下： 12345678910111213141516171819202122void test4()&#123; shared_ptr&lt;Example&gt; ptr4(new Example(4)); // Example: 4（输出内容） if (ptr4.unique()) &#123; cout &lt;&lt; "ptr4 is unique" &lt;&lt; endl; // ptr4 is unique（输出内容） cout &lt;&lt; "ptr4 use count : " &lt;&lt; ptr4.use_count() &lt;&lt; endl;// ptr4 use count : 1（输出内容） &#125; shared_ptr&lt;Example&gt; ptr5 = ptr4; if (ptr4) cout &lt;&lt; "ptr4 is valid" &lt;&lt; endl;// ptr4 is valid(输出内容）说明赋值之后两个智能指针对象都有效 if (ptr5) cout &lt;&lt; "ptr5 is valid" &lt;&lt; endl;// ptr5 is valid(输出内容）说明赋值之后两个智能指针对象都有效 if (ptr4.unique()) cout &lt;&lt; "ptr4 is unique" &lt;&lt; endl; // 没有输出，说明ptr4不是唯一管理内部对象的智能指针了 cout &lt;&lt; "ptr4 use count : " &lt;&lt; ptr4.use_count() &lt;&lt; endl; // ptr4 use count : 2（输出内容） cout &lt;&lt; "ptr5 use count : " &lt;&lt; ptr5.use_count() &lt;&lt; endl; // ptr4 use count : 2（输出内容）&#125; // ~Example: 4（输出内容）,出定义域，释放内部对象 测试用同一个对象指针生成两个shared_ptr对象与auto_ptr一样，我测试的结果是崩溃，官方标准网站上说是结果未定义，基本上就是说不靠谱，别这样干，仔细想想也能理解，虽说shared_ptr是通过引用计数方式实现，但也不是无所不能，比如这种情况，两个对象都是通过构造生成的，对内部对象的指针p都是“唯一”引用的，也就是两个对象的内部引用计数都是1，当第一个智能指针对象销毁时，会析构内部对象，当第二个智能指针对象销毁时，同样会析构内部对象，这样就造成了崩溃，测试如下： 12345678910void test5()&#123; Example *p = new Example(5); // Example: 5（输出内容） shared_ptr&lt;Example&gt; ptr5(p); shared_ptr&lt;Example&gt; ptr6(p); cout &lt;&lt; "ptr4 use count : " &lt;&lt; ptr5.use_count() &lt;&lt; endl;// ptr4 use count : 1（输出内容） cout &lt;&lt; "ptr5 use count : " &lt;&lt; ptr6.use_count() &lt;&lt; endl;// ptr5 use count : 1（输出内容）&#125; // ~Example: 3（输出内容）,出定义域，ptr5释放内部对象 // ~Example : -572662307（输出内容）,出定义域，ptr6释放内部对象，程序崩溃 测试shared_ptr作为函数参数和返回值因为shared_ptr内部是引用计数，而不是独占所有权，所以在赋值的时候只改变引用计数，不会发生所有权转移，所以这两种用法基本没有问题，发生在auto_ptr上的崩溃惨剧也不会在这里上演，测试代码如下： 12345678910111213141516171819202122232425void test6_inner1(shared_ptr&lt;Example&gt; ptr6_1)&#123; ptr6_1-&gt;test_print(); // in test print: number = 6（输出内容） cout &lt;&lt; "ptr6_1 use count : " &lt;&lt; ptr6_1.use_count() &lt;&lt; endl;// ptr6 use count : 2（输出内容）&#125;shared_ptr&lt;Example&gt; test6_inner2()&#123; shared_ptr&lt;Example&gt; ptr6_2(new Example(62)); // Example:62（输出内容） ptr6_2-&gt;test_print(); // in test print: number = 62（输出内容） cout &lt;&lt; "ptr6_2 use count : " &lt;&lt; ptr6_2.use_count() &lt;&lt; endl;// ptr6_2 use count : 1（输出内容） return ptr6_2;&#125;void test6()&#123; shared_ptr&lt;Example&gt; ptr6(new Example(6)); // Example:6（输出内容） ptr6-&gt;test_print(); // in test print: number = 6（输出内容） cout &lt;&lt; "ptr6 use count : " &lt;&lt; ptr6.use_count() &lt;&lt; endl;// ptr6 use count : 1（输出内容） test6_inner1(ptr6); cout &lt;&lt; "ptr6 use count : " &lt;&lt; ptr6.use_count() &lt;&lt; endl;// ptr6 use count : 1（输出内容） ptr6 = test6_inner2(); // ~Example: 6（输出内容）,ptr6接管新的对象，原来对象被析构 cout &lt;&lt; "ptr6 use count : " &lt;&lt; ptr6.use_count() &lt;&lt; endl;// ptr6 use count : 1（输出内容）&#125; // ~Example: 62（输出内容）,出定义域，ptr6释放内部对象 测试shared_ptr作为容器元素在这里也不存在auto_ptr作为容器元素时的争议，同样是引用计数的机制发挥了作用，使得他满足的容器的要求——其元素对象的拷贝与原对象相同或者等价，所以这里也不会出现问题，同时那些针对于容器的算法在shared_ptr上也可以大显身手，比如下面这个排序的例子： 12345678910111213141516171819202122232425262728// 一般会写成只读引用类型，这里为了说明问题才这样定义bool comp(shared_ptr&lt;Example&gt; a, shared_ptr&lt;Example&gt; b)&#123; return a-&gt;get_number() &gt; b-&gt;get_number();&#125;void test7()&#123; vector&lt;shared_ptr&lt;Example&gt;&gt; v(10); for (int i = 0; i &lt; 10; i++) &#123; v[i] = shared_ptr&lt;Example&gt;(new Example(70+i)); &#125;// 依次输出Example:70,Example:71,Example:72...Example:79 // 循环调用 for (int i = 0; i &lt; 10; i++) &#123; v[i]-&gt;test_print(); &#125;// 依次输出in test print: number = 70....in test print: number = 79 sort(v.begin(), v.end(), comp); // 这可以正常运行，但是使用auto_ptr会死的很难看 // 循环调用 for (int i = 0; i &lt; 10; i++) &#123; v[i]-&gt;test_print(); &#125;// 依次输出in test print: number = 79....in test print: number = 70&#125;// 依次输出~Example: 79,~Example: 78...~Example: 70 测试使用指针或者引用作为参数虽然shared_ptr作为参数、返回值、容器元素貌似没有丝毫问题了，但是有时还是使用shared_ptr对象的指针或者引用比较好，因为这样可以减少对对象的拷贝，毕竟对象的拷贝是需要消耗时间的，用更好的方式为什么不用呢，参考下面的用法，没有任何问题： 12345678910111213141516171819202122void test8_inner1(shared_ptr&lt;Example&gt;* ptr8_1)&#123; (*ptr8_1)-&gt;test_print(); // in test print: number = 8（输出内容） cout &lt;&lt; "ptr8_1 use count : " &lt;&lt; (*ptr8_1).use_count() &lt;&lt; endl;// ptr8_1 use count : 1（输出内容）&#125;void test8_inner2(shared_ptr&lt;Example&gt;&amp; ptr8_2)&#123; ptr8_2-&gt;test_print(); // in test print: number = 8（输出内容） cout &lt;&lt; "ptr8_2 use count : " &lt;&lt; ptr8_2.use_count() &lt;&lt; endl;// ptr8_2 use count : 1（输出内容）&#125;void test8()&#123; shared_ptr&lt;Example&gt; ptr8(new Example(8)); // Example:8（输出内容） ptr8-&gt;test_print(); // in test print: number = 8（输出内容） cout &lt;&lt; "ptr8 use count : " &lt;&lt; ptr8.use_count() &lt;&lt; endl;// ptr8 use count : 1（输出内容) test8_inner1(&amp;ptr8); cout &lt;&lt; "ptr8 use count : " &lt;&lt; ptr8.use_count() &lt;&lt; endl;// ptr8 use count : 1（输出内容) test8_inner2(ptr8); cout &lt;&lt; "ptr8 use count : " &lt;&lt; ptr8.use_count() &lt;&lt; endl;// ptr8 use count : 1（输出内容)&#125; // ~Example: 8（输出内容）,出定义域，ptr8释放内部对像 现象分析shared_ptr与auto_ptr相比要优秀的多，这得益于其内部引用计数的实现，正是这种非独占所有权的方式，使其摆脱了auto_ptr的种种限制，并将其踢出了C++标准（auto_ptr在C++17中被移除），但是shared_ptr也不是完美无缺的，引用计数不能解决所的问题，并且可能会带来一些问题，比如“循环引用问题”，这个得靠后面我们即将说到的weak_ptr来解决，所以说没有什么结构是完美的，选择合适的就是最好的，综合前面多个测试的例子，可以得到一些经验。 总结 shared_ptr作为目前最优秀的指针，取代auto_ptr是必然的，所以能使用shared_ptr的地方还是尽量使用shared_ptr。 不要使用同一个原始对象的指针生成多个shared_ptr对象，这样使用会导致未定义的行为，比如test5这个函数就导致了崩溃和错误的输出。 shared_ptr不是万能的，如果不加思考的把原始指针都替换成shared_ptr，虽然大部分能防止内存泄露，但是还会造成其他的问题，比如循环引用，这种情况需要使用weak_ptr来解决问题，如果不解决就会造成另一种形式的内存泄漏。 不要使用get返回的指针来初始化一个shared_ptr对象，这种的做法的本质与第2点一样，会造成未定义的行为。 尽量不要保存get函数返回的指针，因为你不知道什么时候这个指针对应的对象就被析构掉了，所以请“随用随取”。 测试源码示例传送门：shared_ptr用法]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>shared_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[auto_ptr浅析]]></title>
    <url>%2Fblog%2F2018%2F08%2F08%2Fauto-ptr%E6%B5%85%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言之前简单的列举了一下各种智能指针的特点，其中提到了这个历经沧桑的指针，C++98中引入，C++11中弃用，C++17中被移除，弃用的原因主要是使用不当容易造成内存崩溃，不能够作为函数的返回值和函数的参数，也不能在容器中保存auto_ptr。其实说这个指针“不能够作为函数的返回值和函数的参数，也不能在容器中保存”，这个结论过于武断了，经过一系列的测试后发现，原来真正的结论不应该说“不能”，准确来说是“不建议”。 auto_ptr本身是一个模板类，那么一般情况下直接用它来定义一个智能指针的对象，例如std::auto_ptr&lt;Test&gt; pa(new Test);需要注意的是pa虽然叫智能指针，但是它是一个对象，在它的内部保存着一个原始的对象的指针，其原理就是 RAII(Resource Acquisition Is Initialization) ，在智能指针构造的时候获取资源，在析构的时候释放资源，并进行相关指针操作的重载，使其使用起来就像普通的指针一样方便。 查看auto_ptr的代码时发现，它主要有get、release、reset、operator*、operator-&gt;、operator=几个函数，下面通过一些例子来了解一下auto_ptr的具体用法。 使用环境 VS2015 + Windows7（应该是C++11标准） 头文件#include &lt;memory&gt; 命名空间using namespace std; 测试过程首先我们先编写一些测试类，用来测试智能指针各个函数的作用，以及可能出现的问题，测试类的代码如下：123456789101112131415161718class Example&#123;public: Example(int param = 0) &#123; number = param; cout &lt;&lt; "Example: " &lt;&lt; number &lt;&lt; endl; &#125; ~Example() &#123; cout &lt;&lt; "~Example: " &lt;&lt; number &lt;&lt; endl; &#125; void test_print() &#123; cout &lt;&lt; "in test print: number = " &lt;&lt; number &lt;&lt; endl; &#125; void set_number(int num) &#123; number = num; &#125;private: int number;&#125;; 测试函数get、operator*、operator-&gt;get函数可以获得智能指针包装的原始指针，可以用来判断被包装对象的有效性，也可以用来访问被包装对象，operator*可以直接对智能指针包装的原始指针解引用，获得被包装的对象，operator-&gt;用来取得原始对象的指针，引用成员时与get函数作用相同，示例代码如下： 1234567891011void test1()&#123; auto_ptr&lt;Example&gt; ptr1(new Example(6)); // Example: 6(输出内容) if (ptr1.get()) // 判断内部指针的有效性 &#123; // 以下为访问成员的3种方法 ptr1.get()-&gt;test_print(); // in test print: number = 6(输出内容) ptr1-&gt;set_number(8); (*ptr1).test_print(); // in test print: number = 8(输出内容) &#125;&#125; // ~Example: 8(输出内容) // 出作用域被析构 测试函数release错误用法release函数是很容易让人误解的函数，一般看到release会想起释放、回收的含义，函数的作用通常就是回收掉申请的资源，但是这里就要注意了，auto_ptr对象的release函数只有释放的意思，指的是释放指针的所有权，说简单点就是auto_ptr的对象与原始的指针脱离关系，但是并不回收原始指针申请的内存，如果不主动释放就会造成内存泄露，就像下面这样： 12345678910111213void test2()&#123; //auto_ptr&lt;Example&gt; ptr2 = new Example(6); // 编译错误，不支持不同指针到智能指针的隐式转换 auto_ptr&lt;Example&gt; ptr2(new Example(6)); // Example: 6(输出内容) if (ptr2.get()) // 判断内部指针的有效性 &#123; ptr2.release(); // 调用release之后会释放内存所有权，但是不会析构，造成内存泄漏 if (!ptr2.get()) cout &lt;&lt; "ptr2 is invalid" &lt;&lt; endl; // ptr2 is invalid(输出内容) ptr2.release(); // 多写一遍没有任何作用 &#125;&#125; 测试函数release正确用法知道了relsease函数的错误用法，那么正确用法也就应该清楚了，需要自己调用delete，话说如果自己调用了delete那还用智能指针干什么，下面展示正常的用法： 123456789101112void test3()&#123; auto_ptr&lt;Example&gt; ptr3(new Example(3)); // Example: 3(输出内容) if (ptr3.get()) // 判断内部指针的有效性 &#123; Example *p = ptr3.release(); // release函数调用之后会释放内存的所有权，并且返回原始指针 if (!ptr3.get()) cout &lt;&lt; "ptr3 is invalid" &lt;&lt; endl; // ptr3 is invalid(输出内容) delete p; // ~Example: 3(输出内容) // 主动析构Example对象 &#125;&#125; 测试函数reset用法reset函数取其字面含义，就是重新设置的意思，也就是给一个指着对象设置一个新的内存对象让其管理，如果设置之前智能指针的已经管理了一个对象，那么在设置之后原来的对象会被析构掉，具体看测试结果： 12345678void test4()&#123; auto_ptr&lt;Example&gt; ptr4(new Example(4)); // Example: 4(输出内容) cout &lt;&lt; "after declare ptr4" &lt;&lt; endl; // after declare ptr4 ptr4.reset(new Example(5)); // Example: 5 // ~Example: 4 cout &lt;&lt; "after function reset" &lt;&lt; endl; // after function reset&#125; 测试函数operator=用法operator=也就是赋值运算符，是智能指针auto_ptr最具争议的一个方法，或者说一种特性，它的种种限制完全来自于这个赋值操作，作为面向的对象中的一部分，如果把一个对象赋值给另一个对象，那么两个对象就是完全一样的，但是这一点却在auto_ptr上打破了，智能指针auto_ptr的赋值，只是移交了所有权，将内部对象的控制所有权从等号的右侧转移到左侧，等号右侧的智能指针丧失对原有内部对象的控制，如果右侧的对象不检测内部对象的有效性，就会造成程序崩溃，测试如下： 1234567891011121314void test5()&#123; auto_ptr&lt;Example&gt; ptr5(new Example(5)); // Example: 5(输出内容) auto_ptr&lt;Example&gt; ptr6 = ptr5; // 没有输出 if (ptr5.get()) cout &lt;&lt; "ptr5 is valid" &lt;&lt; endl; // 没有输出，说明ptr5已经无效，如果再调用就会崩溃 if (ptr6.get()) cout &lt;&lt; "ptr6 is valid" &lt;&lt; endl; // ptr6 is valid(输出内容) ptr6-&gt;test_print(); // in test print: number = 5(输出内容) //ptr5-&gt;test_print(); // 直接崩溃 &#125; 测试auto_ptr类型返回一些文章中指出，auto_ptr不能作为函数的返回值，但是在我的测试环境下，可以正常执行，并且结果正确，但是还是不建议这样做，原因就是operator=，后面统一总结，先看下这个正常的例子： 1234567891011auto_ptr&lt;Example&gt; test6_inner()&#123; auto_ptr&lt;Example&gt; ptr6(new Example(6)); // Example: 6(输出内容) return ptr6;&#125;void test6()&#123; auto_ptr&lt;Example&gt; ptr6 = test6_inner(); // 测试auto_ptr类型返回值 ptr6-&gt;test_print(); // in test print: number = 6(输出内容)&#125; // ~Example: 6(输出内容) // 主动析构Example对 测试auto_ptr作为参数这是常常容易出错的情况，原因还是operator=的操作引起的，因为auto_ptr的赋值会转移控制权，所以你把auto_ptr的对象作为参数传递给一个函数的时候，后面再使用这个对象就会直接崩溃： 1234567891011void test7_inner(auto_ptr&lt;Example&gt; ptr7)&#123; ptr7-&gt;test_print(); // in test print: number = 6(输出内容)&#125; // ~Example: 7(输出内容) // 主动析构Example对象void test7()&#123; auto_ptr&lt;Example&gt; ptr7(new Example(7)); // Example: 7(输出内容) test7_inner(ptr7); // 传递参数 //ptr7-&gt;test_print(); // 直接崩溃&#125; 两个auto_ptr管理一个指针这种错误稍微出现的明显一点，因为智能指针的对象在析构时会回收内部对象的内存，如果两个智能指针同时管理一个内部对象，那么两个auto_ptr对象析构时都会试图释放内部对象的资源，造成崩溃问题： 1234567void test8()&#123; Example *p = new Example(8); // Example: 7(输出内容) auto_ptr&lt;Example&gt; ptr8(p); auto_ptr&lt;Example&gt; ptr9(p);&#125; //~Example: 8(输出内容) // 主动析构Example对象 //~Example: -572662307(输出内容) // 第二次析构崩溃 测试auto_ptr作为容器元素这是一个被广泛讨论的问题，可能你已经猜到了，一般说auto_ptr不能作为容器的元素也是因为operator=操作，但是我在Windows平台上成功运行了下面的代码，并且输出了正常的对象构造信息和析构信息，但是在Linux平台根本就编译不过去，出现大段的编译错误，其中重要的一句就是.../bits/stl_construct.h:73: 错误：对‘std::auto_ptr&lt;Example&gt;::auto_ptr(const std::auto_ptr&lt;Example&gt;&amp;)’的调用没有匹配的函数，其实可以说是operator=的锅，也可以说是拷贝构造函数的锅，但最根本的问题还是赋值时控制权转移导致的，测试代码如下： 123456789void test9()&#123; vector&lt;auto_ptr&lt;Example&gt;&gt; v(10); int i = 0; for (; i &lt; 10; i++) &#123; v[i] = auto_ptr&lt;Example&gt;(new Example(i));// windows下正常构造、析构，linux下无法通过编译 &#125;&#125; 测试auto_ptr的引用作为参数传递这个例子比较正常，就是将auto_ptr的对象进行引用传递，这种方式不会造成控制权转移，所以不会出现问题： 1234567891011void test10_inner(auto_ptr&lt;Example&gt;&amp; ptr10)&#123; ptr10-&gt;test_print(); // in test print: number = 6(输出内容)&#125; // 这里没有析构void test10()&#123; auto_ptr&lt;Example&gt; ptr10(new Example(10)); // Example: 10(输出内容) test10_inner(ptr10); // 传递引用参数 ptr10-&gt;test_print(); // in test print: number = 10(输出内容)&#125; //~Example: 10(输出内容) // 主动析构Example对象 测试auto_ptr的指针作为参数传递这个例子本质上同上个例子一样，就是将auto_ptr的对象的地址传递，这种指针的方式不会造成控制权转移，所以也不会出现问题： 1234567891011void test11_inner(auto_ptr&lt;Example&gt;* ptr11)&#123; (*ptr11)-&gt;test_print(); // in test print: number = 11(输出内容)&#125; // 这里没有析构void test11()&#123; auto_ptr&lt;Example&gt; ptr11(new Example(11)); // Example:11(输出内容) test11_inner(&amp;ptr11); // 传递地址参数 ptr11-&gt;test_print(); // in test print: number = 11(输出内容)&#125; // ~Example: 11(输出内容) // 主动析构Example对象 现象分析上述这些例子比较简单，主要是说明auto_ptr的用法，其中比较有争议的也就是6,7,9三个例子，也就是我们前文所说的“不建议”将auto_ptr作为函数返回值、函数参数、容器内的元素，这三个例子中只有作为函数参数的那个例子崩溃了，但是如果我们调用完函数test7_inner之后，不在使用智能指针ptr7也就不会崩溃了，那么是不是说只要我们注意到可能发生的问题，就可以使用auto_ptr在这些情况呢，目前来看是这样的。 但是为什么在Windows上成功运行的test9在Linux上却编译不过呢？简单点说就是为了怕你犯错，而对你采取管制措施，实际上你可以把auto_ptr作为容器的元素，但是因为这样太容易出错了，所以压根就不允许你这样做。 那么Linux是怎样在编译时期就提示auto_ptr这种错误，而Windows又是怎样绕过这种错误的呢？其实从应用的方便性和安全角度出发，容器应该要求其元素对象的拷贝与原对象相同或者等价，但是很明显auto_ptr做不到这一点，因为它的赋值是实质上是控制权的转移，而不是等价的复制，所以拷贝之后原对象必然被改变，linux版本的auto_ptr就是利用了这一点，使其违反C++的静态类型安全规则，这个版本的auto_ptr只实现构造函数auto_ptr(auto_ptr&amp; other)和赋值函数auto_ptr&amp; operator=(auto_ptr&amp; other)，因为参数都是非const，在构造或者赋值的时候原对象可能会发生变化，所以与容器对元素要求的不符合，这样在编译阶段就会检查出错误，也就是我们上面test9函数中提示的错误.../bits/stl_construct.h:73: 错误：对‘std::auto_ptr&lt;Example&gt;::auto_ptr(const std::auto_ptr&lt;Example&gt;&amp;)’的调用没有匹配的函数，这样就避免了把auto_ptr作为容器的元素。 关于Windows平台上正常运行test9函数的疑惑，实际上可以从两个方面来考虑，一种方式就是放宽容器对元素的要求，也就是说允许容器中的元素赋值之后，原对象被改变；另一种方式就是auto_ptr只提供构造函数auto_ptr(const auto_ptr&amp; other)和赋值函数auto_ptr&amp; operator=(const auto_ptr&amp; other)，这样就就可以通过容器的检测了，但是还有一个问题需要解决，那就是auto_ptr肯定要改变原对象，const类型就没法改变了，其实还有一种神奇的操作叫强制类型转换，使用const_cast就可以改变const对象，这样就达到了使用auto_ptr作为容器元素的目的，具体细节参考: auto_ptr到底能不能作为容器的元素? 前面提到把auto_ptr作为容器元素时很容易出错，这是为什么各个版本的auto_ptr实现的差异会这么大的原因，出错的根本原因就是auto_ptr构造和赋值时控制权的转移，试想一下，对一个容器进行排序，然后提供一个排序函数，然后排序时把容器中的元素传入比较函数，结果容器中元素的内部对象全都被清空了，这显然不是我们想要的，但是如果你不使用类似操作，那么把auto_ptr作为容器元素也没有什么不可。 总结 既然auto_ptr在C++17中已经被移除，那么我们也应该顺应潮流，尽量不使用auto_ptr了。 虽然不建议使用auto_ptr了，但是他的用法和注意事项我们还是应该了解，毕竟存在了这么多年，还有很多老代码中在用着。 由于各平台差异很大，目前auto_ptr作为容器元素不可移植，无论你使用的STL平台是否允许auto_ptr容器，你都不应该这样做。 通过分析发现auto_ptr能不能作为容器的元素并非绝对的，不仅与STL的实现有关，而且与STL容器的需求和安全性以及容器的语义有关。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>auto_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能指针分类及简单特性]]></title>
    <url>%2Fblog%2F2018%2F08%2F06%2F%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88%E5%88%86%E7%B1%BB%E5%8F%8A%E7%AE%80%E5%8D%95%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[前言智能指针的种类繁多，我听说过的就有这些：auto_ptr、shared_ptr、weak_ptr、unique_ptr、scoped_ptr、scoped_array、shared_array、intrusive_ptr，这些智能指针看起来种类繁多，但实际上常用的就只有两三种，他们是shared_ptr、weak_ptr和unique_ptr，先简单了解一下这几个指针，后续再列出具体的例子和选择标准。 分类及特性 auto_ptr 这个指针历经沧桑，C++98中引入，C++11中弃用，C++17中被移除，弃用的原因主要是使用不当容易造成内存崩溃，不能够作为函数的返回值和函数的参数，也不能在容器中保存auto_ptr。 shared_ptr 据说是最好用的智能指针，使用引用计数实现，每使用它一次，内部的引用计数加1，每析构一次，内部的引用计数减1，减为0时，自动删除所指向的堆内存。shared_ptr内部的引用计数是线程安全的，但是对象的读取需要加锁。 weak_ptr 没有什么存在感，基本只在解除 shared_ptr循环引用时使用，weak_ptr没有共享资源，它的构造不会引起指针引用计数的增加，使用weak_ptr的成员函数use_count()可以观测资源的引用计数，使用成员函数lock()从被观测的shared_ptr获得一个可用的shared_ptr对象。 unique_ptr 一种比auto_ptr更加优秀的指针，可以唯一的拥有一个对象，auto_ptr通过等号赋值改变所有权后，再次引用原对象会造成内存崩溃，但是unique_ptr可以用过std::move改变所有权，并且引用原对象会在编译时期就指出错误，同时在容器算法中也可以使用，另有一种说法是说unique_ptr是scoped_ptr在标准库中的一个分身。 scoped_ptr 存在于boost库而非标准库中，要把资源限制在作用域里的，并且永远不能被复制，是一种轻量级的智能指针，和const auto_ptr很像，但是可以被reset，并可以更加清楚地表明意图。 scoped_array 跟scoped_ptr一样，也是独享所有权的，用于管理动态数组，不支持复制，并且初始化的时候需要使用动态数组，没有重载operator*，需要使用get()函数。 shared_array 跟 shared_ptr 一样，内部使用了引用计数，可以复制，通过参数来传递等，需要使用动态数组来初始化。 intrusive_ptr 这是一种侵入式的智能指针，内部不含有引用计数，要求被存储的对象自己实现引用计数功能，不然编译不过，还要提供intrusive_ptr_add_ref和intrusive_ptr_release函数接口供intrusive_ptr调用。 总结 智能指针的种类很多，但是只要掌握shared_ptr、weak_ptr、unique_ptr这三种指针的用法，就可以处理绝大多数问题。 智能指针的选择就根据特性来选，但是auto_ptr尽量不要用了，虽然历史悠久，但是毕竟由于各种诟病被抛弃了。 以上只给出了分类和简单特性，后续有时间会依次给出示例，指出用法和需要注意的点。]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>智能指针</tag>
        <tag>shared_ptr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简述TCP三次握手和四次挥手流程]]></title>
    <url>%2Fblog%2F2018%2F07%2F11%2F%E7%AE%80%E8%BF%B0TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言关于TCP的连接过程，很多从事程序开发的小伙伴应该都听过三次握手，可这三次握手的细节还是有很多人不太清楚的，特别是有些参数记不清楚，我也经常弄错，所以我根据自己的理解画了两张图，将TCP连接和断开的流程简单记录一下，以方便后续查找复习之用。 三次握手 初始状态：客户端A和服务器B均处于CLOSED状态，然后服务器B创建socket，调用监听接口使得服务器处于LISTEN状态，等待客户端连接。（后续内容用A，B简称代替） A首先向B发起连接，这时TCP头部中的SYN标识位值为1，然后选定一个初始序号seq=x（一般是随机的），消息发送后，A进入SYN_SENT状态，SYN=1的报文段不能携带数据，但要消耗一个序号。 B收到A的连接请求后，同意建立连接，向A发送确认数据，这时TCP头部中的SYN和ACK标识位值均为1，确认序号为ack=x+1，然后选定自己的初始序号seq=y（一般是随机的），确认消息发送后，B进入SYN_RCVD状态，与连接消息一样，这条消息也不能携带数据，同时消耗一个序号。 A收到B的确认消息后，需要给B回复确认数据，这时TCP头部中的ACK标识位值为1，确认序号是ack=y+1，自己的序号在连接请求的序号上加1，也就是seq=x+1，此时A进入ESTABLISHED状态，当B收到A的确认回复后，B也进入ESTABLISHED状态，至此TCP成功建立连接，A和B之间就可以通过这个连接互相发送数据了。 四次挥手 初始状态：客户端A和服务器B之间已经建立了TCP连接，并且数据发送完成，打算断开连接，此时客户端A和服务器B是等价的，双方都可以发送断开请求，下面以客户端A主动发起断开请求为例。（后续内容用A，B简称代替） A首先向B发送断开连接消息，这时TCP头部中的FIN标识位值为1，序号是seq=m，m为A前面正常发送数据最后一个字节序号加1得到的，消息发送后A进入FNI_WAIT_1状态，FIN=1的报文段不能携带数据，但要消耗一个序号。 B收到A的断开连接请求需要发出确认消息，这时TCP头部中的ACK标识位值为1，确认号为ack=m+1，而自己的序号为seq=n,n为B前面正常发送数据最后一个字节序号加1得到的，然后B进入CLOSE_WAIT状态，此时就关闭了A到B的连接，A无法再给B发数据，但是B仍然可以给A发数据（此处存疑），同时B端通知上方应用层，处理完成后被动关闭连接。然后A收到B的确认信息后，就进入了FIN_WAIT_2状态。 B端应用层处理完数据后，通知关闭连接，B向A发送关闭连接的消息，这时TCP头部中的FIN和ACK标识位值均为1，确认号ack=m+1，自己的序号为seq=k，（B发出确认消息后有发送了一段数据，此处存疑），消息发送后B进入LACK_ACK状态。 A收到B的断开连接的消息后，需要发送确认消息，这是这时TCP头部中的ACK标识位值为1，确认号ack=k+1，序号为m+1（因为A向B发送断开连接的消息时消耗了一个消息号），然后A进入TIME_WAIT状态，若等待时间经过2MSL后，没有收到B的重传请求，则表明B收到了自己的确认，A进入CLOSED状态，B收到A的确认消息后则直接进入CLOSED状态。至此TCP成功断开连接。 总结 关于三次握手，参考了很多资料说服务器是被动打开连接，对此有些不解，希望知道的朋友给出提示和建议。 关于四次挥手，在我的叙述中有两处存疑，就是B收到的A的主动断开请求后，进入CLOSE_WAIT状态，是否还能发送数据到A，参考了一些资料说A不能发数据给B，但是B能发数据给A，并且A也可以接收，但是无论我在Windows环境测试还是Linux环境下测试这种状态A都无法收到B的数据，不知道我是不是理解错了，希望明白原理的小伙伴能解答一下。 在四次挥手的最后阶段，有一个等待时间2MSL，这个不是一个时间单位，而是一个表明时间段的名词，这段等待时间就是为了在B没收到确认消息时，接收B的重传请求的，如果不等待这一段时间直接进入CLOSED状态，那么B未收到A的确认消息就会发送重传请求，而此时A已经关闭，就不会再给B重传了，其中MSL的是Maximum Segment Lifetime英文的缩写，可简单译为“报文最大生存时间”，也就是说如果B没有收到确认信息，那么在2MSL这段时间内很大概率就会发送重传请求，并且被A收到，RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。 在连接和断开的过程都有提到ACK和ack，这一点要注意区分，大写的ACK代表TCP头部中6个标识位之一，是表明这是个确认报文，而小写的ack拜师确认序号，表明对方发来的数据到ack这个序号前的都已经收到了。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Socket</tag>
        <tag>网络连接</tag>
        <tag>网络断开</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[结构体sockaddr、sockaddr_in、sockaddr_in6之间的区别和联系]]></title>
    <url>%2Fblog%2F2018%2F07%2F10%2F%E7%BB%93%E6%9E%84%E4%BD%93sockaddr%E3%80%81sockaddr-in%E3%80%81sockaddr-in6%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[前言最近在学习网络相关的知识，虽然之前代码写了不少，但是长时间不写难免会忘记，简单地复习了一下IO多路复用的方式，对比了解了一下epoll模式和select模式的异同，不过写代码的时候发现，这个socket连接中有几个结构还是挺让人头大的，用着用着突然就强转成其他的类型了，加上年前改了半天IPv6的连接，这几个结构体更加混乱，所以今天角色放到一起，从源码的角度看一下sockaddr、sockaddr_in、sockaddr_in6这三个结构体之间的联系，以及为什么有些情况可以直接强转。 代码分析 看一下这三个结构的定义，先说明一下版本，操作系统为CentOS，头文件版本应该挺古老了，在’/usr/include/netinet/in.h’ 中发现版权信息：Copyright (C) 1991, 1992, 1994-2001, 2004, 2006, 2007, 2008, 2009, 2010，看着很古老，但之后的版本应该没有改动很大吧，反正不太清楚，我们就分析当前这一个版本吧。 1234567891011121314151617181920212223242526272829303132333435/* /usr/include/bits/socket.h *//* Structure describing a generic socket address. */struct sockaddr&#123; __SOCKADDR_COMMON (sa_); /* Common data: address family and length. */ char sa_data[14]; /* Address data. */&#125;;/* /usr/include/netinet/in.h *//* Structure describing an Internet socket address. */struct sockaddr_in&#123; __SOCKADDR_COMMON (sin_); in_port_t sin_port; /* Port number. */ struct in_addr sin_addr; /* Internet address. */ /* Pad to size of `struct sockaddr'. */ unsigned char sin_zero[sizeof (struct sockaddr) - __SOCKADDR_COMMON_SIZE - sizeof (in_port_t) - sizeof (struct in_addr)];&#125;;/* /usr/include/netinet/in.h */#ifndef __USE_KERNEL_IPV6_DEFS/* Ditto, for IPv6. */struct sockaddr_in6&#123; __SOCKADDR_COMMON (sin6_); in_port_t sin6_port; /* Transport layer port # */ uint32_t sin6_flowinfo; /* IPv6 flow information */ struct in6_addr sin6_addr; /* IPv6 address */ uint32_t sin6_scope_id; /* IPv6 scope-id */&#125;;#endif /* !__USE_KERNEL_IPV6_DEFS */ 看到3个结构的定义想到了什么？只是看着有点像吧，真正的区别我们往下看，其中3个结构里都包含了 __SOCKADDR_COMMON 这个宏，我们先把它的定义找到，最后在’usr/inlcue/bits/sockaddr.h’中找到如下代码， 1234567891011/* POSIX.1g specifies this type name for the `sa_family' member. */typedef unsigned short int sa_family_t;/* This macro is used to declare the initial common members of the data types used for socket addresses, `struct sockaddr', `struct sockaddr_in', `struct sockaddr_un', etc. */#define __SOCKADDR_COMMON(sa_prefix) \ sa_family_t sa_prefix##family#define __SOCKADDR_COMMON_SIZE (sizeof (unsigned short int)) 由此我们知道，这三个结构的第一个字段都是一个unsigned short int 类型，只不过用宏来定义了三个不同的名字，至此第一个结构就清楚了，在一般环境下（short一般为2个字节），整个结构占用16个字节，变量sa_family占用2个字节，变量sa_data 保留14个字节用于保存IP地址信息。 接着我们发现第二个结构中还有in_port_t和struct in_addr两个类型没有定义，继续找下去吧，在文件‘/usr/include/netinet/in.h’发现以下定义 123456789/* Type to represent a port. */typedef uint16_t in_port_t;/* Internet address. */typedef uint32_t in_addr_t;struct in_addr&#123; in_addr_t s_addr;&#125;; 这么看来sockaddr_in这个结构也不复杂，除了一开始的2个字节表示sin_family，然后是2个字节的变量sin_port表示端口，接着是4个字节的变量sin_addr表示IP地址，最后是8个字节变量sin_zero填充尾部，用来与结构sockaddr对齐 现在我们该分析结构sockaddr_in6了，这里边只有一个未知的结构in6_addr，经过寻找发现其定义也在’/usr/include/netinet/in.h’中 12345678910111213141516171819#ifndef __USE_KERNEL_IPV6_DEFS/* IPv6 address */struct in6_addr&#123; union &#123; uint8_t __u6_addr8[16];#if defined __USE_MISC || defined __USE_GNU uint16_t __u6_addr16[8]; uint32_t __u6_addr32[4];#endif &#125; __in6_u;#define s6_addr __in6_u.__u6_addr8#if defined __USE_MISC || defined __USE_GNU# define s6_addr16 __in6_u.__u6_addr16# define s6_addr32 __in6_u.__u6_addr32#endif&#125;;#endif /* !__USE_KERNEL_IPV6_DEFS */ 这个结构看起来有点乱，但是如果抛开其中的预编译选项，其实就是8个字节，用来表示IPV6版本的IP地址，一共128位，只不过划分字节的段数有些不同，每段字节多一点那么段数就少一点，反义亦然。 那接下来我们整理一下，为了看的清楚，部分结构使用伪代码，不能通过编译，主要是方便对比，整理如下 12345678910111213141516171819202122232425/* Structure describing a generic socket address. */struct sockaddr&#123; uint16 sa_family; /* Common data: address family and length. */ char sa_data[14]; /* Address data. */&#125;;/* Structure describing an Internet socket address. */struct sockaddr_in&#123; uint16 sin_family; /* Address family AF_INET */ uint16 sin_port; /* Port number. */ uint32 sin_addr.s_addr; /* Internet address. */ unsigned char sin_zero[8]; /* Pad to size of `struct sockaddr'. */&#125;;/* Ditto, for IPv6. */struct sockaddr_in6&#123; uint16 sin6_family; /* Address family AF_INET6 */ uint16 sin6_port; /* Transport layer port # */ uint32 sin6_flowinfo; /* IPv6 flow information */ uint8 sin6_addr[16]; /* IPv6 address */ uint32 sin6_scope_id; /* IPv6 scope-id */&#125;; 这么来看是不是就清晰多了，由此我们发现结构 sockaddr 和 sockaddr_in 字节数完全相同，都是16个字节，所以可以直接强转，但是结构 sockaddr_in6 有28个字节，为什么在使用的时候也是直接将地址强制转化成(sockaddr*)类型呢？ 强转的可能性其实sockaddr 和 sockaddr_in 之间的转化很容易理解，因为他们开头一样，内存大小也一样，但是sockaddr和sockaddr_in6之间的转换就有点让人搞不懂了，其实你有可能被结构所占的内存迷惑了，这几个结构在作为参数时基本上都是以指针的形式传入的，我们拿函数bind()为例，这个函数一共接收三个参数，第一个为监听的文件描述符，第二个参数是sockaddr*类型，第三个参数是传入指针原结构的内存大小，所以有了后两个信息，无所谓原结构怎么变化，因为他们的头都是一样的，也就是uint16 sa_family，那么我们也能根据这个头做处理，原本我没有看过bind()函数的源代码，但是可以猜一下: 1234567891011121314151617int bind(int socket_fd, sockaddr* p_addr, int add_size)&#123; if (p_addr-&gt;sa_family == AF_INET) &#123; sockaddr_in* p_addr_in = (sockaddr_in*)p_addr; //... &#125; else if (p_addr-&gt;sa_family == AF_INET6) &#123; sockaddr_in6* p_addr_in = (sockaddr_in6*)p_addr; //... &#125; else &#123; //... &#125;&#125; 由以上代码完全可以实现IPv4和IPv6的版本区分，所以不需要纠结内存大小的不同 总结 通过等价替换的方式我们可以更好的了解sockaddr、sockaddr_in、sockaddr_in6之间的异同。 网路接口函数针对于IPv4和IPv6虽然有不同的结构，但是接口基本相同，主要是为了用户（开发者）使用方便吧。 有时间可以看一下bind()、accept()等函数，看看其中对于结构的使用到底是怎样的。]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Socket</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS2015调试dump文件时提示未找到xxx.exe或xxx.dll]]></title>
    <url>%2Fblog%2F2018%2F06%2F23%2FVS%E8%B0%83%E8%AF%95dump%E6%96%87%E4%BB%B6%E6%97%B6%E6%8F%90%E7%A4%BA%E6%9C%AA%E6%89%BE%E5%88%B0Xxxx-exe%E6%88%96xxx-dlll%2F</url>
    <content type="text"><![CDATA[前言游戏开发的过程中，经常会出现客户端宕机的问题，这时候一个小小的dump文件可以记录当时的内存及堆栈情况，对于解决崩溃的问题有巨大的帮助，之前用VS2008的时候调试过dump文件，但是最近客户端升级为VS2015以后，调试dump文件时经常会出现未找到xxx.exe或xxx.dll的情况，之前一直好使的方法现在却行不通了，于是决定找找解决的办法。 问题原因起初尝试过新建dump文件所显示的路径，复制exe或dll到指定路径下，复制dump文件到exe所在路径下都提示找不到，甚至是手动指定dll或者exe文件都无法打开，这就很奇怪了，原来只要把dump文件放在exe所在目录就可以啊，怎么这次不行了呢？终于，经过多次试验之后发现，原来在VS2015上调试dump文件，要求dump文件的版本与产生dump文件的exe或者dll必须一致，也就是说你要调试一个dump文件，就必须找到找到对应版本dll和exe，否则就会提示无法找到xxx.exe或xxx.dll，下面我们来试验一下。 产生dump文件产生dump文件的方法网上很容易找到，如果想测试的话可以自己找一找，也可以使用下面的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include "stdafx.h"#include "Windows.h"#include "DbgHelp.h"int GenerateMiniDump(PEXCEPTION_POINTERS pExceptionPointers)&#123; // 定义函数指针 typedef BOOL(WINAPI * MiniDumpWriteDumpT)( HANDLE, DWORD, HANDLE, MINIDUMP_TYPE, PMINIDUMP_EXCEPTION_INFORMATION, PMINIDUMP_USER_STREAM_INFORMATION, PMINIDUMP_CALLBACK_INFORMATION ); // 从 "DbgHelp.dll" 库中获取 "MiniDumpWriteDump" 函数 MiniDumpWriteDumpT pfnMiniDumpWriteDump = NULL; HMODULE hDbgHelp = LoadLibrary(_T("DbgHelp.dll")); if (NULL == hDbgHelp) &#123; return EXCEPTION_CONTINUE_EXECUTION; &#125; pfnMiniDumpWriteDump = (MiniDumpWriteDumpT)GetProcAddress(hDbgHelp, "MiniDumpWriteDump"); if (NULL == pfnMiniDumpWriteDump) &#123; FreeLibrary(hDbgHelp); return EXCEPTION_CONTINUE_EXECUTION; &#125; // 创建 dmp 文件件 TCHAR szFileName[MAX_PATH] = &#123; 0 &#125;; TCHAR* szVersion = _T("dump_file_v1.0"); SYSTEMTIME stLocalTime; GetLocalTime(&amp;stLocalTime); wsprintf(szFileName, L"%s-%04d%02d%02d-%02d%02d%02d.dmp", szVersion, stLocalTime.wYear, stLocalTime.wMonth, stLocalTime.wDay, stLocalTime.wHour, stLocalTime.wMinute, stLocalTime.wSecond); HANDLE hDumpFile = CreateFile(szFileName, GENERIC_READ | GENERIC_WRITE, FILE_SHARE_WRITE | FILE_SHARE_READ, 0, CREATE_ALWAYS, 0, 0); if (INVALID_HANDLE_VALUE == hDumpFile) &#123; FreeLibrary(hDbgHelp); return EXCEPTION_CONTINUE_EXECUTION; &#125; // 写入 dmp 文件 MINIDUMP_EXCEPTION_INFORMATION expParam; expParam.ThreadId = GetCurrentThreadId(); expParam.ExceptionPointers = pExceptionPointers; expParam.ClientPointers = FALSE; pfnMiniDumpWriteDump(GetCurrentProcess(), GetCurrentProcessId(), hDumpFile, MiniDumpWithDataSegs, (pExceptionPointers ? &amp;expParam : NULL), NULL, NULL); // 释放文件 CloseHandle(hDumpFile); FreeLibrary(hDbgHelp); return EXCEPTION_EXECUTE_HANDLER;&#125;LONG WINAPI ExceptionFilter(LPEXCEPTION_POINTERS lpExceptionInfo)&#123; // 这里做一些异常的过滤或提示 if (IsDebuggerPresent()) &#123; return EXCEPTION_CONTINUE_SEARCH; &#125; return GenerateMiniDump(lpExceptionInfo);&#125;void create_dump()&#123; // 给空指针赋值，使程序崩溃产生 Dump 文件 int *ptr = NULL; *ptr = 101;&#125;int main()&#123; // 加入崩溃dump文件功能 SetUnhandledExceptionFilter(ExceptionFilter); create_dump();&#125; 将上述代码编译成exe文件，然后点击运行就会在exe所在目录产生一个dump文件，例如我产生的dump文件为dump_file_v1.0-20180623-123940.dmp 调试dump文件双击打开刚刚生成的dump文件，会出现如下界面： 点击右侧 “使用 仅限本机 进行调试” 按钮，就会显示出程序崩溃时的堆栈信息和内存情况以及崩溃位置的代码，如下图： 以上是正常的调试情况，接下来不需要改变代码，重新编译一下程序，得到新版本的exe文件，然后双击刚刚的dump文件dump_file_v1.0-20180623-123940.dmp，点击右侧 “使用 仅限本机 进行调试” 按钮，情况就会发生变化，显示结果如下图： 点击 “中断” 按钮，就会出现标题所说的未找到vsDump.exe。在小型转储中未找到 vsDump.exe。 您需要加载二进制文件才能查找当前堆栈帧的源代码。 看到了吧，只要是dump文件不是这个exe产生的，不管源代码是不是一样，结果都会提示找不到exe，至此我们就找到了“VS2015调试dump文件时提示未找到xxx.exe或xxx.dll”的原因。 总结 VS2015调试dump文件时需要保证dump文件和exe、dll版本一致 遇到奇怪的问题可以手动模拟一下，往往可以重现，然后找到具体的原因]]></content>
      <categories>
        <category>VS</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>VS</tag>
        <tag>dump文件调试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作指向类成员的指针需要了解的两个操作符->*和.*]]></title>
    <url>%2Fblog%2F2018%2F05%2F12%2F%E6%93%8D%E4%BD%9C%E6%8C%87%E5%90%91%E7%B1%BB%E6%88%90%E5%91%98%E7%9A%84%E6%8C%87%E9%92%88%E9%9C%80%E8%A6%81%E4%BA%86%E8%A7%A3%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%93%8D%E4%BD%9C%E7%AC%A6-%E5%92%8C%2F</url>
    <content type="text"><![CDATA[前言关于 -&gt;* 这种写法在很早就在项目代码里见过了，并且还写过，不过当时并没有正确的理解这样写的含义，一直到最近发现这样写很奇怪，于是根据自己的理解，开始改代码，发现无论怎么改都无法通过编译，仔细搜索后才发现这是一种固定的写法，也就是说 -&gt;* 是一个操作符，无法拆分，同时还有一个 .* 也是相同的作用，只不过是用于对象上，而 -&gt;* 是用于对象的指针上。 那么这两个操作符究竟有什么作用呢？实际上它们主要用于操作指向类成员的指针，可能你会说指向类成员的指针直接定义就好了，为什么这么麻烦，还要是用这两个操作符呢？接下来我们举几个例子就明白了。 指向类数据成员的指针12345678910111213141516#include &lt;iostream&gt;class C&#123;public: int m;&#125;;int main()&#123; int C::* p = &amp;C::m; // pointer to data member m of class C C c = &#123;7&#125;; std::cout &lt;&lt; c.*p &lt;&lt; '\n'; // prints 7 C* cp = &amp;c; cp-&gt;m = 10; std::cout &lt;&lt; cp-&gt;*p &lt;&lt; '\n'; // prints 10&#125; 看到上述代码中的p指针有什么不同了吧，这是一个指向类成员变量的指针，如果我们不这样定义p也想操作c对象的成员变量m要怎么办呢？我们可以这样写： 123456789101112131415#include &lt;iostream&gt;class C&#123;public: int m;&#125;;int main()&#123; C c = &#123;7&#125;; int *p = &amp;c.m; std::cout &lt;&lt; *p &lt;&lt; '\n'; // prints 7 *p = 10; std::cout &lt;&lt; *p &lt;&lt; '\n'; // prints 10&#125; 这样代码中的变量p就变成了一个简单的指向整型数据的指针，我们也可以通过它访问c对象的m变量，并且给它赋值，但是你有没有发现区别，前一种指针p只依赖于类C的定义，可以在类C创建对象之前就给指针p定义赋值，但是后一种数据指针p就只能在类C创建对象之后才能给它赋值，还有一点，前一种指针p可以根据调用它的对象不同而访问不同类C对象的值，而后一种指针p就只能访问它所指向的那个对象的m值，如果要访问其他对象，需要重新给p赋值。 注意指向类成员指针的定义和赋值方法，是int C::* p = &amp;C::m;，取变量m的地址还有两种写法，&amp;(C::m) 或者 &amp;m这两种写法只能写在类C的成员函数中，所表示的也就是一个简单的指向整型变量的指针，即int*，与 &amp;C::m的含义是大不相同的。 而操作符-&gt;* 和.*在代码中起什么作用呢，我们只看这一句std::cout &lt;&lt; cp-&gt;*p &lt;&lt; &#39;\n&#39;;，其中表达式cp-&gt;*p用到了操作符-&gt;*，根据我的理解这个操作符的作用就是将后面的指针解引用，然后再被前面的对象调用，首先我们看cp是一个指向c对象的指针，如果想访问m变量，可以直接使用cp-&gt;m，假设现在不想这么写，我们有一个指向类C中m变量的指针p，那么直接写成cp-&gt;p肯定是不行的，因为p并不是类C的成员，它只是一个指向类C成员的指针，所以需要将其解引用，转换成真正的成员才能被cp指针引用到，那么*cp其实就是类C中的m，组合到一起就是cp-&gt; *p，这只是理解，其实-&gt;*是一个不可分割的操作符，需要紧挨着写成cp-&gt;*p才能编译通过。 另外关于指向类成员指针，在操作对象是父类对象和子类对象时有什么不同呢?答案是：指向可访问的非虚拟基类的数据成员的指针可以隐式地转换为指向派生类的同一数据成员的指针，反过来结果就是未定义的了，可以参考代码： 1234567891011121314151617#include &lt;iostream&gt;class Base&#123;public: int m;&#125;;class Derived : public Base &#123;&#125;;int main()&#123; int Base::* bp = &amp;Base::m; int Derived::* dp = bp; Derived d; d.m = 1; std::cout &lt;&lt; d.*dp &lt;&lt; '\n'; // prints 1 std::cout &lt;&lt; d.*bp &lt;&lt; '\n'; // prints 1&#125; 指向类成员函数的指针其实前面的例子我在工作中还真没遇到过，但是指向类数据成员的指针确实经常用，熟悉函数指针的工程师都知道，类似于void (*func)(); 就是定义了指向一个无返回值无参数函数的指针，调用时只要写成(*func)();就行，但是如果定义指向类成员函数的指针可就麻烦一点了，接下来看一个例子： 1234567891011121314class C&#123;public: void f(int n) &#123; std::cout &lt;&lt; n &lt;&lt; '\n'; &#125;&#125;;int main()&#123; void (C::* p)(int) = &amp;C::f; // pointer to member function f of class C C c; (c.*p)(1); // prints 1 C* cp = &amp;c; (cp-&gt;*p)(2); // prints 2&#125; 这个例子中的函数指针p是有作用域的，也就是只能指向类C中的无返回值并且有一个整型参数的函数，代码中赋值为&amp;C::f，这个形式与数据成员指针的赋值一样，其实函数f就是类C的一个成员而已。 那么它是怎么通过p指针调用到函数f的呢？我们看一句代码(cp-&gt;*p)(2);其实-&gt;*在这里还是起到了解引用并访问的作用，如果要访问f函数，只要cp-&gt;f(2)即可，但是这里没有f只有一个指向f的指针p，所以将f替换成*p编程cp-&gt;*p(2);但是这样无法通过编译，它无法区分那一部分是函数体，那一部分是参数，所以加个括号指明一下变成(cp-&gt;*p)(2);就可以正常访问f函数了。 实际上对面向对象编程了解的深入一点就会知道，调用对象的成员函数，实际上就是把对象的指针this作为函数第一个参数传进去，比如cp-&gt;f(2)，假如函数f的函数指针是func，那么cp-&gt;f(2)就是调用func(cp, 2)，这样在函数f中就可以调用对象的成员变量或者其他的成员函数了，但是如果你的成员函数中没有访问成员内容，那么这个this指针传什么都可以，也就是说func(cp, 2)和func(0, 2)、func(0x1234567890, 2)都是等价的，在这个例子中就是这样，所有你可以这样来写一段代码：(((C*)0)-&gt;*p)(2)，也是可以打印出数字2的。 另外关于指向类成员函数指针，在操作对象是父类对象和子类对象时与成员变量的规则一致：指向可访问的非虚拟基类的成员函数的指针可以隐式地转换为指向派生类的同一成员函数的指针，反过来也是未定义，可以参考代码： 12345678910111213141516#include &lt;iostream&gt;class Base&#123;public: void f(int n) &#123; std::cout &lt;&lt; n &lt;&lt; '\n'; &#125;&#125;;class Derived : public Base &#123;&#125;;int main()&#123; void (Base::* bp)(int) = &amp;Base::f; void (Derived::* dp)(int) = bp; Derived d; (d.*dp)(1); (d.*bp)(2);&#125; 具体使用前面提到过指向类数据成员的指针我之前真的没用到过，但是指向成员函数的指针，我却用了不少，一般都是放在函数数组中使用，比如有这样一个场景，游戏npc根据状态执行对应的状态函数，这些状态函数是成员函数，为此我们需要将npc所有的状态函数添加到一个函数数组中，假设有idle、run、walk、jump四种状态，下面是实现代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;iostream&gt;class CNpc&#123; typedef void (CNpc::*StateFunction)();public: int state; // 0,1,2,3 对应 idle、run、walk、jump StateFunction state_function_array[4];public: CNpc() &#123; state = 0; state_function_array [0] = &amp;CNpc::idle; state_function_array [1] = &amp;CNpc::run; state_function_array [2] = &amp;CNpc::walk; state_function_array [3] = &amp;CNpc::jump; &#125; void change_state(int new_state) &#123; state = new_state; &#125; void process_state() &#123; if (state_function_array[state]) &#123; (this-&gt;*state_function_array[state])(); // 调用函数指针的地方 &#125; &#125;private: void idle() &#123; std::cout &lt;&lt; "state = idle" &lt;&lt; std::endl; &#125; void run() &#123; std::cout &lt;&lt; "state = run" &lt;&lt; std::endl; &#125; void walk() &#123; std::cout &lt;&lt; "state = walk" &lt;&lt; std::endl; &#125; void jump() &#123; std::cout &lt;&lt; "state = jump" &lt;&lt; std::endl; &#125;&#125;;int main()&#123; CNpc npc; npc.process_state(); npc.process_state(); npc.change_state(1); npc.process_state(); npc.change_state(3); npc.process_state(); npc.process_state(); npc.process_state(); npc.change_state(2); npc.process_state(); npc.change_state(0); npc.process_state(); npc.process_state(); return 0;&#125; 运行结果 123456789state = idlestate = idlestate = runstate = jumpstate = jumpstate = jumpstate = walkstate = idlestate = idle 总结 牢记-&gt;*和.*也是一种操作符，使用的时候不要拆开 理解操作符中的*符号的解引用的作用 如果有理解不正确的地方欢迎大家批评指正]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>类成员访问</tag>
        <tag>成员指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bat批处理替换字符串中匹配的子串]]></title>
    <url>%2Fblog%2F2018%2F04%2F12%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%E6%9B%BF%E6%8D%A2%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E5%8C%B9%E9%85%8D%E7%9A%84%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[前言关于字符串的操作通常是编程生涯中不可避免的，在各种竞赛中、工作中常常能使用到，许多语言中都有专门负责处理字符串的模块或者类，对于字符串的替换一般也有专门的函数，比如Lua中的string.gsub()、Python中的replece()等，那么批处理在进行字符串操作的时候，有没有好用的替换函数呢？ 前两天在使用批处理更新资源文件的时候发现，批处理中也有专门处理字符串替换的方法，并且这是我见到的最有意思的字符串替换方式，就是利用A:B=C的方式来替换字符串，具体含义就是在字符串变量A中查找所有的子串B并且替换成子串C，看起来很有意思吧？下面举一个具体的示例看一下。 代码示例12345678910111213141516171819202122@echo offSET INPUT_PARAM=%1rem 替换输入变量中的world为Chinaecho source string is %INPUT_PARAM%echo === China replace world ===echo replace result is %INPUT_PARAM:world=China%echo.rem 将路径中的反斜杠替换成斜杠SET IMAGE_PATH=C:\NVIDIA\AndroidWorks\001echo source string is %IMAGE_PATH%echo === \ replace / ===echo replace result is %IMAGE_PATH:\=/%echo.echo ABCD:A=apause 代码中举了两个例子，将变量中的world为China、将路径中的反斜杠替换成斜杠都成功地替换了子串的内容，但是我们发现这个的作用对象只能是变量，对于最后一句echo ABCD:A=a并没有发生替换，下面可以看一下运行结果。 运行结果1234567891011E:\batTool&gt;Replace.bat &quot;Hello wolrd, All over the world!&quot;source string is &quot;Hello wolrd, All over the world!&quot;=== China replace world ===replace result is &quot;Hello wolrd, All over the China!&quot;source string is C:\NVIDIA\AndroidWorks\001=== \ replace / ===replace result is C:/NVIDIA/AndroidWorks/001ABCD:A=a请按任意键继续. . . 总结 bat处理字符串替换的方式比较有意思，需要知道A:B=C形式的替换方法 字符串替换只能是针对变量，对于文本貌似不起作用。]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[略显神秘的快速排序]]></title>
    <url>%2Fblog%2F2018%2F04%2F08%2F%E7%95%A5%E6%98%BE%E7%A5%9E%E7%A7%98%E7%9A%84%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[前言继续我的填坑旅程，上次说到《排序算法系列之（二）——冒泡排序名字最为形象的一个》2017-09-16 10:42:07，又过了半年多，终于再一次鼓起勇气决定聊一聊快速排序的思路，不过与冒泡排序不同的是，这个快速排序的名字似乎和算法的思路没有什么关系，这个名字太抽象了，起这个名字可能当初仅仅是因为它比别的排序快一点。咳咳！ 抽象的名字不利于我们对于算法思路的理解，或许这就是我为什么当初认为快速排序是最难理解的排序算法，也可能是当初还没接触过堆排序、希尔排序等这些另类的排序吧！毕竟工作5年之后再来看这个快速排序，思路也是很清晰的，忽然发现它当初那份神秘的气息消散了许多。 快速排序我们今天同样略过各种复杂度，直奔主题——快速排序，既然它的名字不是说算法思路，那就是说性质了，通俗点说就是在一般情况下它比选择排序、冒泡排序要快，先不用关心它为什么快，我们先来模拟一个最简单的快速排序。 快速排序的核心思想是分治、递归，将原本问题的规模不断缩小，递归求解，这类算法往往代码很简单，但是理解起来难度大一点，说一下总体思路，我们先来举个例子。假设将N个数从小到大排序，首先是在等待排序的数组N中随便选一个数M，为了简单我们选择第一个，然后遍历待排数组，把比M小的数放到M的左边，把比M大的数放到M的右边，一次遍历结束M左边有m1个数，右边有m2个数(m1+m2+1=N)，然后就形成了两个待排数组N1和N2，对于每个待排数组重复上述操作，直到待排数组缩小到一个数字，则待排数据排序完毕，整个数组变为有序。 因为这个排序比较抽象，所以前面的橘子、苹果的例子很难解释清楚，但是我们可以用标了号的橙子来理解，是不是感觉橙子伟大了一点，为什么橙子可以，因为早上刚刚吃过橙子，嗯！就是这么任性！假设桌上摆着一排橙子，他们的重量分别是6, 2, 7, 3, 8, 9，什么？你问我重量单位是什么，那就是斤吧，谁叫这些橙子变异了呢，大的大，小的小，好了，能帮助我们理解算法就好了，自从有了转基因，今后多重的橙子都可能遇到。 事先解释一下，我们这些橙子在桌上排成了一排，并且每一个橙子都放在了盘子里，盘子不移动，我们只移动盘子里的橙子，空盘子用*表示，手里的橙子用M表示，为了省点力气，我们尽可能的少移动橙子。 起初桌子上盘子里的橙子情况是这样的:6, 2, 7, 3, 8, 9，M=* 用手拿起第一个盘子里的橙子后：*, 2, 7, 3, 8, 9 ，M=6 从后往前找到第一个比M小的橙子放到前面，9、8、3，发现3是第一个符合条件的，把它拿到前面的盘子，变成了这样：3, 2, 7, *, 8, 9 ，M=6 然后第一个不算从前往后找到第一个比M大的橙子放到后面，2、7，发现7是第一个符合条件的，把它放在后面的空盘子：3, 2, *, 7, 8, 9 ，M=6 到此为止，我们已经把所有位置都遍历一遍了，这就是所谓的一趟排序，如果中间还有位置没有比较，重复步骤3和步骤4，直到所有的位置的橙子都被遍历到，把M=6放到最后的空盘子中就变成了：3, 2, 6, 7, 8, 9 ，M=* 执行到这个步骤，原来的这些橙子就被分成了两部分，比M=6小的放到了它的前面，比M=6大的放到了它的后面，现在就变成了两个规模较小的数组排序，我们以前面的待排数组N1为例，重复步骤2，先取出第一个橙子，拿在手里：*, 2 ，M=3 重复步骤3，从后往前找到第一个比M小的橙子放到前面，发现2这个橙子，然后把它放到前面的空盘子，现在的情况如下：2, * ，M=3 本来应该重复步骤4，但是此时发现所有的位置已经遍历过了，所以步骤4省略，直接步骤5，把M=3放在空盘子中：2, 3 ，M=* 此时被M=3分割的就过只有一部分，并且不大于一个橙子，所以左半部分排序结果，总体来看顺序为：2, 3, 6, 7, 8, 9 ，M=* 接着就要对步骤5后面的右半部分排序了，也就算是对7, 8, 9，虽然现在数据少我们一眼就能看出这结果数是有序的，但是如果在程序代码中，还是会对这部分橙子重复步骤3、4、5来达到有序，这里就不再逐步解释了，最后的结果就是：2, 3, 6, 7, 8, 9 ，M=* 代码实现1234567891011121314151617181920212223242526272829/*功能： 快速排序，实现数组元素从小到大排列参数： array--表示待排序的数组，此处会退化成指针 low --待排序区间的起始索引 high --待排序区间的结束索引返回值：无注意： 只用来表示思路，不考虑指针为空等特殊情况*/void quick_sort(int array[], int low, int high)&#123; if (low &gt;= high) return; int front = low, back = high, key = array[low]; // 选取第一个元素作为中轴 while (front &lt; back) &#123; while (front &lt; back &amp;&amp; array[back] &gt;= key) --back; array[front] = array[back]; // 从后面找到第一个比中轴小的交换 while (front &lt; back &amp;&amp; array[front] &lt;= key) ++front; array[back] = array[front]; // 从前面找到第一个比中轴大的交换 &#125; array[front] = key; quick_sort(array, low, front - 1); // 递归快排前半段 quick_sort(array, low, front + 1); // 递归快排后半段&#125; 代码分析上述代码与橙子排序的示例思路完全一致，key = array[low]是步骤2，选取第一个元素作为中轴；最外层的while循环是反复重复步骤3和步骤4，保证遍历所有位置的橙子；内部的第一个while循环是步骤3，从后面找到第一个比中轴小的；内部的第二个while循环是步骤4，从前面找到第一个比中轴大的；array[front] = key;就是步骤5，把手里的橙子放回到空盘子中；接下来的两个函数调用都是调用自己，也就是递归调用，分别处理小于M的一段和大于M的一段，怎么样？代码是不是好理解多了？如果觉得我理解的有问题或者代码有错，也欢迎大家批评指正。 运行测试快速排序–源码 如果你想运行测试一下，可以复制或者下载源代码，在本机运行测试一下，当然也可以选择在线编辑器，这是我新发现的在线编译器，样子还挺好看的，把源代码复制到网页中运行查看结果。]]></content>
      <categories>
        <category>Sort</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>排序</tag>
        <tag>概念</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神秘莫测的时间复杂度]]></title>
    <url>%2Fblog%2F2018%2F03%2F29%2F%E7%A5%9E%E7%A7%98%E8%8E%AB%E6%B5%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[前言说到时间复杂度，作为程序员或多或少都会有所接触，特别是算法工程师肯定是天天和这个概念打交道，其实前几篇总结排序的文章我一直没有提时间复杂度，因为网上太多的文章讲这个概念了，所以我只总结了一下我对几种排序算法的理解，以及简单的实现代码，而当我今天准备总结一下快速排序的时候，我发现各个关于快速排序的文章都有讲到时间复杂度，有的甚至直接就给出 O(N*log(N))，这个对数是以2为底的，还是以10为底的都没有说清楚，这简直让人感到莫名其妙，所以我决定还是简单说一下我对时间复杂度这个概念的理解。 其实时间复杂的作用很简单，就是用来表示算法运行时间与数据规模的关系，其中的数据规模常常用字母N或者n表示，将算法的运行时间表示为n的函数表达式应该为 t=f(n) ，而时间复杂度就是表达式中幂最高的那一项，然后去掉常数系数，比如当算法运行时间表达式为 t = f(n) = 3 * n^2 + n 时，那么这个算法的时间复杂度就是O(n^2)，字母O用来表示时间复杂度，也就是该算法与数据规模成平方阶的关系，为什么要去掉一次项n，因为在数据规模扩大时，它相对于平方阶来说基本可以忽略不计。 时间复杂度计算方法虽然很多编程工作者都接触过时间复杂度，但是真正要想把时间复杂度算明白可不是件容易的事，特别是遇到时间复杂度中包含对数项的，那就更让人糊涂了，很多人搞不明白为什么会有对数次的运算，实际上二分法常常是导致时间复杂度中出现对数项的一种算法，要算出时间复杂度，本质上就是算出执行完算法算所需要的操作次数，这种操作通常是比较、赋值、交换等等，而时间复杂度与数据规模相关，通常把一个算法执行完所需的操作次数，表示成和数据规模n相关的函数，这就是我们所说的时间复杂度。时间复杂度是用来衡量算法所需时间资源的，所以只保留最高次幂的项就可以，不用纠结于是O(n^2+n)还是O(n^2)，在大规模的数据面前，这两种时间复杂度几乎相等，总之一句话，看一个算法的时间复杂度，就是数这个算法执行完所需要的操作次数，并且把这个次数表示成与n相关的简单函数。 时间复杂度示例接下来我们举几个简单的例子，来理解一下什么是时间复杂度，并且了解一下“计算时间复杂度就是数算法执行操作的次数”这句话的意思，接下来我们一起数一下： 常规方法计算区间[1,n]中所有整数的和 12345int sum = 0, n = 10000;for (int i = 1; i &lt;= n; i++)&#123; sum += i;&#125; 上述代码就是一个很简单的计算方法，具体操作就是遍历区间[1,n]内的所有整，然后依次相加，执行次数很好数吧？就是n次，因为每遍历一个数，就会执行一次累加操作，一共有n个数字，所以执行完这段代码需要进行n次累加运算，随着数据规模的扩大，也就是数字n的扩大，计算次数也在扩大，但是还是n次，所以可以用n来表示这段代码的时间复杂度，也就是O(n)。 利用等差数列公式计算区间[1,n]中所有整数的和 12int n = 10000;int sum = (1 + n) * n / 2; 等差数列的求和公式相信很多人都是知道的，那么这种计算方法对于遍历来说快了太多，因为它是直接计算出来的，也就是1次就能计算出结果，计算次数不会随着数据规模的扩大而扩大，那么这个算法的时间复杂度就是O(1)，也许有的人会纠结这里有加法、乘法、除法，不应该是3次运算吗？实际上就是3次运算，但是常数级的时间复杂度都会用O(1)来表示，它是一个衡量的指标，不需要精确到具体的次数，即使这个常数次数是1000也写成O(1)即可，所以通过这点可以看出，时间复杂度是O(1)的算法未必就比时间复杂度是O(n)的算法计算次数少，比如这个例子中，当n小于3时，第一种算法反而计算的次数少，但是时间复杂度通常是用来衡量数据规模很大的时候，算法所需时间的情况，所以通常情况下O(1)的算法在时间上还是优于O(n)的算法。 利用冒泡排序对所给数组进行排序 12345678910111213void bubble_sort(int array[], int n)&#123; for (int bubble_count = 0; bubble_count &lt; n - 1; ++bubble_count) &#123; for (int bubble_pos = 0; bubble_pos &lt; n - 1 - bubble_count; ++bubble_pos) &#123; if (array[bubble_pos] &gt; array[bubble_pos + 1]) &#123; swap_data(&amp;array[bubble_pos], &amp;array[bubble_pos + 1]); // 交换数据 &#125; &#125; &#125;&#125; 冒泡排序算是排序算法中规则比较简单的了，那么它的时间复杂度怎样来计算呢，或者说怎样来数它的执行次数呢，本例的执行操作次数指的是比较和交换，随着数据规模的扩大，也就只有这些操作次数是跟着变的，那么我们来数一数执行这些操作的次数，首先这是个双重循环，外层循环会遍历n-1次，随着外层循环增多内层循环次数会逐渐减少，听起来很麻烦的，外层和内层都在变化，怎么计算呢？其实可以回归本质，我们看看一共执行了多少次比较运算就可以，第一遍冒泡，内层循环执行了n-1次比较，第二遍冒泡，内层循环执行了n-2次比较，依次类推最后肯定是执行了1次比较，一共比较了多少次是不是就很好计算了，这些次数就是一个等差数列，求和就是这个算法的执行次数，f(n) = (1 + n - 1) * (n - 1) / 2 = n * (n - 1) / 2 = n^2 / 2 - n /2，根据时间复杂度定义，取最高次幂的项去掉系数就得到冒泡排序的时间复杂度O(n^2)。 计算一个十进制数的所有位上（个位、十位、百位…）上1的个数，例如12341这个数中包含2个1 123456789101112int count_one(int n)&#123; int count = 0; while(n &gt; 0) &#123; if ((n % 10) == 1) ++count; n /= 10; &#125; return count;&#125; 这也是一个很简单的算法，我们只要取出每一位上的数字，看看是不是1就可以，如果是1的话统计的变量count就加1就可以，那么这个算法的操作次数与数据规模n有什么关系呢，实际上这次数我们很清楚，就是n一共有几位数，就需要执行几次操作，当数字是34时，我们需要执行两次操作，当数字是24353的时候我们需要执行5次操作，那么怎么把这个次数表示成n的函数呢，仔细想想者原来就是对数，在本例中f(n) = (int)lg(n) + 1，也就是对n取10的对数，然后取整后加1，那么这个算法的时间复杂度就出来了，就是去掉常数项和修饰符变为O(lg(n))。 总结写这篇总结的初衷就是网上太多的算法直接给出了时间复杂度，而缺乏必要的说明，其实时间复杂度的计算并不算太复杂，只要你回归定义的本质，仔细的算算究竟需要多少次操作就可以得到大概的时间复杂度，并且一个算法的时间复杂度也不是固定的，比如快速排序，一般大家都喜欢说它是N乘以对数级的，但是当它的最坏情况发生时，它会退化成平方级的。]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>概念</tag>
        <tag>O(n)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bat批处理遍历指定目录下文件并更新]]></title>
    <url>%2Fblog%2F2018%2F03%2F21%2Fbat%E6%89%B9%E5%A4%84%E7%90%86%E9%81%8D%E5%8E%86%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B%E6%96%87%E4%BB%B6%E5%B9%B6%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[前言先来看这样一个需求，假设有A，B两个目录，其中A目录是资源目录，B目录是工作目录，其中资源目录不定期更新，资源文件都在A目录下，并且没有子目录层级关系，但是资源要被使用时需要更新到B工作目录，B目录根据工程需要建立了一个包含众多子目录的层级体系，这样当A目录中的一个资源文件更新后，需要手动复制A目录中更新的资源文件，然后在B目录中找到对应的位置，然后进行粘贴替换，这样的操作如果很久一次、或者每次只有1、2个文件还好，如果资源文件大范围更新，那么要一个个找到更新文件在B目录中的位置然后替换就成了一件令人苦恼的事情，所以根据这个需求，才有了下面的探索过程。 思路的转变一开始想把A目录作为出发点，毕竟A目录中包含了修改后的资源文件，但是A目录更新后怎样才能准确的修改对应的B目录呢？我想到了配表，每次新增资源后，都会修改配置表，将A目录中的各个文件资源与B目录中的位置建立对应关系，这样A目录下的资源更新后就可以根据配置文件统一更新B目录了。 但这样的做法就是，需要经常维护配置文件，特别是增加或者删除资源的时候，然后我就想到了现在的这个做法，从B目录出发，注意本文主要解决的是资源文件的更新，而不是新增，更新就说明是原有的文件，只是内容发生了变化，比如一些UI文件，这些文件经常会做布局格式的调整，控件的增加和删除等等，调整结束后需要更新到工作目录。 实现过程实现的过程并没有想象的那么顺利，期间遇到了诸多问题和一些新的概念，比如for循环的语法，for循环中的变量定义，if条件的语法，字符串变量的替换，文件目录的处理，延迟环境变量扩展等等，这些问题每一项都可以作为一个单独的知识点，后续我会抽时间慢慢总结到一起，总之最后终于可以用了，前后大约花了1个半小时的时间，想想也是醉了，下面是一个具体的示例及对应的实现代码。 A资源目录对应实际的”E:/dirZ”，结构如下： 12345678910root:[E:/dirZ]+--aaa.txt+--bbb.txt+--ccc.txt+--ddd.txt+--eee.txt+--extra.c+--extra.h+--fff.txt+--ggg.txt B工作目录对应实际的”E:/dirA”，结构如下： 123456789101112131415root:[E:/dirA]+--aaa.txt+--bbb.txt+--dirB| +--ccc.txt| +--extra.c| +--extra.h+--dirC| +--ddd.txt| +--dirD| | +--eee.txt+--dirE| +--dirF| | +--fff.txt| | +--ggg.txt 现在需要把E:/dirZ目录中的txt文件，按照E:/dirA目录的层级结构，更新到对应位置，并且不更新ggg.txt文件，以下是实现的代码: 1234567891011121314151617181920212223242526272829303132333435@echo offrem 启用延迟环境变量扩展setlocal enabledelayedexpansionrem 定义不需要更新的文件SET EXCEPT_FILE=ggg.txtrem 定义工作目录和资源目录SET WORK_PATH=E:\dirA\SET RESO_PATH=E:\dirZ\rem 简单输出查看一下echo WORK_PATH is %WORK_PATH%echo RESO_PATH is %RESO_PATH%echo ------------------------rem for循环递归遍历WORK_PATH目录中的.txt文件，文件的全路径放在变量f中for /R %WORK_PATH% %%f in (*.txt) do ( rem 使用TARGET_FILE变量记录绝对文件名，注意延迟变量的使用 SET TARGET_FILE=%%f echo !TARGET_FILE! rem 去掉路径，只保留文件名及扩展名 SET "FILE_PATH_NO_EXT=%%~nxf" rem 利用资源路径和文件名，拼接出资源的绝对全路径 SET SOURCE_FILE=%RESO_PATH%!FILE_PATH_NO_EXT! echo !SOURCE_FILE! rem 条件判断是否是不需要更新的文件 if NOT !FILE_PATH_NO_EXT!==%EXCEPT_FILE% ( copy !SOURCE_FILE! !TARGET_FILE! ))pause 运行结果 123456789101112131415161718192021222324WORK_PATH is E:\dirA\RESO_PATH is E:\dirZ\------------------------E:\dirA\aaa.txtE:\dirZ\aaa.txt已复制 1 个文件。E:\dirA\bbb.txtE:\dirZ\bbb.txt已复制 1 个文件。E:\dirA\dirB\ccc.txtE:\dirZ\ccc.txt已复制 1 个文件。E:\dirA\dirC\ddd.txtE:\dirZ\ddd.txt已复制 1 个文件。E:\dirA\dirC\dirD\eee.txtE:\dirZ\eee.txt已复制 1 个文件。E:\dirA\dirE\dirF\fff.txtE:\dirZ\fff.txt已复制 1 个文件。E:\dirA\dirE\dirF\ggg.txtE:\dirZ\ggg.txt请按任意键继续. . . 总结到此为止我们就解决了这个资源更新的实际问题，每次资源更新后只要运行这个批处理文件就可以更新工作目录中对应的资源文件了，在这个例子中关于目录的截取，一开始走了很多弯路，其实有很多现成的方式，所以需要在此记录一下，方便以后查找使用，具体查看示例代码： 12345678910111213141516171819202122232425ECHO offSETlOCAL enabledelayedexpansion SET FIND_DIR=E:\dirA\dirC\dirDfor /R %FIND_DIR% %%f in (*.txt) do ( SET FULL_PATH=%%f ECHO 完整的路径: !FULL_PATH! SET FILE_DIR=%%~dpf ECHO 所在的目录: !FILE_DIR! SET FILE_NAME=%%~nf ECHO 无后缀文件: !FILE_NAME! SET FILE_EXT=%%~xf ECHO 文件名后缀: !FILE_EXT! SET "FILE_NAME_NOT_PATH=%%~nxf" ECHO 无路径文件: !FILE_NAME_NOT_PATH! SET "FULL_PATH_NOT_EXT=%%~dpnf" ECHO 无后缀全名: !FULL_PATH_NOT_EXT!)pause 运行结果： 123456完整的路径: E:\dirA\dirC\dirD\eee.txt所在的目录: E:\dirA\dirC\dirD\无后缀文件: eee文件名后缀: .txt无路径文件: eee.txt无后缀全名: E:\dirA\dirC\dirD\eee]]></content>
      <categories>
        <category>bat</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>bat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于游戏中仓库类的设计]]></title>
    <url>%2Fblog%2F2018%2F02%2F25%2F%E5%85%B3%E4%BA%8E%E6%B8%B8%E6%88%8F%E4%B8%AD%E4%BB%93%E5%BA%93%E7%B1%BB%E7%9A%84%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[前言这个游戏中的仓库类设计开始于春节前，和大家一样，我也是期盼着放假而无心工作，所以在放假前一天虽然蹦出了思维的火花，我却没有使用文字记录下来，但是大致的思路我已经记录到脑子中了，这一次的突然感悟，与上次突然明白什么是选择排序，什么是冒泡排序很类似，都是一瞬间突然明白，是一个从量变到质变的过程，接下来简单记录下我关于仓库的理解。 初觉不妥游戏中的仓库是用来存放道具的，这是我在接触这套游戏代码时形成的稳固的印象，结果就是代码中充斥着道具属性的判断，因为是很古老的代码，一开始我并没有产生疑问，同时也是修修补补的解决了许多BUG，可是渐渐的问题暴露了出来，设计上仓库里存储的是道具的索引，通过索引可以找到唯一的一个道具，这个思想根深蒂固，导致在写代码时自然而然的就在仓库的类里直接判断了道具属性，仔细想想这是不正确的。 起初感觉有问题时，大概是工作两年后，第一次重构道具系统的时候，当时在写放入道具和取出道具的时候总感觉怪怪的，但是又说不出问题出在哪里，其实就是在放入和取出的逻辑中，操作了道具的属性，修改了道具的坐标。也就是在仓库类的代码中设置了道具的属性，但是他们两个类不是依赖关系，硬生生的产生了依赖关系。 新的任务道具系统的第一次重构，我并没有找到为什么代码怪怪的，也就没有修改，但是新的任务在工作4年之后给了我一个新的机会，再写一遍道具系统，这时候那段奇怪的代码给我的感觉更强烈了，绝对有问题，也就是那么一瞬，我似乎明白了，仓库这个类被我们强加了太多的东西，谁说仓库中就一定要放道具了，我们在游戏中也没有直接把道具的对象保存在仓库中，而是把道具的索引存在了仓库中，也就是仓库中存储了道具的身份证，同理如果我们把人的身份中存在仓库中，那么仓库就是管理人的，如果我们把车牌号存储在仓库中，那么仓库就是管理车辆的。 因为起初游戏中的仓库只保存了道具的索引，所以我们想当然的认为仓库中只能保存道具，所以把一大堆的道具操作代码写到了仓库类中，是时候把代码提出来了，仓库就是仓库，它只根据坐标存储对应数据的ID，而这个ID对应的数据，应该在仓库以外的类中操作，这个ID可能对应道具、可能对应人、也可能对应车辆，干干净净的仓库管理了一组数据的ID，至于对ID对应数据的操作，一概不应该放在仓库类中进行。 重构的结果仓库类简简单单，保存着道具ID，只提供按位置放入ID，按位置取出ID，能够给出仓库的使用情况，能够初始化仓库的状态，仓库类有以上这些操作足以，仓库本身并不应该知道自己存的是道具还是车辆，真正要修改道具的属性，或者要查找指定属性的道具，放到道具管理类中来编写逻辑即可。]]></content>
      <categories>
        <category>Thinking</category>
      </categories>
      <tags>
        <tag>游戏</tag>
        <tag>架构设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim、Xshell、远程终端莫名卡死的原因]]></title>
    <url>%2Fblog%2F2018%2F02%2F03%2FVim%E3%80%81Xshell%E3%80%81%E8%BF%9C%E7%A8%8B%E7%BB%88%E7%AB%AF%E8%8E%AB%E5%90%8D%E5%8D%A1%E6%AD%BB%E7%9A%84%E5%8E%9F%E5%9B%A0%2F</url>
    <content type="text"><![CDATA[前言最近通过Xshell连接远程服务器，然后使用Vim修改文件时会莫名其妙的卡死，一开始我还没注意，因为近期的网络一直不太好，经常出现按下一个字母半天才反应过来的情况，所以我没有太在意，直接关闭终端重新打开就好。直到有一天我开着两个终端的时候，Vim又卡着不动了，而另一个终端还以流畅的处理我敲击的命令，我就断定这肯定不是网络原因了。 原因既然是Vim卡住了那就查查Vim本身有什么BUG吧，结果上网搜了一圈发现原来是远程终端的问题，根本就不关Vim的事，它只是躺着中枪了而已(*^▽^*)，实际上就是不小心按下了快捷键 Ctrl+S 导致的，为什么常常是Vim卡住呢？那是因为很多人习惯了在 Windows上 的保存快捷键，写写文档总是习惯性按下快捷键 Ctrl+S 保存一下，来避免程序突然崩溃导致文档丢失，这就解释了为什么出问题的总是Vim，因为使用Vim编辑文本有时会习惯性的按下 Ctrl+S 保存，而在执行Shell命令是很小的概率会按 Ctrl+S，所以大多数人卡住往往是在使用Vim的时候。 可是快捷键 Ctrl+S 为什么会导致终端卡死呢？实际上这个快捷键的含义是“阻断向终端输出内容”，很多人说这个快捷键的作用是暂停终端，我个人感觉这种说法并不准确，实际是上终端并没有暂停，按下 Ctrl+S快捷键后，你依然可以像终端发送命令，终端也会正常执行，只是不会将反馈内容和结果显示在终端上而已，这个特性可以用来暂停显示快速滚动输出的内容，比如在编译大型项目的时候。 解决办法解除这种状态的方法很简单，按下快捷键 Ctrl+Q 就可以“恢复向终端输出内容”，只是很多时候我们并不知道，以为是终端卡死了然后错杀了程序！ 附注关于这个问题，Vim文档中“SECTION 32 - VIM ON UNIX”一节也给出了回答，有兴趣的小伙伴可以自己看一下： 32.1. I am running Vim in a xterm. When I press the CTRL-S key, Vim freezes. What should I do now? vimdoc.sourceforge.net]]></content>
      <categories>
        <category>Vim</category>
      </categories>
      <tags>
        <tag>Vim</tag>
        <tag>Xshell</tag>
        <tag>终端卡死</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim中简单格式化代码]]></title>
    <url>%2Fblog%2F2018%2F02%2F02%2FVim%E4%B8%AD%E7%AE%80%E5%8D%95%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[前言写这个总结的起因是我在把Windows上VS中的代码粘贴在Linux服务器的Vim中时，代码格式惨不忍睹，我就搞不明白为什么它每一行都要向后缩进，搞得我的代码最后像倒立的楼梯似的，就像这样： 123456789int main(int argc, char *argv[])&#123; int i=0,j=0; if (i != 0) &#123;i++; &#125; else &#123;i--; &#125; for(i=0;i&lt;5;i++)j++; printf("i=%d,j=%d\n",i,j); return 0;&#125; 往常我一般就是切换到插入模式，然后使用删除键删除掉前面多余的空格，可是这一次我决定不再忍受了，我要找到快速格式化的方法，还别说，方法其实很简单，各种格式化方法的核心就是符号=。 何谓“简单”其实一开始我想标题的时候并没有加上“简单”二字，直到我发现了一个求知者看似“无理”的要求，他要求在Vim中把上面格式的代码格式化成下面这样： 123456789101112131415161718int main(int argc, char *argv[])&#123; int i=0,j=0; if (i != 0) &#123; i++; &#125; else &#123; i--; &#125; for(i=0;i&lt;5;i++) j++; printf("i=%d,j=%d\n",i,j); return 0;&#125; 如果你也是这样想的，很抱歉，你可以关掉这个页面了，本文提供的方法无法满足你的要求，这就是我的标题中为什么加上了“简单”二字，而Vim中的简单格式化只能是格式化成下面这样，以行为单位，保证每行的缩进都是正确的： 123456789int main(int argc, char *argv[])&#123; int i=0,j=0; if (i != 0) &#123;i++; &#125; else &#123;i--; &#125; for(i=0;i&lt;5;i++)j++; printf("i=%d,j=%d\n",i,j); return 0;&#125; 如果你确实要把大括号的换行也显示正确，那么只能通过安装插件、编写脚本、或者把源代码中对应的位置敲如回车，变成下面这样格式的代码，然后再使用本文后面叙述的方法来格式化就可以了。 1234567891011121314151617int main(int argc, char *argv[])&#123; int i=0,j=0; if (i != 0) &#123; i++; &#125; else &#123; i--; &#125; for(i=0;i&lt;5;i++) j++; printf("i=%d,j=%d\n",i,j); return 0;&#125; 神奇的=其实格式化的核心内容就是这个 =，其中绝大部分的方法都是 = 的变种，只是让人不容易发觉，甚至有些方法例如 gg=G 包装的让人都无法注意到真正起作用的就是那个 =，格式化的前提是处于命令模式，也就是按完 ESC 时的模式，而格式化时 = 真正起作用的只有两种情况： 先按=，再选区域 先说应用最广泛的全文格式化的方法gg=G，就是这种情况的变种，分析一下命令的含义，先是gg表示回到文档最开始，= 表示要格式化，G 表示到文档末尾，也就是说 gg=G 的含义就是： 跳到文档开头-&gt;开始格式化-&gt;一直格式化到文档末尾 既然明白了原理，假如此时光标就在文档开始处，那么使用命令 =G 也是可以格式化全文的，同理命令 G=gg也可以达到格式化全文的效果，而命令 =100j 就是从文档当前位置向下格式化100行。 先选区域，再按= 这种方式我反正用不习惯，不过也说一下，就是先按 v (可视化编辑)或 shift+v (可视化编辑行模式)或 ctrl+v (可视化编辑块模式)，然后利用方向键 h,j,k,l 选择区域，最后按 = 完成格式化，简单操作例如 vjjj= 就是从当前位置向下格式化3行代码。 直接输入== 不是说两种情况吗，为什么会有第3条呢？其实在命令模式下输入 == ，也就是连着输入两个等号，就是格式化当前行的方法，我感觉它和上两种情况一样，可能是又不知道归入哪一种情况比较好，所以就单列出来咯。 总结 本文中所提到的格式化代码只是很简单的格式化，以行为单位保证缩进正常，无法处理大括号换行等情况。 如果要挑起“大括号换行”的战争，麻烦装一个格式化插件吧，Vim只和Emacs打架，不想参与“大括号换行”战争。 如果要部分格式化，首先保证要格式化的代码之前的内容是格式化好的，否则格式化无效，请选择全文格式化吧！]]></content>
      <categories>
        <category>Vim</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>Vim</tag>
        <tag>代码格式化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql查询时case when语句的使用]]></title>
    <url>%2Fblog%2F2018%2F02%2F01%2FMysql%E6%9F%A5%E8%AF%A2%E6%97%B6case-when%E8%AF%AD%E5%8F%A5%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言前几天在技术论坛论坛上发现一个求助帖，大体意思就是要把一个表中的数据按条件分成两类，每一类排序方式不同，然后整体作为查询的结果集，乍一看这问题不是很难，很多人给出的答案是分别查询排序后再 union合并到一起，但是后来楼主明确指出不想使用 union 操作，这时有一位高人巧用 case when 语句解决了问题，其实这是我第一次接触 case when 语句，于是查询了一下具体用法，在此做个小结，方便日后查询使用。 创建示例表格数据库表格结构很简单，马上要期末了，就以学习成绩为数据来建立一张数据表，表中包含唯一ID、学号、姓名、性别、分数等列，其中性别这一列用整数代表，0表示男，1表示女，建立表格的sql语句如下： 123456789CREATE TABLE `grade` ( `id` int(4) NOT NULL AUTO_INCREMENT, `number` int(4) NOT NULL DEFAULT '0', `name` varbinary(32) NOT NULL DEFAULT '', `sex` int(4) NOT NULL DEFAULT '0', `score` int(4) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `find_index` (`number`,`name`)) ENGINE=InnoDB DEFAULT CHARSET=binary ROW_FORMAT=DYNAMIC; 插入测试数据表格建立完成可以使用可视化工具或者insert语句插入测试数据，插入测试数据后查询结果如下： 123456789101112131415161718mysql&gt; select number,name,sex,score from grade;+----------+----------+-----+-------+| number | name | sex | score |+----------+----------+-----+-------+| 20180001 | xiaoming | 0 | 68 || 20180002 | xiaohong | 1 | 98 || 20180003 | xiaobing | 0 | 78 || 20180004 | xiaoli | 0 | 88 || 20180005 | zhangsan | 0 | 32 || 20180006 | zhaosi | 0 | 58 || 20180007 | marry | 1 | 78 || 20180008 | tom | 0 | 100 || 20180009 | feifei | 1 | 90 || 20180010 | lili | 1 | 92 || 20180011 | xiaozhao | 0 | 52 || 20180012 | xiaowang | 0 | 62 |+----------+----------+-----+-------+12 rows in set (0.00 sec) 获取平均成绩班主任们坐在一起做喜欢做的事就是比一下自己的学生和别人班的差距，谁让他们每个人带的学生都是一届不如一届呢！（你们是我带过的学生中最差的一届！！！）说到比成绩一般都是比较并均分，sql语句可能会写成下面这样： 1234567mysql&gt; select avg(score) as 平均分 from grade;+-----------+| 平均分 |+-----------+| 74.6667 |+-----------+1 row in set (0.02 sec) 是的，很简单就能获得班级的平均分，如果要分组呢？比如分别查一下男生和女生的平均分，因为我们知道表中的sex表示性别，所以直接按照sex分组就可以实现，可以将语句简单写成这样： 12345678mysql&gt; select sex as 性别, avg(score) as 平均分 from grade group by sex;+--------+-----------+| 性别 | 平均分 |+--------+-----------+| 0 | 67.2500 || 1 | 89.5000 |+--------+-----------+2 rows in set (0.00 sec) 是不是很简单？可是性别显示成0和1确实不利于阅读，但是表中又没有保存0、1与男、女的对应关系，应该怎么办呢？这就要用到我们今天所要用到的case when语句了，语法上共有两种写法，看着具体例子体会一下吧。 case when 语句的使用 第一种用法：case后面跟列名，when后面跟对应值 12345CASE case_value WHEN when_value THEN statement_list [WHEN when_value THEN statement_list] ... [ELSE statement_list]END 这种用法正好解决我们刚刚提出的问题，当sex值为0时当前列显示“男”，否则显示“女”，sql写法如下： 123456789mysql&gt; select (case sex when 0 then '男' else '女' end) as 性别, avg(score) as 平均分 -&gt; from grade group by sex;+--------+-----------+| 性别 | 平均分 |+--------+-----------+| 男 | 67.2500 || 女 | 89.5000 |+--------+-----------+2 rows in set (0.00 sec) 第二种用法：case后面空白，when后面跟着判断条件 12345CASE WHEN search_condition THEN statement_list [WHEN search_condition THEN statement_list] ... [ELSE statement_list]END 针对于这种写法，我们考虑这样一种需求，学生成绩是有评分的，大于等于90分的学生是A，小于90分大于等于60分的学生是B， 其余的学生是C，现在要查询评分为A、B、C的学生成绩的平均分分别是多少，因为成绩评分并不是单独的一列，所以不能简单的 使用 group by 来分组实现了，但是可以利用 case when 语句实现，写起来也很简单，看看下面的sql语句就知道了！ 12345678910mysql&gt; select (case when score &gt;= 90 then 'A' when score &lt; 60 then 'C' else 'B' end) as 等级, -&gt; avg(score) as 平均分 from grade group by 等级;+--------+-----------+| 等级 | 平均分 |+--------+-----------+| A | 95.0000 || B | 74.8000 || C | 47.3333 |+--------+-----------+3 rows in set (0.00 sec) 总结 case when 语句共有两种写法，使用时要区别两种用法的差异。 使用 case when 语句可以实现修改数值的对应关系，还可以按照复杂的条件进行分组。 关于 case when 语句的详细用法，有兴趣的同学可以参考一下官方文档：13.6.5.1 CASE Syntax]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>分组查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql验证联合索引的最左原则]]></title>
    <url>%2Fblog%2F2018%2F01%2F29%2FMysql%E9%AA%8C%E8%AF%81%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95%E7%9A%84%E6%9C%80%E5%B7%A6%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[前言如果你接触过数据库，应该听说过在数据库表的某些列上建立索引能够加快查找速度，如果研究更深入一点的人，可能还听说过联合索引，那么索引为什么能够加快查找速度呢？联合索引究竟又是什么呢？下面说说我的简单理解。 索引试想一下，把1~10000这1万个数字打乱顺序存储在数组中，如果要找到5000这个数字在哪，那就得从数组第0个元素开始，依次遍历找到5000这个数字所在的位置，运气好了1次就能找到，运气不好需要查询1万个数，可是如果把这1万个数作为map的key，每个数存在数组中的位置作为value，存储在map结构中很快就能找到，通常情况下要比直接遍历快的多。 其实这里的map充当的是一个索引的作用，我们知道map存储数据时使用树形结构，会根据要查找的值和当前节点比较，来确定继续查找左分支还是右分支，而数据库中的索引充当的也是这样的作用，mysql中的索引是BTree结构（多路搜索树），就是利用建立索引的列中的所有值建立了一棵树，通过有序的树形查找一般要比全局搜索快多了吧！ 联合索引简单了解了一下索引的含义，那么什么是联合索引呢？其实mysql数据库中的索引不止可以建立在一个列上，它可以将一个索引同时建立在说多个列上，也就是我们所说的联合索引，联合索引的作用特别大，有时会超过单列索引，至于什么时候建立单列索引，什么时候建立联合索引同样是个很复杂的问题，在此不做描述。有兴趣的读者可以自行搜索一下。 最左原则当你在多个列上建立一个索引时，怎样的查找才能利用索引加快速度呢？说到这我们先建立一个带有索引的表格，具体的分析一下什么叫做索引的最左原则。 123456789CREATE TABLE IF NOT EXISTS `test_index`( `id` int(4) NOT NULL AUTO_INCREMENT, `a` int(4) NOT NULL DEFAULT '0', `b` int(4) NOT NULL DEFAULT '0', `c` int(4) NOT NULL DEFAULT '0', `data` int(4) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `union_index` (`a`,`b`,`c`))ENGINE=InnoDB ROW_FORMAT=DYNAMIC DEFAULT CHARSET=binary; 分析上述建表语句，创建了一个名为test_index 的数据库表格，然后在a、b、c三列上建立了联合索引，索引名字为union_index，而最左原则指的就是当你建立了这样一个索引的时候，等于建立了(a)、 (a,b)、 (a,b,c)三个索引，通过条件 (a), (a,b), (a,b,c) 这三种条件查询的时候都可以利用索引加快速度，所以在建立索引的时候要把最常用的条件列放到联合索引的最左边，接下来我们来验证一下，工具就是mysql自带的explain命令。 测试版本 Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 13Server version: 5.7.21-log MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. 验证过程 首先以列a作为条件查询数据，我们看到 type: ref 表示引用查找， key_len: 4 表示索引长度为4，也就说明利用了索引来进行查找 123456789101112131415mysql&gt; explain select data from test_index where a = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.01 sec) 然后以列b作为条件查询数据，可以看到type: ALL表示全表查找， key_len: NULL 表示没有索引，也就说明如果只使用b作为查询条件，不能利用索引来加快查找速度 123456789101112131415mysql&gt; explain select data from test_index where b = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 接着以列c作为条件查询数据，可以看到type: ALL表示全表查找， key_len: NULL 表示没有索引，情况与用b作为条件一样，只使用c作为查询条件也不能利用索引来加快查找速度 123456789101112131415mysql&gt; explain select data from test_index where c = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 现在来测一下使用a、b作为条件的情况，我们看到 type: ref 表示引用查找， key_len: 8 表示索引长度为8，也就是说我们利用上了a、b联合索引来进行查找 123456789101112131415mysql&gt; explain select data from test_index where a = 1 and b = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 紧接着来测一下使用a、c作为条件的情况，我们看到 type: ref 表示引用查找， key_len: 4 表示索引长度为4，这就奇怪了，按照最左原则来说，a、c上是不会建立索引的，为什么会有索引长度呢？其实与a、b上的索引一比较我们就能发现，a、c上的索引长度只有4，而且单独的c上是没有索引的，所以4字节长度的索引只能是a上的，也就是说这种情况我们只使用了a列上的索引来进行查找 123456789101112131415mysql&gt; explain select data from test_index where a = 1 and c = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 为了进一步验证上面的想法，这一次测一下使用b、c作为条件的情况，我们看到 type: ALL 表示全表查找， key_len: NULL 表示没有索引可以使用，按照最左原则来说，b列上没有索引，c列上也没有索引，同时b、c的上也不存在联合索引，所以使用b、c作为查询条件时无法利用联合索引 123456789101112131415mysql&gt; explain select data from test_index where b = 1 and c = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 716173 filtered: 1.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 测试完两个条件的情况，接下来测试一下使用a、b、c作为条件的情况，我们看到 type: ref 表示引用查找， key_len: 12 表示索引长度为12，这完全符合联合索引的最左原则，同时使用3个条件查询可以利用联合索引 123456789101112131415mysql&gt; explain select data from test_index where a = 1 and b = 1 and c = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 12 ref: const,const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 测试进行到现在，我测试了所有的情况吗？不是的！还可以颠倒顺序啊，我原来一直以为联合索引是有顺序的，结果测试后才发现，利用索引的条件符合“交换律”，也就是下面这种情况也能利用a、b上的联合索引，索引长度为8 123456789101112131415mysql&gt; explain select data from test_index where b = 1 and a = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 8 ref: const,const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 再来试试这种情况，按照最左原则，c上没有建立索引，a上有索引，c、a没有建立联合索引，所以只能使用a上的索引进行查找，结果索引长度只有4，验证了我们的想法，联合查询条件使用索引时满足“交换律” 123456789101112131415mysql&gt; explain select data from test_index where c = 1 and a = 1\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: test_index partitions: NULL type: refpossible_keys: union_index key: union_index key_len: 4 ref: const rows: 70 filtered: 10.00 Extra: Using index condition1 row in set, 1 warning (0.00 sec) 接下来几种交换顺序的情况(c,b)、(a,c,b)、(c,b,a)等，大家可以自己进行验证，到此为止，mysql联合索引的最左原则也就验证结束了！ 总结 联合索引的最左原则就是建立索引KEY union_index (a,b,c)时，等于建立了(a)、(a,b)、(a,b,c)三个索引，从形式上看就是索引向左侧聚集，所以叫做最左原则，因此最常用的条件应该放到联合索引的组左侧。 利用联合索引加速查询时，联合查询条件符合“交换律”，也就是where a = 1 and b = 1 等价于 where b = 1 and a = 1，这两种写法都能利用索引KEY union_index (a,b,c)。 遇到这种不确定的问题还是需要实际测试一下，简单的调整一下索引顺序可能会极大的提升效率哦！]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>实用工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python统计指定号码的历史中奖情况]]></title>
    <url>%2Fblog%2F2018%2F01%2F11%2FPython%E7%BB%9F%E8%AE%A1%E6%8C%87%E5%AE%9A%E5%8F%B7%E7%A0%81%E7%9A%84%E5%8E%86%E5%8F%B2%E4%B8%AD%E5%A5%96%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[前言生活在寸土寸金的帝都，买房变成了一件可遇而不可求的事情，作为一个朝九晚九挣工资的人来说，买房或许只是出现在梦里，但是梦想总是要有的，万一实现了呢？其实真正赚钱的方式都明明白白地写在了刑法里，可是咱可是本分人，不能做哪些违法的事情，想要赚钱买房，还有一种比较随意的方式——买彩票，如今这帝都的房价，不是你中一张500万彩票就能买得起的，如果要买的起，那就需要中两张！ 经历中两张500万的彩票，想想就知道这种概率有多大了，跟你被天上掉下的陨石砸中脑袋差不多，是不是感觉没有希望了，不过不要灰心嘛，万一你要被砸中了呢？ 对于预测彩票号码这件事情，很抱歉，我不是神仙，没有方法可以办到，不过有件事你需要知道一下：那就是双色球自从问世以来，十几年间从未出现过一次号码完全相同的情况，也就是说你买个和历史号码一样的，基本中不了大奖了，但是也不一定，万一就有两颗陨石同时砸中你呢! 对于买彩票这件事，有的人喜欢买一个号，坚持多年从不动摇，一心做着发财梦；而有的人却是很随意，每天机选，靠天吃饭；还有一部分大神就比较高端了，每天窝在彩票投注站里，写写画画，仿佛可以窥探天机一样，每期少则几十，多则几百的砸着自己的血汗钱，我劝你们还是醒醒吧。 而我呢，就属于半个第一种人，坚持着一个号，做着发财梦，偶然间看到彩票投注站就买一张，遇不到就算了，典型的佛系彩票购买者，这样买了一段时间，忽然有个想法，我买的这个号到底在历史上中没中过大奖呢？于是作为程序猿的我决定写个程序查一下不就好了，所以才有了下面这段代码。 代码 引入库函数，其实需要的函数特别简单，就是要处理csv格式的双色球历史开奖数据。 1import csv 定义奖项，也就是中奖号码情况与奖项的对应关系，注意中一个篮球是6等奖哟！ 1234567891011121314151617# list award classifyaward_classify = &#123; (6,1): 1, (6,0): 2, (5,1): 3, (5,0): 4, (4,1): 4, (4,0): 5, (3,1): 5, (3,0): 0, (2,1): 6, (2,0): 0, (1,1): 6, (1,0): 0, (0,1): 6, (0,0): 0&#125; 定义比对函数，这个函数要能判断出我的号码跟一个历史号码相比，中了几个红球和蓝球。 123456789101112131415161718# count red balls and blue ballsdef count_red_blue_balls(my_number, history_number): red_count, blue_count = 0, 0 my_index, history_index = 0, 0 while my_index &lt; 6 and history_index &lt; 6: if my_number[my_index] == history_number[history_index]: my_index += 1 history_index += 1 red_count += 1 elif my_number[my_index] &lt; history_number[history_index]: my_index += 1 else: history_index += 1 if my_number[6] == history_number[6]: blue_count = 1 return red_count, blue_count 查询历史中奖情况，使用我选择的号码和历史开奖情况逐一比对，得到每个奖项中奖次数。 123456789# count award situation of my numberdef count_award_situation(my_number): local_award_statistics = [0,0,0,0,0,0,0] with open('lottery_history_data.csv', 'r') as file: data_content = csv.reader(file) for row_data in data_content: local_award_statistics[award_classify[count_red_blue_balls(my_number, list(map(int, row_data[1:8])))]] += 1 return local_award_statistics 展示查询结果，将统计结果以友好的方式呈现。 1234567# show award statictics for a numberdef show_award_result(my_number, award_statistics): print("my number is %s\n" % my_number) print("history award record list:") for index in range(0,7): print("award %d: %4d times" % (index, award_statistics[index])) 启动函数，读取自己定义的号码，然后进行统计 123456789# main functionif __name__ == '__main__': #my_number = [5,6,10,11,25,30,11] #my_number = [5,8,10,15,26,30,6] print("请输入6个红球和1个蓝球号码，空格分隔：") my_number = list(map(int, input().split())) award_statistics = count_award_situation(my_number) show_award_result(my_number, award_statistics) 运行结果 总结 这段代码只是一时好奇的产物，所以说好奇心带来了生产力。 统计代码只是做简单使用，所以一些特殊情况并未判断，比如输入字母或者未排序的数字。 看到我的号码连个4等奖都没有中过，不知道是该高兴还是难过，是不是这个大奖在等着我啊！ 说到这也该结束了，不好意思上周六(20180113)买的双色球又中了6等奖，明天(20180121)要去领奖喽！ 源码代码传送门(附双色球历史数据)：一触即达]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>实用工具</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python查找文件中包含中文的行]]></title>
    <url>%2Fblog%2F2018%2F01%2F06%2FPython%E6%9F%A5%E6%89%BE%E6%96%87%E4%BB%B6%E4%B8%AD%E5%8C%85%E5%90%AB%E4%B8%AD%E6%96%87%E7%9A%84%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[前言近几天在做多语言版本的时候再次发现，区分各种语言真的是一件比较困难的事情，上一次做中文提取工具的就花了不少时间，这次决定用python试一试，结果写起来发现真是方便不少，自己整理了一下方便以后查找使用。 代码123456789101112131415161718192021222324#!/usr/bin/env python3# -*- coding: utf-8 -*-# find the line of containing chinese in files__author__ = 'AlbertS'import redef start_find_chinese(): find_count = 0; with open('ko_untranslated.txt', 'wb') as outfile: with open('source_ko.txt', 'rb') as infile: while True: content = infile.readline() if re.match(r'(.*[\u4E00-\u9FA5]+)|([\u4E00-\u9FA5]+.*)', content.decode('utf-8')): outfile.write(content) find_count += 1; if not content: return find_count# start to findif __name__ == '__main__': count = start_find_chinese() print("find complete! count =", count) 文件 输入：source_ko.txt 3 캐릭터 Lv.50 달성8 캐릭터 Lv.80 달성10 캐릭터 Lv.90 달성……2840 飞黄腾达4841 同归于尽8848 캐릭터 Lv.50 달 输出：ko_untranslated.txt 2840 飞黄腾达4841 同归于尽 总结 其实这段小小的代码中包含了两个常用的功能，那就是读写文件和正则表达式。 这也是两个重要的知识点，其中with操作可能防止资源泄漏，操作起来更加方便。 正则表达式可是一个文字处理的利器，代码中的正则可能还不太完善，后续我会继续补充更新。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2018%2F01%2F05%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy Picture Watermark1?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9hbGJlcnRnaXRodWJob21lLmdpdGh1Yi5pby9ibG9nL2Fib3V0,size_18,color_FFFFFF,t_70#pic_center) Fans aticlehttps://blog.csdn.net/albertsh/article/details/100594143 https://blog.csdn.net/albertsh/article/details/100540338 !-- https://blog.csdn.net/albertsh/article/details/52788106 https://blog.csdn.net/albertsh/article/details/52797519 --https://blog.csdn.net/albertsh/article/details/82286999 https://blog.csdn.net/albertsh/article/details/90736859 Blog Record博客记录QQ：347070901微信公众号：写代码的苏东坡===============================我的Github：AlbertGithubHome===============================刚刚起步 2013-10-05 08:39 写了第一篇转载博文 2015-11-19 11:20 收到第一条博客评论 2016-12-27 20:18 翻译第一篇英文资料 2017-03-27 09:55 访问第一次突破三万 2017-05-21 09:25 积分第一回到达一仟 2017-05-21 09:25 等级第一次满足四级 2017-09-14 21:00 版式第一次不让更改 2017-11-14 11:59 排名第一次有了数字 2018-07-16 19:39 访问第一次达二十万2018-12-08 10:41 访问第一回破三十万2019-06-22 18:30 访问第一次破四十万2019-09-24 17:04 博客嗖一下到了六级2020-01-02 15:55 访问第一回达五十万2020-02-17 20:17 粉丝第一回超过千人2020-06-20 12:09 评论第一次跨越千次2020-06-25 16:31 等级飞一般来到七级2020-07-31 11:05 点赞第一次达到千人2020-08-09 20:48 积分第一回突破一万2021-01-27 23:46 访问嗡一下到了百万2021-08-27 10:21 粉丝刷一下到破一万 imgroot More info: Deployment]]></content>
  </entry>
</search>
